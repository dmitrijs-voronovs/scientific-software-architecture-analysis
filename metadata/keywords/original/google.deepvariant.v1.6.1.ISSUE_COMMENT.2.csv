id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:2520,Safety,recover,recovery,2520,"**Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 41450 (95.3%) full SNP recovery; 39678 / 41450 (95.7%) partial SNP recovery; ```. I am omitted the indel statistics from my script because Veritas used freebayes for variant calling (and I’m not converting the indel format, causing the indel count to be quite low, presumably because most overlapped a homopolymer of at least 2 nucleotides). **Still, maybe it is interesting that the “*full*” ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:3171,Safety,recover,recovery,3171,"partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 41450 (95.3%) full SNP recovery; 39678 / 41450 (95.7%) partial SNP recovery; ```. I am omitted the indel statistics from my script because Veritas used freebayes for variant calling (and I’m not converting the indel format, causing the indel count to be quite low, presumably because most overlapped a homopolymer of at least 2 nucleotides). **Still, maybe it is interesting that the “*full*” SNP recovery was higher for the provided variants, but the “*partial*” SNP recovery was higher for DeepVariant?** The precisionFDA comparison (for my samples) shows better results for DeepVariant than the provided VCF files (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:3215,Safety,recover,recovery,3215,"comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 41450 (95.3%) full SNP recovery; 39678 / 41450 (95.7%) partial SNP recovery; ```. I am omitted the indel statistics from my script because Veritas used freebayes for variant calling (and I’m not converting the indel format, causing the indel count to be quite low, presumably because most overlapped a homopolymer of at least 2 nucleotides). **Still, maybe it is interesting that the “*full*” SNP recovery was higher for the provided variants, but the “*partial*” SNP recovery was higher for DeepVariant?** The precisionFDA comparison (for my samples) shows better results for DeepVariant than the provided VCF files (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:3545,Safety,recover,recovery,3545,"dn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 41450 (95.3%) full SNP recovery; 39678 / 41450 (95.7%) partial SNP recovery; ```. I am omitted the indel statistics from my script because Veritas used freebayes for variant calling (and I’m not converting the indel format, causing the indel count to be quite low, presumably because most overlapped a homopolymer of at least 2 nucleotides). **Still, maybe it is interesting that the “*full*” SNP recovery was higher for the provided variants, but the “*partial*” SNP recovery was higher for DeepVariant?** The precisionFDA comparison (for my samples) shows better results for DeepVariant than the provided VCF files (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as sho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:3616,Safety,recover,recovery,3616,"dn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 41450 (95.3%) full SNP recovery; 39678 / 41450 (95.7%) partial SNP recovery; ```. I am omitted the indel statistics from my script because Veritas used freebayes for variant calling (and I’m not converting the indel format, causing the indel count to be quite low, presumably because most overlapped a homopolymer of at least 2 nucleotides). **Still, maybe it is interesting that the “*full*” SNP recovery was higher for the provided variants, but the “*partial*” SNP recovery was higher for DeepVariant?** The precisionFDA comparison (for my samples) shows better results for DeepVariant than the provided VCF files (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as sho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4306,Safety,recover,recovery,4306,"y; ```. I am omitted the indel statistics from my script because Veritas used freebayes for variant calling (and I’m not converting the indel format, causing the indel count to be quite low, presumably because most overlapped a homopolymer of at least 2 nucleotides). **Still, maybe it is interesting that the “*full*” SNP recovery was higher for the provided variants, but the “*partial*” SNP recovery was higher for DeepVariant?** The precisionFDA comparison (for my samples) shows better results for DeepVariant than the provided VCF files (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4713,Safety,recover,recovery,4713," for DeepVariant than the provided VCF files (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4757,Safety,recover,recovery,4757,"les (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4800,Safety,recover,recovery,4800,"ly fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4846,Safety,recover,recovery,4846,"g back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are high, like the other variant calling results).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4888,Safety,recover,recovery,4888,"h both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are high, like the other variant calling results). I also created a [discussion group](https:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4933,Safety,recover,recovery,4933,"er combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are high, like the other variant calling results). I also created a [discussion group](https://precision.fda.gov/discussions/55-hg002",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4982,Safety,recover,recovery,4982,"esulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are high, like the other variant calling results). I also created a [discussion group](https://precision.fda.gov/discussions/55-hg002-truth-dataset) on precisionFDA asking about how the “truth” set was defined. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:40,Testability,test,testing,40,"Hi Again,. I’ve had a chance to do some testing with my own samples, so I thought I should report back:. Going back to _point 4_, I noticed that you could choose to output a gVCF file in DeepVariant. However, that is not what is done by default (and it’s not what I did), and I can also tell that’s probably not what they did either. Namely, there are over 3 _billion_ base pairs in the human genome: if you add up the passed filters and failed filters for DeepVariant, then that is still only about 9 _million_ sites. However, I wanted to wait until I had my own DeepVariant results before I said anything else, since it is always possible there may have been something that I wouldn’t expecting in the VCF from DeepVariant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:1364,Testability,test,test,1364,"led filters for DeepVariant, then that is still only about 9 _million_ sites. However, I wanted to wait until I had my own DeepVariant results before I said anything else, since it is always possible there may have been something that I wouldn’t expecting in the VCF from DeepVariant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://pre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:6589,Testability,test,tested,6589,"fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are high, like the other variant calling results). I also created a [discussion group](https://precision.fda.gov/discussions/55-hg002-truth-dataset) on precisionFDA asking about how the “truth” set was defined. They mentioned that they **focused on regions where variants could be made most confidently** (genome-wide), and I’m assuming that is why most of the numbers are so high. Otherwise, they are more in the range of using [my same WGS sample and variant caller (DeepVariant) while only changing the alignment](https://precision.fda.gov/comparisons/3437), which isn’t really an independent verification (matching my original concern that the percentages being reported seemed unrealistically high). **In other words, I would say DeepVariant performed well, *along with other strategies tested***. I genuinely believe it is good to have a variety of freely available programs to use, but I believe the ""winner"" designations from the precisionFDA challenge can be a bit misleading (even though, to be fair, they do color similar high percentiles, even though they also award a designation to one group per category). If I was the winner of a precisionFDA challenge, I would probably want to mention that somewhere. However, I don't typically see sections like ""Why DeepVariant"" at the top of most program READMEs. So, along with some observations about [run-time and cost](https://github.com/google/deepvariant/issues/171#issuecomment-483903505), I think it may respectfully be worth considering trimming back some of that information (**while continuing to provide excellent support on the issues section of GitHub!**). **7)** My understanding is that there is not a DeepVariant App on precisionFDA. I think they use AWS, and I may be able to creat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:7603,Testability,test,test,7603,"long with other strategies tested***. I genuinely believe it is good to have a variety of freely available programs to use, but I believe the ""winner"" designations from the precisionFDA challenge can be a bit misleading (even though, to be fair, they do color similar high percentiles, even though they also award a designation to one group per category). If I was the winner of a precisionFDA challenge, I would probably want to mention that somewhere. However, I don't typically see sections like ""Why DeepVariant"" at the top of most program READMEs. So, along with some observations about [run-time and cost](https://github.com/google/deepvariant/issues/171#issuecomment-483903505), I think it may respectfully be worth considering trimming back some of that information (**while continuing to provide excellent support on the issues section of GitHub!**). **7)** My understanding is that there is not a DeepVariant App on precisionFDA. I think they use AWS, and I may be able to create something unofficial using code similar to [my AWS test](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) (relating to issues #166 and #167). However, perhaps at some point, you could consider offering something that can be more officially (and better) supported by DeepVariant developers? This would be free to the users (since the FDA is covering the costs of using the DNAnexus-based interface), but there are some unique differences (like I had to change the chromosome formatting for my .vcf files, and there was an issue with my [Veritas WGS header](https://www.biostars.org/p/361415/#366669) that I had to fix). I am currently uploading my .fastq files (the .bam alignments are up there and public, but I think the chr format may cause issue with variant calling comparisons). However, all relevant information for these two samples will be publicly available in precisionFDA (from my [charles.warden](https://precision.fda.gov/users/charles.warden) account). Y",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:756,Usability,feedback,feedback,756,"Hi Again,. I’ve had a chance to do some testing with my own samples, so I thought I should report back:. Going back to _point 4_, I noticed that you could choose to output a gVCF file in DeepVariant. However, that is not what is done by default (and it’s not what I did), and I can also tell that’s probably not what they did either. Namely, there are over 3 _billion_ base pairs in the human genome: if you add up the passed filters and failed filters for DeepVariant, then that is still only about 9 _million_ sites. However, I wanted to wait until I had my own DeepVariant results before I said anything else, since it is always possible there may have been something that I wouldn’t expecting in the VCF from DeepVariant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/165#issuecomment-485283894:8634,Usability,feedback,feedback,8634,"lieve the ""winner"" designations from the precisionFDA challenge can be a bit misleading (even though, to be fair, they do color similar high percentiles, even though they also award a designation to one group per category). If I was the winner of a precisionFDA challenge, I would probably want to mention that somewhere. However, I don't typically see sections like ""Why DeepVariant"" at the top of most program READMEs. So, along with some observations about [run-time and cost](https://github.com/google/deepvariant/issues/171#issuecomment-483903505), I think it may respectfully be worth considering trimming back some of that information (**while continuing to provide excellent support on the issues section of GitHub!**). **7)** My understanding is that there is not a DeepVariant App on precisionFDA. I think they use AWS, and I may be able to create something unofficial using code similar to [my AWS test](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) (relating to issues #166 and #167). However, perhaps at some point, you could consider offering something that can be more officially (and better) supported by DeepVariant developers? This would be free to the users (since the FDA is covering the costs of using the DNAnexus-based interface), but there are some unique differences (like I had to change the chromosome formatting for my .vcf files, and there was an issue with my [Veritas WGS header](https://www.biostars.org/p/361415/#366669) that I had to fix). I am currently uploading my .fastq files (the .bam alignments are up there and public, but I think the chr format may cause issue with variant calling comparisons). However, all relevant information for these two samples will be publicly available in precisionFDA (from my [charles.warden](https://precision.fda.gov/users/charles.warden) account). You don’t have to re-open the ticket, but I would certainly welcome any feedback / thoughts that you might have. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894
https://github.com/google/deepvariant/issues/166#issuecomment-478393809:463,Availability,down,download,463,"If it helps, this is what the model folder looks like:. ```; total 401316; -rw-rw-r-- 1 ec2-user ec2-user 348681272 Dec 11 23:41 model.ckpt.data-00000-of-00001; -rw-rw-r-- 1 ec2-user ec2-user 18473 Dec 11 23:42 model.ckpt.index; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta.1; ```; (I think there are two meta files because I had to change the HTTPS permissions and re-download the files). and these were the commands to download those files:. ```; MODEL_VERSION=""0.7.2""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wes_standard""; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/${MODEL_VERSION}/${MODEL_NAME}"". mkdir -p ${MODEL_NAME}; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.data-00000-of-00001; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.index; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.meta; ```. I also tested deleting those files and re-downloading them (but I got the same error message).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478393809
https://github.com/google/deepvariant/issues/166#issuecomment-478393809:515,Availability,down,download,515,"If it helps, this is what the model folder looks like:. ```; total 401316; -rw-rw-r-- 1 ec2-user ec2-user 348681272 Dec 11 23:41 model.ckpt.data-00000-of-00001; -rw-rw-r-- 1 ec2-user ec2-user 18473 Dec 11 23:42 model.ckpt.index; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta.1; ```; (I think there are two meta files because I had to change the HTTPS permissions and re-download the files). and these were the commands to download those files:. ```; MODEL_VERSION=""0.7.2""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wes_standard""; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/${MODEL_VERSION}/${MODEL_NAME}"". mkdir -p ${MODEL_NAME}; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.data-00000-of-00001; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.index; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.meta; ```. I also tested deleting those files and re-downloading them (but I got the same error message).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478393809
https://github.com/google/deepvariant/issues/166#issuecomment-478393809:1009,Availability,down,downloading,1009,"If it helps, this is what the model folder looks like:. ```; total 401316; -rw-rw-r-- 1 ec2-user ec2-user 348681272 Dec 11 23:41 model.ckpt.data-00000-of-00001; -rw-rw-r-- 1 ec2-user ec2-user 18473 Dec 11 23:42 model.ckpt.index; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta.1; ```; (I think there are two meta files because I had to change the HTTPS permissions and re-download the files). and these were the commands to download those files:. ```; MODEL_VERSION=""0.7.2""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wes_standard""; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/${MODEL_VERSION}/${MODEL_NAME}"". mkdir -p ${MODEL_NAME}; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.data-00000-of-00001; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.index; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.meta; ```. I also tested deleting those files and re-downloading them (but I got the same error message).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478393809
https://github.com/google/deepvariant/issues/166#issuecomment-478393809:1046,Availability,error,error,1046,"If it helps, this is what the model folder looks like:. ```; total 401316; -rw-rw-r-- 1 ec2-user ec2-user 348681272 Dec 11 23:41 model.ckpt.data-00000-of-00001; -rw-rw-r-- 1 ec2-user ec2-user 18473 Dec 11 23:42 model.ckpt.index; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta.1; ```; (I think there are two meta files because I had to change the HTTPS permissions and re-download the files). and these were the commands to download those files:. ```; MODEL_VERSION=""0.7.2""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wes_standard""; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/${MODEL_VERSION}/${MODEL_NAME}"". mkdir -p ${MODEL_NAME}; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.data-00000-of-00001; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.index; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.meta; ```. I also tested deleting those files and re-downloading them (but I got the same error message).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478393809
https://github.com/google/deepvariant/issues/166#issuecomment-478393809:1052,Integrability,message,message,1052,"If it helps, this is what the model folder looks like:. ```; total 401316; -rw-rw-r-- 1 ec2-user ec2-user 348681272 Dec 11 23:41 model.ckpt.data-00000-of-00001; -rw-rw-r-- 1 ec2-user ec2-user 18473 Dec 11 23:42 model.ckpt.index; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta.1; ```; (I think there are two meta files because I had to change the HTTPS permissions and re-download the files). and these were the commands to download those files:. ```; MODEL_VERSION=""0.7.2""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wes_standard""; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/${MODEL_VERSION}/${MODEL_NAME}"". mkdir -p ${MODEL_NAME}; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.data-00000-of-00001; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.index; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.meta; ```. I also tested deleting those files and re-downloading them (but I got the same error message).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478393809
https://github.com/google/deepvariant/issues/166#issuecomment-478393809:974,Testability,test,tested,974,"If it helps, this is what the model folder looks like:. ```; total 401316; -rw-rw-r-- 1 ec2-user ec2-user 348681272 Dec 11 23:41 model.ckpt.data-00000-of-00001; -rw-rw-r-- 1 ec2-user ec2-user 18473 Dec 11 23:42 model.ckpt.index; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta.1; ```; (I think there are two meta files because I had to change the HTTPS permissions and re-download the files). and these were the commands to download those files:. ```; MODEL_VERSION=""0.7.2""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wes_standard""; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/${MODEL_VERSION}/${MODEL_NAME}"". mkdir -p ${MODEL_NAME}; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.data-00000-of-00001; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.index; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.meta; ```. I also tested deleting those files and re-downloading them (but I got the same error message).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478393809
https://github.com/google/deepvariant/issues/166#issuecomment-478394293:198,Availability,error,error,198,"I also tested running _make_examples_ without _parallel_ (so, without the 4 threads), and the run-time was similar on the same instance (I believe a few hours). However, I am still getting the same error message that the **call_variant** step (with or without the threads / shards). Thank you again for your help! :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478394293
https://github.com/google/deepvariant/issues/166#issuecomment-478394293:204,Integrability,message,message,204,"I also tested running _make_examples_ without _parallel_ (so, without the 4 threads), and the run-time was similar on the same instance (I believe a few hours). However, I am still getting the same error message that the **call_variant** step (with or without the threads / shards). Thank you again for your help! :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478394293
https://github.com/google/deepvariant/issues/166#issuecomment-478394293:7,Testability,test,tested,7,"I also tested running _make_examples_ without _parallel_ (so, without the 4 threads), and the run-time was similar on the same instance (I believe a few hours). However, I am still getting the same error message that the **call_variant** step (with or without the threads / shards). Thank you again for your help! :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478394293
https://github.com/google/deepvariant/issues/166#issuecomment-478396748:10,Availability,error,error,10,"From your error, it seems like here is the relevant part:. ```; 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?; ```. For the `call_variants` step, in this line:; ```; --checkpoint ""${MODEL}""; ```. `${MODEL}` is actually a prefix of the model file, not the directory.; So, in your case, `${MODEL}` here should be:; `/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard/model`; not just the name of the directory. Please give that a try and see if it works. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478396748
https://github.com/google/deepvariant/issues/166#issuecomment-478396748:514,Availability,checkpoint,checkpoint,514,"From your error, it seems like here is the relevant part:. ```; 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?; ```. For the `call_variants` step, in this line:; ```; --checkpoint ""${MODEL}""; ```. `${MODEL}` is actually a prefix of the model file, not the directory.; So, in your case, `${MODEL}` here should be:; `/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard/model`; not just the name of the directory. Please give that a try and see if it works. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478396748
https://github.com/google/deepvariant/issues/166#issuecomment-478414504:469,Availability,error,error,469,"Thank you very much for your prompt response, particularly on the weekend!. I can see that I overlooked the line `MODEL=""${HOME}/${MODEL_NAME}/model.ckpt""` in this [quick-start](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) guide. I apologize about that. So, if I use `/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard/model.ckpt`, then that works!. If I only put the prefix with ""model,"" I get another error mentioning a checkpoint, but I am assuming that is what the .ckpt extension stands for. Thank you again. **Update (4/1)**: I just remembered that I am supposed to close the issues on GitHub, which is what I did :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478414504
https://github.com/google/deepvariant/issues/166#issuecomment-478414504:488,Availability,checkpoint,checkpoint,488,"Thank you very much for your prompt response, particularly on the weekend!. I can see that I overlooked the line `MODEL=""${HOME}/${MODEL_NAME}/model.ckpt""` in this [quick-start](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) guide. I apologize about that. So, if I use `/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard/model.ckpt`, then that works!. If I only put the prefix with ""model,"" I get another error mentioning a checkpoint, but I am assuming that is what the .ckpt extension stands for. Thank you again. **Update (4/1)**: I just remembered that I am supposed to close the issues on GitHub, which is what I did :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478414504
https://github.com/google/deepvariant/issues/166#issuecomment-478414504:582,Deployability,Update,Update,582,"Thank you very much for your prompt response, particularly on the weekend!. I can see that I overlooked the line `MODEL=""${HOME}/${MODEL_NAME}/model.ckpt""` in this [quick-start](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) guide. I apologize about that. So, if I use `/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard/model.ckpt`, then that works!. If I only put the prefix with ""model,"" I get another error mentioning a checkpoint, but I am assuming that is what the .ckpt extension stands for. Thank you again. **Update (4/1)**: I just remembered that I am supposed to close the issues on GitHub, which is what I did :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478414504
https://github.com/google/deepvariant/issues/166#issuecomment-478414504:259,Usability,guid,guide,259,"Thank you very much for your prompt response, particularly on the weekend!. I can see that I overlooked the line `MODEL=""${HOME}/${MODEL_NAME}/model.ckpt""` in this [quick-start](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) guide. I apologize about that. So, if I use `/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard/model.ckpt`, then that works!. If I only put the prefix with ""model,"" I get another error mentioning a checkpoint, but I am assuming that is what the .ckpt extension stands for. Thank you again. **Update (4/1)**: I just remembered that I am supposed to close the issues on GitHub, which is what I did :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478414504
https://github.com/google/deepvariant/issues/166#issuecomment-478851260:272,Availability,error,error-prone,272,"Yup that's right. Sorry, I meant `model.ckpt`, not just `model`.; Feel free to open another issue if you have other questions. In the next release, I'm hoping to make it easier to use. One thing I'm looking into is to not have to specify a super long model path, which is error-prone. When the next release comes out, it'll be good to have your feedback again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478851260
https://github.com/google/deepvariant/issues/166#issuecomment-478851260:139,Deployability,release,release,139,"Yup that's right. Sorry, I meant `model.ckpt`, not just `model`.; Feel free to open another issue if you have other questions. In the next release, I'm hoping to make it easier to use. One thing I'm looking into is to not have to specify a super long model path, which is error-prone. When the next release comes out, it'll be good to have your feedback again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478851260
https://github.com/google/deepvariant/issues/166#issuecomment-478851260:299,Deployability,release,release,299,"Yup that's right. Sorry, I meant `model.ckpt`, not just `model`.; Feel free to open another issue if you have other questions. In the next release, I'm hoping to make it easier to use. One thing I'm looking into is to not have to specify a super long model path, which is error-prone. When the next release comes out, it'll be good to have your feedback again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478851260
https://github.com/google/deepvariant/issues/166#issuecomment-478851260:345,Usability,feedback,feedback,345,"Yup that's right. Sorry, I meant `model.ckpt`, not just `model`.; Feel free to open another issue if you have other questions. In the next release, I'm hoping to make it easier to use. One thing I'm looking into is to not have to specify a super long model path, which is error-prone. When the next release comes out, it'll be good to have your feedback again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166#issuecomment-478851260
https://github.com/google/deepvariant/issues/167#issuecomment-479348077:27,Availability,error,error,27,"Hi Charles,; the bad_alloc error could indicate that you're running out of memory on this machine.; It seemes like m5.4xlarge have 64 GB RAM? Currently postprocess_variants reads in everything in memory and sorts them, which can take quite a lot of memory depending on how many records the previous step generated.; I suggest trying this step on a machine with more RAM, or keep an an eye on whether the memory usage is an issue. Currently for our WGS Case Study , we recommend getting a machine with 128GB RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479348077
https://github.com/google/deepvariant/issues/167#issuecomment-479348077:256,Integrability,depend,depending,256,"Hi Charles,; the bad_alloc error could indicate that you're running out of memory on this machine.; It seemes like m5.4xlarge have 64 GB RAM? Currently postprocess_variants reads in everything in memory and sorts them, which can take quite a lot of memory depending on how many records the previous step generated.; I suggest trying this step on a machine with more RAM, or keep an an eye on whether the memory usage is an issue. Currently for our WGS Case Study , we recommend getting a machine with 128GB RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479348077
https://github.com/google/deepvariant/issues/167#issuecomment-479365566:172,Performance,Perform,Performance,172,"Thank you - I will look into this. I was definitely not using 128 GB of RAM on AWS, so I will keep increasing this. I think you almost have to use a Cloud Resource or High-Performance Computing Cluster/Server for that: definitely not my personal home computer :(. While I also have WGS data, this is Exome data. I thought the _call_variants_ output seemed kind of small, but I don't know what is the typical size for this program.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479365566
https://github.com/google/deepvariant/issues/167#issuecomment-479522653:325,Deployability,install,installed,325,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
https://github.com/google/deepvariant/issues/167#issuecomment-479522653:578,Deployability,configurat,configuration,578,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
https://github.com/google/deepvariant/issues/167#issuecomment-479522653:1155,Deployability,update,update,1155,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
https://github.com/google/deepvariant/issues/167#issuecomment-479522653:578,Modifiability,config,configuration,578,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
https://github.com/google/deepvariant/issues/167#issuecomment-479522653:1043,Performance,perform,perform,1043,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
https://github.com/google/deepvariant/issues/167#issuecomment-479522653:235,Testability,test,testing,235,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
https://github.com/google/deepvariant/issues/167#issuecomment-479522653:823,Testability,test,test,823,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
https://github.com/google/deepvariant/issues/167#issuecomment-479522653:62,Usability,learn,learning,62,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
https://github.com/google/deepvariant/issues/167#issuecomment-479522653:703,Usability,clear,clear,703,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
https://github.com/google/deepvariant/issues/167#issuecomment-479522653:751,Usability,learn,learning,751,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653
https://github.com/google/deepvariant/issues/167#issuecomment-479584377:270,Availability,error,error,270,"Hi Charles,. In addition to what Pi-Chuan said. We run a case study for exome on 128G instance (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-exome-case-study.md). postprocess_variants simply converts variants from internal format to VCF format. This error is very generic and from the information you provided there is no way to say it is related to TensorFlow.; It would be helpful if you could provide logs for postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479584377
https://github.com/google/deepvariant/issues/167#issuecomment-479584377:424,Testability,log,logs,424,"Hi Charles,. In addition to what Pi-Chuan said. We run a case study for exome on 128G instance (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-exome-case-study.md). postprocess_variants simply converts variants from internal format to VCF format. This error is very generic and from the information you provided there is no way to say it is related to TensorFlow.; It would be helpful if you could provide logs for postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479584377
https://github.com/google/deepvariant/issues/167#issuecomment-479584377:204,Usability,simpl,simply,204,"Hi Charles,. In addition to what Pi-Chuan said. We run a case study for exome on 128G instance (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-exome-case-study.md). postprocess_variants simply converts variants from internal format to VCF format. This error is very generic and from the information you provided there is no way to say it is related to TensorFlow.; It would be helpful if you could provide logs for postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479584377
https://github.com/google/deepvariant/issues/167#issuecomment-479627680:95,Security,access,access,95,"Thank you for the reply about TensorFlow. I'll double-check when I get home (I have limited IP access to my AWS Cloud account), but the call_variants file seemed much smaller than I would expect for a file format conversion at the next step. However, I think is probably that it is a binary file. However, the proportions of run time in that case study does match my own experience (the ""call_variants"" step ran the most quickly). I apologize that it will take me a little while to look into all of these things, but I do plan on comparing Google Cloud at some point (possibly for this particular issue). Total time / cost is important, but I am not currently certain what I would recommend to be as clear as possible to users. Using Docker made a huge difference for me for usability. However, at a later point, I am very grateful that you have all of the code open-source (so, if I wanted, I could use DeepVariant to better understand how to use TensorFlow in other applications). For example, even if they don't use COHCAP directly, you can use the [source code](https://github.com/cwarden45/COHCAP/tree/master/src) to see how the Boost libraries for [statistical distributions](https://www.boost.org/doc/libs/1_67_0/libs/math/doc/html/dist.html) (rather than parallel operation) can substantially decrease the run time (relative to the R-base functions). _[I apologize for being a bit off topic, but that is the best analogy that I can think of]_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479627680
https://github.com/google/deepvariant/issues/167#issuecomment-479627680:700,Usability,clear,clear,700,"Thank you for the reply about TensorFlow. I'll double-check when I get home (I have limited IP access to my AWS Cloud account), but the call_variants file seemed much smaller than I would expect for a file format conversion at the next step. However, I think is probably that it is a binary file. However, the proportions of run time in that case study does match my own experience (the ""call_variants"" step ran the most quickly). I apologize that it will take me a little while to look into all of these things, but I do plan on comparing Google Cloud at some point (possibly for this particular issue). Total time / cost is important, but I am not currently certain what I would recommend to be as clear as possible to users. Using Docker made a huge difference for me for usability. However, at a later point, I am very grateful that you have all of the code open-source (so, if I wanted, I could use DeepVariant to better understand how to use TensorFlow in other applications). For example, even if they don't use COHCAP directly, you can use the [source code](https://github.com/cwarden45/COHCAP/tree/master/src) to see how the Boost libraries for [statistical distributions](https://www.boost.org/doc/libs/1_67_0/libs/math/doc/html/dist.html) (rather than parallel operation) can substantially decrease the run time (relative to the R-base functions). _[I apologize for being a bit off topic, but that is the best analogy that I can think of]_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479627680
https://github.com/google/deepvariant/issues/167#issuecomment-479627680:775,Usability,usab,usability,775,"Thank you for the reply about TensorFlow. I'll double-check when I get home (I have limited IP access to my AWS Cloud account), but the call_variants file seemed much smaller than I would expect for a file format conversion at the next step. However, I think is probably that it is a binary file. However, the proportions of run time in that case study does match my own experience (the ""call_variants"" step ran the most quickly). I apologize that it will take me a little while to look into all of these things, but I do plan on comparing Google Cloud at some point (possibly for this particular issue). Total time / cost is important, but I am not currently certain what I would recommend to be as clear as possible to users. Using Docker made a huge difference for me for usability. However, at a later point, I am very grateful that you have all of the code open-source (so, if I wanted, I could use DeepVariant to better understand how to use TensorFlow in other applications). For example, even if they don't use COHCAP directly, you can use the [source code](https://github.com/cwarden45/COHCAP/tree/master/src) to see how the Boost libraries for [statistical distributions](https://www.boost.org/doc/libs/1_67_0/libs/math/doc/html/dist.html) (rather than parallel operation) can substantially decrease the run time (relative to the R-base functions). _[I apologize for being a bit off topic, but that is the best analogy that I can think of]_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479627680
https://github.com/google/deepvariant/issues/167#issuecomment-479685222:358,Deployability,pipeline,pipelines,358,"Hi Charles,; there are a few 3rd party Cloud orchestrations for DeepVariant that you can consider. I'll provide the links below. To get the most accurate information, please visit their websites. (Our team does not maintain these). * [Running DeepVariant on Google Cloud Platform](https://cloud.google.com/genomics/docs/tutorials/deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222
https://github.com/google/deepvariant/issues/167#issuecomment-479685222:609,Deployability,pipeline,pipeline,609,"Hi Charles,; there are a few 3rd party Cloud orchestrations for DeepVariant that you can consider. I'll provide the links below. To get the most accurate information, please visit their websites. (Our team does not maintain these). * [Running DeepVariant on Google Cloud Platform](https://cloud.google.com/genomics/docs/tutorials/deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222
https://github.com/google/deepvariant/issues/167#issuecomment-479685222:825,Deployability,pipeline,pipeline,825,"Hi Charles,; there are a few 3rd party Cloud orchestrations for DeepVariant that you can consider. I'll provide the links below. To get the most accurate information, please visit their websites. (Our team does not maintain these). * [Running DeepVariant on Google Cloud Platform](https://cloud.google.com/genomics/docs/tutorials/deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222
https://github.com/google/deepvariant/issues/167#issuecomment-479685222:875,Deployability,pipeline,pipeline,875,"Hi Charles,; there are a few 3rd party Cloud orchestrations for DeepVariant that you can consider. I'll provide the links below. To get the most accurate information, please visit their websites. (Our team does not maintain these). * [Running DeepVariant on Google Cloud Platform](https://cloud.google.com/genomics/docs/tutorials/deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222
https://github.com/google/deepvariant/issues/167#issuecomment-479685222:1192,Deployability,Pipeline,Pipeline,1192,"Our team does not maintain these). * [Running DeepVariant on Google Cloud Platform](https://cloud.google.com/genomics/docs/tutorials/deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the input file for postprocess_variants (output from call_variants), that might ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222
https://github.com/google/deepvariant/issues/167#issuecomment-479685222:1321,Deployability,Pipeline,Pipeline,1321,"ials/deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the input file for postprocess_variants (output from call_variants), that might be useful information. (2) I'll follow up with you through a message to see what we can do in order to diagnose this particular",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222
https://github.com/google/deepvariant/issues/167#issuecomment-479685222:1749,Deployability,pipeline,pipeline,1749,"deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the input file for postprocess_variants (output from call_variants), that might be useful information. (2) I'll follow up with you through a message to see what we can do in order to diagnose this particular run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222
https://github.com/google/deepvariant/issues/167#issuecomment-479685222:1140,Integrability,interface,interface,1140,"below. To get the most accurate information, please visit their websites. (Our team does not maintain these). * [Running DeepVariant on Google Cloud Platform](https://cloud.google.com/genomics/docs/tutorials/deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222
https://github.com/google/deepvariant/issues/167#issuecomment-479685222:1377,Integrability,protocol,protocol,1377,"deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the input file for postprocess_variants (output from call_variants), that might be useful information. (2) I'll follow up with you through a message to see what we can do in order to diagnose this particular run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222
https://github.com/google/deepvariant/issues/167#issuecomment-479685222:2004,Integrability,depend,depends,2004,"deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the input file for postprocess_variants (output from call_variants), that might be useful information. (2) I'll follow up with you through a message to see what we can do in order to diagnose this particular run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222
https://github.com/google/deepvariant/issues/167#issuecomment-479685222:2259,Integrability,message,message,2259,"deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the input file for postprocess_variants (output from call_variants), that might be useful information. (2) I'll follow up with you through a message to see what we can do in order to diagnose this particular run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222
https://github.com/google/deepvariant/issues/167#issuecomment-479685222:368,Performance,optimiz,optimized,368,"Hi Charles,; there are a few 3rd party Cloud orchestrations for DeepVariant that you can consider. I'll provide the links below. To get the most accurate information, please visit their websites. (Our team does not maintain these). * [Running DeepVariant on Google Cloud Platform](https://cloud.google.com/genomics/docs/tutorials/deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222
https://github.com/google/deepvariant/issues/167#issuecomment-479685222:1727,Performance,optimiz,optimized,1727,"deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the input file for postprocess_variants (output from call_variants), that might be useful information. (2) I'll follow up with you through a message to see what we can do in order to diagnose this particular run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222
https://github.com/google/deepvariant/issues/167#issuecomment-479700070:276,Integrability,Bridg,Bridges,276,"Hi Pi-Chuan,. I'm still not back home yet (so, I apologize that I'm not 100% answering your question). However, I can say this:. Even thought I would expect the direct command-line analysis (in AWS or Google Cloud) should cost less, I did briefly look into DNAnexus and Seven Bridges. However, DNAnexus requires that you create an account with an institutional e-mail, and I was checking about what were options for people who want to re-analyze that that is directly provided to them (who may not be scientists). I also didn't see a way to create a Seven Bridges account with a G-mail address, but both companies gave me an initial reply about my account creation question within 24 hours. That said, I am also currently working on uploading my data to PrecisionFDA (which uses DNAnexus, but I can't use my PrecisionFDA account to sign into DNAnexus). For the user, I think this would be free, and it provides a way to test results provided by companies (**and** I could create an account with a Gmail address). However, PrecisionFDA doesn't currently have a DeepVariant App. I passed along an earlier version of this thread to them, but I will also now ask if it might be helpful that DNAnexus already has a DeepVariant app (to see how easily that can be applied within PrecisionFDA). I'll upload the call_variants file when I get home. Unless there is something obvious that you can see, I think it would probably be more fair for your time to start a Google Cloud test (although that means you may have to wait a little while before I get to the same step on another platform). Thank you again for all of your prompt replies!. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479700070
https://github.com/google/deepvariant/issues/167#issuecomment-479700070:556,Integrability,Bridg,Bridges,556,"Hi Pi-Chuan,. I'm still not back home yet (so, I apologize that I'm not 100% answering your question). However, I can say this:. Even thought I would expect the direct command-line analysis (in AWS or Google Cloud) should cost less, I did briefly look into DNAnexus and Seven Bridges. However, DNAnexus requires that you create an account with an institutional e-mail, and I was checking about what were options for people who want to re-analyze that that is directly provided to them (who may not be scientists). I also didn't see a way to create a Seven Bridges account with a G-mail address, but both companies gave me an initial reply about my account creation question within 24 hours. That said, I am also currently working on uploading my data to PrecisionFDA (which uses DNAnexus, but I can't use my PrecisionFDA account to sign into DNAnexus). For the user, I think this would be free, and it provides a way to test results provided by companies (**and** I could create an account with a Gmail address). However, PrecisionFDA doesn't currently have a DeepVariant App. I passed along an earlier version of this thread to them, but I will also now ask if it might be helpful that DNAnexus already has a DeepVariant app (to see how easily that can be applied within PrecisionFDA). I'll upload the call_variants file when I get home. Unless there is something obvious that you can see, I think it would probably be more fair for your time to start a Google Cloud test (although that means you may have to wait a little while before I get to the same step on another platform). Thank you again for all of your prompt replies!. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479700070
https://github.com/google/deepvariant/issues/167#issuecomment-479700070:920,Testability,test,test,920,"Hi Pi-Chuan,. I'm still not back home yet (so, I apologize that I'm not 100% answering your question). However, I can say this:. Even thought I would expect the direct command-line analysis (in AWS or Google Cloud) should cost less, I did briefly look into DNAnexus and Seven Bridges. However, DNAnexus requires that you create an account with an institutional e-mail, and I was checking about what were options for people who want to re-analyze that that is directly provided to them (who may not be scientists). I also didn't see a way to create a Seven Bridges account with a G-mail address, but both companies gave me an initial reply about my account creation question within 24 hours. That said, I am also currently working on uploading my data to PrecisionFDA (which uses DNAnexus, but I can't use my PrecisionFDA account to sign into DNAnexus). For the user, I think this would be free, and it provides a way to test results provided by companies (**and** I could create an account with a Gmail address). However, PrecisionFDA doesn't currently have a DeepVariant App. I passed along an earlier version of this thread to them, but I will also now ask if it might be helpful that DNAnexus already has a DeepVariant app (to see how easily that can be applied within PrecisionFDA). I'll upload the call_variants file when I get home. Unless there is something obvious that you can see, I think it would probably be more fair for your time to start a Google Cloud test (although that means you may have to wait a little while before I get to the same step on another platform). Thank you again for all of your prompt replies!. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479700070
https://github.com/google/deepvariant/issues/167#issuecomment-479700070:1468,Testability,test,test,1468,"Hi Pi-Chuan,. I'm still not back home yet (so, I apologize that I'm not 100% answering your question). However, I can say this:. Even thought I would expect the direct command-line analysis (in AWS or Google Cloud) should cost less, I did briefly look into DNAnexus and Seven Bridges. However, DNAnexus requires that you create an account with an institutional e-mail, and I was checking about what were options for people who want to re-analyze that that is directly provided to them (who may not be scientists). I also didn't see a way to create a Seven Bridges account with a G-mail address, but both companies gave me an initial reply about my account creation question within 24 hours. That said, I am also currently working on uploading my data to PrecisionFDA (which uses DNAnexus, but I can't use my PrecisionFDA account to sign into DNAnexus). For the user, I think this would be free, and it provides a way to test results provided by companies (**and** I could create an account with a Gmail address). However, PrecisionFDA doesn't currently have a DeepVariant App. I passed along an earlier version of this thread to them, but I will also now ask if it might be helpful that DNAnexus already has a DeepVariant app (to see how easily that can be applied within PrecisionFDA). I'll upload the call_variants file when I get home. Unless there is something obvious that you can see, I think it would probably be more fair for your time to start a Google Cloud test (although that means you may have to wait a little while before I get to the same step on another platform). Thank you again for all of your prompt replies!. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479700070
https://github.com/google/deepvariant/issues/167#issuecomment-479738587:334,Availability,error,error,334,"I've attached the output from the _call_variants_ step. **I'm not sure how much may have to do with having a total of ~2 weeks AWS experience**, but the current situation may be kind of interesting:. **1)** I believe I started running **postprocess_variants** this morning; **2)** After an 1-2 hours I would have expected to see that error message with the previous runs.; **3)** Using _r4.8xlarge_ EC2 instance launched from the ECS cluster (listed as having have 32 cores and 244 GB of RAM), **postprocess_variants** step is still running. I was admittedly surprised with I had problems with 64 GB of RAM (and initiated this issue). So, **using 244 GB of RAM seems to have gotten around the memory issue (_at least after having been running for ~12 hours_)**, but the run-time seems extraordinarily long (if I correctly understand that this step is supposed to be just file reformatting, for an Exome dataset). I kind of want to see if the job is capable of finishing running (and giving the expected result). However, I think I need to either figure out what is going on or test other options (since I want to test an alternative .bam alignment for that Exome file, as well as testing 2 alignments for my WGS data). In other words, if the job completes and produces a .vcf file, I will close this issue. If not, I will keep looking into things, but I think it may be a little while before I follow-up and get to the point where I would close the issue. [call_variants_output.tfrecord.gz](https://github.com/google/deepvariant/files/3041631/call_variants_output.tfrecord.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479738587
https://github.com/google/deepvariant/issues/167#issuecomment-479738587:340,Integrability,message,message,340,"I've attached the output from the _call_variants_ step. **I'm not sure how much may have to do with having a total of ~2 weeks AWS experience**, but the current situation may be kind of interesting:. **1)** I believe I started running **postprocess_variants** this morning; **2)** After an 1-2 hours I would have expected to see that error message with the previous runs.; **3)** Using _r4.8xlarge_ EC2 instance launched from the ECS cluster (listed as having have 32 cores and 244 GB of RAM), **postprocess_variants** step is still running. I was admittedly surprised with I had problems with 64 GB of RAM (and initiated this issue). So, **using 244 GB of RAM seems to have gotten around the memory issue (_at least after having been running for ~12 hours_)**, but the run-time seems extraordinarily long (if I correctly understand that this step is supposed to be just file reformatting, for an Exome dataset). I kind of want to see if the job is capable of finishing running (and giving the expected result). However, I think I need to either figure out what is going on or test other options (since I want to test an alternative .bam alignment for that Exome file, as well as testing 2 alignments for my WGS data). In other words, if the job completes and produces a .vcf file, I will close this issue. If not, I will keep looking into things, but I think it may be a little while before I follow-up and get to the point where I would close the issue. [call_variants_output.tfrecord.gz](https://github.com/google/deepvariant/files/3041631/call_variants_output.tfrecord.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479738587
https://github.com/google/deepvariant/issues/167#issuecomment-479738587:1077,Testability,test,test,1077,"I've attached the output from the _call_variants_ step. **I'm not sure how much may have to do with having a total of ~2 weeks AWS experience**, but the current situation may be kind of interesting:. **1)** I believe I started running **postprocess_variants** this morning; **2)** After an 1-2 hours I would have expected to see that error message with the previous runs.; **3)** Using _r4.8xlarge_ EC2 instance launched from the ECS cluster (listed as having have 32 cores and 244 GB of RAM), **postprocess_variants** step is still running. I was admittedly surprised with I had problems with 64 GB of RAM (and initiated this issue). So, **using 244 GB of RAM seems to have gotten around the memory issue (_at least after having been running for ~12 hours_)**, but the run-time seems extraordinarily long (if I correctly understand that this step is supposed to be just file reformatting, for an Exome dataset). I kind of want to see if the job is capable of finishing running (and giving the expected result). However, I think I need to either figure out what is going on or test other options (since I want to test an alternative .bam alignment for that Exome file, as well as testing 2 alignments for my WGS data). In other words, if the job completes and produces a .vcf file, I will close this issue. If not, I will keep looking into things, but I think it may be a little while before I follow-up and get to the point where I would close the issue. [call_variants_output.tfrecord.gz](https://github.com/google/deepvariant/files/3041631/call_variants_output.tfrecord.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479738587
https://github.com/google/deepvariant/issues/167#issuecomment-479738587:1113,Testability,test,test,1113,"I've attached the output from the _call_variants_ step. **I'm not sure how much may have to do with having a total of ~2 weeks AWS experience**, but the current situation may be kind of interesting:. **1)** I believe I started running **postprocess_variants** this morning; **2)** After an 1-2 hours I would have expected to see that error message with the previous runs.; **3)** Using _r4.8xlarge_ EC2 instance launched from the ECS cluster (listed as having have 32 cores and 244 GB of RAM), **postprocess_variants** step is still running. I was admittedly surprised with I had problems with 64 GB of RAM (and initiated this issue). So, **using 244 GB of RAM seems to have gotten around the memory issue (_at least after having been running for ~12 hours_)**, but the run-time seems extraordinarily long (if I correctly understand that this step is supposed to be just file reformatting, for an Exome dataset). I kind of want to see if the job is capable of finishing running (and giving the expected result). However, I think I need to either figure out what is going on or test other options (since I want to test an alternative .bam alignment for that Exome file, as well as testing 2 alignments for my WGS data). In other words, if the job completes and produces a .vcf file, I will close this issue. If not, I will keep looking into things, but I think it may be a little while before I follow-up and get to the point where I would close the issue. [call_variants_output.tfrecord.gz](https://github.com/google/deepvariant/files/3041631/call_variants_output.tfrecord.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479738587
https://github.com/google/deepvariant/issues/167#issuecomment-479738587:1180,Testability,test,testing,1180,"I've attached the output from the _call_variants_ step. **I'm not sure how much may have to do with having a total of ~2 weeks AWS experience**, but the current situation may be kind of interesting:. **1)** I believe I started running **postprocess_variants** this morning; **2)** After an 1-2 hours I would have expected to see that error message with the previous runs.; **3)** Using _r4.8xlarge_ EC2 instance launched from the ECS cluster (listed as having have 32 cores and 244 GB of RAM), **postprocess_variants** step is still running. I was admittedly surprised with I had problems with 64 GB of RAM (and initiated this issue). So, **using 244 GB of RAM seems to have gotten around the memory issue (_at least after having been running for ~12 hours_)**, but the run-time seems extraordinarily long (if I correctly understand that this step is supposed to be just file reformatting, for an Exome dataset). I kind of want to see if the job is capable of finishing running (and giving the expected result). However, I think I need to either figure out what is going on or test other options (since I want to test an alternative .bam alignment for that Exome file, as well as testing 2 alignments for my WGS data). In other words, if the job completes and produces a .vcf file, I will close this issue. If not, I will keep looking into things, but I think it may be a little while before I follow-up and get to the point where I would close the issue. [call_variants_output.tfrecord.gz](https://github.com/google/deepvariant/files/3041631/call_variants_output.tfrecord.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479738587
https://github.com/google/deepvariant/issues/167#issuecomment-479788147:96,Availability,error,error,96,"You are very welcome - thank you for your help!. FYI, the script recently stopped with the same error message. So, unless that file gives you another troubleshooting idea, I may look into other possible strategies (and/or look into more AWS documentation) and return to this issue when I get to the same stage with running DeepVariant (or at least report the alternative solution that I worked out).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479788147
https://github.com/google/deepvariant/issues/167#issuecomment-479788147:102,Integrability,message,message,102,"You are very welcome - thank you for your help!. FYI, the script recently stopped with the same error message. So, unless that file gives you another troubleshooting idea, I may look into other possible strategies (and/or look into more AWS documentation) and return to this issue when I get to the same stage with running DeepVariant (or at least report the alternative solution that I worked out).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-479788147
https://github.com/google/deepvariant/issues/167#issuecomment-480150395:984,Availability,down,downloaded,984,"e to try this on a GCE (Google Compute Engine) instance. First, when I looked at your file, I suspect it won't take much resource and time at all. And from my experiment below, it matches my expectation. Quick summary:; It took me about **24 seconds**, on a [n1-standard-1](https://cloud.google.com/compute/docs/machine-types#standard_machine_types) instance (which has **3.75GB** RAM) to run postprocess_variants on your file. Below, I will share all the steps I used to produce the result.; And, my next step (which won't be today) will be to try to run this on an AWS instance. . Here are the steps I've done:. 1. On my machine, I used this command to get a GCE instance:; ```; gcloud compute instances create ""${USER}-1"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-1"" \; --zone ""us-west1-b""; ```. 2. I ssh'ed into the machine using this command:; ```; gcloud compute ssh ""${USER}-1""; ```. 3. I downloaded the input file needed:; ```; # Downloading the reference file takes a while.; # It's only used for the header in postprocess_variants.; wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.fasta.gz; gunzip ucsc.hg19.fasta.gz; wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.fasta.fai.gz; gunzip ucsc.hg19.fasta.fai.gz. # Download the output from call_variants step:; wget https://github.com/google/deepvariant/files/3041631/call_variants_output.tfrecord.gz; ```. 4. I pull the docker image, and run the command:; ```; sudo docker pull gcr.io/deepvariant-docker/deepvariant:0.7.2. time sudo docker run \; -v ${PWD}:/data \; gcr.io/deepvariant-docker/deepvariant:0.7.2 \; /opt/deepvariant/bin/postprocess_variants \; --ref /data/ucsc.hg19.fasta \; --infile /data/call_variants_output.tfrecord.gz \; --outfile /data/output.vcf.gz; ```. As I mentioned, this took:; ```; real 0m24.779s; user 0m0.033s; sys 0m0.022s; ```. I used `top` to keep an eye on the memory usage. The highest I'v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480150395
https://github.com/google/deepvariant/issues/167#issuecomment-480150395:1026,Availability,Down,Downloading,1026,"e to try this on a GCE (Google Compute Engine) instance. First, when I looked at your file, I suspect it won't take much resource and time at all. And from my experiment below, it matches my expectation. Quick summary:; It took me about **24 seconds**, on a [n1-standard-1](https://cloud.google.com/compute/docs/machine-types#standard_machine_types) instance (which has **3.75GB** RAM) to run postprocess_variants on your file. Below, I will share all the steps I used to produce the result.; And, my next step (which won't be today) will be to try to run this on an AWS instance. . Here are the steps I've done:. 1. On my machine, I used this command to get a GCE instance:; ```; gcloud compute instances create ""${USER}-1"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-1"" \; --zone ""us-west1-b""; ```. 2. I ssh'ed into the machine using this command:; ```; gcloud compute ssh ""${USER}-1""; ```. 3. I downloaded the input file needed:; ```; # Downloading the reference file takes a while.; # It's only used for the header in postprocess_variants.; wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.fasta.gz; gunzip ucsc.hg19.fasta.gz; wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.fasta.fai.gz; gunzip ucsc.hg19.fasta.fai.gz. # Download the output from call_variants step:; wget https://github.com/google/deepvariant/files/3041631/call_variants_output.tfrecord.gz; ```. 4. I pull the docker image, and run the command:; ```; sudo docker pull gcr.io/deepvariant-docker/deepvariant:0.7.2. time sudo docker run \; -v ${PWD}:/data \; gcr.io/deepvariant-docker/deepvariant:0.7.2 \; /opt/deepvariant/bin/postprocess_variants \; --ref /data/ucsc.hg19.fasta \; --infile /data/call_variants_output.tfrecord.gz \; --outfile /data/output.vcf.gz; ```. As I mentioned, this took:; ```; real 0m24.779s; user 0m0.033s; sys 0m0.022s; ```. I used `top` to keep an eye on the memory usage. The highest I'v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480150395
https://github.com/google/deepvariant/issues/167#issuecomment-480150395:1367,Availability,Down,Download,1367,"3.75GB** RAM) to run postprocess_variants on your file. Below, I will share all the steps I used to produce the result.; And, my next step (which won't be today) will be to try to run this on an AWS instance. . Here are the steps I've done:. 1. On my machine, I used this command to get a GCE instance:; ```; gcloud compute instances create ""${USER}-1"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-1"" \; --zone ""us-west1-b""; ```. 2. I ssh'ed into the machine using this command:; ```; gcloud compute ssh ""${USER}-1""; ```. 3. I downloaded the input file needed:; ```; # Downloading the reference file takes a while.; # It's only used for the header in postprocess_variants.; wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.fasta.gz; gunzip ucsc.hg19.fasta.gz; wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg19/ucsc.hg19.fasta.fai.gz; gunzip ucsc.hg19.fasta.fai.gz. # Download the output from call_variants step:; wget https://github.com/google/deepvariant/files/3041631/call_variants_output.tfrecord.gz; ```. 4. I pull the docker image, and run the command:; ```; sudo docker pull gcr.io/deepvariant-docker/deepvariant:0.7.2. time sudo docker run \; -v ${PWD}:/data \; gcr.io/deepvariant-docker/deepvariant:0.7.2 \; /opt/deepvariant/bin/postprocess_variants \; --ref /data/ucsc.hg19.fasta \; --infile /data/call_variants_output.tfrecord.gz \; --outfile /data/output.vcf.gz; ```. As I mentioned, this took:; ```; real 0m24.779s; user 0m0.033s; sys 0m0.022s; ```. I used `top` to keep an eye on the memory usage. The highest I've noticed is roughly the following:; ```; PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND; 4882 root 20 0 1445956 229032 103824 R 99.3 6.1 0:22.46 python; ```. This produced 90,359 total lines, of which 78,085 are actual variant calls (PASS); ```; $ zcat output.vcf.gz | grep -v '^#' | wc -l; 90359. $ zcat output.vcf.gz | grep -v '^#' | grep PASS | wc -l; 78085; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480150395
https://github.com/google/deepvariant/issues/167#issuecomment-480271688:114,Testability,test,test,114,Thank you - I think these commands will be helpful as I learn about using Google Cloud. Please don't rush the AWS test - I am guessing I will have to wait until at least Sunday to be able to set up an account and confirm that using Google Cloud can resolve this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480271688
https://github.com/google/deepvariant/issues/167#issuecomment-480271688:56,Usability,learn,learn,56,Thank you - I think these commands will be helpful as I learn about using Google Cloud. Please don't rush the AWS test - I am guessing I will have to wait until at least Sunday to be able to set up an account and confirm that using Google Cloud can resolve this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480271688
https://github.com/google/deepvariant/issues/167#issuecomment-480324512:161,Testability,test,test,161,"No problem. I will still want to make sure there isn't a problem on AWS though. Our team develop DeepVariant to run everywhere (with a compatible OS). We mostly test on Ubuntu 16.04 so far because we haven't had much resource to test much more broadly. But a Ubuntu either on GCP or AWS shouldn't be an issue. And if it is, I think our team needs to know why.; Once I have a chance to test and confirm, I'll report back here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480324512
https://github.com/google/deepvariant/issues/167#issuecomment-480324512:229,Testability,test,test,229,"No problem. I will still want to make sure there isn't a problem on AWS though. Our team develop DeepVariant to run everywhere (with a compatible OS). We mostly test on Ubuntu 16.04 so far because we haven't had much resource to test much more broadly. But a Ubuntu either on GCP or AWS shouldn't be an issue. And if it is, I think our team needs to know why.; Once I have a chance to test and confirm, I'll report back here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480324512
https://github.com/google/deepvariant/issues/167#issuecomment-480324512:385,Testability,test,test,385,"No problem. I will still want to make sure there isn't a problem on AWS though. Our team develop DeepVariant to run everywhere (with a compatible OS). We mostly test on Ubuntu 16.04 so far because we haven't had much resource to test much more broadly. But a Ubuntu either on GCP or AWS shouldn't be an issue. And if it is, I think our team needs to know why.; Once I have a chance to test and confirm, I'll report back here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480324512
https://github.com/google/deepvariant/issues/167#issuecomment-480563774:688,Deployability,install,install,688,"@cwarden45 ; I got a `t1.micro` machine on AWS , and I basically repeated what I did in https://github.com/google/deepvariant/issues/167#issuecomment-480150395. With the same command:; ```; sudo docker pull gcr.io/deepvariant-docker/deepvariant:0.7.2. time sudo docker run \; -v ${PWD}:/data \; gcr.io/deepvariant-docker/deepvariant:0.7.2 \; /opt/deepvariant/bin/postprocess_variants \; --ref /data/ucsc.hg19.fasta \; --infile /data/call_variants_output.tfrecord.gz \; --outfile /data/output.vcf.gz; ```. It took me:; ```; real 0m24.779s; user 0m0.033s; sys 0m0.022s; ```; on a t1.micro AWS instance. A few differences from my GCP experience is:; (1) I had to use yum (instead of apt) to install docker.; (2) I had to first run `sudo service docker start` before I pull and run the docker image. Other than these, everything seems mostly the same. At this point I have one more question for you -- where are your files located? In your command in the original post, they're from `/mnt`. Where are these files mounted from?. In my setting, I wget all the files first. So that could be one difference that I can think of now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480563774
https://github.com/google/deepvariant/issues/167#issuecomment-480616982:295,Availability,error,error,295,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
https://github.com/google/deepvariant/issues/167#issuecomment-480616982:645,Deployability,install,installed,645,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
https://github.com/google/deepvariant/issues/167#issuecomment-480616982:301,Integrability,message,message,301,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
https://github.com/google/deepvariant/issues/167#issuecomment-480616982:1677,Security,access,accessible,1677,"ve a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount it whenever I create a new instance, but I don't have to re-upload the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
https://github.com/google/deepvariant/issues/167#issuecomment-480616982:163,Testability,test,test,163,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
https://github.com/google/deepvariant/issues/167#issuecomment-480616982:206,Testability,test,tested,206,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
https://github.com/google/deepvariant/issues/167#issuecomment-480616982:132,Usability,feedback,feedback,132,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
https://github.com/google/deepvariant/issues/167#issuecomment-480616982:1589,Usability,feedback,feedback,1589,"ve a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount it whenever I create a new instance, but I don't have to re-upload the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982
https://github.com/google/deepvariant/issues/167#issuecomment-480628311:887,Availability,error,error,887,"FYI, I'm not sure if this is the best solution, but I noticed that running in my home directory avoids the need to use _sudo_. While I am still encountering the same result (and I thought I encountered some issue with running Docker interactively at another step), I do have the ability to launch Docker in my home directory in interactive mode:. `docker run -it -v /mnt/efs-genome:/mnt/efs-genome gcr.io/deepvariant-docker/deepvariant`. and then run the commands for :. ```; OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta; CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz""; FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. Again, I am neither seeing an error message nor a result file (and the command stops running within seconds). However, if this provides a useful option for troubleshooting, I thought I should mention it. Also, I am starting the Google Cloud testing, but I am currently only at the file upload stage (I want to make sure I can run the 1st two steps, before checking that as a solution for the 3rd step). Thank you very much for your help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480628311
https://github.com/google/deepvariant/issues/167#issuecomment-480628311:893,Integrability,message,message,893,"FYI, I'm not sure if this is the best solution, but I noticed that running in my home directory avoids the need to use _sudo_. While I am still encountering the same result (and I thought I encountered some issue with running Docker interactively at another step), I do have the ability to launch Docker in my home directory in interactive mode:. `docker run -it -v /mnt/efs-genome:/mnt/efs-genome gcr.io/deepvariant-docker/deepvariant`. and then run the commands for :. ```; OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta; CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz""; FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. Again, I am neither seeing an error message nor a result file (and the command stops running within seconds). However, if this provides a useful option for troubleshooting, I thought I should mention it. Also, I am starting the Google Cloud testing, but I am currently only at the file upload stage (I want to make sure I can run the 1st two steps, before checking that as a solution for the 3rd step). Thank you very much for your help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480628311
https://github.com/google/deepvariant/issues/167#issuecomment-480628311:96,Safety,avoid,avoids,96,"FYI, I'm not sure if this is the best solution, but I noticed that running in my home directory avoids the need to use _sudo_. While I am still encountering the same result (and I thought I encountered some issue with running Docker interactively at another step), I do have the ability to launch Docker in my home directory in interactive mode:. `docker run -it -v /mnt/efs-genome:/mnt/efs-genome gcr.io/deepvariant-docker/deepvariant`. and then run the commands for :. ```; OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta; CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz""; FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. Again, I am neither seeing an error message nor a result file (and the command stops running within seconds). However, if this provides a useful option for troubleshooting, I thought I should mention it. Also, I am starting the Google Cloud testing, but I am currently only at the file upload stage (I want to make sure I can run the 1st two steps, before checking that as a solution for the 3rd step). Thank you very much for your help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480628311
https://github.com/google/deepvariant/issues/167#issuecomment-480628311:1098,Testability,test,testing,1098,"FYI, I'm not sure if this is the best solution, but I noticed that running in my home directory avoids the need to use _sudo_. While I am still encountering the same result (and I thought I encountered some issue with running Docker interactively at another step), I do have the ability to launch Docker in my home directory in interactive mode:. `docker run -it -v /mnt/efs-genome:/mnt/efs-genome gcr.io/deepvariant-docker/deepvariant`. and then run the commands for :. ```; OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta; CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz""; FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. Again, I am neither seeing an error message nor a result file (and the command stops running within seconds). However, if this provides a useful option for troubleshooting, I thought I should mention it. Also, I am starting the Google Cloud testing, but I am currently only at the file upload stage (I want to make sure I can run the 1st two steps, before checking that as a solution for the 3rd step). Thank you very much for your help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480628311
https://github.com/google/deepvariant/issues/167#issuecomment-480640009:858,Availability,error,error,858,"While AWS was better than my local computer (with 8 GB of RAM and 4 cores) for running the 1st step (_make_examples_), I can likewise try running the Docker container in interactive mode on my own computer:. `docker run -it -v /c/Users/Charles/Documents/WGS_Exome_Analysis/My_Veritas_WGS:/mnt/wgs gcr.io/deepvariant-docker/deepvariant`. followed by moving to the appropriate directory and running the following script:. ```; OUTPUT_DIR=Genos_Provided; REF=../hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz""; FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. This is different than running interactive mode for **postprocess_variants** on AWS (which almost immediately ends, without error message or results file), but it has been running for more than one hour. So, I will provide an update if this works, but this sounds different than your experience with the t1.micro AWS instance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480640009
https://github.com/google/deepvariant/issues/167#issuecomment-480640009:960,Deployability,update,update,960,"While AWS was better than my local computer (with 8 GB of RAM and 4 cores) for running the 1st step (_make_examples_), I can likewise try running the Docker container in interactive mode on my own computer:. `docker run -it -v /c/Users/Charles/Documents/WGS_Exome_Analysis/My_Veritas_WGS:/mnt/wgs gcr.io/deepvariant-docker/deepvariant`. followed by moving to the appropriate directory and running the following script:. ```; OUTPUT_DIR=Genos_Provided; REF=../hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz""; FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. This is different than running interactive mode for **postprocess_variants** on AWS (which almost immediately ends, without error message or results file), but it has been running for more than one hour. So, I will provide an update if this works, but this sounds different than your experience with the t1.micro AWS instance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480640009
https://github.com/google/deepvariant/issues/167#issuecomment-480640009:864,Integrability,message,message,864,"While AWS was better than my local computer (with 8 GB of RAM and 4 cores) for running the 1st step (_make_examples_), I can likewise try running the Docker container in interactive mode on my own computer:. `docker run -it -v /c/Users/Charles/Documents/WGS_Exome_Analysis/My_Veritas_WGS:/mnt/wgs gcr.io/deepvariant-docker/deepvariant`. followed by moving to the appropriate directory and running the following script:. ```; OUTPUT_DIR=Genos_Provided; REF=../hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz""; FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. This is different than running interactive mode for **postprocess_variants** on AWS (which almost immediately ends, without error message or results file), but it has been running for more than one hour. So, I will provide an update if this works, but this sounds different than your experience with the t1.micro AWS instance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480640009
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:409,Availability,down,down,409,"Also, if I try to follow your instructions for an EC2 instance, I can't actually create a _t1.micro_ EC2 instance, but I can create a **t2.micro** EC2 instance (with 1 vCPU and 1 GB of RAM). I remember spending some effort to try and install Docker on the EC2 instance. That installation is quite quick, and I almost wondered if I hadn't somehow didn't test `sudo yum install docker`. However, there was slow-down after pulling the docker image (possibly due to the type of instance?). If I sign into another terminal and try to run `docker images` (to confirm that everything worked OK), I get the following error message:. `Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:609,Availability,error,error,609,"Also, if I try to follow your instructions for an EC2 instance, I can't actually create a _t1.micro_ EC2 instance, but I can create a **t2.micro** EC2 instance (with 1 vCPU and 1 GB of RAM). I remember spending some effort to try and install Docker on the EC2 instance. That installation is quite quick, and I almost wondered if I hadn't somehow didn't test `sudo yum install docker`. However, there was slow-down after pulling the docker image (possibly due to the type of instance?). If I sign into another terminal and try to run `docker images` (to confirm that everything worked OK), I get the following error message:. `Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:1025,Availability,error,error,1025,"Also, if I try to follow your instructions for an EC2 instance, I can't actually create a _t1.micro_ EC2 instance, but I can create a **t2.micro** EC2 instance (with 1 vCPU and 1 GB of RAM). I remember spending some effort to try and install Docker on the EC2 instance. That installation is quite quick, and I almost wondered if I hadn't somehow didn't test `sudo yum install docker`. However, there was slow-down after pulling the docker image (possibly due to the type of instance?). If I sign into another terminal and try to run `docker images` (to confirm that everything worked OK), I get the following error message:. `Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:1661,Availability,Down,Downloaded,1661,"ow-down after pulling the docker image (possibly due to the type of instance?). If I sign into another terminal and try to run `docker images` (to confirm that everything worked OK), I get the following error message:. `Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `docker ps -a` after the script stopped from that error message. So, that puts me back where I started, and I actually have that extra error message (over trying to use Docker from the instance launched via ECS). Nevertheless, if I make any additional progress, I will let you know. **Update (4/10/2019)**: FYI, if anybody else has a similar problem, I eventually remembered that I needed to add myself to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:1970,Availability,error,error,1970," while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `docker ps -a` after the script stopped from that error message. So, that puts me back where I started, and I actually have that extra error message (over trying to use Docker from the instance launched via ECS). Nevertheless, if I make any additional progress, I will let you know. **Update (4/10/2019)**: FYI, if anybody else has a similar problem, I eventually remembered that I needed to add myself to the Docker group using `sudo usermod -a -G docker ec2-user`, exiting, and then starting a new ssh session (a similar command was useful when I wanted to use gcsfuse along with Docker on Google Cloud, which is what I am currently testing...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:2050,Availability,error,error,2050," while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `docker ps -a` after the script stopped from that error message. So, that puts me back where I started, and I actually have that extra error message (over trying to use Docker from the instance launched via ECS). Nevertheless, if I make any additional progress, I will let you know. **Update (4/10/2019)**: FYI, if anybody else has a similar problem, I eventually remembered that I needed to add myself to the Docker group using `sudo usermod -a -G docker ec2-user`, exiting, and then starting a new ssh session (a similar command was useful when I wanted to use gcsfuse along with Docker on Google Cloud, which is what I am currently testing...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:2135,Availability,error,error,2135," while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `docker ps -a` after the script stopped from that error message. So, that puts me back where I started, and I actually have that extra error message (over trying to use Docker from the instance launched via ECS). Nevertheless, if I make any additional progress, I will let you know. **Update (4/10/2019)**: FYI, if anybody else has a similar problem, I eventually remembered that I needed to add myself to the Docker group using `sudo usermod -a -G docker ec2-user`, exiting, and then starting a new ssh session (a similar command was useful when I wanted to use gcsfuse along with Docker on Google Cloud, which is what I am currently testing...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:234,Deployability,install,install,234,"Also, if I try to follow your instructions for an EC2 instance, I can't actually create a _t1.micro_ EC2 instance, but I can create a **t2.micro** EC2 instance (with 1 vCPU and 1 GB of RAM). I remember spending some effort to try and install Docker on the EC2 instance. That installation is quite quick, and I almost wondered if I hadn't somehow didn't test `sudo yum install docker`. However, there was slow-down after pulling the docker image (possibly due to the type of instance?). If I sign into another terminal and try to run `docker images` (to confirm that everything worked OK), I get the following error message:. `Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:275,Deployability,install,installation,275,"Also, if I try to follow your instructions for an EC2 instance, I can't actually create a _t1.micro_ EC2 instance, but I can create a **t2.micro** EC2 instance (with 1 vCPU and 1 GB of RAM). I remember spending some effort to try and install Docker on the EC2 instance. That installation is quite quick, and I almost wondered if I hadn't somehow didn't test `sudo yum install docker`. However, there was slow-down after pulling the docker image (possibly due to the type of instance?). If I sign into another terminal and try to run `docker images` (to confirm that everything worked OK), I get the following error message:. `Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:368,Deployability,install,install,368,"Also, if I try to follow your instructions for an EC2 instance, I can't actually create a _t1.micro_ EC2 instance, but I can create a **t2.micro** EC2 instance (with 1 vCPU and 1 GB of RAM). I remember spending some effort to try and install Docker on the EC2 instance. That installation is quite quick, and I almost wondered if I hadn't somehow didn't test `sudo yum install docker`. However, there was slow-down after pulling the docker image (possibly due to the type of instance?). If I sign into another terminal and try to run `docker images` (to confirm that everything worked OK), I get the following error message:. `Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:2285,Deployability,Update,Update,2285," while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `docker ps -a` after the script stopped from that error message. So, that puts me back where I started, and I actually have that extra error message (over trying to use Docker from the instance launched via ECS). Nevertheless, if I make any additional progress, I will let you know. **Update (4/10/2019)**: FYI, if anybody else has a similar problem, I eventually remembered that I needed to add myself to the Docker group using `sudo usermod -a -G docker ec2-user`, exiting, and then starting a new ssh session (a similar command was useful when I wanted to use gcsfuse along with Docker on Google Cloud, which is what I am currently testing...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:615,Integrability,message,message,615,"Also, if I try to follow your instructions for an EC2 instance, I can't actually create a _t1.micro_ EC2 instance, but I can create a **t2.micro** EC2 instance (with 1 vCPU and 1 GB of RAM). I remember spending some effort to try and install Docker on the EC2 instance. That installation is quite quick, and I almost wondered if I hadn't somehow didn't test `sudo yum install docker`. However, there was slow-down after pulling the docker image (possibly due to the type of instance?). If I sign into another terminal and try to run `docker images` (to confirm that everything worked OK), I get the following error message:. `Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:1031,Integrability,message,message,1031,"Also, if I try to follow your instructions for an EC2 instance, I can't actually create a _t1.micro_ EC2 instance, but I can create a **t2.micro** EC2 instance (with 1 vCPU and 1 GB of RAM). I remember spending some effort to try and install Docker on the EC2 instance. That installation is quite quick, and I almost wondered if I hadn't somehow didn't test `sudo yum install docker`. However, there was slow-down after pulling the docker image (possibly due to the type of instance?). If I sign into another terminal and try to run `docker images` (to confirm that everything worked OK), I get the following error message:. `Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:1976,Integrability,message,message,1976," while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `docker ps -a` after the script stopped from that error message. So, that puts me back where I started, and I actually have that extra error message (over trying to use Docker from the instance launched via ECS). Nevertheless, if I make any additional progress, I will let you know. **Update (4/10/2019)**: FYI, if anybody else has a similar problem, I eventually remembered that I needed to add myself to the Docker group using `sudo usermod -a -G docker ec2-user`, exiting, and then starting a new ssh session (a similar command was useful when I wanted to use gcsfuse along with Docker on Google Cloud, which is what I am currently testing...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:2056,Integrability,message,message,2056," while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `docker ps -a` after the script stopped from that error message. So, that puts me back where I started, and I actually have that extra error message (over trying to use Docker from the instance launched via ECS). Nevertheless, if I make any additional progress, I will let you know. **Update (4/10/2019)**: FYI, if anybody else has a similar problem, I eventually remembered that I needed to add myself to the Docker group using `sudo usermod -a -G docker ec2-user`, exiting, and then starting a new ssh session (a similar command was useful when I wanted to use gcsfuse along with Docker on Google Cloud, which is what I am currently testing...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:2141,Integrability,message,message,2141," while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `docker ps -a` after the script stopped from that error message. So, that puts me back where I started, and I actually have that extra error message (over trying to use Docker from the instance launched via ECS). Nevertheless, if I make any additional progress, I will let you know. **Update (4/10/2019)**: FYI, if anybody else has a similar problem, I eventually remembered that I needed to add myself to the Docker group using `sudo usermod -a -G docker ec2-user`, exiting, and then starting a new ssh session (a similar command was useful when I wanted to use gcsfuse along with Docker on Google Cloud, which is what I am currently testing...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:353,Testability,test,test,353,"Also, if I try to follow your instructions for an EC2 instance, I can't actually create a _t1.micro_ EC2 instance, but I can create a **t2.micro** EC2 instance (with 1 vCPU and 1 GB of RAM). I remember spending some effort to try and install Docker on the EC2 instance. That installation is quite quick, and I almost wondered if I hadn't somehow didn't test `sudo yum install docker`. However, there was slow-down after pulling the docker image (possibly due to the type of instance?). If I sign into another terminal and try to run `docker images` (to confirm that everything worked OK), I get the following error message:. `Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480642492:2635,Testability,test,testing,2635," while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `docker ps -a` after the script stopped from that error message. So, that puts me back where I started, and I actually have that extra error message (over trying to use Docker from the instance launched via ECS). Nevertheless, if I make any additional progress, I will let you know. **Update (4/10/2019)**: FYI, if anybody else has a similar problem, I eventually remembered that I needed to add myself to the Docker group using `sudo usermod -a -G docker ec2-user`, exiting, and then starting a new ssh session (a similar command was useful when I wanted to use gcsfuse along with Docker on Google Cloud, which is what I am currently testing...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492
https://github.com/google/deepvariant/issues/167#issuecomment-480702286:385,Deployability,configurat,configuration,385,"It took about 8 hours, but I could run the **postprocess_variants** step on my local computer (using the commands [specified above](https://github.com/google/deepvariant/issues/167#issuecomment-480640009)). If Official Amazon Support doesn't have a solution for running this program on AWS, I might cross-post this on StackExchange (to see if I can figure out if there is some sort of configuration issue on AWS, and/or if I am not using ECS efficiently/correctly). However, I realize you have a lot of support to provide, so I will close this ticket and provide the successful output from my local computer:. ```; 2019-04-07 21:06:30.591035: I deepvariant/postprocess_variants.cc:88] Read from: Genos_Provided/call_variants_output.tfrecord.gz; 2019-04-07 21:06:33.504711: I deepvariant/postprocess_variants.cc:97] Done reading: Genos_Provided/call_variants_output.tfrecord.gz. #entries in single_site_calls = 91732; 2019-04-07 21:06:33.505181: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 91732; 2019-04-07 21:06:33.505270: I deepvariant/postprocess_variants.cc:103] Start SortSingleSiteCalls; 2019-04-07 21:06:34.914308: I deepvariant/postprocess_variants.cc:105] Done SortSingleSiteCalls; I0407 21:06:36.217032 139687245461248 postprocess_variants.py:596] Writing output to VCF file: Genos_Provided/output.vcf.gz; I0407 21:06:36.221911 139687245461248 genomics_writer.py:163] Writing Genos_Provided/output.vcf.gz with NativeVcfWriter; I0407 21:06:36.231071 139687245461248 postprocess_variants.py:601] 1 variants written.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480702286
https://github.com/google/deepvariant/issues/167#issuecomment-480702286:442,Energy Efficiency,efficient,efficiently,442,"It took about 8 hours, but I could run the **postprocess_variants** step on my local computer (using the commands [specified above](https://github.com/google/deepvariant/issues/167#issuecomment-480640009)). If Official Amazon Support doesn't have a solution for running this program on AWS, I might cross-post this on StackExchange (to see if I can figure out if there is some sort of configuration issue on AWS, and/or if I am not using ECS efficiently/correctly). However, I realize you have a lot of support to provide, so I will close this ticket and provide the successful output from my local computer:. ```; 2019-04-07 21:06:30.591035: I deepvariant/postprocess_variants.cc:88] Read from: Genos_Provided/call_variants_output.tfrecord.gz; 2019-04-07 21:06:33.504711: I deepvariant/postprocess_variants.cc:97] Done reading: Genos_Provided/call_variants_output.tfrecord.gz. #entries in single_site_calls = 91732; 2019-04-07 21:06:33.505181: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 91732; 2019-04-07 21:06:33.505270: I deepvariant/postprocess_variants.cc:103] Start SortSingleSiteCalls; 2019-04-07 21:06:34.914308: I deepvariant/postprocess_variants.cc:105] Done SortSingleSiteCalls; I0407 21:06:36.217032 139687245461248 postprocess_variants.py:596] Writing output to VCF file: Genos_Provided/output.vcf.gz; I0407 21:06:36.221911 139687245461248 genomics_writer.py:163] Writing Genos_Provided/output.vcf.gz with NativeVcfWriter; I0407 21:06:36.231071 139687245461248 postprocess_variants.py:601] 1 variants written.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480702286
https://github.com/google/deepvariant/issues/167#issuecomment-480702286:385,Modifiability,config,configuration,385,"It took about 8 hours, but I could run the **postprocess_variants** step on my local computer (using the commands [specified above](https://github.com/google/deepvariant/issues/167#issuecomment-480640009)). If Official Amazon Support doesn't have a solution for running this program on AWS, I might cross-post this on StackExchange (to see if I can figure out if there is some sort of configuration issue on AWS, and/or if I am not using ECS efficiently/correctly). However, I realize you have a lot of support to provide, so I will close this ticket and provide the successful output from my local computer:. ```; 2019-04-07 21:06:30.591035: I deepvariant/postprocess_variants.cc:88] Read from: Genos_Provided/call_variants_output.tfrecord.gz; 2019-04-07 21:06:33.504711: I deepvariant/postprocess_variants.cc:97] Done reading: Genos_Provided/call_variants_output.tfrecord.gz. #entries in single_site_calls = 91732; 2019-04-07 21:06:33.505181: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 91732; 2019-04-07 21:06:33.505270: I deepvariant/postprocess_variants.cc:103] Start SortSingleSiteCalls; 2019-04-07 21:06:34.914308: I deepvariant/postprocess_variants.cc:105] Done SortSingleSiteCalls; I0407 21:06:36.217032 139687245461248 postprocess_variants.py:596] Writing output to VCF file: Genos_Provided/output.vcf.gz; I0407 21:06:36.221911 139687245461248 genomics_writer.py:163] Writing Genos_Provided/output.vcf.gz with NativeVcfWriter; I0407 21:06:36.231071 139687245461248 postprocess_variants.py:601] 1 variants written.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-480702286
https://github.com/google/deepvariant/issues/167#issuecomment-482393946:133,Availability,error,error,133,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
https://github.com/google/deepvariant/issues/167#issuecomment-482393946:235,Availability,error,error,235,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
https://github.com/google/deepvariant/issues/167#issuecomment-482393946:891,Availability,error,error,891,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
https://github.com/google/deepvariant/issues/167#issuecomment-482393946:139,Integrability,message,message,139,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
https://github.com/google/deepvariant/issues/167#issuecomment-482393946:768,Integrability,message,messages,768,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
https://github.com/google/deepvariant/issues/167#issuecomment-482393946:897,Integrability,message,message,897,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
https://github.com/google/deepvariant/issues/167#issuecomment-482393946:555,Modifiability,variab,variable,555,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
https://github.com/google/deepvariant/issues/167#issuecomment-482393946:40,Testability,test,testing,40,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
https://github.com/google/deepvariant/issues/167#issuecomment-482393946:112,Usability,simpl,simple,112,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946
https://github.com/google/deepvariant/issues/168#issuecomment-481424791:308,Deployability,pipeline,pipelines,308,"Hi Tomasz,; We currently do not support running make_examples on GPUs, just the parallelized threads that you are currently using. However, you could consider [this external solution](https://docs.parabricks.com/standalone-tools/deepvariant) from Parabricks, which provides multi-GPU support for running our pipelines. Please note that we are not involved with creating or maintaining this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168#issuecomment-481424791
https://github.com/google/deepvariant/issues/169#issuecomment-481854459:60,Availability,error,error,60,"Please provide more details on the issue (commands run, the error you saw, etc.) You can start with the [quickstart doc](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) or one of the case studies (linked on the top-level page) to better understand how to install and run DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/169#issuecomment-481854459
https://github.com/google/deepvariant/issues/169#issuecomment-481854459:288,Deployability,install,install,288,"Please provide more details on the issue (commands run, the error you saw, etc.) You can start with the [quickstart doc](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) or one of the case studies (linked on the top-level page) to better understand how to install and run DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/169#issuecomment-481854459
https://github.com/google/deepvariant/issues/170#issuecomment-482316115:387,Usability,clear,clear,387,"@xunjieli yes, I can. I only extracted the variants where the analyzed sample had the alternative allele (genotype 1/1,0/1,./1). I did this for the single vcf obtained from deepvariant and for the combined vcf (obtained from gatk using the gvcf created with deepvariant).; Then, I compared only the variants (chromosome, position, reference and alternate allele) of this two sets. Is it clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170#issuecomment-482316115
https://github.com/google/deepvariant/issues/170#issuecomment-482327182:192,Usability,feedback,feedback,192,"When you say ""_when we use another tool to process the gvcf created by deepvariant, some information are changed_,"" it might worth figuring out what exactly is changed. Without that info, the feedback isn't actionable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170#issuecomment-482327182
https://github.com/google/deepvariant/issues/170#issuecomment-482328482:326,Usability,feedback,feedback,326,"Thanks @jaqueytw . I don't believe we're currently encouraging our users to use GATK tools to process our files. But if you find any documentation that mentioned/encouraged that, please do let me know and I'd like to fix it. We're actively working on coming up with our own recommendation for best practice. Your analysis and feedback is very valuable. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170#issuecomment-482328482
https://github.com/google/deepvariant/issues/170#issuecomment-553659345:27,Deployability,update,update,27,"Hi @jaqueytw . I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170#issuecomment-553659345
https://github.com/google/deepvariant/issues/170#issuecomment-553659345:116,Deployability,release,release,116,"Hi @jaqueytw . I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). We encourage you or other users interested in multi-sample calling to follow these recommendations to combine multiple DeepVariant gVCFs. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170#issuecomment-553659345
https://github.com/google/deepvariant/issues/171#issuecomment-483448362:1539,Availability,down,downloads,1539,"CP cloud-orchestration framework developed external to our team. The speed-optimized one here is 70 min. However, as there are now multiple orchestration options (including Parabricks and ATGENOMIX) we have decided to separate the single-machine runtime from what can be achieved with orchestration. I understand you are running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard instance class should consume 288 CPU-hours, which on a pre-emptible GCP machine should cost $0.01 per core-hour at current pricing, for $2.88. This is also not the most cost-efficient way to run DeepVariant. 2\); Your script runs `mkdir -p ${OUTPUT_DIR}`, which I believe does not work since you need sudo permission to create this directory. You will want to modify this command to ensure that `/home/cwarden/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM` exists locally prior to mounting. We also recommend updating your scrip",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362
https://github.com/google/deepvariant/issues/171#issuecomment-483448362:2718,Availability,checkpoint,checkpoint,2718,"iant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard instance class should consume 288 CPU-hours, which on a pre-emptible GCP machine should cost $0.01 per core-hour at current pricing, for $2.88. This is also not the most cost-efficient way to run DeepVariant. 2\); Your script runs `mkdir -p ${OUTPUT_DIR}`, which I believe does not work since you need sudo permission to create this directory. You will want to modify this command to ensure that `/home/cwarden/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM` exists locally prior to mounting. We also recommend updating your script to explicitly use the 0.8.0 image at ` gcr.io/deepvariant-docker/deepvariant:0.8.0`, which is now tagged as `latest`. Models are included in this image. The WES model checkpoint for this image is at `/opt/models/wes/model.ckpt`. To run the v0.8.0 container in interactive mode, you will need to modify the entrypoint as below. ```; docker run -it --entrypoint bash -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant:0.8.0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362
https://github.com/google/deepvariant/issues/171#issuecomment-483448362:280,Deployability,configurat,configuration,280,"Hi @cwarden45, just to make sure I understand correctly, does your time estimate of 24 hours on Google Cloud include upload time to the machine?. 1a) ; The 5 hour runtime is based on performance on a specific set-up: a 64-core CPU-only setup (which is not the most cost-optimized configuration for running DeepVariant). We state this runtime as a baseline, and mention that many accelerated pipelines exist that allow WGS to run as fast as 40 minutes. I want to clarify the runtimes a bit more. Previously, our estimates for speed was based on the GCP cloud-orchestration framework developed external to our team. The speed-optimized one here is 70 min. However, as there are now multiple orchestration options (including Parabricks and ATGENOMIX) we have decided to separate the single-machine runtime from what can be achieved with orchestration. I understand you are running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362
https://github.com/google/deepvariant/issues/171#issuecomment-483448362:391,Deployability,pipeline,pipelines,391,"Hi @cwarden45, just to make sure I understand correctly, does your time estimate of 24 hours on Google Cloud include upload time to the machine?. 1a) ; The 5 hour runtime is based on performance on a specific set-up: a 64-core CPU-only setup (which is not the most cost-optimized configuration for running DeepVariant). We state this runtime as a baseline, and mention that many accelerated pipelines exist that allow WGS to run as fast as 40 minutes. I want to clarify the runtimes a bit more. Previously, our estimates for speed was based on the GCP cloud-orchestration framework developed external to our team. The speed-optimized one here is 70 min. However, as there are now multiple orchestration options (including Parabricks and ATGENOMIX) we have decided to separate the single-machine runtime from what can be achieved with orchestration. I understand you are running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362
https://github.com/google/deepvariant/issues/171#issuecomment-483448362:1304,Energy Efficiency,efficient,efficiently,1304,"t cost-optimized configuration for running DeepVariant). We state this runtime as a baseline, and mention that many accelerated pipelines exist that allow WGS to run as fast as 40 minutes. I want to clarify the runtimes a bit more. Previously, our estimates for speed was based on the GCP cloud-orchestration framework developed external to our team. The speed-optimized one here is 70 min. However, as there are now multiple orchestration options (including Parabricks and ATGENOMIX) we have decided to separate the single-machine runtime from what can be achieved with orchestration. I understand you are running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard instance class should consume 288 CPU-hours, which on a pre-emptible GCP machine should cost $0.01 per core-hour at current pricing, for $2.88. This is also not the most cost-efficient way to run DeepVariant. 2\); Your script runs `mkdir -p ${OUTPUT_DIR}`, which I b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362
https://github.com/google/deepvariant/issues/171#issuecomment-483448362:2172,Energy Efficiency,efficient,efficient,2172,"iant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard instance class should consume 288 CPU-hours, which on a pre-emptible GCP machine should cost $0.01 per core-hour at current pricing, for $2.88. This is also not the most cost-efficient way to run DeepVariant. 2\); Your script runs `mkdir -p ${OUTPUT_DIR}`, which I believe does not work since you need sudo permission to create this directory. You will want to modify this command to ensure that `/home/cwarden/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM` exists locally prior to mounting. We also recommend updating your script to explicitly use the 0.8.0 image at ` gcr.io/deepvariant-docker/deepvariant:0.8.0`, which is now tagged as `latest`. Models are included in this image. The WES model checkpoint for this image is at `/opt/models/wes/model.ckpt`. To run the v0.8.0 container in interactive mode, you will need to modify the entrypoint as below. ```; docker run -it --entrypoint bash -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant:0.8.0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362
https://github.com/google/deepvariant/issues/171#issuecomment-483448362:280,Modifiability,config,configuration,280,"Hi @cwarden45, just to make sure I understand correctly, does your time estimate of 24 hours on Google Cloud include upload time to the machine?. 1a) ; The 5 hour runtime is based on performance on a specific set-up: a 64-core CPU-only setup (which is not the most cost-optimized configuration for running DeepVariant). We state this runtime as a baseline, and mention that many accelerated pipelines exist that allow WGS to run as fast as 40 minutes. I want to clarify the runtimes a bit more. Previously, our estimates for speed was based on the GCP cloud-orchestration framework developed external to our team. The speed-optimized one here is 70 min. However, as there are now multiple orchestration options (including Parabricks and ATGENOMIX) we have decided to separate the single-machine runtime from what can be achieved with orchestration. I understand you are running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362
https://github.com/google/deepvariant/issues/171#issuecomment-483448362:183,Performance,perform,performance,183,"Hi @cwarden45, just to make sure I understand correctly, does your time estimate of 24 hours on Google Cloud include upload time to the machine?. 1a) ; The 5 hour runtime is based on performance on a specific set-up: a 64-core CPU-only setup (which is not the most cost-optimized configuration for running DeepVariant). We state this runtime as a baseline, and mention that many accelerated pipelines exist that allow WGS to run as fast as 40 minutes. I want to clarify the runtimes a bit more. Previously, our estimates for speed was based on the GCP cloud-orchestration framework developed external to our team. The speed-optimized one here is 70 min. However, as there are now multiple orchestration options (including Parabricks and ATGENOMIX) we have decided to separate the single-machine runtime from what can be achieved with orchestration. I understand you are running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362
https://github.com/google/deepvariant/issues/171#issuecomment-483448362:270,Performance,optimiz,optimized,270,"Hi @cwarden45, just to make sure I understand correctly, does your time estimate of 24 hours on Google Cloud include upload time to the machine?. 1a) ; The 5 hour runtime is based on performance on a specific set-up: a 64-core CPU-only setup (which is not the most cost-optimized configuration for running DeepVariant). We state this runtime as a baseline, and mention that many accelerated pipelines exist that allow WGS to run as fast as 40 minutes. I want to clarify the runtimes a bit more. Previously, our estimates for speed was based on the GCP cloud-orchestration framework developed external to our team. The speed-optimized one here is 70 min. However, as there are now multiple orchestration options (including Parabricks and ATGENOMIX) we have decided to separate the single-machine runtime from what can be achieved with orchestration. I understand you are running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362
https://github.com/google/deepvariant/issues/171#issuecomment-483448362:624,Performance,optimiz,optimized,624,"Hi @cwarden45, just to make sure I understand correctly, does your time estimate of 24 hours on Google Cloud include upload time to the machine?. 1a) ; The 5 hour runtime is based on performance on a specific set-up: a 64-core CPU-only setup (which is not the most cost-optimized configuration for running DeepVariant). We state this runtime as a baseline, and mention that many accelerated pipelines exist that allow WGS to run as fast as 40 minutes. I want to clarify the runtimes a bit more. Previously, our estimates for speed was based on the GCP cloud-orchestration framework developed external to our team. The speed-optimized one here is 70 min. However, as there are now multiple orchestration options (including Parabricks and ATGENOMIX) we have decided to separate the single-machine runtime from what can be achieved with orchestration. I understand you are running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362
https://github.com/google/deepvariant/issues/171#issuecomment-483448362:1898,Testability,test,tests,1898,"running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard instance class should consume 288 CPU-hours, which on a pre-emptible GCP machine should cost $0.01 per core-hour at current pricing, for $2.88. This is also not the most cost-efficient way to run DeepVariant. 2\); Your script runs `mkdir -p ${OUTPUT_DIR}`, which I believe does not work since you need sudo permission to create this directory. You will want to modify this command to ensure that `/home/cwarden/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM` exists locally prior to mounting. We also recommend updating your script to explicitly use the 0.8.0 image at ` gcr.io/deepvariant-docker/deepvariant:0.8.0`, which is now tagged as `latest`. Models are included in this image. The WES model checkpoint for this image is at `/opt/models/wes/model.ckpt`. To run the v0.8.0 container in interactive mode, you will need to modify the entrypoint as",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3321,Availability,error,error,3321,"o see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:4672,Availability,Error,Error,4672,").; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-16T02:45:46Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; sudo docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/DeepVariant_Output/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; ERRO[0000] error waiting for container: context canceled; ```. While I wasn't specifying a specific version, thank you for pointing that out; even though comparing versions may be interesting, I should try to keep the version similar for all com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:4700,Availability,error,error,4700,").; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-16T02:45:46Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; sudo docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/DeepVariant_Output/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; ERRO[0000] error waiting for container: context canceled; ```. While I wasn't specifying a specific version, thank you for pointing that out; even though comparing versions may be interesting, I should try to keep the version similar for all com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:4847,Availability,error,error,4847,"0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-16T02:45:46Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; sudo docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/DeepVariant_Output/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; ERRO[0000] error waiting for container: context canceled; ```. While I wasn't specifying a specific version, thank you for pointing that out; even though comparing versions may be interesting, I should try to keep the version similar for all comparisons. Also, adding `--entrypoint` bash allows me to run Docker in interactive mode (starting in /opt/models/pacbio). ***However, running that command (with or withou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:4858,Availability,error,error,4858,"0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-16T02:45:46Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; sudo docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/DeepVariant_Output/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; ERRO[0000] error waiting for container: context canceled; ```. While I wasn't specifying a specific version, thank you for pointing that out; even though comparing versions may be interesting, I should try to keep the version similar for all comparisons. Also, adding `--entrypoint` bash allows me to run Docker in interactive mode (starting in /opt/models/pacbio). ***However, running that command (with or withou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:5350,Availability,Error,Error,5350,"el to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-16T02:45:46Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; sudo docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/DeepVariant_Output/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; ERRO[0000] error waiting for container: context canceled; ```. While I wasn't specifying a specific version, thank you for pointing that out; even though comparing versions may be interesting, I should try to keep the version similar for all comparisons. Also, adding `--entrypoint` bash allows me to run Docker in interactive mode (starting in /opt/models/pacbio). ***However, running that command (with or without sudo) shows an empty file directory in ""/mnt/cdw-genome"" (within the Docker container).***. Do you know what may be causing that issue?. Thank you very much for your assistance!. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:5378,Availability,error,error,5378,"el to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-16T02:45:46Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; sudo docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/DeepVariant_Output/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; ERRO[0000] error waiting for container: context canceled; ```. While I wasn't specifying a specific version, thank you for pointing that out; even though comparing versions may be interesting, I should try to keep the version similar for all comparisons. Also, adding `--entrypoint` bash allows me to run Docker in interactive mode (starting in /opt/models/pacbio). ***However, running that command (with or without sudo) shows an empty file directory in ""/mnt/cdw-genome"" (within the Docker container).***. Do you know what may be causing that issue?. Thank you very much for your assistance!. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:5502,Availability,error,error,5502,"el to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-16T02:45:46Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; sudo docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/DeepVariant_Output/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; ERRO[0000] error waiting for container: context canceled; ```. While I wasn't specifying a specific version, thank you for pointing that out; even though comparing versions may be interesting, I should try to keep the version similar for all comparisons. Also, adding `--entrypoint` bash allows me to run Docker in interactive mode (starting in /opt/models/pacbio). ***However, running that command (with or without sudo) shows an empty file directory in ""/mnt/cdw-genome"" (within the Docker container).***. Do you know what may be causing that issue?. Thank you very much for your assistance!. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:1261,Deployability,pipeline,pipeline,1261,"cs/tutorials/deepvariant):. ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=cdw-deepvariant-wgs-exome; OUTPUT_BUCKET=gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/DeepVariant_Output; STAGING_FOLDER_NAME=wgs_staging; OUTPUT_FILE_NAME=WGS_Provided.vcf. ## Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard. IMAGE_VERSION=0.7.2; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --sample_name VeritasProvided \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/veritas_wgs.filter.bam \; --ref gs://cdw-genome/Ref/hg19.gatk.fasta \; --gcsfuse""; ; # Run the pipeline.; # run after 'gcloud config set compute/region """"'; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --regions us-west2 \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; 	--docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I admittedly didn't know what were the default settings, and I saw larger numbers in the later commands. While that gives me other options to test to decrease run-time, it sounds like I should point out that these are two cloud comparisons (*not* being run on my local machine - that was only GATK). It also looks like I used an earlier version of one of the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:1345,Deployability,pipeline,pipelines,1345,"iant-wgs-exome; OUTPUT_BUCKET=gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/DeepVariant_Output; STAGING_FOLDER_NAME=wgs_staging; OUTPUT_FILE_NAME=WGS_Provided.vcf. ## Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard. IMAGE_VERSION=0.7.2; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --sample_name VeritasProvided \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/veritas_wgs.filter.bam \; --ref gs://cdw-genome/Ref/hg19.gatk.fasta \; --gcsfuse""; ; # Run the pipeline.; # run after 'gcloud config set compute/region """"'; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --regions us-west2 \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; 	--docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I admittedly didn't know what were the default settings, and I saw larger numbers in the later commands. While that gives me other options to test to decrease run-time, it sounds like I should point out that these are two cloud comparisons (*not* being run on my local machine - that was only GATK). It also looks like I used an earlier version of one of the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3532,Deployability,upgrade,upgraded,3532,"r (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3550,Deployability,install,installed,3550,"r (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3583,Deployability,upgrade,upgraded,3583,"r (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3742,Deployability,upgrade,upgraded,3742,"ctory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3760,Deployability,install,installed,3760,"ctory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3793,Deployability,upgrade,upgraded,3793,"ctory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:4204,Energy Efficiency,Power,Power,4204,"h test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-16T02:45:46Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; sudo docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARG",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3327,Integrability,message,message,3327,"o see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3427,Integrability,depend,dependency,3427,"le Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3634,Integrability,depend,dependency,3634," I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:1292,Modifiability,config,config,1292,"iant-wgs-exome; OUTPUT_BUCKET=gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/DeepVariant_Output; STAGING_FOLDER_NAME=wgs_staging; OUTPUT_FILE_NAME=WGS_Provided.vcf. ## Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard. IMAGE_VERSION=0.7.2; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --sample_name VeritasProvided \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/veritas_wgs.filter.bam \; --ref gs://cdw-genome/Ref/hg19.gatk.fasta \; --gcsfuse""; ; # Run the pipeline.; # run after 'gcloud config set compute/region """"'; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --regions us-west2 \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; 	--docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I admittedly didn't know what were the default settings, and I saw larger numbers in the later commands. While that gives me other options to test to decrease run-time, it sounds like I should point out that these are two cloud comparisons (*not* being run on my local machine - that was only GATK). It also looks like I used an earlier version of one of the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:1492,Testability,log,logging,1492,"FILE_NAME=WGS_Provided.vcf. ## Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard. IMAGE_VERSION=0.7.2; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --sample_name VeritasProvided \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/veritas_wgs.filter.bam \; --ref gs://cdw-genome/Ref/hg19.gatk.fasta \; --gcsfuse""; ; # Run the pipeline.; # run after 'gcloud config set compute/region """"'; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --regions us-west2 \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; 	--docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I admittedly didn't know what were the default settings, and I saw larger numbers in the later commands. While that gives me other options to test to decrease run-time, it sounds like I should point out that these are two cloud comparisons (*not* being run on my local machine - that was only GATK). It also looks like I used an earlier version of one of the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:1576,Testability,log,log,1576,"g data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard. IMAGE_VERSION=0.7.2; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --sample_name VeritasProvided \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/veritas_wgs.filter.bam \; --ref gs://cdw-genome/Ref/hg19.gatk.fasta \; --gcsfuse""; ; # Run the pipeline.; # run after 'gcloud config set compute/region """"'; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --regions us-west2 \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; 	--docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I admittedly didn't know what were the default settings, and I saw larger numbers in the later commands. While that gives me other options to test to decrease run-time, it sounds like I should point out that these are two cloud comparisons (*not* being run on my local machine - that was only GATK). It also looks like I used an earlier version of one of the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:1843,Testability,test,test,1843,"-sample_name VeritasProvided \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/veritas_wgs.filter.bam \; --ref gs://cdw-genome/Ref/hg19.gatk.fasta \; --gcsfuse""; ; # Run the pipeline.; # run after 'gcloud config set compute/region """"'; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --regions us-west2 \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; 	--docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I admittedly didn't know what were the default settings, and I saw larger numbers in the later commands. While that gives me other options to test to decrease run-time, it sounds like I should point out that these are two cloud comparisons (*not* being run on my local machine - that was only GATK). It also looks like I used an earlier version of one of the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:2229,Testability,test,testing,2229,"ha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --regions us-west2 \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; 	--docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I admittedly didn't know what were the default settings, and I saw larger numbers in the later commands. While that gives me other options to test to decrease run-time, it sounds like I should point out that these are two cloud comparisons (*not* being run on my local machine - that was only GATK). It also looks like I used an earlier version of one of the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:2956,Testability,test,tested,2956,"f the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3184,Testability,test,test,3184," in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - Th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3213,Testability,test,test,3213," in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - Th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3836,Testability,log,logs,3836,"nt it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-16T02:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483490946:4218,Testability,log,login,4218,"h test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-16T02:45:46Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; sudo docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARG",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946
https://github.com/google/deepvariant/issues/171#issuecomment-483500506:924,Availability,error,error,924,"Hi Charles,. Thank you for the machine information. For the AWS instance, the 8-core machine has 8-fold fewer CPUs than the 64-cores used in the case study. Given this, the total number of core-hours is 144. This is actually quite efficient for a DeepVariant run. For more similar time to the case study, you could consider the m5.12xlarge or m5.24xlarge, with 48 and 96 cores respectively. The 96-core instance will likely be faster than the case study, though with worse economics. In all cases, the use of spot instances will be far more cost-favorable. For the GCP run posted, this looks to be the GCP cost-optimized execution framework. This framework is built by the Google Cloud, so although I cannot speak as directly to this, I can refer your question to the right place. That GitHub repository is here: https://github.com/googlegenomics/gcp-deepvariant-runner/issues. For your Docker question, I believe that this error reflects that you are not mounting the volumes in the proper way in your script. In your .sh script, you may want to try to more closely align the way your script mounts and creates input and output directories with the case study. I don't feel that I can clearly communicate the details of mounting the Docker volumes. It may be valuable to read Docker's description of this: https://docs.docker.com/storage/volumes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483500506
https://github.com/google/deepvariant/issues/171#issuecomment-483500506:231,Energy Efficiency,efficient,efficient,231,"Hi Charles,. Thank you for the machine information. For the AWS instance, the 8-core machine has 8-fold fewer CPUs than the 64-cores used in the case study. Given this, the total number of core-hours is 144. This is actually quite efficient for a DeepVariant run. For more similar time to the case study, you could consider the m5.12xlarge or m5.24xlarge, with 48 and 96 cores respectively. The 96-core instance will likely be faster than the case study, though with worse economics. In all cases, the use of spot instances will be far more cost-favorable. For the GCP run posted, this looks to be the GCP cost-optimized execution framework. This framework is built by the Google Cloud, so although I cannot speak as directly to this, I can refer your question to the right place. That GitHub repository is here: https://github.com/googlegenomics/gcp-deepvariant-runner/issues. For your Docker question, I believe that this error reflects that you are not mounting the volumes in the proper way in your script. In your .sh script, you may want to try to more closely align the way your script mounts and creates input and output directories with the case study. I don't feel that I can clearly communicate the details of mounting the Docker volumes. It may be valuable to read Docker's description of this: https://docs.docker.com/storage/volumes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483500506
https://github.com/google/deepvariant/issues/171#issuecomment-483500506:611,Performance,optimiz,optimized,611,"Hi Charles,. Thank you for the machine information. For the AWS instance, the 8-core machine has 8-fold fewer CPUs than the 64-cores used in the case study. Given this, the total number of core-hours is 144. This is actually quite efficient for a DeepVariant run. For more similar time to the case study, you could consider the m5.12xlarge or m5.24xlarge, with 48 and 96 cores respectively. The 96-core instance will likely be faster than the case study, though with worse economics. In all cases, the use of spot instances will be far more cost-favorable. For the GCP run posted, this looks to be the GCP cost-optimized execution framework. This framework is built by the Google Cloud, so although I cannot speak as directly to this, I can refer your question to the right place. That GitHub repository is here: https://github.com/googlegenomics/gcp-deepvariant-runner/issues. For your Docker question, I believe that this error reflects that you are not mounting the volumes in the proper way in your script. In your .sh script, you may want to try to more closely align the way your script mounts and creates input and output directories with the case study. I don't feel that I can clearly communicate the details of mounting the Docker volumes. It may be valuable to read Docker's description of this: https://docs.docker.com/storage/volumes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483500506
https://github.com/google/deepvariant/issues/171#issuecomment-483500506:1186,Usability,clear,clearly,1186,"Hi Charles,. Thank you for the machine information. For the AWS instance, the 8-core machine has 8-fold fewer CPUs than the 64-cores used in the case study. Given this, the total number of core-hours is 144. This is actually quite efficient for a DeepVariant run. For more similar time to the case study, you could consider the m5.12xlarge or m5.24xlarge, with 48 and 96 cores respectively. The 96-core instance will likely be faster than the case study, though with worse economics. In all cases, the use of spot instances will be far more cost-favorable. For the GCP run posted, this looks to be the GCP cost-optimized execution framework. This framework is built by the Google Cloud, so although I cannot speak as directly to this, I can refer your question to the right place. That GitHub repository is here: https://github.com/googlegenomics/gcp-deepvariant-runner/issues. For your Docker question, I believe that this error reflects that you are not mounting the volumes in the proper way in your script. In your .sh script, you may want to try to more closely align the way your script mounts and creates input and output directories with the case study. I don't feel that I can clearly communicate the details of mounting the Docker volumes. It may be valuable to read Docker's description of this: https://docs.docker.com/storage/volumes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171#issuecomment-483500506
https://github.com/google/deepvariant/issues/172#issuecomment-483412986:52,Security,validat,validation,52,This could be due to no examples were generated for validation set. When you run make_example step there multiple files generated. Could you attach one of the text files generated during make_example validation set step?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172#issuecomment-483412986
https://github.com/google/deepvariant/issues/172#issuecomment-483412986:200,Security,validat,validation,200,This could be due to no examples were generated for validation set. When you run make_example step there multiple files generated. Could you attach one of the text files generated during make_example validation set step?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172#issuecomment-483412986
https://github.com/google/deepvariant/issues/172#issuecomment-483490813:38,Security,validat,validation,38,"I attached two files. One is shuffled validation set, the other one is without shuffled. These data sets are from google quick-start test dataset. Region is chr20:10,005,000-10,008,000. ; [validation_set_with_shuffled.tar.gz](https://github.com/google/deepvariant/files/3082816/validation_set_with_shuffled.tar.gz); [validation_set_without_shuffled.tar.gz](https://github.com/google/deepvariant/files/3082817/validation_set_without_shuffled.tar.gz); @akolesnikov Thanks very much.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172#issuecomment-483490813
https://github.com/google/deepvariant/issues/172#issuecomment-483490813:133,Testability,test,test,133,"I attached two files. One is shuffled validation set, the other one is without shuffled. These data sets are from google quick-start test dataset. Region is chr20:10,005,000-10,008,000. ; [validation_set_with_shuffled.tar.gz](https://github.com/google/deepvariant/files/3082816/validation_set_with_shuffled.tar.gz); [validation_set_without_shuffled.tar.gz](https://github.com/google/deepvariant/files/3082817/validation_set_without_shuffled.tar.gz); @akolesnikov Thanks very much.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172#issuecomment-483490813
https://github.com/google/deepvariant/issues/172#issuecomment-484975077:30,Testability,log,log,30,"It is at the beginning of the log. I copied it from your first post. `; Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantModel(name=inception_v3); I0415 07:34:19.585236 140713377441536 model_eval.py:198] Dataset has 8 samples, doing eval over 0; max_examples is 1000000, num_batches is 0; `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172#issuecomment-484975077
https://github.com/google/deepvariant/issues/173#issuecomment-483410868:177,Usability,simpl,simplicity,177,"Hi @zstephens, thanks for reaching out! The goal of `run_deepvariant` was to provide users with a single command they could use to run DeepVariant using Docker. For the sake of simplicity, this approach only accepts [a few different flags](https://github.com/google/deepvariant/blob/r0.8/scripts/run_deepvariant.py#L53). We still do support the `sample_name` flag for `make_examples`, but it is not possible to use with `run_deepvariant`. To use this and other flags, I would suggest using separate commands for each step of DeepVariant (`make_examples`, `call_variants`, or `postprocess_variants`). Each of these binaries is included in the Docker image and can be run using a command such as below (with any additional desired flags). ```; sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0\ ; /opt/deepvariant/bin/make_examples; ```. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/173#issuecomment-483410868
https://github.com/google/deepvariant/issues/174#issuecomment-483406535:86,Testability,test,test,86,"Hi Peter,; We used multiple HG002 samples generated with PacBio (CCS mode). We didn't test this model for CLR reads.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/174#issuecomment-483406535
https://github.com/google/deepvariant/issues/174#issuecomment-487190949:263,Performance,perform,performed,263,"Hi, so I noticed in the Deepvariant manuscript supplementary text that the 44x NA12878 CLR data (sorted_final_merged.bam) from GIAB was used for benchmarking Deepvariant with PacBio CLR reads (specifically, that chroms 1-19 were used for training and testing was performed on chroms 20-22). Would it be possible for me to access the result VCF for chroms 20-22 (and ideally also the trained model used in the manuscript)? The reason I'm interested in this is that we have developed our own CLR variant calling method and I would like to fairly compare it with Deepvariant. If I have the VCF, then I can be sure that our precision/recall calculations on chr20-22 are done in exactly the same way, and if I have the trained model then I also have the option to run Deepvariant myself on other CLR data. Unfortunately, the training procedure for Deepvariant seems to be complicated at this time and requires significant compute resources. I would be very grateful if it would be possible to meet this request -- you can reach me by email at pedge AT eng.ucsd.edu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/174#issuecomment-487190949
https://github.com/google/deepvariant/issues/174#issuecomment-487190949:322,Security,access,access,322,"Hi, so I noticed in the Deepvariant manuscript supplementary text that the 44x NA12878 CLR data (sorted_final_merged.bam) from GIAB was used for benchmarking Deepvariant with PacBio CLR reads (specifically, that chroms 1-19 were used for training and testing was performed on chroms 20-22). Would it be possible for me to access the result VCF for chroms 20-22 (and ideally also the trained model used in the manuscript)? The reason I'm interested in this is that we have developed our own CLR variant calling method and I would like to fairly compare it with Deepvariant. If I have the VCF, then I can be sure that our precision/recall calculations on chr20-22 are done in exactly the same way, and if I have the trained model then I also have the option to run Deepvariant myself on other CLR data. Unfortunately, the training procedure for Deepvariant seems to be complicated at this time and requires significant compute resources. I would be very grateful if it would be possible to meet this request -- you can reach me by email at pedge AT eng.ucsd.edu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/174#issuecomment-487190949
https://github.com/google/deepvariant/issues/174#issuecomment-487190949:145,Testability,benchmark,benchmarking,145,"Hi, so I noticed in the Deepvariant manuscript supplementary text that the 44x NA12878 CLR data (sorted_final_merged.bam) from GIAB was used for benchmarking Deepvariant with PacBio CLR reads (specifically, that chroms 1-19 were used for training and testing was performed on chroms 20-22). Would it be possible for me to access the result VCF for chroms 20-22 (and ideally also the trained model used in the manuscript)? The reason I'm interested in this is that we have developed our own CLR variant calling method and I would like to fairly compare it with Deepvariant. If I have the VCF, then I can be sure that our precision/recall calculations on chr20-22 are done in exactly the same way, and if I have the trained model then I also have the option to run Deepvariant myself on other CLR data. Unfortunately, the training procedure for Deepvariant seems to be complicated at this time and requires significant compute resources. I would be very grateful if it would be possible to meet this request -- you can reach me by email at pedge AT eng.ucsd.edu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/174#issuecomment-487190949
https://github.com/google/deepvariant/issues/174#issuecomment-487190949:251,Testability,test,testing,251,"Hi, so I noticed in the Deepvariant manuscript supplementary text that the 44x NA12878 CLR data (sorted_final_merged.bam) from GIAB was used for benchmarking Deepvariant with PacBio CLR reads (specifically, that chroms 1-19 were used for training and testing was performed on chroms 20-22). Would it be possible for me to access the result VCF for chroms 20-22 (and ideally also the trained model used in the manuscript)? The reason I'm interested in this is that we have developed our own CLR variant calling method and I would like to fairly compare it with Deepvariant. If I have the VCF, then I can be sure that our precision/recall calculations on chr20-22 are done in exactly the same way, and if I have the trained model then I also have the option to run Deepvariant myself on other CLR data. Unfortunately, the training procedure for Deepvariant seems to be complicated at this time and requires significant compute resources. I would be very grateful if it would be possible to meet this request -- you can reach me by email at pedge AT eng.ucsd.edu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/174#issuecomment-487190949
https://github.com/google/deepvariant/issues/175#issuecomment-560473436:346,Energy Efficiency,schedul,scheduled,346,"@pichuan I just stumbled upon the same thing, and it took me quite a while to figure out what's going on there. I am running deepvariant v0.9.0 (docker container), and I found that there is quite a lot of files left behind for failed jobs under /tmp on the execution host. Can you comment on deepvariant's behavior when two (or more) DV jobs are scheduled to the same execution host in a cluster setup? Should that be avoided?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/175#issuecomment-560473436
https://github.com/google/deepvariant/issues/175#issuecomment-560473436:418,Safety,avoid,avoided,418,"@pichuan I just stumbled upon the same thing, and it took me quite a while to figure out what's going on there. I am running deepvariant v0.9.0 (docker container), and I found that there is quite a lot of files left behind for failed jobs under /tmp on the execution host. Can you comment on deepvariant's behavior when two (or more) DV jobs are scheduled to the same execution host in a cluster setup? Should that be avoided?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/175#issuecomment-560473436
https://github.com/google/deepvariant/issues/175#issuecomment-560533134:495,Availability,error,errors,495,"@ptrebert ; One clarifying question - Do you mean that you have multiple jobs that write to the same files at the same time, or multiple jobs that writes to different file names?. Previously, the issues was that if the user has some jobs they thought they killed (because they ctrl-c) but were still running in the background, this could cause the new jobs to write in the same files, and resulted in corrupted files. If they're with different names, I think it should be fine. If you're seeing errors with different names, I can give it a try and report back. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/175#issuecomment-560533134
https://github.com/google/deepvariant/issues/175#issuecomment-560625427:546,Security,expose,exposed,546,"I just checked the code, and you're right that the temp file names will be the same:; https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L264-L266. For now, please pass in different `intermediate_results_dir` for each run. For example:; `--intermediate_results_dir=""/tmp/deepvariant_tmp_output/chr1""` for chr1, and so on. I'll think about how we want to improve this in the future. I can think of a few options for future improvements, such as :. 1. Use a random name for the internal /tmp files. Given that these are not exposed to the users anyway.; 2. Use a unique name derived from the output VCF file, instead of calling all temp files the same name. For now, using the `--intermediate_results_dir` should hopefully resolve your issue. Let me know if it works. If you have a suggestion on what's the best future improvement for better user experience, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/175#issuecomment-560625427
https://github.com/google/deepvariant/issues/175#issuecomment-560625427:864,Usability,user experience,user experience,864,"I just checked the code, and you're right that the temp file names will be the same:; https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L264-L266. For now, please pass in different `intermediate_results_dir` for each run. For example:; `--intermediate_results_dir=""/tmp/deepvariant_tmp_output/chr1""` for chr1, and so on. I'll think about how we want to improve this in the future. I can think of a few options for future improvements, such as :. 1. Use a random name for the internal /tmp files. Given that these are not exposed to the users anyway.; 2. Use a unique name derived from the output VCF file, instead of calling all temp files the same name. For now, using the `--intermediate_results_dir` should hopefully resolve your issue. Let me know if it works. If you have a suggestion on what's the best future improvement for better user experience, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/175#issuecomment-560625427
https://github.com/google/deepvariant/issues/176#issuecomment-484947045:359,Deployability,release,released,359,"Hi @yingchen69 ; from your questions, it seems like you're asking whether you can change the height of the pileup image.; I believe this previous GitHub issue is very similar, and it has a bunch of in-depth discussion: https://github.com/google/deepvariant/issues/62. A direct answer to your question:; DeepVariant is currently a germline variant caller. Our released models are trained for that purpose. The models are trained on data that are 221x100x6 in dimension (221 bases wide, 100 reads deep, 6 channels). You can see an example for visualization [here](https://github.com/google/deepvariant/blob/master/docs/visualizing_examples.ipynb). From [my answer in the earlier issue](https://github.com/google/deepvariant/issues/62#issuecomment-379107194), it is possible to change the pileup image height. However, even if you do that, it won't just work out of the box for you. I'm closing this issue, but feel free to ask if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/176#issuecomment-484947045
https://github.com/google/deepvariant/issues/177#issuecomment-486033904:54,Deployability,install,install,54,"Peter;; Thanks for the report and apologies about the install issues. It looks like you're installing deepvariant in a python 3 environment, and it only works with python 2.7, which might be the source of the problems. Hopefully if you try installing with `'python=2.7'` as an additional requirement that will help avoid it. If you still hit issues, it might be a legit problem with the latest `google-cloud-sdk` and you could also add `'google-cloud-sdk<243.0.0'` as another requirement. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486033904
https://github.com/google/deepvariant/issues/177#issuecomment-486033904:91,Deployability,install,installing,91,"Peter;; Thanks for the report and apologies about the install issues. It looks like you're installing deepvariant in a python 3 environment, and it only works with python 2.7, which might be the source of the problems. Hopefully if you try installing with `'python=2.7'` as an additional requirement that will help avoid it. If you still hit issues, it might be a legit problem with the latest `google-cloud-sdk` and you could also add `'google-cloud-sdk<243.0.0'` as another requirement. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486033904
https://github.com/google/deepvariant/issues/177#issuecomment-486033904:240,Deployability,install,installing,240,"Peter;; Thanks for the report and apologies about the install issues. It looks like you're installing deepvariant in a python 3 environment, and it only works with python 2.7, which might be the source of the problems. Hopefully if you try installing with `'python=2.7'` as an additional requirement that will help avoid it. If you still hit issues, it might be a legit problem with the latest `google-cloud-sdk` and you could also add `'google-cloud-sdk<243.0.0'` as another requirement. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486033904
https://github.com/google/deepvariant/issues/177#issuecomment-486033904:315,Safety,avoid,avoid,315,"Peter;; Thanks for the report and apologies about the install issues. It looks like you're installing deepvariant in a python 3 environment, and it only works with python 2.7, which might be the source of the problems. Hopefully if you try installing with `'python=2.7'` as an additional requirement that will help avoid it. If you still hit issues, it might be a legit problem with the latest `google-cloud-sdk` and you could also add `'google-cloud-sdk<243.0.0'` as another requirement. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486033904
https://github.com/google/deepvariant/issues/177#issuecomment-486066532:21,Deployability,install,installing,21,Ah ok I figured that installing it fresh in an empty environment would by default force that environment to be python 2.7. let me try it with that extra requirement,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486066532
https://github.com/google/deepvariant/issues/177#issuecomment-486070285:15,Availability,error,errors,15,I get the same errors with ```conda create -n deepvariant python=2.7 deepvariant```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486070285
https://github.com/google/deepvariant/issues/177#issuecomment-486163437:386,Deployability,install,installed,386,"Peter;; Thanks for testing, it sounds like there is a problem with the recent google-cloud-sdk packages. I'll take a look to see if I can figure out what is going wrong but an immediate thing you could try is to restrict that dependency version to try and avoid the issue:; ```; conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; ```; Hope this helps get it installed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486163437
https://github.com/google/deepvariant/issues/177#issuecomment-486163437:226,Integrability,depend,dependency,226,"Peter;; Thanks for testing, it sounds like there is a problem with the recent google-cloud-sdk packages. I'll take a look to see if I can figure out what is going wrong but an immediate thing you could try is to restrict that dependency version to try and avoid the issue:; ```; conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; ```; Hope this helps get it installed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486163437
https://github.com/google/deepvariant/issues/177#issuecomment-486163437:256,Safety,avoid,avoid,256,"Peter;; Thanks for testing, it sounds like there is a problem with the recent google-cloud-sdk packages. I'll take a look to see if I can figure out what is going wrong but an immediate thing you could try is to restrict that dependency version to try and avoid the issue:; ```; conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; ```; Hope this helps get it installed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486163437
https://github.com/google/deepvariant/issues/177#issuecomment-486163437:19,Testability,test,testing,19,"Peter;; Thanks for testing, it sounds like there is a problem with the recent google-cloud-sdk packages. I'll take a look to see if I can figure out what is going wrong but an immediate thing you could try is to restrict that dependency version to try and avoid the issue:; ```; conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; ```; Hope this helps get it installed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486163437
https://github.com/google/deepvariant/issues/177#issuecomment-486821218:537,Deployability,install,installed,537,"constraining google-cloud-sdk version worked! thank you. On Wed, Apr 24, 2019 at 3:17 AM Brad Chapman <notifications@github.com>; wrote:. > Peter;; > Thanks for testing, it sounds like there is a problem with the recent; > google-cloud-sdk packages. I'll take a look to see if I can figure out what; > is going wrong but an immediate thing you could try is to restrict that; > dependency version to try and avoid the issue:; >; > conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; >; > Hope this helps get it installed.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/177#issuecomment-486163437>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABUBV2DKBMPCNW6H6H2OGKLPSAXVJANCNFSM4HH7EBWQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486821218
https://github.com/google/deepvariant/issues/177#issuecomment-486821218:377,Integrability,depend,dependency,377,"constraining google-cloud-sdk version worked! thank you. On Wed, Apr 24, 2019 at 3:17 AM Brad Chapman <notifications@github.com>; wrote:. > Peter;; > Thanks for testing, it sounds like there is a problem with the recent; > google-cloud-sdk packages. I'll take a look to see if I can figure out what; > is going wrong but an immediate thing you could try is to restrict that; > dependency version to try and avoid the issue:; >; > conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; >; > Hope this helps get it installed.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/177#issuecomment-486163437>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABUBV2DKBMPCNW6H6H2OGKLPSAXVJANCNFSM4HH7EBWQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486821218
https://github.com/google/deepvariant/issues/177#issuecomment-486821218:407,Safety,avoid,avoid,407,"constraining google-cloud-sdk version worked! thank you. On Wed, Apr 24, 2019 at 3:17 AM Brad Chapman <notifications@github.com>; wrote:. > Peter;; > Thanks for testing, it sounds like there is a problem with the recent; > google-cloud-sdk packages. I'll take a look to see if I can figure out what; > is going wrong but an immediate thing you could try is to restrict that; > dependency version to try and avoid the issue:; >; > conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; >; > Hope this helps get it installed.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/177#issuecomment-486163437>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABUBV2DKBMPCNW6H6H2OGKLPSAXVJANCNFSM4HH7EBWQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486821218
https://github.com/google/deepvariant/issues/177#issuecomment-486821218:161,Testability,test,testing,161,"constraining google-cloud-sdk version worked! thank you. On Wed, Apr 24, 2019 at 3:17 AM Brad Chapman <notifications@github.com>; wrote:. > Peter;; > Thanks for testing, it sounds like there is a problem with the recent; > google-cloud-sdk packages. I'll take a look to see if I can figure out what; > is going wrong but an immediate thing you could try is to restrict that; > dependency version to try and avoid the issue:; >; > conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; >; > Hope this helps get it installed.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/177#issuecomment-486163437>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABUBV2DKBMPCNW6H6H2OGKLPSAXVJANCNFSM4HH7EBWQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486821218
https://github.com/google/deepvariant/issues/177#issuecomment-486826886:67,Deployability,update,update,67,Thanks @chapmanb so much for you help. And Thanks @pjedge for your update. ; I'm closing this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-486826886
https://github.com/google/deepvariant/issues/177#issuecomment-487313448:225,Availability,down,download,225,"Peter;; Thanks for following up and glad to hear that this got it installed for you. I've been trying to replicate to fix the issue and avoid the manual pinning but can't seem to do on my system. Perhaps this was a temporary download issue with the google-cloud-sdk package, but it seems okay now. If anyone else stumbles across this same issue we can dig more but hopefully it was just something transient and we're good going forward. Thanks again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-487313448
https://github.com/google/deepvariant/issues/177#issuecomment-487313448:66,Deployability,install,installed,66,"Peter;; Thanks for following up and glad to hear that this got it installed for you. I've been trying to replicate to fix the issue and avoid the manual pinning but can't seem to do on my system. Perhaps this was a temporary download issue with the google-cloud-sdk package, but it seems okay now. If anyone else stumbles across this same issue we can dig more but hopefully it was just something transient and we're good going forward. Thanks again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-487313448
https://github.com/google/deepvariant/issues/177#issuecomment-487313448:136,Safety,avoid,avoid,136,"Peter;; Thanks for following up and glad to hear that this got it installed for you. I've been trying to replicate to fix the issue and avoid the manual pinning but can't seem to do on my system. Perhaps this was a temporary download issue with the google-cloud-sdk package, but it seems okay now. If anyone else stumbles across this same issue we can dig more but hopefully it was just something transient and we're good going forward. Thanks again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-487313448
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:223,Availability,down,download,223,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:305,Availability,error,error,305,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:407,Availability,ERROR,ERROR,407,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:465,Availability,error,error,465,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:889,Availability,ERROR,ERROR,889,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:929,Availability,error,error,929,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:1685,Availability,error,error,1685,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:1944,Availability,error,error,1944,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:16,Deployability,install,installing,16,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:52,Deployability,install,install,52,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:232,Deployability,install,install,232,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:240,Deployability,update,update,240,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:486,Deployability,install,installing,486,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:950,Deployability,install,installing,950,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:1344,Deployability,Rolling,Rolling,1344,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:1691,Deployability,install,installing,1691,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:1716,Deployability,install,install,1716,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:1760,Deployability,install,install,1760,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:833,Integrability,message,messages,833,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:1297,Integrability,message,messages,1297,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504749118:1642,Integrability,message,messages,1642,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118
https://github.com/google/deepvariant/issues/177#issuecomment-504940567:74,Availability,down,downloads,74,Sorry about the install issues. The `post-link.sh` script that is failing downloads the trained models that DeepVariant uses from Google Buckets:. https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/post-link.sh. Failures here are typically due to problems with gsutil picking up other configuration options on your system. This thread has a bunch of debugging and some suggestions to try to work around the gsutil problem:. https://github.com/bcbio/bcbio-nextgen/issues/2613. If you can try the tips there and let us know more details of the failures if you get stuck happy to help with specific suggestions. Hope this gets deepvariant installed for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504940567
https://github.com/google/deepvariant/issues/177#issuecomment-504940567:238,Availability,Failure,Failures,238,Sorry about the install issues. The `post-link.sh` script that is failing downloads the trained models that DeepVariant uses from Google Buckets:. https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/post-link.sh. Failures here are typically due to problems with gsutil picking up other configuration options on your system. This thread has a bunch of debugging and some suggestions to try to work around the gsutil problem:. https://github.com/bcbio/bcbio-nextgen/issues/2613. If you can try the tips there and let us know more details of the failures if you get stuck happy to help with specific suggestions. Hope this gets deepvariant installed for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504940567
https://github.com/google/deepvariant/issues/177#issuecomment-504940567:568,Availability,failure,failures,568,Sorry about the install issues. The `post-link.sh` script that is failing downloads the trained models that DeepVariant uses from Google Buckets:. https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/post-link.sh. Failures here are typically due to problems with gsutil picking up other configuration options on your system. This thread has a bunch of debugging and some suggestions to try to work around the gsutil problem:. https://github.com/bcbio/bcbio-nextgen/issues/2613. If you can try the tips there and let us know more details of the failures if you get stuck happy to help with specific suggestions. Hope this gets deepvariant installed for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504940567
https://github.com/google/deepvariant/issues/177#issuecomment-504940567:16,Deployability,install,install,16,Sorry about the install issues. The `post-link.sh` script that is failing downloads the trained models that DeepVariant uses from Google Buckets:. https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/post-link.sh. Failures here are typically due to problems with gsutil picking up other configuration options on your system. This thread has a bunch of debugging and some suggestions to try to work around the gsutil problem:. https://github.com/bcbio/bcbio-nextgen/issues/2613. If you can try the tips there and let us know more details of the failures if you get stuck happy to help with specific suggestions. Hope this gets deepvariant installed for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504940567
https://github.com/google/deepvariant/issues/177#issuecomment-504940567:311,Deployability,configurat,configuration,311,Sorry about the install issues. The `post-link.sh` script that is failing downloads the trained models that DeepVariant uses from Google Buckets:. https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/post-link.sh. Failures here are typically due to problems with gsutil picking up other configuration options on your system. This thread has a bunch of debugging and some suggestions to try to work around the gsutil problem:. https://github.com/bcbio/bcbio-nextgen/issues/2613. If you can try the tips there and let us know more details of the failures if you get stuck happy to help with specific suggestions. Hope this gets deepvariant installed for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504940567
https://github.com/google/deepvariant/issues/177#issuecomment-504940567:662,Deployability,install,installed,662,Sorry about the install issues. The `post-link.sh` script that is failing downloads the trained models that DeepVariant uses from Google Buckets:. https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/post-link.sh. Failures here are typically due to problems with gsutil picking up other configuration options on your system. This thread has a bunch of debugging and some suggestions to try to work around the gsutil problem:. https://github.com/bcbio/bcbio-nextgen/issues/2613. If you can try the tips there and let us know more details of the failures if you get stuck happy to help with specific suggestions. Hope this gets deepvariant installed for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504940567
https://github.com/google/deepvariant/issues/177#issuecomment-504940567:311,Modifiability,config,configuration,311,Sorry about the install issues. The `post-link.sh` script that is failing downloads the trained models that DeepVariant uses from Google Buckets:. https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/post-link.sh. Failures here are typically due to problems with gsutil picking up other configuration options on your system. This thread has a bunch of debugging and some suggestions to try to work around the gsutil problem:. https://github.com/bcbio/bcbio-nextgen/issues/2613. If you can try the tips there and let us know more details of the failures if you get stuck happy to help with specific suggestions. Hope this gets deepvariant installed for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-504940567
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:1518,Availability,Avail,Available,1518,"d conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:2267,Availability,Avail,Available,2267,"package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:3726,Availability,avail,available,3726,"tput:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Your python: python=2.7. If python is on the left-most side of the chain, that's the version you've asked for.; When python appears to the right, that indicates that the thing on the left is somehow; not available for the python version you are constrained to. Note that conda will not; change your python version to a different minor version unless you explicitly specify; that.; ```. Do you have any idea what is causing the problem and how to proceed, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:15,Deployability,Install,Installation,15,"**_DeepVariant Installation problem using Anaconda version 4.8.2_** ; https://anaconda.org/bioconda/deepvariant. **Tried to installed with default python version which is Python 3.7.6**. ```conda create --name deepvariant; conda activate deepvariant; conda install -c bioconda deepvariant; ```; Output:. ```; Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: - ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Colle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:124,Deployability,install,installed,124,"**_DeepVariant Installation problem using Anaconda version 4.8.2_** ; https://anaconda.org/bioconda/deepvariant. **Tried to installed with default python version which is Python 3.7.6**. ```conda create --name deepvariant; conda activate deepvariant; conda install -c bioconda deepvariant; ```; Output:. ```; Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: - ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Colle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:257,Deployability,install,install,257,"**_DeepVariant Installation problem using Anaconda version 4.8.2_** ; https://anaconda.org/bioconda/deepvariant. **Tried to installed with default python version which is Python 3.7.6**. ```conda create --name deepvariant; conda activate deepvariant; conda install -c bioconda deepvariant; ```; Output:. ```; Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: - ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Colle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:655,Deployability,install,install,655,"**_DeepVariant Installation problem using Anaconda version 4.8.2_** ; https://anaconda.org/bioconda/deepvariant. **Tried to installed with default python version which is Python 3.7.6**. ```conda create --name deepvariant; conda activate deepvariant; conda install -c bioconda deepvariant; ```; Output:. ```; Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: - ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Colle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:1103,Deployability,install,install,1103,"nda/deepvariant. **Tried to installed with default python version which is Python 3.7.6**. ```conda create --name deepvariant; conda activate deepvariant; conda install -c bioconda deepvariant; ```; Output:. ```; Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: - ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:2812,Deployability,install,install,2812,"oogle/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Your python: python=2.7. If python is on the left-most side of the chain, that's the version you've asked for.; When python appears to the right, that indicates that the thing on the left is somehow; not available for the python version you are constrained to. Note that conda will not; change your py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:2861,Deployability,install,installing,2861,"ain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Your python: python=2.7. If python is on the left-most side of the chain, that's the version you've asked for.; When python appears to the right, that indicates that the thing on the left is somehow; not available for the python version you are constrained to. Note that conda will not; change your python version to a different minor version unle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:2966,Deployability,install,install,2966,"-sdk<243.0.0'`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Your python: python=2.7. If python is on the left-most side of the chain, that's the version you've asked for.; When python appears to the right, that indicates that the thing on the left is somehow; not available for the python version you are constrained to. Note that conda will not; change your python version to a different minor version unless you explicitly specify; that.; ```. Do you have any idea what is causing the problem and how t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:3344,Deployability,install,installation,3344,"tput:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Your python: python=2.7. If python is on the left-most side of the chain, that's the version you've asked for.; When python appears to the right, that indicates that the thing on the left is somehow; not available for the python version you are constrained to. Note that conda will not; change your python version to a different minor version unless you explicitly specify; that.; ```. Do you have any idea what is causing the problem and how to proceed, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:429,Modifiability,flexible,flexible,429,"**_DeepVariant Installation problem using Anaconda version 4.8.2_** ; https://anaconda.org/bioconda/deepvariant. **Tried to installed with default python version which is Python 3.7.6**. ```conda create --name deepvariant; conda activate deepvariant; conda install -c bioconda deepvariant; ```; Output:. ```; Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: - ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Colle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:838,Modifiability,flexible,flexible,838,"**_DeepVariant Installation problem using Anaconda version 4.8.2_** ; https://anaconda.org/bioconda/deepvariant. **Tried to installed with default python version which is Python 3.7.6**. ```conda create --name deepvariant; conda activate deepvariant; conda install -c bioconda deepvariant; ```; Output:. ```; Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: - ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Colle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:3138,Modifiability,flexible,flexible,3138,"tput:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Your python: python=2.7. If python is on the left-most side of the chain, that's the version you've asked for.; When python appears to the right, that indicates that the thing on the left is somehow; not available for the python version you are constrained to. Note that conda will not; change your python version to a different minor version unless you explicitly specify; that.; ```. Do you have any idea what is causing the problem and how to proceed, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:571,Safety,abort,abort,571,"**_DeepVariant Installation problem using Anaconda version 4.8.2_** ; https://anaconda.org/bioconda/deepvariant. **Tried to installed with default python version which is Python 3.7.6**. ```conda create --name deepvariant; conda activate deepvariant; conda install -c bioconda deepvariant; ```; Output:. ```; Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: - ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Colle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584370219:980,Safety,abort,abort,980,"**_DeepVariant Installation problem using Anaconda version 4.8.2_** ; https://anaconda.org/bioconda/deepvariant. **Tried to installed with default python version which is Python 3.7.6**. ```conda create --name deepvariant; conda activate deepvariant; conda install -c bioconda deepvariant; ```; Output:. ```; Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: - ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Colle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219
https://github.com/google/deepvariant/issues/177#issuecomment-584441591:375,Availability,avail,available,375,Anna;; Apologies about the issues. If you also include the conda-forge channel in your install it should resolve cleanly:; ```; conda create -n deepvariant -c conda-forge -c bioconda python=2.7 deepvariant; ```; bioconda is heavily dependent on conda-forge packages so you'll want to include that whenever installing anything from bioconda to ensure all the dependencies are available. Hope this helps get it running for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584441591
https://github.com/google/deepvariant/issues/177#issuecomment-584441591:87,Deployability,install,install,87,Anna;; Apologies about the issues. If you also include the conda-forge channel in your install it should resolve cleanly:; ```; conda create -n deepvariant -c conda-forge -c bioconda python=2.7 deepvariant; ```; bioconda is heavily dependent on conda-forge packages so you'll want to include that whenever installing anything from bioconda to ensure all the dependencies are available. Hope this helps get it running for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584441591
https://github.com/google/deepvariant/issues/177#issuecomment-584441591:306,Deployability,install,installing,306,Anna;; Apologies about the issues. If you also include the conda-forge channel in your install it should resolve cleanly:; ```; conda create -n deepvariant -c conda-forge -c bioconda python=2.7 deepvariant; ```; bioconda is heavily dependent on conda-forge packages so you'll want to include that whenever installing anything from bioconda to ensure all the dependencies are available. Hope this helps get it running for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584441591
https://github.com/google/deepvariant/issues/177#issuecomment-584441591:232,Integrability,depend,dependent,232,Anna;; Apologies about the issues. If you also include the conda-forge channel in your install it should resolve cleanly:; ```; conda create -n deepvariant -c conda-forge -c bioconda python=2.7 deepvariant; ```; bioconda is heavily dependent on conda-forge packages so you'll want to include that whenever installing anything from bioconda to ensure all the dependencies are available. Hope this helps get it running for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584441591
https://github.com/google/deepvariant/issues/177#issuecomment-584441591:358,Integrability,depend,dependencies,358,Anna;; Apologies about the issues. If you also include the conda-forge channel in your install it should resolve cleanly:; ```; conda create -n deepvariant -c conda-forge -c bioconda python=2.7 deepvariant; ```; bioconda is heavily dependent on conda-forge packages so you'll want to include that whenever installing anything from bioconda to ensure all the dependencies are available. Hope this helps get it running for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/177#issuecomment-584441591
https://github.com/google/deepvariant/issues/178#issuecomment-487218238:1637,Availability,error,error,1637,"h:; ```; gcloud beta compute instances create ""${USER}-centos-singularity"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image=centos-7-drawfork-v20181102 \; --image-project=eip-images \; --machine-type ""n1-standard-32"" \; --zone ""us-west1-b""; ```. 2. On the machine, I installed Singularity 2.5.2 with instructions on: https://github.com/sylabs/singularity/blob/2.5.2/INSTALL.md. 3. I copied a Singularity image that I built with [the instructions I posted before](https://github.com/google/deepvariant/issues/132#issuecomment-482430728) on a Ubutun 16.04 machine to this CentOS 7 machine. Then I got the [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) data and run the command:; ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. The run completed without an issue. My CentOS machine has:; ```; $ lsb_release ; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; ```; ```; $ cat /etc/centos-release; CentOS Linux release 7.6.1810 (Core) ; ```. ---. From the original error message:; `ImportError: No module named _multiarray_umath`; It seems like an issue with numpy installation. But given we're using a singularity image, I am having a hard time thinking why this would be the case. (Unless it's not created correctly?). @drtamermansour ; Two questions for you:; (a) When you create the image, can you try to run it on the machine where you create it to make sure it worked there? ; (b) Do you think it'll help if either I or @williamrowell share our *simg file with you to try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-487218238
https://github.com/google/deepvariant/issues/178#issuecomment-487218238:439,Deployability,install,installed,439,"Hi @drtamermansour,; In my first attempt to reproduce this, I wasn't able to reproduce the issue with the following setting:. 1. I got a CentOS 7 machine with:; ```; gcloud beta compute instances create ""${USER}-centos-singularity"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image=centos-7-drawfork-v20181102 \; --image-project=eip-images \; --machine-type ""n1-standard-32"" \; --zone ""us-west1-b""; ```. 2. On the machine, I installed Singularity 2.5.2 with instructions on: https://github.com/sylabs/singularity/blob/2.5.2/INSTALL.md. 3. I copied a Singularity image that I built with [the instructions I posted before](https://github.com/google/deepvariant/issues/132#issuecomment-482430728) on a Ubutun 16.04 machine to this CentOS 7 machine. Then I got the [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) data and run the command:; ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. The run completed without an issue. My CentOS machine has:; ```; $ lsb_release ; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; ```; ```; $ cat /etc/centos-release; CentOS Linux release 7.6.1810 (Core) ; ```. ---. From the original error message:; `ImportError: No module named _multiarray_umath`; It seems like an issue with numpy installation. But given we're using a singularity image, I am having a hard time thinking why this would be the case. (Unless it's not created correctly?). @drtamermansour ; Two questions for you:; (a) When you create the image, can you try to run it on the machin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-487218238
https://github.com/google/deepvariant/issues/178#issuecomment-487218238:538,Deployability,INSTALL,INSTALL,538,"Hi @drtamermansour,; In my first attempt to reproduce this, I wasn't able to reproduce the issue with the following setting:. 1. I got a CentOS 7 machine with:; ```; gcloud beta compute instances create ""${USER}-centos-singularity"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image=centos-7-drawfork-v20181102 \; --image-project=eip-images \; --machine-type ""n1-standard-32"" \; --zone ""us-west1-b""; ```. 2. On the machine, I installed Singularity 2.5.2 with instructions on: https://github.com/sylabs/singularity/blob/2.5.2/INSTALL.md. 3. I copied a Singularity image that I built with [the instructions I posted before](https://github.com/google/deepvariant/issues/132#issuecomment-482430728) on a Ubutun 16.04 machine to this CentOS 7 machine. Then I got the [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) data and run the command:; ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. The run completed without an issue. My CentOS machine has:; ```; $ lsb_release ; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; ```; ```; $ cat /etc/centos-release; CentOS Linux release 7.6.1810 (Core) ; ```. ---. From the original error message:; `ImportError: No module named _multiarray_umath`; It seems like an issue with numpy installation. But given we're using a singularity image, I am having a hard time thinking why this would be the case. (Unless it's not created correctly?). @drtamermansour ; Two questions for you:; (a) When you create the image, can you try to run it on the machin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-487218238
https://github.com/google/deepvariant/issues/178#issuecomment-487218238:1561,Deployability,release,release,1561,"h:; ```; gcloud beta compute instances create ""${USER}-centos-singularity"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image=centos-7-drawfork-v20181102 \; --image-project=eip-images \; --machine-type ""n1-standard-32"" \; --zone ""us-west1-b""; ```. 2. On the machine, I installed Singularity 2.5.2 with instructions on: https://github.com/sylabs/singularity/blob/2.5.2/INSTALL.md. 3. I copied a Singularity image that I built with [the instructions I posted before](https://github.com/google/deepvariant/issues/132#issuecomment-482430728) on a Ubutun 16.04 machine to this CentOS 7 machine. Then I got the [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) data and run the command:; ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. The run completed without an issue. My CentOS machine has:; ```; $ lsb_release ; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; ```; ```; $ cat /etc/centos-release; CentOS Linux release 7.6.1810 (Core) ; ```. ---. From the original error message:; `ImportError: No module named _multiarray_umath`; It seems like an issue with numpy installation. But given we're using a singularity image, I am having a hard time thinking why this would be the case. (Unless it's not created correctly?). @drtamermansour ; Two questions for you:; (a) When you create the image, can you try to run it on the machine where you create it to make sure it worked there? ; (b) Do you think it'll help if either I or @williamrowell share our *simg file with you to try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-487218238
https://github.com/google/deepvariant/issues/178#issuecomment-487218238:1583,Deployability,release,release,1583,"h:; ```; gcloud beta compute instances create ""${USER}-centos-singularity"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image=centos-7-drawfork-v20181102 \; --image-project=eip-images \; --machine-type ""n1-standard-32"" \; --zone ""us-west1-b""; ```. 2. On the machine, I installed Singularity 2.5.2 with instructions on: https://github.com/sylabs/singularity/blob/2.5.2/INSTALL.md. 3. I copied a Singularity image that I built with [the instructions I posted before](https://github.com/google/deepvariant/issues/132#issuecomment-482430728) on a Ubutun 16.04 machine to this CentOS 7 machine. Then I got the [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) data and run the command:; ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. The run completed without an issue. My CentOS machine has:; ```; $ lsb_release ; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; ```; ```; $ cat /etc/centos-release; CentOS Linux release 7.6.1810 (Core) ; ```. ---. From the original error message:; `ImportError: No module named _multiarray_umath`; It seems like an issue with numpy installation. But given we're using a singularity image, I am having a hard time thinking why this would be the case. (Unless it's not created correctly?). @drtamermansour ; Two questions for you:; (a) When you create the image, can you try to run it on the machine where you create it to make sure it worked there? ; (b) Do you think it'll help if either I or @williamrowell share our *simg file with you to try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-487218238
https://github.com/google/deepvariant/issues/178#issuecomment-487218238:1737,Deployability,install,installation,1737,"h:; ```; gcloud beta compute instances create ""${USER}-centos-singularity"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image=centos-7-drawfork-v20181102 \; --image-project=eip-images \; --machine-type ""n1-standard-32"" \; --zone ""us-west1-b""; ```. 2. On the machine, I installed Singularity 2.5.2 with instructions on: https://github.com/sylabs/singularity/blob/2.5.2/INSTALL.md. 3. I copied a Singularity image that I built with [the instructions I posted before](https://github.com/google/deepvariant/issues/132#issuecomment-482430728) on a Ubutun 16.04 machine to this CentOS 7 machine. Then I got the [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) data and run the command:; ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. The run completed without an issue. My CentOS machine has:; ```; $ lsb_release ; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; ```; ```; $ cat /etc/centos-release; CentOS Linux release 7.6.1810 (Core) ; ```. ---. From the original error message:; `ImportError: No module named _multiarray_umath`; It seems like an issue with numpy installation. But given we're using a singularity image, I am having a hard time thinking why this would be the case. (Unless it's not created correctly?). @drtamermansour ; Two questions for you:; (a) When you create the image, can you try to run it on the machine where you create it to make sure it worked there? ; (b) Do you think it'll help if either I or @williamrowell share our *simg file with you to try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-487218238
https://github.com/google/deepvariant/issues/178#issuecomment-487218238:1643,Integrability,message,message,1643,"h:; ```; gcloud beta compute instances create ""${USER}-centos-singularity"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image=centos-7-drawfork-v20181102 \; --image-project=eip-images \; --machine-type ""n1-standard-32"" \; --zone ""us-west1-b""; ```. 2. On the machine, I installed Singularity 2.5.2 with instructions on: https://github.com/sylabs/singularity/blob/2.5.2/INSTALL.md. 3. I copied a Singularity image that I built with [the instructions I posted before](https://github.com/google/deepvariant/issues/132#issuecomment-482430728) on a Ubutun 16.04 machine to this CentOS 7 machine. Then I got the [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) data and run the command:; ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. The run completed without an issue. My CentOS machine has:; ```; $ lsb_release ; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; ```; ```; $ cat /etc/centos-release; CentOS Linux release 7.6.1810 (Core) ; ```. ---. From the original error message:; `ImportError: No module named _multiarray_umath`; It seems like an issue with numpy installation. But given we're using a singularity image, I am having a hard time thinking why this would be the case. (Unless it's not created correctly?). @drtamermansour ; Two questions for you:; (a) When you create the image, can you try to run it on the machine where you create it to make sure it worked there? ; (b) Do you think it'll help if either I or @williamrowell share our *simg file with you to try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-487218238
https://github.com/google/deepvariant/issues/178#issuecomment-487657800:78,Deployability,release,release,78,"Hi @drtamermansour ; I have not tried building Singularity before this latest release, so I can't guarantee how easy it would be. Regarding your issue with `--regions`, I'm pretty sure it's because the intervaltree version issue. Please see this comment for a solution if you have to build from DeepVariant before 0.8.0:; https://github.com/google/deepvariant/issues/131#issuecomment-449034956. We highly recommend building from the latest one (0.8.0) if possible, though. . In terms of sharing file, we'll see if @williamrowell can share his file. I can also see if I can find a reasonable place to share mine (which is also built on a CentOS machine on GCP)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-487657800
https://github.com/google/deepvariant/issues/178#issuecomment-488824724:202,Testability,test,testing,202,"Sorry, I've been traveling the past week without a solid network connection. I've uploaded my simg to google drive at the address below. This will likely not be a permanent link, but you can use it for testing now. https://drive.google.com/open?id=12NZKJwRTroKofGB6KrZ4JVyN36DRbPuF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-488824724
https://github.com/google/deepvariant/issues/178#issuecomment-494913584:0,Deployability,Update,Update,0,"Update: I've emailed @drtamermansour but haven't back from him. I'll close this issue for now. If anyone had a chance to try @williamrowell 's image above, let us know how it wokr.s; If anyone else is encountering the same issue, please feel free to comment on this issue again. I will close it now since there's no information right now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-494913584
https://github.com/google/deepvariant/issues/178#issuecomment-499709284:63,Availability,error,error,63,"Sorry for the late replay. Unfortunately, it did not work. The error message says:; ERROR : Unknown image format/type: deepvariant.0.8.0.simg; ABORT : Retval = 255",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-499709284
https://github.com/google/deepvariant/issues/178#issuecomment-499709284:84,Availability,ERROR,ERROR,84,"Sorry for the late replay. Unfortunately, it did not work. The error message says:; ERROR : Unknown image format/type: deepvariant.0.8.0.simg; ABORT : Retval = 255",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-499709284
https://github.com/google/deepvariant/issues/178#issuecomment-499709284:69,Integrability,message,message,69,"Sorry for the late replay. Unfortunately, it did not work. The error message says:; ERROR : Unknown image format/type: deepvariant.0.8.0.simg; ABORT : Retval = 255",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-499709284
https://github.com/google/deepvariant/issues/178#issuecomment-499709284:143,Safety,ABORT,ABORT,143,"Sorry for the late replay. Unfortunately, it did not work. The error message says:; ERROR : Unknown image format/type: deepvariant.0.8.0.simg; ABORT : Retval = 255",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-499709284
https://github.com/google/deepvariant/issues/178#issuecomment-503758185:141,Testability,test,testing,141,"Hmm interesting. ; @williamrowell FYI - I tried out your image you shared, and I'm having the same issue as well. @drtamermansour Thanks for testing it out. Let me rebuild with the steps I shared in https://github.com/google/deepvariant/issues/132#issuecomment-482430728 and see if you can try mine as well. I'll follow up soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-503758185
https://github.com/google/deepvariant/issues/178#issuecomment-503798268:329,Testability,test,tested,329,@drtamermansour ; Here is a simg file I just built:; https://drive.google.com/file/d/1P768MjXLa4xPmqj12UZqnmDqUZsUpbNh/view?usp=sharing; (which should be a file with the name `deepvariant.0.8.0.issue-178.simg`). I built this with the instruction here: https://github.com/google/deepvariant/issues/132#issuecomment-482430728; and tested with the steps here: https://github.com/google/deepvariant/issues/178#issuecomment-487218238. Please let me know if this works for you or not.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/178#issuecomment-503798268
https://github.com/google/deepvariant/issues/179#issuecomment-488287371:58,Deployability,update,updated,58,"Turns out I was on some nodes of the HPC that hadn't been updated, issue solved itself when on current nodes. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/179#issuecomment-488287371
https://github.com/google/deepvariant/issues/180#issuecomment-488147736:317,Integrability,contract,contraction,317,"Hi @njbernstein . Performance on STR will be a function of the size of the event. In Illumina data, DeepVariant will likely stop calling events as they start to reach 100bp in size and larger. DeepVariant will call STR events below this size (for example, here is a HET call repeat expansion in one allele and repeat contraction in the other from a DeepVariant HG002 WGS VCF):. 10	50527727	.	CTATATATATATATATATATATATATATATATATATATATA	C,CTATATATATATATATATATATATATATATATATATATATATA	37.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:17:56:2,22,29:0.392857,0.517857:37,20,52,20,0,40. I don't have stratified accuracy metrics for STR performance, nor do I have comparisons of this to dedicated STR tools. I would imagine that dedicated STR callers perform better for the long (100bp+) events, due to specific approaches for that class of problem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736
https://github.com/google/deepvariant/issues/180#issuecomment-488147736:18,Performance,Perform,Performance,18,"Hi @njbernstein . Performance on STR will be a function of the size of the event. In Illumina data, DeepVariant will likely stop calling events as they start to reach 100bp in size and larger. DeepVariant will call STR events below this size (for example, here is a HET call repeat expansion in one allele and repeat contraction in the other from a DeepVariant HG002 WGS VCF):. 10	50527727	.	CTATATATATATATATATATATATATATATATATATATATA	C,CTATATATATATATATATATATATATATATATATATATATATA	37.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:17:56:2,22,29:0.392857,0.517857:37,20,52,20,0,40. I don't have stratified accuracy metrics for STR performance, nor do I have comparisons of this to dedicated STR tools. I would imagine that dedicated STR callers perform better for the long (100bp+) events, due to specific approaches for that class of problem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736
https://github.com/google/deepvariant/issues/180#issuecomment-488147736:614,Performance,perform,performance,614,"Hi @njbernstein . Performance on STR will be a function of the size of the event. In Illumina data, DeepVariant will likely stop calling events as they start to reach 100bp in size and larger. DeepVariant will call STR events below this size (for example, here is a HET call repeat expansion in one allele and repeat contraction in the other from a DeepVariant HG002 WGS VCF):. 10	50527727	.	CTATATATATATATATATATATATATATATATATATATATA	C,CTATATATATATATATATATATATATATATATATATATATATA	37.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:17:56:2,22,29:0.392857,0.517857:37,20,52,20,0,40. I don't have stratified accuracy metrics for STR performance, nor do I have comparisons of this to dedicated STR tools. I would imagine that dedicated STR callers perform better for the long (100bp+) events, due to specific approaches for that class of problem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736
https://github.com/google/deepvariant/issues/180#issuecomment-488147736:728,Performance,perform,perform,728,"Hi @njbernstein . Performance on STR will be a function of the size of the event. In Illumina data, DeepVariant will likely stop calling events as they start to reach 100bp in size and larger. DeepVariant will call STR events below this size (for example, here is a HET call repeat expansion in one allele and repeat contraction in the other from a DeepVariant HG002 WGS VCF):. 10	50527727	.	CTATATATATATATATATATATATATATATATATATATATA	C,CTATATATATATATATATATATATATATATATATATATATATA	37.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:17:56:2,22,29:0.392857,0.517857:37,20,52,20,0,40. I don't have stratified accuracy metrics for STR performance, nor do I have comparisons of this to dedicated STR tools. I would imagine that dedicated STR callers perform better for the long (100bp+) events, due to specific approaches for that class of problem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736
https://github.com/google/deepvariant/issues/180#issuecomment-488147736:893,Performance,perform,perform,893,"Hi @njbernstein . Performance on STR will be a function of the size of the event. In Illumina data, DeepVariant will likely stop calling events as they start to reach 100bp in size and larger. DeepVariant will call STR events below this size (for example, here is a HET call repeat expansion in one allele and repeat contraction in the other from a DeepVariant HG002 WGS VCF):. 10	50527727	.	CTATATATATATATATATATATATATATATATATATATATA	C,CTATATATATATATATATATATATATATATATATATATATATA	37.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:17:56:2,22,29:0.392857,0.517857:37,20,52,20,0,40. I don't have stratified accuracy metrics for STR performance, nor do I have comparisons of this to dedicated STR tools. I would imagine that dedicated STR callers perform better for the long (100bp+) events, due to specific approaches for that class of problem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736
https://github.com/google/deepvariant/issues/180#issuecomment-488147736:2087,Testability,log,logic,2087,"lem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The specific implementation differs from GATK (and from the linked description, the GATK logic sounds more complex). Benchmarks reassembling all regions with DeepVariant consistently show the same accuracy to the region selection version. For the second step, conceptually, the methods are very similar. Both construct a de Bruijn graph of reference and alternate contigs. The same authors of these GATK methods are authors of DeepVariant, so apart from writing in C++ for speed, I expect these two to be conceptually similar. For the third step the methods are entirely different. This is where DeepVariant applies a trained convolutional neural network, looking directly at the raw information across the reads, whereas GATK applies a PairHMM to calulate the likelihood of candidate full haplotypes based on their support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736
https://github.com/google/deepvariant/issues/180#issuecomment-488147736:2115,Testability,Benchmark,Benchmarks,2115,"lem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The specific implementation differs from GATK (and from the linked description, the GATK logic sounds more complex). Benchmarks reassembling all regions with DeepVariant consistently show the same accuracy to the region selection version. For the second step, conceptually, the methods are very similar. Both construct a de Bruijn graph of reference and alternate contigs. The same authors of these GATK methods are authors of DeepVariant, so apart from writing in C++ for speed, I expect these two to be conceptually similar. For the third step the methods are entirely different. This is where DeepVariant applies a trained convolutional neural network, looking directly at the raw information across the reads, whereas GATK applies a PairHMM to calulate the likelihood of candidate full haplotypes based on their support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736
https://github.com/google/deepvariant/issues/180#issuecomment-488147736:857,Usability,intuit,intuition,857,"Hi @njbernstein . Performance on STR will be a function of the size of the event. In Illumina data, DeepVariant will likely stop calling events as they start to reach 100bp in size and larger. DeepVariant will call STR events below this size (for example, here is a HET call repeat expansion in one allele and repeat contraction in the other from a DeepVariant HG002 WGS VCF):. 10	50527727	.	CTATATATATATATATATATATATATATATATATATATATA	C,CTATATATATATATATATATATATATATATATATATATATATA	37.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:17:56:2,22,29:0.392857,0.517857:37,20,52,20,0,40. I don't have stratified accuracy metrics for STR performance, nor do I have comparisons of this to dedicated STR tools. I would imagine that dedicated STR callers perform better for the long (100bp+) events, due to specific approaches for that class of problem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736
https://github.com/google/deepvariant/issues/180#issuecomment-488147736:1925,Usability,simpl,simple,1925,"lem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The specific implementation differs from GATK (and from the linked description, the GATK logic sounds more complex). Benchmarks reassembling all regions with DeepVariant consistently show the same accuracy to the region selection version. For the second step, conceptually, the methods are very similar. Both construct a de Bruijn graph of reference and alternate contigs. The same authors of these GATK methods are authors of DeepVariant, so apart from writing in C++ for speed, I expect these two to be conceptually similar. For the third step the methods are entirely different. This is where DeepVariant applies a trained convolutional neural network, looking directly at the raw information across the reads, whereas GATK applies a PairHMM to calulate the likelihood of candidate full haplotypes based on their support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736
https://github.com/google/deepvariant/issues/180#issuecomment-488154062:364,Integrability,depend,depend,364,"@AndrewCarroll Thanks for the thorough response. This was extremely helpful. Also re the haplotype caller comparison: I thought DeepVariant also uses a PairHMM to score haplotypes to re-align reads to, and then feeds that to the CNN. . From the paper:; > The likelihood function used to score haplotypes is a traditional pair HMM with fixed parameters that do not depend on base quality scores. This likelihood function assumes that each read is independent. Finally, each read is then realigned to its most likely haplotype using a Smith–Waterman-like algorithm with an additional affine gap penalty score for homopolymer indels. So both methods use PairHMM to score haplotypes, and assist in re-aligning reads, ya? After that, the similarity is nil between the methods as you mentioned. . Sorry if that was implicit in your response, but wanted to double check I understand.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488154062
https://github.com/google/deepvariant/issues/180#issuecomment-488762439:26,Deployability,release,released,26,"The code for the current, released version of DeepVariant does not use PairHMM to score haplotypes. Since the submission of the DeepVariant manuscript, there have been 4 releases which have improved various aspects of the code, training regime, and training data for models. The DeepVariant paper does validly describe the methods used in a working version, both the original PrecisionFDA submission and the improvements made for the first open source release (v0.4). However, there are further improvements which are not captured in that publication, and which are instead represented either in other joint publications (e.g. https://www.biorxiv.org/content/10.1101/519025v2) or in blogs produced by our team or close partners (e.g. https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html), (e.g. https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488762439
https://github.com/google/deepvariant/issues/180#issuecomment-488762439:170,Deployability,release,releases,170,"The code for the current, released version of DeepVariant does not use PairHMM to score haplotypes. Since the submission of the DeepVariant manuscript, there have been 4 releases which have improved various aspects of the code, training regime, and training data for models. The DeepVariant paper does validly describe the methods used in a working version, both the original PrecisionFDA submission and the improvements made for the first open source release (v0.4). However, there are further improvements which are not captured in that publication, and which are instead represented either in other joint publications (e.g. https://www.biorxiv.org/content/10.1101/519025v2) or in blogs produced by our team or close partners (e.g. https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html), (e.g. https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488762439
https://github.com/google/deepvariant/issues/180#issuecomment-488762439:452,Deployability,release,release,452,"The code for the current, released version of DeepVariant does not use PairHMM to score haplotypes. Since the submission of the DeepVariant manuscript, there have been 4 releases which have improved various aspects of the code, training regime, and training data for models. The DeepVariant paper does validly describe the methods used in a working version, both the original PrecisionFDA submission and the improvements made for the first open source release (v0.4). However, there are further improvements which are not captured in that publication, and which are instead represented either in other joint publications (e.g. https://www.biorxiv.org/content/10.1101/519025v2) or in blogs produced by our team or close partners (e.g. https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html), (e.g. https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488762439
https://github.com/google/deepvariant/issues/180#issuecomment-488762439:853,Energy Efficiency,power,power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-,853,"The code for the current, released version of DeepVariant does not use PairHMM to score haplotypes. Since the submission of the DeepVariant manuscript, there have been 4 releases which have improved various aspects of the code, training regime, and training data for models. The DeepVariant paper does validly describe the methods used in a working version, both the original PrecisionFDA submission and the improvements made for the first open source release (v0.4). However, there are further improvements which are not captured in that publication, and which are instead represented either in other joint publications (e.g. https://www.biorxiv.org/content/10.1101/519025v2) or in blogs produced by our team or close partners (e.g. https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html), (e.g. https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488762439
https://github.com/google/deepvariant/issues/180#issuecomment-488762439:931,Performance,optimiz,optimizations-,931,"The code for the current, released version of DeepVariant does not use PairHMM to score haplotypes. Since the submission of the DeepVariant manuscript, there have been 4 releases which have improved various aspects of the code, training regime, and training data for models. The DeepVariant paper does validly describe the methods used in a working version, both the original PrecisionFDA submission and the improvements made for the first open source release (v0.4). However, there are further improvements which are not captured in that publication, and which are instead represented either in other joint publications (e.g. https://www.biorxiv.org/content/10.1101/519025v2) or in blogs produced by our team or close partners (e.g. https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html), (e.g. https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/180#issuecomment-488762439
https://github.com/google/deepvariant/issues/181#issuecomment-489169486:66,Availability,error,error,66,"Thanks @gunjanbaid! , it's now one command but I still received a error:. sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=4; docker: invalid reference format.; See 'docker run --help'. These commands are being run in a Unix shell, do you think the commands vary between Linux and Unix?. Best,. -Ashraf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/181#issuecomment-489169486
https://github.com/google/deepvariant/issues/181#issuecomment-489226839:126,Availability,error,error,126,"@ashraf123456789 I don't think the commands should vary. Could you check that you have set `${BIN_VERSION}`? I have seen this error message when the variable is not set, causing the image name to be incorrect. You could try running the following:. ```; BIN_VERSION=""0.8.0"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=4; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/181#issuecomment-489226839
https://github.com/google/deepvariant/issues/181#issuecomment-489226839:132,Integrability,message,message,132,"@ashraf123456789 I don't think the commands should vary. Could you check that you have set `${BIN_VERSION}`? I have seen this error message when the variable is not set, causing the image name to be incorrect. You could try running the following:. ```; BIN_VERSION=""0.8.0"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=4; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/181#issuecomment-489226839
https://github.com/google/deepvariant/issues/181#issuecomment-489226839:149,Modifiability,variab,variable,149,"@ashraf123456789 I don't think the commands should vary. Could you check that you have set `${BIN_VERSION}`? I have seen this error message when the variable is not set, causing the image name to be incorrect. You could try running the following:. ```; BIN_VERSION=""0.8.0"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=4; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/181#issuecomment-489226839
https://github.com/google/deepvariant/issues/181#issuecomment-2160884273:205,Availability,error,error,205,"Hello,; I am using docker command first time, so not much aware of it, i am trying to run deepvariant, tried out almost all possible method to make changes in my command, but i might be missing some minor error in my command, which i may not be able to rectify, The error i am getting is docker: invalid reference format.; See 'docker run --help'. (**For this i followed the given parameter format in run_deepvariant "" --ref: Required. Genome reference to use. Must have an associated FAI index as well. Supports text or gzipped references. Should match the reference used to align the BAM file; provided to --reads.; "")**; it would be great help if anyone help me to sort out this issue.; I am attaching my command here. docker run -v /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3 -v /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result -v /media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker:/media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker google/deepvariant:{BIN_VERSION=""1.6.1""} python /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py --reads /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam --ref /media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker/hg38_chrM.fa.gz --report_title MITO60_Stats --sample_name MITO60 --outp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/181#issuecomment-2160884273
https://github.com/google/deepvariant/issues/181#issuecomment-2160884273:266,Availability,error,error,266,"Hello,; I am using docker command first time, so not much aware of it, i am trying to run deepvariant, tried out almost all possible method to make changes in my command, but i might be missing some minor error in my command, which i may not be able to rectify, The error i am getting is docker: invalid reference format.; See 'docker run --help'. (**For this i followed the given parameter format in run_deepvariant "" --ref: Required. Genome reference to use. Must have an associated FAI index as well. Supports text or gzipped references. Should match the reference used to align the BAM file; provided to --reads.; "")**; it would be great help if anyone help me to sort out this issue.; I am attaching my command here. docker run -v /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3 -v /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result -v /media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker:/media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker google/deepvariant:{BIN_VERSION=""1.6.1""} python /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py --reads /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam --ref /media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker/hg38_chrM.fa.gz --report_title MITO60_Stats --sample_name MITO60 --outp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/181#issuecomment-2160884273
https://github.com/google/deepvariant/issues/182#issuecomment-488952977:69,Deployability,update,updated,69,"Hi @TerjeNorderhaug, thanks for the suggestions! We will push out an updated version of the notebook by the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/182#issuecomment-488952977
https://github.com/google/deepvariant/issues/182#issuecomment-488952977:113,Deployability,release,release,113,"Hi @TerjeNorderhaug, thanks for the suggestions! We will push out an updated version of the notebook by the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/182#issuecomment-488952977
https://github.com/google/deepvariant/issues/183#issuecomment-490179510:470,Performance,perform,performance,470,Hi @oschwengers . Thank you for your suggestion. I am curious if you have tried to run DeepVariant on haploid bacteria yet? We have investigated in-bred rice strains and found that it has a strong tendency to call homozygous sites as expected from the genome structure. I was curious if you have evaluated running DeepVariant on bacterial sequencing and seeing whether the results are reasonable. This could be valuable feedback to understand the differences in current performance versus expectations of the field. There may be some additional complexity in understanding how to express subclonal variants (these might manifest as HET calls). I don't have an intuition about the preferences of the field.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/183#issuecomment-490179510
https://github.com/google/deepvariant/issues/183#issuecomment-490179510:420,Usability,feedback,feedback,420,Hi @oschwengers . Thank you for your suggestion. I am curious if you have tried to run DeepVariant on haploid bacteria yet? We have investigated in-bred rice strains and found that it has a strong tendency to call homozygous sites as expected from the genome structure. I was curious if you have evaluated running DeepVariant on bacterial sequencing and seeing whether the results are reasonable. This could be valuable feedback to understand the differences in current performance versus expectations of the field. There may be some additional complexity in understanding how to express subclonal variants (these might manifest as HET calls). I don't have an intuition about the preferences of the field.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/183#issuecomment-490179510
https://github.com/google/deepvariant/issues/183#issuecomment-490179510:660,Usability,intuit,intuition,660,Hi @oschwengers . Thank you for your suggestion. I am curious if you have tried to run DeepVariant on haploid bacteria yet? We have investigated in-bred rice strains and found that it has a strong tendency to call homozygous sites as expected from the genome structure. I was curious if you have evaluated running DeepVariant on bacterial sequencing and seeing whether the results are reasonable. This could be valuable feedback to understand the differences in current performance versus expectations of the field. There may be some additional complexity in understanding how to express subclonal variants (these might manifest as HET calls). I don't have an intuition about the preferences of the field.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/183#issuecomment-490179510
https://github.com/google/deepvariant/issues/183#issuecomment-493038730:208,Testability,test,test,208,"Hi @AndrewCarroll ,; thanks for your reply and the additional info. I'd love to conduct some deeper investigations but unfortunately, I don't have the time. So if you could provide some more bacteria related test results, this would be highly appreciated.; Thanks a lot and best regards!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/183#issuecomment-493038730
https://github.com/google/deepvariant/issues/183#issuecomment-1542901689:166,Integrability,depend,depending,166,@oschwengers DeepVariant was included in [this bacterial variant calling benchmark](https://doi.org/10.1093%2Fgigascience%2Fgiaa007). tl;dr results were quite varied depending on the species and reference genome distance from the sample.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/183#issuecomment-1542901689
https://github.com/google/deepvariant/issues/183#issuecomment-1542901689:73,Testability,benchmark,benchmark,73,@oschwengers DeepVariant was included in [this bacterial variant calling benchmark](https://doi.org/10.1093%2Fgigascience%2Fgiaa007). tl;dr results were quite varied depending on the species and reference genome distance from the sample.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/183#issuecomment-1542901689
https://github.com/google/deepvariant/issues/183#issuecomment-1824632486:632,Modifiability,evolve,evolved,632,"Dear @oschwengers,. I will soon have to call variants from E.coli bacteria genomes and ONT SUP reads and wonder if I can use the newly introduced haploid option to tell Deepvariant that my bacterial reference genome is haploid, like shown in [this page for X and Y](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-xy-calling-case-study.md). `--haploid_contigs ""<Ecoli_chromosome>"" `. Also, will the _ONT_R104_ model be affected by this extra argument?. The paper referred to above by @mbhall88 dates from 2020 and may not be accurate anymore for the haploid aspect of variant calling in bacteria if DeepVariant has evolved in that domain. Thanks for your feedback",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/183#issuecomment-1824632486
https://github.com/google/deepvariant/issues/183#issuecomment-1824632486:672,Usability,feedback,feedback,672,"Dear @oschwengers,. I will soon have to call variants from E.coli bacteria genomes and ONT SUP reads and wonder if I can use the newly introduced haploid option to tell Deepvariant that my bacterial reference genome is haploid, like shown in [this page for X and Y](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-xy-calling-case-study.md). `--haploid_contigs ""<Ecoli_chromosome>"" `. Also, will the _ONT_R104_ model be affected by this extra argument?. The paper referred to above by @mbhall88 dates from 2020 and may not be accurate anymore for the haploid aspect of variant calling in bacteria if DeepVariant has evolved in that domain. Thanks for your feedback",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/183#issuecomment-1824632486
https://github.com/google/deepvariant/issues/184#issuecomment-491138418:166,Availability,Error,Error,166,"I think I know what's happening. The `${USER}` was `/root` when running docker. It was maybe caused by a `sudo bash`? When I changed it, everything seems OK. Similar Error #141 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-491138418
https://github.com/google/deepvariant/issues/184#issuecomment-1699174840:599,Availability,reboot,reboot,599,"I'm not sure I fully understand, I'm not a very experienced docker user. . The command is being run by a non-root user who is part of the docker user group. They are running the commands in the same directories that they ran them in before, rerunning it with alignments that have been filtered differently than their first run, but otherwise identical to their previous runs that worked just fine. The command is even directly copied and pasted). I'm a bit baffled because as far as I can discern, any user/permission issues should have also factored in the last time. But it worked before a recent reboot and now no longer does. I've also tried restarting docker to no avail. In my own non-root account I was previously able to run the quickstart example, but now that will not work for me either. We supplied our working director as ${PWD} rather than ${HOME} which I would have thought would address the location issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699174840
https://github.com/google/deepvariant/issues/184#issuecomment-1699174840:670,Availability,avail,avail,670,"I'm not sure I fully understand, I'm not a very experienced docker user. . The command is being run by a non-root user who is part of the docker user group. They are running the commands in the same directories that they ran them in before, rerunning it with alignments that have been filtered differently than their first run, but otherwise identical to their previous runs that worked just fine. The command is even directly copied and pasted). I'm a bit baffled because as far as I can discern, any user/permission issues should have also factored in the last time. But it worked before a recent reboot and now no longer does. I've also tried restarting docker to no avail. In my own non-root account I was previously able to run the quickstart example, but now that will not work for me either. We supplied our working director as ${PWD} rather than ${HOME} which I would have thought would address the location issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699174840
https://github.com/google/deepvariant/issues/184#issuecomment-1699480255:320,Availability,error,errors,320,"Hi @charlesfeigin,. It's great that things are being run as a non-root user. So to simplify the steps for reaching a solution, I just need three things from you:. 1. The complete set of commands that were typed for launching DeepVariant.; 2. The directories where the input files are located.; 3. The complete output of errors that were seen after DeepVariant was launched. This way we can reconstruct a runnable state. If you don't know everything, that's fine but at least steps 1 & 3 are necessary to begin to reconstruct step 2. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699480255
https://github.com/google/deepvariant/issues/184#issuecomment-1699480255:83,Usability,simpl,simplify,83,"Hi @charlesfeigin,. It's great that things are being run as a non-root user. So to simplify the steps for reaching a solution, I just need three things from you:. 1. The complete set of commands that were typed for launching DeepVariant.; 2. The directories where the input files are located.; 3. The complete output of errors that were seen after DeepVariant was launched. This way we can reconstruct a runnable state. If you don't know everything, that's fine but at least steps 1 & 3 are necessary to begin to reconstruct step 2. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699480255
https://github.com/google/deepvariant/issues/184#issuecomment-1699892625:969,Availability,error,errors,969,"Thank you so much for your assistance Paul. Answered slightly out of order:. 2. The location from which the commands were run is /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/. Inside this directory are two subdirectories: inputs (in which the assembly and bam files are) and outputs (in which the results go). 1. The commands run were:. BIN_VERSION=""1.5.0""; docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""${PWD}/inputs""; OUTPUT_DIR=""${PWD}/outputs"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa --reads=/input/NC_045426.1_A_filt_fixed_markdup_csort.bam --output_vcf=/output/NC_045426.1_A.vcf.gz --output_gvcf=/output/NC_045426.1_A.g.vcf.gz --intermediate_results_dir /output/NC_045426.1_A_intermediate_results_dir --num_shards=1 &. 3. The complete output of errors is:. 2023-08-30 21:45:18.422516: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0830 21:45:20.409601 140002879141696 run_deepvariant.py:364] Re-using the directory for intermediate results in /output/NC_045426.1_A_intermediate_results_dir. ***** Intermediate results will be written to /output/NC_045426.1_A_intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa"" --reads ""/input/NC_045426.1_A_filt_fixed_markdup_csort.bam"" --examples ""/output/NC_045426.1_A_intermediate_results_dir/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/output/NC_045426.1_A_interm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699892625
https://github.com/google/deepvariant/issues/184#issuecomment-1699892625:1088,Performance,optimiz,optimized,1088,"t_calling/. Inside this directory are two subdirectories: inputs (in which the assembly and bam files are) and outputs (in which the results go). 1. The commands run were:. BIN_VERSION=""1.5.0""; docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""${PWD}/inputs""; OUTPUT_DIR=""${PWD}/outputs"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa --reads=/input/NC_045426.1_A_filt_fixed_markdup_csort.bam --output_vcf=/output/NC_045426.1_A.vcf.gz --output_gvcf=/output/NC_045426.1_A.g.vcf.gz --intermediate_results_dir /output/NC_045426.1_A_intermediate_results_dir --num_shards=1 &. 3. The complete output of errors is:. 2023-08-30 21:45:18.422516: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0830 21:45:20.409601 140002879141696 run_deepvariant.py:364] Re-using the directory for intermediate results in /output/NC_045426.1_A_intermediate_results_dir. ***** Intermediate results will be written to /output/NC_045426.1_A_intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa"" --reads ""/input/NC_045426.1_A_filt_fixed_markdup_csort.bam"" --examples ""/output/NC_045426.1_A_intermediate_results_dir/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/output/NC_045426.1_A_intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {}. 2023-08-30 21:45:21.348267: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699892625
https://github.com/google/deepvariant/issues/184#issuecomment-1699892625:1188,Performance,perform,performance-critical,1188,"t_calling/. Inside this directory are two subdirectories: inputs (in which the assembly and bam files are) and outputs (in which the results go). 1. The commands run were:. BIN_VERSION=""1.5.0""; docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""${PWD}/inputs""; OUTPUT_DIR=""${PWD}/outputs"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa --reads=/input/NC_045426.1_A_filt_fixed_markdup_csort.bam --output_vcf=/output/NC_045426.1_A.vcf.gz --output_gvcf=/output/NC_045426.1_A.g.vcf.gz --intermediate_results_dir /output/NC_045426.1_A_intermediate_results_dir --num_shards=1 &. 3. The complete output of errors is:. 2023-08-30 21:45:18.422516: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0830 21:45:20.409601 140002879141696 run_deepvariant.py:364] Re-using the directory for intermediate results in /output/NC_045426.1_A_intermediate_results_dir. ***** Intermediate results will be written to /output/NC_045426.1_A_intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa"" --reads ""/input/NC_045426.1_A_filt_fixed_markdup_csort.bam"" --examples ""/output/NC_045426.1_A_intermediate_results_dir/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/output/NC_045426.1_A_intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {}. 2023-08-30 21:45:21.348267: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699892625
https://github.com/google/deepvariant/issues/184#issuecomment-1699892625:2158,Performance,optimiz,optimized,2158,"rations, rebuild TensorFlow with the appropriate compiler flags.; I0830 21:45:20.409601 140002879141696 run_deepvariant.py:364] Re-using the directory for intermediate results in /output/NC_045426.1_A_intermediate_results_dir. ***** Intermediate results will be written to /output/NC_045426.1_A_intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa"" --reads ""/input/NC_045426.1_A_filt_fixed_markdup_csort.bam"" --examples ""/output/NC_045426.1_A_intermediate_results_dir/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/output/NC_045426.1_A_intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {}. 2023-08-30 21:45:21.348267: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [E::hts_open_format] Failed to open file ""/input/NC_045426.1_A_filt_fixed_markdup_csort.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options; samples_in_order, sample_role_to_train = one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699892625
https://github.com/google/deepvariant/issues/184#issuecomment-1699892625:2258,Performance,perform,performance-critical,2258,"rations, rebuild TensorFlow with the appropriate compiler flags.; I0830 21:45:20.409601 140002879141696 run_deepvariant.py:364] Re-using the directory for intermediate results in /output/NC_045426.1_A_intermediate_results_dir. ***** Intermediate results will be written to /output/NC_045426.1_A_intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa"" --reads ""/input/NC_045426.1_A_filt_fixed_markdup_csort.bam"" --examples ""/output/NC_045426.1_A_intermediate_results_dir/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/output/NC_045426.1_A_intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {}. 2023-08-30 21:45:21.348267: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [E::hts_open_format] Failed to open file ""/input/NC_045426.1_A_filt_fixed_markdup_csort.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options; samples_in_order, sample_role_to_train = one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699892625
https://github.com/google/deepvariant/issues/184#issuecomment-1699969751:556,Availability,echo,echo,556,"Hi Charles,. Great! So if you run the following steps, what do you see (I know some are obvious, but it's just a sanity check):. $`1)`$ The following will test that your file is at the correct location:. ```; ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs . ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`2)`$ The following will test that your `INPUT_DIR` environment variable works properly:. ``` ; INPUT_DIR=""${PWD}/inputs"" . echo ${PWD}. ls -l ${INPUT_DIR}. ls -l ${INPUT_DIR} | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`3)`$ The following will test that your `INPUT_DIR` environment variable works properly inside Docker:. ```; INPUT_DIR=""${PWD}/inputs"" ; BIN_VERSION=""1.5.0"". echo ${PWD}. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. You should see the file `NC_045426.1_A_filt_fixed_markdup_csort.bam` for all 3 steps. Please list for which step you don't see that file, and what you actually see for that step (in terms of the complete output of the step). Of course the other way to explicitly name the input and output variables, by changing them to the following (and not relying on `${PWD}` - the present working directory - which could change):. ```; INPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"". OUTPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/outputs"". ```; Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699969751
https://github.com/google/deepvariant/issues/184#issuecomment-1699969751:823,Availability,echo,echo,823,"Hi Charles,. Great! So if you run the following steps, what do you see (I know some are obvious, but it's just a sanity check):. $`1)`$ The following will test that your file is at the correct location:. ```; ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs . ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`2)`$ The following will test that your `INPUT_DIR` environment variable works properly:. ``` ; INPUT_DIR=""${PWD}/inputs"" . echo ${PWD}. ls -l ${INPUT_DIR}. ls -l ${INPUT_DIR} | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`3)`$ The following will test that your `INPUT_DIR` environment variable works properly inside Docker:. ```; INPUT_DIR=""${PWD}/inputs"" ; BIN_VERSION=""1.5.0"". echo ${PWD}. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. You should see the file `NC_045426.1_A_filt_fixed_markdup_csort.bam` for all 3 steps. Please list for which step you don't see that file, and what you actually see for that step (in terms of the complete output of the step). Of course the other way to explicitly name the input and output variables, by changing them to the following (and not relying on `${PWD}` - the present working directory - which could change):. ```; INPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"". OUTPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/outputs"". ```; Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699969751
https://github.com/google/deepvariant/issues/184#issuecomment-1699969751:496,Modifiability,variab,variable,496,"Hi Charles,. Great! So if you run the following steps, what do you see (I know some are obvious, but it's just a sanity check):. $`1)`$ The following will test that your file is at the correct location:. ```; ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs . ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`2)`$ The following will test that your `INPUT_DIR` environment variable works properly:. ``` ; INPUT_DIR=""${PWD}/inputs"" . echo ${PWD}. ls -l ${INPUT_DIR}. ls -l ${INPUT_DIR} | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`3)`$ The following will test that your `INPUT_DIR` environment variable works properly inside Docker:. ```; INPUT_DIR=""${PWD}/inputs"" ; BIN_VERSION=""1.5.0"". echo ${PWD}. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. You should see the file `NC_045426.1_A_filt_fixed_markdup_csort.bam` for all 3 steps. Please list for which step you don't see that file, and what you actually see for that step (in terms of the complete output of the step). Of course the other way to explicitly name the input and output variables, by changing them to the following (and not relying on `${PWD}` - the present working directory - which could change):. ```; INPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"". OUTPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/outputs"". ```; Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699969751
https://github.com/google/deepvariant/issues/184#issuecomment-1699969751:729,Modifiability,variab,variable,729,"Hi Charles,. Great! So if you run the following steps, what do you see (I know some are obvious, but it's just a sanity check):. $`1)`$ The following will test that your file is at the correct location:. ```; ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs . ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`2)`$ The following will test that your `INPUT_DIR` environment variable works properly:. ``` ; INPUT_DIR=""${PWD}/inputs"" . echo ${PWD}. ls -l ${INPUT_DIR}. ls -l ${INPUT_DIR} | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`3)`$ The following will test that your `INPUT_DIR` environment variable works properly inside Docker:. ```; INPUT_DIR=""${PWD}/inputs"" ; BIN_VERSION=""1.5.0"". echo ${PWD}. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. You should see the file `NC_045426.1_A_filt_fixed_markdup_csort.bam` for all 3 steps. Please list for which step you don't see that file, and what you actually see for that step (in terms of the complete output of the step). Of course the other way to explicitly name the input and output variables, by changing them to the following (and not relying on `${PWD}` - the present working directory - which could change):. ```; INPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"". OUTPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/outputs"". ```; Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699969751
https://github.com/google/deepvariant/issues/184#issuecomment-1699969751:1366,Modifiability,variab,variables,1366,"Hi Charles,. Great! So if you run the following steps, what do you see (I know some are obvious, but it's just a sanity check):. $`1)`$ The following will test that your file is at the correct location:. ```; ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs . ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`2)`$ The following will test that your `INPUT_DIR` environment variable works properly:. ``` ; INPUT_DIR=""${PWD}/inputs"" . echo ${PWD}. ls -l ${INPUT_DIR}. ls -l ${INPUT_DIR} | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`3)`$ The following will test that your `INPUT_DIR` environment variable works properly inside Docker:. ```; INPUT_DIR=""${PWD}/inputs"" ; BIN_VERSION=""1.5.0"". echo ${PWD}. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. You should see the file `NC_045426.1_A_filt_fixed_markdup_csort.bam` for all 3 steps. Please list for which step you don't see that file, and what you actually see for that step (in terms of the complete output of the step). Of course the other way to explicitly name the input and output variables, by changing them to the following (and not relying on `${PWD}` - the present working directory - which could change):. ```; INPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"". OUTPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/outputs"". ```; Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699969751
https://github.com/google/deepvariant/issues/184#issuecomment-1699969751:113,Safety,sanity check,sanity check,113,"Hi Charles,. Great! So if you run the following steps, what do you see (I know some are obvious, but it's just a sanity check):. $`1)`$ The following will test that your file is at the correct location:. ```; ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs . ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`2)`$ The following will test that your `INPUT_DIR` environment variable works properly:. ``` ; INPUT_DIR=""${PWD}/inputs"" . echo ${PWD}. ls -l ${INPUT_DIR}. ls -l ${INPUT_DIR} | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`3)`$ The following will test that your `INPUT_DIR` environment variable works properly inside Docker:. ```; INPUT_DIR=""${PWD}/inputs"" ; BIN_VERSION=""1.5.0"". echo ${PWD}. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. You should see the file `NC_045426.1_A_filt_fixed_markdup_csort.bam` for all 3 steps. Please list for which step you don't see that file, and what you actually see for that step (in terms of the complete output of the step). Of course the other way to explicitly name the input and output variables, by changing them to the following (and not relying on `${PWD}` - the present working directory - which could change):. ```; INPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"". OUTPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/outputs"". ```; Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699969751
https://github.com/google/deepvariant/issues/184#issuecomment-1699969751:155,Testability,test,test,155,"Hi Charles,. Great! So if you run the following steps, what do you see (I know some are obvious, but it's just a sanity check):. $`1)`$ The following will test that your file is at the correct location:. ```; ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs . ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`2)`$ The following will test that your `INPUT_DIR` environment variable works properly:. ``` ; INPUT_DIR=""${PWD}/inputs"" . echo ${PWD}. ls -l ${INPUT_DIR}. ls -l ${INPUT_DIR} | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`3)`$ The following will test that your `INPUT_DIR` environment variable works properly inside Docker:. ```; INPUT_DIR=""${PWD}/inputs"" ; BIN_VERSION=""1.5.0"". echo ${PWD}. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. You should see the file `NC_045426.1_A_filt_fixed_markdup_csort.bam` for all 3 steps. Please list for which step you don't see that file, and what you actually see for that step (in terms of the complete output of the step). Of course the other way to explicitly name the input and output variables, by changing them to the following (and not relying on `${PWD}` - the present working directory - which could change):. ```; INPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"". OUTPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/outputs"". ```; Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699969751
https://github.com/google/deepvariant/issues/184#issuecomment-1699969751:457,Testability,test,test,457,"Hi Charles,. Great! So if you run the following steps, what do you see (I know some are obvious, but it's just a sanity check):. $`1)`$ The following will test that your file is at the correct location:. ```; ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs . ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`2)`$ The following will test that your `INPUT_DIR` environment variable works properly:. ``` ; INPUT_DIR=""${PWD}/inputs"" . echo ${PWD}. ls -l ${INPUT_DIR}. ls -l ${INPUT_DIR} | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`3)`$ The following will test that your `INPUT_DIR` environment variable works properly inside Docker:. ```; INPUT_DIR=""${PWD}/inputs"" ; BIN_VERSION=""1.5.0"". echo ${PWD}. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. You should see the file `NC_045426.1_A_filt_fixed_markdup_csort.bam` for all 3 steps. Please list for which step you don't see that file, and what you actually see for that step (in terms of the complete output of the step). Of course the other way to explicitly name the input and output variables, by changing them to the following (and not relying on `${PWD}` - the present working directory - which could change):. ```; INPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"". OUTPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/outputs"". ```; Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699969751
https://github.com/google/deepvariant/issues/184#issuecomment-1699969751:690,Testability,test,test,690,"Hi Charles,. Great! So if you run the following steps, what do you see (I know some are obvious, but it's just a sanity check):. $`1)`$ The following will test that your file is at the correct location:. ```; ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs . ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`2)`$ The following will test that your `INPUT_DIR` environment variable works properly:. ``` ; INPUT_DIR=""${PWD}/inputs"" . echo ${PWD}. ls -l ${INPUT_DIR}. ls -l ${INPUT_DIR} | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`3)`$ The following will test that your `INPUT_DIR` environment variable works properly inside Docker:. ```; INPUT_DIR=""${PWD}/inputs"" ; BIN_VERSION=""1.5.0"". echo ${PWD}. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. You should see the file `NC_045426.1_A_filt_fixed_markdup_csort.bam` for all 3 steps. Please list for which step you don't see that file, and what you actually see for that step (in terms of the complete output of the step). Of course the other way to explicitly name the input and output variables, by changing them to the following (and not relying on `${PWD}` - the present working directory - which could change):. ```; INPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"". OUTPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/outputs"". ```; Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1699969751
https://github.com/google/deepvariant/issues/184#issuecomment-1700285417:553,Availability,echo,echo,553,"Thanks again Paul:. 1). ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs ; -rw-rw-r-- 1 ajp1 ajp1 3138120789 Aug 31 12:12 GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa; -rw-rw-r-- 1 ajp1 ajp1 3133881452 Aug 31 12:12 NC_045426.1_A_filt_fixed_markdup_csort.bam. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; -rw-rw-r-- 1 ajp1 ajp1 3133881452 Aug 31 12:12 NC_045426.1_A_filt_fixed_markdup_csort.bam. 2) . INPUT_DIR=""${PWD}/inputs"" ; echo ${PWD}; /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling. ls -l ${INPUT_DIR}; total 6125004; -rw-rw-r-- 1 ajp1 ajp1 3138120789 Aug 31 12:12 GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa; -rw-rw-r-- 1 ajp1 ajp1 3133881452 Aug 31 12:12 NC_045426.1_A_filt_fixed_markdup_csort.bam. ls -l ${INPUT_DIR} | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; -rw-rw-r-- 1 ajp1 ajp1 3133881452 Aug 31 12:12 NC_045426.1_A_filt_fixed_markdup_csort.bam. 3) . INPUT_DIR=""${PWD}/inputs"" ; BIN_VERSION=""1.5.0""; echo ${PWD}; /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs. Note: ajp1 is in the docker user group, but does not have sudo permissions. they did not need sudo to run deepvariant on their earlier runs though and it worked just fine... Nonetheless, running without sudo gives the following:. docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input; total 0 . docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; (this gave no outputs at all). Lastly, I tried specifying the directories as you did above:. INPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"". And I still had no output at all when running:. docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1700285417
https://github.com/google/deepvariant/issues/184#issuecomment-1700285417:1074,Availability,echo,echo,1074,"Thanks again Paul:. 1). ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs ; -rw-rw-r-- 1 ajp1 ajp1 3138120789 Aug 31 12:12 GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa; -rw-rw-r-- 1 ajp1 ajp1 3133881452 Aug 31 12:12 NC_045426.1_A_filt_fixed_markdup_csort.bam. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; -rw-rw-r-- 1 ajp1 ajp1 3133881452 Aug 31 12:12 NC_045426.1_A_filt_fixed_markdup_csort.bam. 2) . INPUT_DIR=""${PWD}/inputs"" ; echo ${PWD}; /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling. ls -l ${INPUT_DIR}; total 6125004; -rw-rw-r-- 1 ajp1 ajp1 3138120789 Aug 31 12:12 GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa; -rw-rw-r-- 1 ajp1 ajp1 3133881452 Aug 31 12:12 NC_045426.1_A_filt_fixed_markdup_csort.bam. ls -l ${INPUT_DIR} | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; -rw-rw-r-- 1 ajp1 ajp1 3133881452 Aug 31 12:12 NC_045426.1_A_filt_fixed_markdup_csort.bam. 3) . INPUT_DIR=""${PWD}/inputs"" ; BIN_VERSION=""1.5.0""; echo ${PWD}; /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs. Note: ajp1 is in the docker user group, but does not have sudo permissions. they did not need sudo to run deepvariant on their earlier runs though and it worked just fine... Nonetheless, running without sudo gives the following:. docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input; total 0 . docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; (this gave no outputs at all). Lastly, I tried specifying the directories as you did above:. INPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"". And I still had no output at all when running:. docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1700285417
https://github.com/google/deepvariant/issues/184#issuecomment-1701584604:1196,Availability,error,error,1196,"Hi Charles,. Usually `mount` (`-v`) require specialized access that the administrator can provide. ; Maybe you can show them our tests that worked for you previously. In any case, here are a few more tests:. $`1)`$ The first is to use the `mount` command explicitly:. ```; docker run \; --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input; ```. $`2)`$ This is using volumes, which is a different approach:. ```; docker volume create --name dv-vol. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. touch input-path-cont/file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker rmi --force hello-world:latest. docker volume rm dv-vol; ```. If the volume removal gives you an error like this:. ```; Error response from daemon: remove dv-vol: volume is in use - [ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2]; ```; Just perform `docker container rm` on each individual of the listed container ids in the brackets, like this (before trying `docker volume rm dv-vol` again):. ```; docker container rm ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2; ```. Let me know the results of both steps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701584604
https://github.com/google/deepvariant/issues/184#issuecomment-1701584604:1219,Availability,Error,Error,1219,"Hi Charles,. Usually `mount` (`-v`) require specialized access that the administrator can provide. ; Maybe you can show them our tests that worked for you previously. In any case, here are a few more tests:. $`1)`$ The first is to use the `mount` command explicitly:. ```; docker run \; --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input; ```. $`2)`$ This is using volumes, which is a different approach:. ```; docker volume create --name dv-vol. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. touch input-path-cont/file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker rmi --force hello-world:latest. docker volume rm dv-vol; ```. If the volume removal gives you an error like this:. ```; Error response from daemon: remove dv-vol: volume is in use - [ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2]; ```; Just perform `docker container rm` on each individual of the listed container ids in the brackets, like this (before trying `docker volume rm dv-vol` again):. ```; docker container rm ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2; ```. Let me know the results of both steps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701584604
https://github.com/google/deepvariant/issues/184#issuecomment-1701584604:1359,Performance,perform,perform,1359,"Hi Charles,. Usually `mount` (`-v`) require specialized access that the administrator can provide. ; Maybe you can show them our tests that worked for you previously. In any case, here are a few more tests:. $`1)`$ The first is to use the `mount` command explicitly:. ```; docker run \; --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input; ```. $`2)`$ This is using volumes, which is a different approach:. ```; docker volume create --name dv-vol. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. touch input-path-cont/file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker rmi --force hello-world:latest. docker volume rm dv-vol; ```. If the volume removal gives you an error like this:. ```; Error response from daemon: remove dv-vol: volume is in use - [ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2]; ```; Just perform `docker container rm` on each individual of the listed container ids in the brackets, like this (before trying `docker volume rm dv-vol` again):. ```; docker container rm ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2; ```. Let me know the results of both steps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701584604
https://github.com/google/deepvariant/issues/184#issuecomment-1701584604:56,Security,access,access,56,"Hi Charles,. Usually `mount` (`-v`) require specialized access that the administrator can provide. ; Maybe you can show them our tests that worked for you previously. In any case, here are a few more tests:. $`1)`$ The first is to use the `mount` command explicitly:. ```; docker run \; --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input; ```. $`2)`$ This is using volumes, which is a different approach:. ```; docker volume create --name dv-vol. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. touch input-path-cont/file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker rmi --force hello-world:latest. docker volume rm dv-vol; ```. If the volume removal gives you an error like this:. ```; Error response from daemon: remove dv-vol: volume is in use - [ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2]; ```; Just perform `docker container rm` on each individual of the listed container ids in the brackets, like this (before trying `docker volume rm dv-vol` again):. ```; docker container rm ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2; ```. Let me know the results of both steps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701584604
https://github.com/google/deepvariant/issues/184#issuecomment-1701584604:129,Testability,test,tests,129,"Hi Charles,. Usually `mount` (`-v`) require specialized access that the administrator can provide. ; Maybe you can show them our tests that worked for you previously. In any case, here are a few more tests:. $`1)`$ The first is to use the `mount` command explicitly:. ```; docker run \; --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input; ```. $`2)`$ This is using volumes, which is a different approach:. ```; docker volume create --name dv-vol. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. touch input-path-cont/file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker rmi --force hello-world:latest. docker volume rm dv-vol; ```. If the volume removal gives you an error like this:. ```; Error response from daemon: remove dv-vol: volume is in use - [ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2]; ```; Just perform `docker container rm` on each individual of the listed container ids in the brackets, like this (before trying `docker volume rm dv-vol` again):. ```; docker container rm ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2; ```. Let me know the results of both steps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701584604
https://github.com/google/deepvariant/issues/184#issuecomment-1701584604:200,Testability,test,tests,200,"Hi Charles,. Usually `mount` (`-v`) require specialized access that the administrator can provide. ; Maybe you can show them our tests that worked for you previously. In any case, here are a few more tests:. $`1)`$ The first is to use the `mount` command explicitly:. ```; docker run \; --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input; ```. $`2)`$ This is using volumes, which is a different approach:. ```; docker volume create --name dv-vol. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. touch input-path-cont/file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker rmi --force hello-world:latest. docker volume rm dv-vol; ```. If the volume removal gives you an error like this:. ```; Error response from daemon: remove dv-vol: volume is in use - [ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2]; ```; Just perform `docker container rm` on each individual of the listed container ids in the brackets, like this (before trying `docker volume rm dv-vol` again):. ```; docker container rm ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2; ```. Let me know the results of both steps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701584604
https://github.com/google/deepvariant/issues/184#issuecomment-1701949973:635,Availability,Error,Error,635,"Hi Paul,. Thank you again for your replies and sorry mine are so slow (I think we are in very different time zones). So I’m administering this system but am not really a system administrator (I’m a biologist). Since I have root access, if there is something that can be done (or undone) to re-establish the ability of non-root users to run deepvariant, I can do that, I just don’t know enough to fix this specific issue myself. 1) The following gives me ; docker run --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" google/deepvariant:""1.5.0"" ls -l /input. docker: Error response from daemon: invalid mount config for type ""bind"": bind source path does not exist: /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling.; See 'docker run --help'. 2) I’ve run the following, but confess I don’t understand what is going on past the second command and then deleting what was done at the end. If creating volumes can be skirted by changing some admin permission that is doable on my end. docker volume create --name dv-vol; dv-vol. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world; 4d2015a9c2dbe3fc6d27854c440ce8505222ed6a8f2a3c945d36315081a832b6. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. a-new-file; touch input-path-cont/file-2. -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker rmi --force hello-world:latest. U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973
https://github.com/google/deepvariant/issues/184#issuecomment-1701949973:2221,Availability,error,error,2221,"ility of non-root users to run deepvariant, I can do that, I just don’t know enough to fix this specific issue myself. 1) The following gives me ; docker run --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" google/deepvariant:""1.5.0"" ls -l /input. docker: Error response from daemon: invalid mount config for type ""bind"": bind source path does not exist: /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling.; See 'docker run --help'. 2) I’ve run the following, but confess I don’t understand what is going on past the second command and then deleting what was done at the end. If creating volumes can be skirted by changing some admin permission that is doable on my end. docker volume create --name dv-vol; dv-vol. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world; 4d2015a9c2dbe3fc6d27854c440ce8505222ed6a8f2a3c945d36315081a832b6. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. a-new-file; touch input-path-cont/file-2. -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker rmi --force hello-world:latest. Untagged: hello-world:latest; Untagged: hello-world@sha256:dcba6daec718f547568c562956fa47e1b03673dd010fe6ee58ca806767031d1c; Deleted: sha256:9c7a54a9a43cca047013b82af109fe963fde787f63f9e016fdc3384500c2823d. This gave the error you indicated, and the fix worked as well; docker volume rm dv-vol. Best,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973
https://github.com/google/deepvariant/issues/184#issuecomment-1701949973:677,Modifiability,config,config,677,"Hi Paul,. Thank you again for your replies and sorry mine are so slow (I think we are in very different time zones). So I’m administering this system but am not really a system administrator (I’m a biologist). Since I have root access, if there is something that can be done (or undone) to re-establish the ability of non-root users to run deepvariant, I can do that, I just don’t know enough to fix this specific issue myself. 1) The following gives me ; docker run --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" google/deepvariant:""1.5.0"" ls -l /input. docker: Error response from daemon: invalid mount config for type ""bind"": bind source path does not exist: /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling.; See 'docker run --help'. 2) I’ve run the following, but confess I don’t understand what is going on past the second command and then deleting what was done at the end. If creating volumes can be skirted by changing some admin permission that is doable on my end. docker volume create --name dv-vol; dv-vol. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world; 4d2015a9c2dbe3fc6d27854c440ce8505222ed6a8f2a3c945d36315081a832b6. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. a-new-file; touch input-path-cont/file-2. -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker rmi --force hello-world:latest. U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973
https://github.com/google/deepvariant/issues/184#issuecomment-1701949973:228,Security,access,access,228,"Hi Paul,. Thank you again for your replies and sorry mine are so slow (I think we are in very different time zones). So I’m administering this system but am not really a system administrator (I’m a biologist). Since I have root access, if there is something that can be done (or undone) to re-establish the ability of non-root users to run deepvariant, I can do that, I just don’t know enough to fix this specific issue myself. 1) The following gives me ; docker run --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" google/deepvariant:""1.5.0"" ls -l /input. docker: Error response from daemon: invalid mount config for type ""bind"": bind source path does not exist: /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling.; See 'docker run --help'. 2) I’ve run the following, but confess I don’t understand what is going on past the second command and then deleting what was done at the end. If creating volumes can be skirted by changing some admin permission that is doable on my end. docker volume create --name dv-vol; dv-vol. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world; 4d2015a9c2dbe3fc6d27854c440ce8505222ed6a8f2a3c945d36315081a832b6. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. a-new-file; touch input-path-cont/file-2. -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker rmi --force hello-world:latest. U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973
https://github.com/google/deepvariant/issues/184#issuecomment-1701949973:279,Usability,undo,undone,279,"Hi Paul,. Thank you again for your replies and sorry mine are so slow (I think we are in very different time zones). So I’m administering this system but am not really a system administrator (I’m a biologist). Since I have root access, if there is something that can be done (or undone) to re-establish the ability of non-root users to run deepvariant, I can do that, I just don’t know enough to fix this specific issue myself. 1) The following gives me ; docker run --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" google/deepvariant:""1.5.0"" ls -l /input. docker: Error response from daemon: invalid mount config for type ""bind"": bind source path does not exist: /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling.; See 'docker run --help'. 2) I’ve run the following, but confess I don’t understand what is going on past the second command and then deleting what was done at the end. If creating volumes can be skirted by changing some admin permission that is doable on my end. docker volume create --name dv-vol; dv-vol. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world; 4d2015a9c2dbe3fc6d27854c440ce8505222ed6a8f2a3c945d36315081a832b6. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. a-new-file; touch input-path-cont/file-2. -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker rmi --force hello-world:latest. U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973
https://github.com/google/deepvariant/issues/184#issuecomment-1702017107:1690,Deployability,install,install,1690,"e under some circumstances. . So let's try the `mount` approach. The reason we used `--mount` instead of `-v` is because it is more granular, telling us specifically which directory it is having an issue with. In this case it is getting stuck on the `variant_calling` directory, and somehow Docker is not able to recognize it. So let's try to determine what type of directory, and what permissions it has (including above and below it). $`1)`$ So if you could please type the following commands, then we can inspect the output to determine what might be the issue:. ```; stat /tiger/home/ajp1/analysis/demography/tasmanian_devil. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs. ls -l /tiger/home/ajp1/analysis/demography | grep tasmanian_devil. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/ | grep variant_calling. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling | grep inputs; ```. $`2)`$ Now I have some minor questions about your system and Docker, in order to determine if there could be other underlying causes. For the following questions, it's okay if you don't remember everything: . * What operating system and version are your running? ; * How long ago did you install Docker?; * What commands did you use to install Docker? ; * What version of Docker are you running? (The command is in the commands below.). You previously mention that `ajp1` is part of the docker group, so we can use that as guidance when we inspect the output from the commands above. Below is a set of commands if you could please run, to answer some of the questions above (except for the installation ones). For Linux usually the commands are as follows (some will provide you information, as I included them for multiple operating systems):. ```; uname -a. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. docker --version; ```. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1702017107
https://github.com/google/deepvariant/issues/184#issuecomment-1702017107:1738,Deployability,install,install,1738,"e under some circumstances. . So let's try the `mount` approach. The reason we used `--mount` instead of `-v` is because it is more granular, telling us specifically which directory it is having an issue with. In this case it is getting stuck on the `variant_calling` directory, and somehow Docker is not able to recognize it. So let's try to determine what type of directory, and what permissions it has (including above and below it). $`1)`$ So if you could please type the following commands, then we can inspect the output to determine what might be the issue:. ```; stat /tiger/home/ajp1/analysis/demography/tasmanian_devil. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs. ls -l /tiger/home/ajp1/analysis/demography | grep tasmanian_devil. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/ | grep variant_calling. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling | grep inputs; ```. $`2)`$ Now I have some minor questions about your system and Docker, in order to determine if there could be other underlying causes. For the following questions, it's okay if you don't remember everything: . * What operating system and version are your running? ; * How long ago did you install Docker?; * What commands did you use to install Docker? ; * What version of Docker are you running? (The command is in the commands below.). You previously mention that `ajp1` is part of the docker group, so we can use that as guidance when we inspect the output from the commands above. Below is a set of commands if you could please run, to answer some of the questions above (except for the installation ones). For Linux usually the commands are as follows (some will provide you information, as I included them for multiple operating systems):. ```; uname -a. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. docker --version; ```. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1702017107
https://github.com/google/deepvariant/issues/184#issuecomment-1702017107:2092,Deployability,install,installation,2092,"e under some circumstances. . So let's try the `mount` approach. The reason we used `--mount` instead of `-v` is because it is more granular, telling us specifically which directory it is having an issue with. In this case it is getting stuck on the `variant_calling` directory, and somehow Docker is not able to recognize it. So let's try to determine what type of directory, and what permissions it has (including above and below it). $`1)`$ So if you could please type the following commands, then we can inspect the output to determine what might be the issue:. ```; stat /tiger/home/ajp1/analysis/demography/tasmanian_devil. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs. ls -l /tiger/home/ajp1/analysis/demography | grep tasmanian_devil. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/ | grep variant_calling. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling | grep inputs; ```. $`2)`$ Now I have some minor questions about your system and Docker, in order to determine if there could be other underlying causes. For the following questions, it's okay if you don't remember everything: . * What operating system and version are your running? ; * How long ago did you install Docker?; * What commands did you use to install Docker? ; * What version of Docker are you running? (The command is in the commands below.). You previously mention that `ajp1` is part of the docker group, so we can use that as guidance when we inspect the output from the commands above. Below is a set of commands if you could please run, to answer some of the questions above (except for the installation ones). For Linux usually the commands are as follows (some will provide you information, as I included them for multiple operating systems):. ```; uname -a. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. docker --version; ```. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1702017107
https://github.com/google/deepvariant/issues/184#issuecomment-1702017107:2275,Deployability,release,release,2275,"e under some circumstances. . So let's try the `mount` approach. The reason we used `--mount` instead of `-v` is because it is more granular, telling us specifically which directory it is having an issue with. In this case it is getting stuck on the `variant_calling` directory, and somehow Docker is not able to recognize it. So let's try to determine what type of directory, and what permissions it has (including above and below it). $`1)`$ So if you could please type the following commands, then we can inspect the output to determine what might be the issue:. ```; stat /tiger/home/ajp1/analysis/demography/tasmanian_devil. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs. ls -l /tiger/home/ajp1/analysis/demography | grep tasmanian_devil. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/ | grep variant_calling. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling | grep inputs; ```. $`2)`$ Now I have some minor questions about your system and Docker, in order to determine if there could be other underlying causes. For the following questions, it's okay if you don't remember everything: . * What operating system and version are your running? ; * How long ago did you install Docker?; * What commands did you use to install Docker? ; * What version of Docker are you running? (The command is in the commands below.). You previously mention that `ajp1` is part of the docker group, so we can use that as guidance when we inspect the output from the commands above. Below is a set of commands if you could please run, to answer some of the questions above (except for the installation ones). For Linux usually the commands are as follows (some will provide you information, as I included them for multiple operating systems):. ```; uname -a. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. docker --version; ```. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1702017107
https://github.com/google/deepvariant/issues/184#issuecomment-1702017107:2300,Deployability,release,release,2300,"e under some circumstances. . So let's try the `mount` approach. The reason we used `--mount` instead of `-v` is because it is more granular, telling us specifically which directory it is having an issue with. In this case it is getting stuck on the `variant_calling` directory, and somehow Docker is not able to recognize it. So let's try to determine what type of directory, and what permissions it has (including above and below it). $`1)`$ So if you could please type the following commands, then we can inspect the output to determine what might be the issue:. ```; stat /tiger/home/ajp1/analysis/demography/tasmanian_devil. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs. ls -l /tiger/home/ajp1/analysis/demography | grep tasmanian_devil. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/ | grep variant_calling. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling | grep inputs; ```. $`2)`$ Now I have some minor questions about your system and Docker, in order to determine if there could be other underlying causes. For the following questions, it's okay if you don't remember everything: . * What operating system and version are your running? ; * How long ago did you install Docker?; * What commands did you use to install Docker? ; * What version of Docker are you running? (The command is in the commands below.). You previously mention that `ajp1` is part of the docker group, so we can use that as guidance when we inspect the output from the commands above. Below is a set of commands if you could please run, to answer some of the questions above (except for the installation ones). For Linux usually the commands are as follows (some will provide you information, as I included them for multiple operating systems):. ```; uname -a. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. docker --version; ```. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1702017107
https://github.com/google/deepvariant/issues/184#issuecomment-1702017107:2321,Deployability,release,release,2321,"e under some circumstances. . So let's try the `mount` approach. The reason we used `--mount` instead of `-v` is because it is more granular, telling us specifically which directory it is having an issue with. In this case it is getting stuck on the `variant_calling` directory, and somehow Docker is not able to recognize it. So let's try to determine what type of directory, and what permissions it has (including above and below it). $`1)`$ So if you could please type the following commands, then we can inspect the output to determine what might be the issue:. ```; stat /tiger/home/ajp1/analysis/demography/tasmanian_devil. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs. ls -l /tiger/home/ajp1/analysis/demography | grep tasmanian_devil. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/ | grep variant_calling. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling | grep inputs; ```. $`2)`$ Now I have some minor questions about your system and Docker, in order to determine if there could be other underlying causes. For the following questions, it's okay if you don't remember everything: . * What operating system and version are your running? ; * How long ago did you install Docker?; * What commands did you use to install Docker? ; * What version of Docker are you running? (The command is in the commands below.). You previously mention that `ajp1` is part of the docker group, so we can use that as guidance when we inspect the output from the commands above. Below is a set of commands if you could please run, to answer some of the questions above (except for the installation ones). For Linux usually the commands are as follows (some will provide you information, as I included them for multiple operating systems):. ```; uname -a. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. docker --version; ```. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1702017107
https://github.com/google/deepvariant/issues/184#issuecomment-1702017107:1925,Usability,guid,guidance,1925,"e under some circumstances. . So let's try the `mount` approach. The reason we used `--mount` instead of `-v` is because it is more granular, telling us specifically which directory it is having an issue with. In this case it is getting stuck on the `variant_calling` directory, and somehow Docker is not able to recognize it. So let's try to determine what type of directory, and what permissions it has (including above and below it). $`1)`$ So if you could please type the following commands, then we can inspect the output to determine what might be the issue:. ```; stat /tiger/home/ajp1/analysis/demography/tasmanian_devil. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs. ls -l /tiger/home/ajp1/analysis/demography | grep tasmanian_devil. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/ | grep variant_calling. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling | grep inputs; ```. $`2)`$ Now I have some minor questions about your system and Docker, in order to determine if there could be other underlying causes. For the following questions, it's okay if you don't remember everything: . * What operating system and version are your running? ; * How long ago did you install Docker?; * What commands did you use to install Docker? ; * What version of Docker are you running? (The command is in the commands below.). You previously mention that `ajp1` is part of the docker group, so we can use that as guidance when we inspect the output from the commands above. Below is a set of commands if you could please run, to answer some of the questions above (except for the installation ones). For Linux usually the commands are as follows (some will provide you information, as I included them for multiple operating systems):. ```; uname -a. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. docker --version; ```. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/184#issuecomment-1702017107
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:70,Availability,checkpoint,checkpoint,70,"Hi @jumpyknight ,. We have also noticed that when warmstarting from a checkpoint, there seems to be dip in accuracy at the beginning. You can see an example like this in this plot in our training case study from r0.7:; https://raw.githubusercontent.com/google/deepvariant/r0.7/docs/images/TensorBoardAccuracy.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1395,Availability,checkpoint,checkpoint,1395,"uch closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU train",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:2318,Availability,checkpoint,checkpoint,2318,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:2651,Availability,robust,robustness,2651,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:813,Modifiability,variab,variables,813,"Hi @jumpyknight ,. We have also noticed that when warmstarting from a checkpoint, there seems to be dip in accuracy at the beginning. You can see an example like this in this plot in our training case study from r0.7:; https://raw.githubusercontent.com/google/deepvariant/r0.7/docs/images/TensorBoardAccuracy.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:897,Modifiability,variab,variables,897,"Hi @jumpyknight ,. We have also noticed that when warmstarting from a checkpoint, there seems to be dip in accuracy at the beginning. You can see an example like this in this plot in our training case study from r0.7:; https://raw.githubusercontent.com/google/deepvariant/r0.7/docs/images/TensorBoardAccuracy.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1287,Modifiability,variab,variables,1287,"y.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1948,Modifiability,variab,variables,1948,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1990,Modifiability,refactor,refactoring,1990,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:2190,Modifiability,variab,variables,2190,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:569,Performance,load,loaded,569,"Hi @jumpyknight ,. We have also noticed that when warmstarting from a checkpoint, there seems to be dip in accuracy at the beginning. You can see an example like this in this plot in our training case study from r0.7:; https://raw.githubusercontent.com/google/deepvariant/r0.7/docs/images/TensorBoardAccuracy.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1002,Performance,optimiz,optimizer,1002,"Hi @jumpyknight ,. We have also noticed that when warmstarting from a checkpoint, there seems to be dip in accuracy at the beginning. You can see an example like this in this plot in our training case study from r0.7:; https://raw.githubusercontent.com/google/deepvariant/r0.7/docs/images/TensorBoardAccuracy.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1328,Performance,load,loaded,1328,"y.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1563,Performance,load,load,1563,"ed here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make su",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1745,Performance,load,loading,1745,"onal] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1848,Performance,load,loads,1848," (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:2549,Performance,tune,tune,2549,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1475,Safety,predict,prediction,1475,"uch closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU train",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:2064,Testability,log,log,2064,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:2624,Testability,benchmark,benchmark,2624,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-494919509:2252,Usability,learn,learning,2252,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509
https://github.com/google/deepvariant/issues/185#issuecomment-495054046:83,Modifiability,variab,variables,83,"Thanks for the reply. I think it solves my problem.; I also agree that loading all variables is not the best, but I'd like to try the suggested code and check if the model would be the same first.; But still I think some vars like the EMA ones should be loaded in the warm-up stage, and I'll try to figure out what vars are needed.; I'll reply here if I make any further progress.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-495054046
https://github.com/google/deepvariant/issues/185#issuecomment-495054046:71,Performance,load,loading,71,"Thanks for the reply. I think it solves my problem.; I also agree that loading all variables is not the best, but I'd like to try the suggested code and check if the model would be the same first.; But still I think some vars like the EMA ones should be loaded in the warm-up stage, and I'll try to figure out what vars are needed.; I'll reply here if I make any further progress.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-495054046
https://github.com/google/deepvariant/issues/185#issuecomment-495054046:254,Performance,load,loaded,254,"Thanks for the reply. I think it solves my problem.; I also agree that loading all variables is not the best, but I'd like to try the suggested code and check if the model would be the same first.; But still I think some vars like the EMA ones should be loaded in the warm-up stage, and I'll try to figure out what vars are needed.; I'll reply here if I make any further progress.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-495054046
https://github.com/google/deepvariant/issues/185#issuecomment-495306202:85,Deployability,update,update,85,Great! Looking forward to hearing what you find. I'll close this issue. Feel free to update here or open another issue if you encounter other problems.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/185#issuecomment-495306202
https://github.com/google/deepvariant/issues/186#issuecomment-497158522:107,Deployability,release,release,107,"Hi @arostamianfar, thanks for reporting this issue. We plan to push out a fix for this problem in a future release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/186#issuecomment-497158522
https://github.com/google/deepvariant/issues/186#issuecomment-618598771:14,Deployability,patch,patched,14,Has this been patched yet?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/186#issuecomment-618598771
https://github.com/google/deepvariant/issues/186#issuecomment-618676397:254,Availability,error,error,254,"Hi @JoshuaUrrutia , thanks for checking back.; The issue has been fixed in this change:; https://github.com/google/deepvariant/commit/7ed8c6bbcfb2dc0da9b1011ba21d12791239de79. If you're using the latest version (v0.10.0), we don't expect you to see this error before. If you still observe some issues with this, please let us know by reopening this issue or filing a new one.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/186#issuecomment-618676397
https://github.com/google/deepvariant/pull/187#issuecomment-498540454:889,Security,authoriz,authorized,889,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F187) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/187#issuecomment-498540454
https://github.com/google/deepvariant/pull/187#issuecomment-498540454:957,Security,authoriz,authorized,957,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F187) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/187#issuecomment-498540454
https://github.com/google/deepvariant/pull/187#issuecomment-498540454:1234,Security,authoriz,authorized,1234,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F187) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/187#issuecomment-498540454
https://github.com/google/deepvariant/pull/187#issuecomment-498540454:1529,Security,authoriz,authorized,1529,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F187) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/187#issuecomment-498540454
https://github.com/google/deepvariant/pull/187#issuecomment-503755200:187,Deployability,release,release,187,"Thanks for the fix!; Because the way our repo is set up, I have to make this fix in our internal codebase. I have made the fix internally now and it'll come out next time we make another release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/187#issuecomment-503755200
https://github.com/google/deepvariant/issues/188#issuecomment-501453743:42,Availability,avail,available,42,"Hi dhwani2410,. That functionality is not available. If you could tell me what exactly you're trying to achieve I may have some suggestions for you. Thank you; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/188#issuecomment-501453743
https://github.com/google/deepvariant/issues/190#issuecomment-504067535:27,Availability,error,error,27,"Hello! Yes, it is a benign error. All it means is that your BAM file has a blank line in its header section. We will be removing this technically-correct but actually-pointlessly-annoying warning from future releases of DeepVariant and Nucleus.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/190#issuecomment-504067535
https://github.com/google/deepvariant/issues/190#issuecomment-504067535:208,Deployability,release,releases,208,"Hello! Yes, it is a benign error. All it means is that your BAM file has a blank line in its header section. We will be removing this technically-correct but actually-pointlessly-annoying warning from future releases of DeepVariant and Nucleus.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/190#issuecomment-504067535
https://github.com/google/deepvariant/issues/191#issuecomment-503826212:102,Integrability,wrap,wrapper,102,"Hi Phil,; it doesn't seem like this is relevant to our codebase. I suspect this is something from the wrapper from nf-core. I see that you're also asking on the nf-core GitHub issues, which is likely the right place to ask. ; If you think there might still be some issues related to the DeepVariant codebase, please feel free to add more information and reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-503826212
https://github.com/google/deepvariant/issues/191#issuecomment-504330013:280,Availability,error,error,280,"Hi,. Sorry, I could be wrong but I suspect this may be relevant to the codebase. The nf-core script is responsible for passing the parameters correctly to `dv_make_examples.py`. Here the parameters seem to have been passed correctly and the `dv_make_examples.py` script throws an error. . Do you know why this is the case? Could it be to do with the data? Is it okay that the organism is a *Plasmodium*? Do you know what the frequency file is which it is referring to?. Thanks again,; Phil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504330013
https://github.com/google/deepvariant/issues/191#issuecomment-504481029:54,Availability,error,error,54,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029
https://github.com/google/deepvariant/issues/191#issuecomment-504481029:488,Availability,error,error,488,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029
https://github.com/google/deepvariant/issues/191#issuecomment-504481029:494,Integrability,message,message,494,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029
https://github.com/google/deepvariant/issues/191#issuecomment-504481029:1068,Security,access,accessible,1068,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029
https://github.com/google/deepvariant/issues/191#issuecomment-504481029:34,Testability,log,log,34,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029
https://github.com/google/deepvariant/issues/191#issuecomment-504481029:661,Usability,clear,clear,661,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029
https://github.com/google/deepvariant/issues/191#issuecomment-504481029:979,Usability,clear,clear,979,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029
https://github.com/google/deepvariant/issues/191#issuecomment-504492772:491,Availability,robust,robust,491,"Hi Phil,; an update:; @cmclean pointed out that it comes from this line of our code:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/resources.py#L158. We're getting this information for debugging purpose only (DeepVariant outputs some information about the run in case developers need to remember how the run was done). ; I suspect your run was done on a system where the method wasn't implemented. One possible fix is to make our code more robust is to:; ```; try:; freq = psutil.cpu_freq(); return freq.current if freq is not None else 0.0; except NotImplementedError:; return 0.0; ```. We'll fix this internally soon, and it should come out next time we make a release. Thanks for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504492772
https://github.com/google/deepvariant/issues/191#issuecomment-504492772:13,Deployability,update,update,13,"Hi Phil,; an update:; @cmclean pointed out that it comes from this line of our code:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/resources.py#L158. We're getting this information for debugging purpose only (DeepVariant outputs some information about the run in case developers need to remember how the run was done). ; I suspect your run was done on a system where the method wasn't implemented. One possible fix is to make our code more robust is to:; ```; try:; freq = psutil.cpu_freq(); return freq.current if freq is not None else 0.0; except NotImplementedError:; return 0.0; ```. We'll fix this internally soon, and it should come out next time we make a release. Thanks for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504492772
https://github.com/google/deepvariant/issues/191#issuecomment-504492772:714,Deployability,release,release,714,"Hi Phil,; an update:; @cmclean pointed out that it comes from this line of our code:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/resources.py#L158. We're getting this information for debugging purpose only (DeepVariant outputs some information about the run in case developers need to remember how the run was done). ; I suspect your run was done on a system where the method wasn't implemented. One possible fix is to make our code more robust is to:; ```; try:; freq = psutil.cpu_freq(); return freq.current if freq is not None else 0.0; except NotImplementedError:; return 0.0; ```. We'll fix this internally soon, and it should come out next time we make a release. Thanks for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/191#issuecomment-504492772
https://github.com/google/deepvariant/issues/192#issuecomment-506979241:69,Availability,checkpoint,checkpoint,69,The number is referring to the number of steps in training when this checkpoint is saved.; You can see https://www.tensorflow.org/guide/checkpoints for more information.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/192#issuecomment-506979241
https://github.com/google/deepvariant/issues/192#issuecomment-506979241:136,Availability,checkpoint,checkpoints,136,The number is referring to the number of steps in training when this checkpoint is saved.; You can see https://www.tensorflow.org/guide/checkpoints for more information.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/192#issuecomment-506979241
https://github.com/google/deepvariant/issues/192#issuecomment-506979241:130,Usability,guid,guide,130,The number is referring to the number of steps in training when this checkpoint is saved.; You can see https://www.tensorflow.org/guide/checkpoints for more information.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/192#issuecomment-506979241
https://github.com/google/deepvariant/issues/192#issuecomment-507247544:423,Testability,log,logs,423,"I am working with singularity on HPC and it is so slow compared with DeepVariant docker image. It took 3 days to reach model.ckpt-4854. Do you have any recommendations to change some parameters (like the batch size, keep_checkpoint_every_n_hours, or save_interval_secs) in order to speed up model_train? ; ; The parameters I am using is here: . `INPUT_DIR=""${PWD}/input""; OUTPUT_DIR=""${PWD}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; OUTPUT_DIR_TRAINING = ""${OUTPUT_DIR}/training_output""; mkdir -p ""{OUTPUT_DIR_TRAINING}""; WES_PRETRAINED_MODEL = ""${PWD}/models/wes/model.ckpt"". singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ --bind; input:${OUTPUT_DIR}/ \; deepvariant.simg \; /opt/deepvariant/bin//model_train \; --dataset_config_pbtxt = ""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \; --train_dir = ""${OUTPUT_DIR_TRAINING}"" \; --keep_checkpoint_every_n_hours = 0.05 \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.01 \; --start_from_checkpoint = ""${WES_PRETRAINED_MODEL}"" >; ""${LOG_DIR}/train.log"" 2>&1 &`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/192#issuecomment-507247544
https://github.com/google/deepvariant/issues/192#issuecomment-507247544:1085,Testability,log,log,1085,"I am working with singularity on HPC and it is so slow compared with DeepVariant docker image. It took 3 days to reach model.ckpt-4854. Do you have any recommendations to change some parameters (like the batch size, keep_checkpoint_every_n_hours, or save_interval_secs) in order to speed up model_train? ; ; The parameters I am using is here: . `INPUT_DIR=""${PWD}/input""; OUTPUT_DIR=""${PWD}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; OUTPUT_DIR_TRAINING = ""${OUTPUT_DIR}/training_output""; mkdir -p ""{OUTPUT_DIR_TRAINING}""; WES_PRETRAINED_MODEL = ""${PWD}/models/wes/model.ckpt"". singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ --bind; input:${OUTPUT_DIR}/ \; deepvariant.simg \; /opt/deepvariant/bin//model_train \; --dataset_config_pbtxt = ""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" \; --train_dir = ""${OUTPUT_DIR_TRAINING}"" \; --keep_checkpoint_every_n_hours = 0.05 \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.01 \; --start_from_checkpoint = ""${WES_PRETRAINED_MODEL}"" >; ""${LOG_DIR}/train.log"" 2>&1 &`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/192#issuecomment-507247544
https://github.com/google/deepvariant/issues/192#issuecomment-507491998:200,Energy Efficiency,power,powerful,200,"Hi @melkerdawy ; are you using CPUs only? There is a reason why our current training case study uses TPU. If you can't run with TPU, at least considering using GPUs to train. Fundamentally you need a powerful enough processing unit to train an inception v3 model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/192#issuecomment-507491998
https://github.com/google/deepvariant/issues/192#issuecomment-507682235:129,Availability,avail,available,129,"I will try to use GPUs. Thanks for the advice. I have one more question. I just wanted to make sure that model_train use all the available CPUs or GPUs in the machine I am using by default. In other words, If I dont use GNU parallel with model_train and model_eval, it wont utilize only one core by default.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/192#issuecomment-507682235
https://github.com/google/deepvariant/issues/192#issuecomment-507880713:623,Energy Efficiency,power,powerful,623,"Hi @melkerdawy ,; currently our code doesn't support training with multiple GPUs. I haven't really tried training with CPUs, so I don't know whether model_train automatically utilize multiple CPUs or not. (We know that `call_variants` does. But I've never used CPU for training.); If you want, you can try running it and see how many CPUs it utilizes. However, I don't recommend training with CPUs becaues I think it'll be really slow. And, using GPU parallel with training also won't work. In the case if training, it's not easily parallelizable like inference. The most practical path forward is probably to get a really powerful GPU and see how well that works for you. I've been hoping to benchmark for training on GPU as well, but haven't got time to do so. If you have some results you want to share, that will be great.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/192#issuecomment-507880713
https://github.com/google/deepvariant/issues/192#issuecomment-507880713:693,Testability,benchmark,benchmark,693,"Hi @melkerdawy ,; currently our code doesn't support training with multiple GPUs. I haven't really tried training with CPUs, so I don't know whether model_train automatically utilize multiple CPUs or not. (We know that `call_variants` does. But I've never used CPU for training.); If you want, you can try running it and see how many CPUs it utilizes. However, I don't recommend training with CPUs becaues I think it'll be really slow. And, using GPU parallel with training also won't work. In the case if training, it's not easily parallelizable like inference. The most practical path forward is probably to get a really powerful GPU and see how well that works for you. I've been hoping to benchmark for training on GPU as well, but haven't got time to do so. If you have some results you want to share, that will be great.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/192#issuecomment-507880713
https://github.com/google/deepvariant/issues/193#issuecomment-508203315:520,Usability,simpl,simple,520,"> Is there a way that I could just modify the gcp_deepvariant_runner.py script . Yes, you can easily make deepvariant_runner to generate gVCF output by using `--gvcf_outfile` flag, please refer to [our documentation](https://cloud.google.com/genomics/docs/tutorials/deepvariant#genomic_vcf_gvcf_configuration) for more details. > I'm guessing I would need to fork the gcp-deepvariant-runner repo. That's one way to do it, however, the easier way is to use our docker image; in that case launching DeepVariant will be as simple as running a `gcloud ...` command. Again please refer to [our documentations](https://cloud.google.com/genomics/docs/tutorials/deepvariant#before_you_begin) for more details.; The only issue at the moment is that deepvariant_runner is still working with previous version of DeepVariant (0.7.2) and we are in the process of releasing a new docker image that will be compatible with the latest DeepVairant (0.8.8). Please let me know if you have any difficulties with deepvariant_runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/193#issuecomment-508203315
https://github.com/google/deepvariant/issues/194#issuecomment-508186152:58,Testability,log,logs,58,"Hi, can you share the commands you ran and the associated logs so we can take a look?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/194#issuecomment-508186152
https://github.com/google/deepvariant/issues/194#issuecomment-508325465:199,Security,validat,validation,199,"![train_loss_and_error](https://user-images.githubusercontent.com/13111474/60637174-f2844000-9e4b-11e9-9ed0-6461041cf407.png); Hi, I looked in my log file and plotted a figure of training losses and validation f-measures, and it turned out that I might have overlooked the improves in the trained models. The improvments after 10000 steps become very small (still important though) compared with that in the first steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/194#issuecomment-508325465
https://github.com/google/deepvariant/issues/194#issuecomment-508325465:146,Testability,log,log,146,"![train_loss_and_error](https://user-images.githubusercontent.com/13111474/60637174-f2844000-9e4b-11e9-9ed0-6461041cf407.png); Hi, I looked in my log file and plotted a figure of training losses and validation f-measures, and it turned out that I might have overlooked the improves in the trained models. The improvments after 10000 steps become very small (still important though) compared with that in the first steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/194#issuecomment-508325465
https://github.com/google/deepvariant/issues/194#issuecomment-509337028:157,Usability,simpl,simple,157,"It's normal for the loss curve to start flattening out, although yours does change pretty abruptly. What exactly did you change (you mentioned using a `very simple topology`, not sure if that means you changed the architecture or something)? Also the commands you used would be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/194#issuecomment-509337028
https://github.com/google/deepvariant/issues/194#issuecomment-513082707:44,Usability,simpl,simple,44,"Thanks for the comments. I tried to train a simple 10-cnn-layer architecture and got the above results. Although the final loss still seems to be rather high, the accuracy is acceptable for the simple cnn network. I will close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/194#issuecomment-513082707
https://github.com/google/deepvariant/issues/194#issuecomment-513082707:194,Usability,simpl,simple,194,"Thanks for the comments. I tried to train a simple 10-cnn-layer architecture and got the above results. Although the final loss still seems to be rather high, the accuracy is acceptable for the simple cnn network. I will close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/194#issuecomment-513082707
https://github.com/google/deepvariant/issues/195#issuecomment-509390976:28,Modifiability,variab,variable,28,"You need to specify all the variable in the same script. For example:. ```; #!/usr/bin/zsh; BIN_VERSION=""0.8.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/195#issuecomment-509390976
https://github.com/google/deepvariant/issues/195#issuecomment-509390976:142,Testability,test,testdata,142,"You need to specify all the variable in the same script. For example:. ```; #!/usr/bin/zsh; BIN_VERSION=""0.8.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/195#issuecomment-509390976
https://github.com/google/deepvariant/issues/195#issuecomment-509390976:222,Testability,test,testdata,222,"You need to specify all the variable in the same script. For example:. ```; #!/usr/bin/zsh; BIN_VERSION=""0.8.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/195#issuecomment-509390976
https://github.com/google/deepvariant/issues/196#issuecomment-511731809:604,Availability,mask,mask,604,"Hi @kokyriakidis. Thank you for your question. There are a few possibilities for this. First, chrX and chrY do share regions of homology, the pseudo autosomal regions (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2435358/), which allows them to pair and segregate appropriately during meiosis. Because the reference sequence for both chrX and chrY contains these sequences, even in a female individual, reads will map to either and this can manifest as HET calls. It could be good to look at whether these calls occur in PAR regions. This will depend on which reference you are using, as some references mask this region. Can you tell us the reference build that you are using?. Second, reads may end up mismapped onto chrY from autosomes. When mismapping occurs, there is signal that will look like variation. The variant callers are not told whether a sample is male or female, and have to judge whether the signal is consistent with a variant or if the sample is female. Errors in this process can occur. How many variants are you seeing as PASS, it could be illustrative to compare this number to other benchmark human samples. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/196#issuecomment-511731809
https://github.com/google/deepvariant/issues/196#issuecomment-511731809:974,Availability,Error,Errors,974,"Hi @kokyriakidis. Thank you for your question. There are a few possibilities for this. First, chrX and chrY do share regions of homology, the pseudo autosomal regions (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2435358/), which allows them to pair and segregate appropriately during meiosis. Because the reference sequence for both chrX and chrY contains these sequences, even in a female individual, reads will map to either and this can manifest as HET calls. It could be good to look at whether these calls occur in PAR regions. This will depend on which reference you are using, as some references mask this region. Can you tell us the reference build that you are using?. Second, reads may end up mismapped onto chrY from autosomes. When mismapping occurs, there is signal that will look like variation. The variant callers are not told whether a sample is male or female, and have to judge whether the signal is consistent with a variant or if the sample is female. Errors in this process can occur. How many variants are you seeing as PASS, it could be illustrative to compare this number to other benchmark human samples. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/196#issuecomment-511731809
https://github.com/google/deepvariant/issues/196#issuecomment-511731809:544,Integrability,depend,depend,544,"Hi @kokyriakidis. Thank you for your question. There are a few possibilities for this. First, chrX and chrY do share regions of homology, the pseudo autosomal regions (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2435358/), which allows them to pair and segregate appropriately during meiosis. Because the reference sequence for both chrX and chrY contains these sequences, even in a female individual, reads will map to either and this can manifest as HET calls. It could be good to look at whether these calls occur in PAR regions. This will depend on which reference you are using, as some references mask this region. Can you tell us the reference build that you are using?. Second, reads may end up mismapped onto chrY from autosomes. When mismapping occurs, there is signal that will look like variation. The variant callers are not told whether a sample is male or female, and have to judge whether the signal is consistent with a variant or if the sample is female. Errors in this process can occur. How many variants are you seeing as PASS, it could be illustrative to compare this number to other benchmark human samples. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/196#issuecomment-511731809
https://github.com/google/deepvariant/issues/196#issuecomment-511731809:1107,Testability,benchmark,benchmark,1107,"Hi @kokyriakidis. Thank you for your question. There are a few possibilities for this. First, chrX and chrY do share regions of homology, the pseudo autosomal regions (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2435358/), which allows them to pair and segregate appropriately during meiosis. Because the reference sequence for both chrX and chrY contains these sequences, even in a female individual, reads will map to either and this can manifest as HET calls. It could be good to look at whether these calls occur in PAR regions. This will depend on which reference you are using, as some references mask this region. Can you tell us the reference build that you are using?. Second, reads may end up mismapped onto chrY from autosomes. When mismapping occurs, there is signal that will look like variation. The variant callers are not told whether a sample is male or female, and have to judge whether the signal is consistent with a variant or if the sample is female. Errors in this process can occur. How many variants are you seeing as PASS, it could be illustrative to compare this number to other benchmark human samples. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/196#issuecomment-511731809
https://github.com/google/deepvariant/issues/196#issuecomment-512585902:51,Availability,mask,masked,51,I used the right one for hg38 with the PAR regions masked as stated here:; ```; http://lh3.github.io/2017/11/13/which-human-reference-genome-to-use; ```; I see 401 variants on Y out of total 117.009,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/196#issuecomment-512585902
https://github.com/google/deepvariant/issues/196#issuecomment-513911038:41,Availability,mask,mask,41,"Hi @kokyriakidis . That reference should mask the PAR regions. It's unclear what could be going on for those variants which DeepVariant calls. They may be FP variants in difficult regions, or could be positions where additional copies exist that are similar on autosomes. In general, there are always likely to be some calls made on chrY. For reference, the New York Genome Center has sequenced all 1000genomes at 30x coverage and mapped to hg38. For the NA12878 (a female sample), GATK4 makes 6,935 heterozygous variant calls and 6,422 homozygous variant calls. (This VCF file is available at: http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/20190425_NYGC_GATK/raw_calls/CEU/Sample_NA12878/analysis/NA12878.haplotypeCalls.er.raw.vcf.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/196#issuecomment-513911038
https://github.com/google/deepvariant/issues/196#issuecomment-513911038:581,Availability,avail,available,581,"Hi @kokyriakidis . That reference should mask the PAR regions. It's unclear what could be going on for those variants which DeepVariant calls. They may be FP variants in difficult regions, or could be positions where additional copies exist that are similar on autosomes. In general, there are always likely to be some calls made on chrY. For reference, the New York Genome Center has sequenced all 1000genomes at 30x coverage and mapped to hg38. For the NA12878 (a female sample), GATK4 makes 6,935 heterozygous variant calls and 6,422 homozygous variant calls. (This VCF file is available at: http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/20190425_NYGC_GATK/raw_calls/CEU/Sample_NA12878/analysis/NA12878.haplotypeCalls.er.raw.vcf.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/196#issuecomment-513911038
https://github.com/google/deepvariant/issues/197#issuecomment-512061556:106,Testability,log,logic,106,"Hi @anands-repo ,; to your original question about how the multiple images are combined, you can find the logic in https://github.com/google/deepvariant/blob/r0.8/deepvariant/postprocess_variants.py; Specifically, the function `merge_predictions` is the one that takes multiple images for multi-allelic cases and merge them.; As you can see, the comment in that function also refers to: ""See the logic described in the class PileupImageCreator pileup_image.py"", which is probably the logic you found. For your last question, I'll have to read and reply later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512061556
https://github.com/google/deepvariant/issues/197#issuecomment-512061556:396,Testability,log,logic,396,"Hi @anands-repo ,; to your original question about how the multiple images are combined, you can find the logic in https://github.com/google/deepvariant/blob/r0.8/deepvariant/postprocess_variants.py; Specifically, the function `merge_predictions` is the one that takes multiple images for multi-allelic cases and merge them.; As you can see, the comment in that function also refers to: ""See the logic described in the class PileupImageCreator pileup_image.py"", which is probably the logic you found. For your last question, I'll have to read and reply later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512061556
https://github.com/google/deepvariant/issues/197#issuecomment-512061556:484,Testability,log,logic,484,"Hi @anands-repo ,; to your original question about how the multiple images are combined, you can find the logic in https://github.com/google/deepvariant/blob/r0.8/deepvariant/postprocess_variants.py; Specifically, the function `merge_predictions` is the one that takes multiple images for multi-allelic cases and merge them.; As you can see, the comment in that function also refers to: ""See the logic described in the class PileupImageCreator pileup_image.py"", which is probably the logic you found. For your last question, I'll have to read and reply later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512061556
https://github.com/google/deepvariant/issues/197#issuecomment-512112524:383,Availability,error,error,383,"That paper is an early version of the code - so some things changed and some didn't. So let's parse this out:. 1) In the blog, if you look at [Jason's Jupyter notebooks](https://github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the genera",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524
https://github.com/google/deepvariant/issues/197#issuecomment-512112524:1649,Availability,error,error,1649,"/github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the general idea behind DeepVariant. The [details doc](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details.md) might help, but it's a bit general. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524
https://github.com/google/deepvariant/issues/197#issuecomment-512112524:866,Deployability,update,update,866,"That paper is an early version of the code - so some things changed and some didn't. So let's parse this out:. 1) In the blog, if you look at [Jason's Jupyter notebooks](https://github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the genera",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524
https://github.com/google/deepvariant/issues/197#issuecomment-512112524:644,Performance,optimiz,optimized,644,"That paper is an early version of the code - so some things changed and some didn't. So let's parse this out:. 1) In the blog, if you look at [Jason's Jupyter notebooks](https://github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the genera",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524
https://github.com/google/deepvariant/issues/197#issuecomment-512112524:1099,Performance,perform,performed,1099,"1) In the blog, if you look at [Jason's Jupyter notebooks](https://github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the general idea behind DeepVariant. The [details doc](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524
https://github.com/google/deepvariant/issues/197#issuecomment-512112524:1416,Safety,predict,predict,1416,"/github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the general idea behind DeepVariant. The [details doc](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details.md) might help, but it's a bit general. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524
https://github.com/google/deepvariant/issues/197#issuecomment-512112524:1750,Safety,predict,predict,1750,"/github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the general idea behind DeepVariant. The [details doc](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details.md) might help, but it's a bit general. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524
https://github.com/google/deepvariant/issues/197#issuecomment-512112524:1678,Usability,learn,learning,1678,"/github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the general idea behind DeepVariant. The [details doc](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details.md) might help, but it's a bit general. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524
https://github.com/google/deepvariant/issues/197#issuecomment-512120721:823,Deployability,release,release,823,"Overall, I feel like this thread has mentioned a few different things that in my mind are pretty different. Realigner, ( @pgrosu 's pointer 2. 4. above) , which does a local realignment, which was proven helpful especially for our Indel accuracy. Note that this was turned off by default for PacBio model. This is something that @akolesnikov will be able to answer more questions on. How candidates are proposed (which seems to be the main question at the initial question) is based on set of thresholds, which can be adjusted with these flags:; https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py#L189-L202; If you change these thresholds in your make_examples during calling, you will be able to get different number of candidates proposed in the `make_examples` stage. But note that our default release models are trained with examples based on the default thresholds. @pgrosu 's pointer 6 above is the pointer of how these initial set of candidates are proposed before they're passed to the classifier (used in `call_variants` step). And in fact, if you look at the final output VCF at the end, it should include all the candidates. If the classifier decided a candidate is not actually a variant, it will get a `0/0` genotype and have ""RefCall"" in its FILTER field. @anands-repo , I'm not sure if my answer and Paul's answer cover everything you want to ask now. If not, feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512120721
https://github.com/google/deepvariant/issues/197#issuecomment-512127253:194,Performance,perform,performing,194,"@pichuan @pgrosu This improves my understanding substantially. After reading the paper, my initial impression was that DeepVariant consists of two parts - one part following traditional callers performing local assembly, haplotype detection and candidate allele generation based on the top-two haplotypes, and a second part which is a DNN used to perform candidate filtration. In such a scheme, the second part would be presented a single image per site and it makes a determination among homozygous-ref, heterozygous, and homozygous-alt in an allele-agnostic way - hence it needs a single image per site. In such a caller, 3 allele candidates at a site wouldn't be possible. I was also surprised that such a method could outperform all others, since this approach simply (approximately) replaces the VQSR step in GATK or the RandomForest in Strelka, and a lot would be riding on heuristic-based algorithms for candidate selection. I understand now that all (reasonable) candidates at a site are evaluated and scored against each other using the DNN, which seems to me to be a much more satisfying approach. I am still interested in how different predictions are combined into a single variant quality score, since they are not normalized; hence my request for guidance towards the code source that combines multiple predictions into a variant call. I will go through these resources over the next day. Thanks a lot! Much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512127253
https://github.com/google/deepvariant/issues/197#issuecomment-512127253:347,Performance,perform,perform,347,"@pichuan @pgrosu This improves my understanding substantially. After reading the paper, my initial impression was that DeepVariant consists of two parts - one part following traditional callers performing local assembly, haplotype detection and candidate allele generation based on the top-two haplotypes, and a second part which is a DNN used to perform candidate filtration. In such a scheme, the second part would be presented a single image per site and it makes a determination among homozygous-ref, heterozygous, and homozygous-alt in an allele-agnostic way - hence it needs a single image per site. In such a caller, 3 allele candidates at a site wouldn't be possible. I was also surprised that such a method could outperform all others, since this approach simply (approximately) replaces the VQSR step in GATK or the RandomForest in Strelka, and a lot would be riding on heuristic-based algorithms for candidate selection. I understand now that all (reasonable) candidates at a site are evaluated and scored against each other using the DNN, which seems to me to be a much more satisfying approach. I am still interested in how different predictions are combined into a single variant quality score, since they are not normalized; hence my request for guidance towards the code source that combines multiple predictions into a variant call. I will go through these resources over the next day. Thanks a lot! Much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512127253
https://github.com/google/deepvariant/issues/197#issuecomment-512127253:231,Safety,detect,detection,231,"@pichuan @pgrosu This improves my understanding substantially. After reading the paper, my initial impression was that DeepVariant consists of two parts - one part following traditional callers performing local assembly, haplotype detection and candidate allele generation based on the top-two haplotypes, and a second part which is a DNN used to perform candidate filtration. In such a scheme, the second part would be presented a single image per site and it makes a determination among homozygous-ref, heterozygous, and homozygous-alt in an allele-agnostic way - hence it needs a single image per site. In such a caller, 3 allele candidates at a site wouldn't be possible. I was also surprised that such a method could outperform all others, since this approach simply (approximately) replaces the VQSR step in GATK or the RandomForest in Strelka, and a lot would be riding on heuristic-based algorithms for candidate selection. I understand now that all (reasonable) candidates at a site are evaluated and scored against each other using the DNN, which seems to me to be a much more satisfying approach. I am still interested in how different predictions are combined into a single variant quality score, since they are not normalized; hence my request for guidance towards the code source that combines multiple predictions into a variant call. I will go through these resources over the next day. Thanks a lot! Much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512127253
https://github.com/google/deepvariant/issues/197#issuecomment-512127253:1147,Safety,predict,predictions,1147,"@pichuan @pgrosu This improves my understanding substantially. After reading the paper, my initial impression was that DeepVariant consists of two parts - one part following traditional callers performing local assembly, haplotype detection and candidate allele generation based on the top-two haplotypes, and a second part which is a DNN used to perform candidate filtration. In such a scheme, the second part would be presented a single image per site and it makes a determination among homozygous-ref, heterozygous, and homozygous-alt in an allele-agnostic way - hence it needs a single image per site. In such a caller, 3 allele candidates at a site wouldn't be possible. I was also surprised that such a method could outperform all others, since this approach simply (approximately) replaces the VQSR step in GATK or the RandomForest in Strelka, and a lot would be riding on heuristic-based algorithms for candidate selection. I understand now that all (reasonable) candidates at a site are evaluated and scored against each other using the DNN, which seems to me to be a much more satisfying approach. I am still interested in how different predictions are combined into a single variant quality score, since they are not normalized; hence my request for guidance towards the code source that combines multiple predictions into a variant call. I will go through these resources over the next day. Thanks a lot! Much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512127253
https://github.com/google/deepvariant/issues/197#issuecomment-512127253:1317,Safety,predict,predictions,1317,"@pichuan @pgrosu This improves my understanding substantially. After reading the paper, my initial impression was that DeepVariant consists of two parts - one part following traditional callers performing local assembly, haplotype detection and candidate allele generation based on the top-two haplotypes, and a second part which is a DNN used to perform candidate filtration. In such a scheme, the second part would be presented a single image per site and it makes a determination among homozygous-ref, heterozygous, and homozygous-alt in an allele-agnostic way - hence it needs a single image per site. In such a caller, 3 allele candidates at a site wouldn't be possible. I was also surprised that such a method could outperform all others, since this approach simply (approximately) replaces the VQSR step in GATK or the RandomForest in Strelka, and a lot would be riding on heuristic-based algorithms for candidate selection. I understand now that all (reasonable) candidates at a site are evaluated and scored against each other using the DNN, which seems to me to be a much more satisfying approach. I am still interested in how different predictions are combined into a single variant quality score, since they are not normalized; hence my request for guidance towards the code source that combines multiple predictions into a variant call. I will go through these resources over the next day. Thanks a lot! Much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512127253
https://github.com/google/deepvariant/issues/197#issuecomment-512127253:765,Usability,simpl,simply,765,"@pichuan @pgrosu This improves my understanding substantially. After reading the paper, my initial impression was that DeepVariant consists of two parts - one part following traditional callers performing local assembly, haplotype detection and candidate allele generation based on the top-two haplotypes, and a second part which is a DNN used to perform candidate filtration. In such a scheme, the second part would be presented a single image per site and it makes a determination among homozygous-ref, heterozygous, and homozygous-alt in an allele-agnostic way - hence it needs a single image per site. In such a caller, 3 allele candidates at a site wouldn't be possible. I was also surprised that such a method could outperform all others, since this approach simply (approximately) replaces the VQSR step in GATK or the RandomForest in Strelka, and a lot would be riding on heuristic-based algorithms for candidate selection. I understand now that all (reasonable) candidates at a site are evaluated and scored against each other using the DNN, which seems to me to be a much more satisfying approach. I am still interested in how different predictions are combined into a single variant quality score, since they are not normalized; hence my request for guidance towards the code source that combines multiple predictions into a variant call. I will go through these resources over the next day. Thanks a lot! Much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512127253
https://github.com/google/deepvariant/issues/197#issuecomment-512127253:1261,Usability,guid,guidance,1261,"@pichuan @pgrosu This improves my understanding substantially. After reading the paper, my initial impression was that DeepVariant consists of two parts - one part following traditional callers performing local assembly, haplotype detection and candidate allele generation based on the top-two haplotypes, and a second part which is a DNN used to perform candidate filtration. In such a scheme, the second part would be presented a single image per site and it makes a determination among homozygous-ref, heterozygous, and homozygous-alt in an allele-agnostic way - hence it needs a single image per site. In such a caller, 3 allele candidates at a site wouldn't be possible. I was also surprised that such a method could outperform all others, since this approach simply (approximately) replaces the VQSR step in GATK or the RandomForest in Strelka, and a lot would be riding on heuristic-based algorithms for candidate selection. I understand now that all (reasonable) candidates at a site are evaluated and scored against each other using the DNN, which seems to me to be a much more satisfying approach. I am still interested in how different predictions are combined into a single variant quality score, since they are not normalized; hence my request for guidance towards the code source that combines multiple predictions into a variant call. I will go through these resources over the next day. Thanks a lot! Much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/197#issuecomment-512127253
https://github.com/google/deepvariant/issues/198#issuecomment-512031369:211,Availability,checkpoint,checkpoints,211,"Hi,. Evaluation is done using [Estimator class](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate). You may create a simple script calling Estimator.evaluate in a loop passing different checkpoints. > checkpoint_path: Path of a specific checkpoint to evaluate. If None, the latest checkpoint in model_dir is used. If there are no checkpoints in model_dir, evaluation is run with newly initialized Variables instead of ones restored from checkpoint. Hope it helps.; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/198#issuecomment-512031369
https://github.com/google/deepvariant/issues/198#issuecomment-512031369:262,Availability,checkpoint,checkpoint,262,"Hi,. Evaluation is done using [Estimator class](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate). You may create a simple script calling Estimator.evaluate in a loop passing different checkpoints. > checkpoint_path: Path of a specific checkpoint to evaluate. If None, the latest checkpoint in model_dir is used. If there are no checkpoints in model_dir, evaluation is run with newly initialized Variables instead of ones restored from checkpoint. Hope it helps.; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/198#issuecomment-512031369
https://github.com/google/deepvariant/issues/198#issuecomment-512031369:306,Availability,checkpoint,checkpoint,306,"Hi,. Evaluation is done using [Estimator class](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate). You may create a simple script calling Estimator.evaluate in a loop passing different checkpoints. > checkpoint_path: Path of a specific checkpoint to evaluate. If None, the latest checkpoint in model_dir is used. If there are no checkpoints in model_dir, evaluation is run with newly initialized Variables instead of ones restored from checkpoint. Hope it helps.; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/198#issuecomment-512031369
https://github.com/google/deepvariant/issues/198#issuecomment-512031369:355,Availability,checkpoint,checkpoints,355,"Hi,. Evaluation is done using [Estimator class](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate). You may create a simple script calling Estimator.evaluate in a loop passing different checkpoints. > checkpoint_path: Path of a specific checkpoint to evaluate. If None, the latest checkpoint in model_dir is used. If there are no checkpoints in model_dir, evaluation is run with newly initialized Variables instead of ones restored from checkpoint. Hope it helps.; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/198#issuecomment-512031369
https://github.com/google/deepvariant/issues/198#issuecomment-512031369:462,Availability,checkpoint,checkpoint,462,"Hi,. Evaluation is done using [Estimator class](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate). You may create a simple script calling Estimator.evaluate in a loop passing different checkpoints. > checkpoint_path: Path of a specific checkpoint to evaluate. If None, the latest checkpoint in model_dir is used. If there are no checkpoints in model_dir, evaluation is run with newly initialized Variables instead of ones restored from checkpoint. Hope it helps.; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/198#issuecomment-512031369
https://github.com/google/deepvariant/issues/198#issuecomment-512031369:422,Modifiability,Variab,Variables,422,"Hi,. Evaluation is done using [Estimator class](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate). You may create a simple script calling Estimator.evaluate in a loop passing different checkpoints. > checkpoint_path: Path of a specific checkpoint to evaluate. If None, the latest checkpoint in model_dir is used. If there are no checkpoints in model_dir, evaluation is run with newly initialized Variables instead of ones restored from checkpoint. Hope it helps.; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/198#issuecomment-512031369
https://github.com/google/deepvariant/issues/198#issuecomment-512031369:142,Usability,simpl,simple,142,"Hi,. Evaluation is done using [Estimator class](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate). You may create a simple script calling Estimator.evaluate in a loop passing different checkpoints. > checkpoint_path: Path of a specific checkpoint to evaluate. If None, the latest checkpoint in model_dir is used. If there are no checkpoints in model_dir, evaluation is run with newly initialized Variables instead of ones restored from checkpoint. Hope it helps.; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/198#issuecomment-512031369
https://github.com/google/deepvariant/issues/199#issuecomment-513999168:32,Deployability,install,installed,32,What Python version do you have installed?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-513999168
https://github.com/google/deepvariant/issues/199#issuecomment-514034399:107,Deployability,install,install,107,"Thanks @akolesnikov for responding! . I'm running Python `Python 2.7.15+`. This was on a fresh Ubuntu 14.4 install, using your default settings in DeepVariant v0.8/master and running `./build-prereq.sh` followed by `./build_and_test.sh`. What I'm confused by is that `bazel-bin/deepvariant/make_examples_test` runs and everything passes... ```; $ bazel-bin/deepvariant/make_examples_test; Running tests under Python 2.7.15: /usr/bin/python; ...; .; ----------------------------------------------------------------------; Ran 101 tests in 6.501s. OK; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514034399
https://github.com/google/deepvariant/issues/199#issuecomment-514034399:397,Testability,test,tests,397,"Thanks @akolesnikov for responding! . I'm running Python `Python 2.7.15+`. This was on a fresh Ubuntu 14.4 install, using your default settings in DeepVariant v0.8/master and running `./build-prereq.sh` followed by `./build_and_test.sh`. What I'm confused by is that `bazel-bin/deepvariant/make_examples_test` runs and everything passes... ```; $ bazel-bin/deepvariant/make_examples_test; Running tests under Python 2.7.15: /usr/bin/python; ...; .; ----------------------------------------------------------------------; Ran 101 tests in 6.501s. OK; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514034399
https://github.com/google/deepvariant/issues/199#issuecomment-514034399:529,Testability,test,tests,529,"Thanks @akolesnikov for responding! . I'm running Python `Python 2.7.15+`. This was on a fresh Ubuntu 14.4 install, using your default settings in DeepVariant v0.8/master and running `./build-prereq.sh` followed by `./build_and_test.sh`. What I'm confused by is that `bazel-bin/deepvariant/make_examples_test` runs and everything passes... ```; $ bazel-bin/deepvariant/make_examples_test; Running tests under Python 2.7.15: /usr/bin/python; ...; .; ----------------------------------------------------------------------; Ran 101 tests in 6.501s. OK; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514034399
https://github.com/google/deepvariant/issues/199#issuecomment-514348301:252,Testability,test,test,252,"@akolesnikov do you have any suggestions for other commands to try? Roll back to previous version? Different parameters passed to the function? Other ways to run the Python code? I am not very familair with `Bazel` to be honest. Just confusing how the test pass, and *same exact* command works when running from pulled Docker container... . Happy to provide more details. Any ideas? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514348301
https://github.com/google/deepvariant/issues/199#issuecomment-514360900:14,Testability,test,test,14,Our build and test was tested on Ubuntu 16.4. I can do a quick test run on Ubuntu 14.4.; Can you remind us what version of `bazel` you're using?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514360900
https://github.com/google/deepvariant/issues/199#issuecomment-514360900:23,Testability,test,tested,23,Our build and test was tested on Ubuntu 16.4. I can do a quick test run on Ubuntu 14.4.; Can you remind us what version of `bazel` you're using?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514360900
https://github.com/google/deepvariant/issues/199#issuecomment-514360900:63,Testability,test,test,63,Our build and test was tested on Ubuntu 16.4. I can do a quick test run on Ubuntu 14.4.; Can you remind us what version of `bazel` you're using?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514360900
https://github.com/google/deepvariant/issues/199#issuecomment-514397821:8,Deployability,release,release,8,"`[bazel release 0.21.0]` -- should I try a more recent version?. I believe this was prescribed by default `settings.sh`.; https://github.com/google/deepvariant/blob/r0.8/settings.sh. Is there a good review stable release from which to try? Sorry. Really at a loss here, as tests pass.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514397821
https://github.com/google/deepvariant/issues/199#issuecomment-514397821:213,Deployability,release,release,213,"`[bazel release 0.21.0]` -- should I try a more recent version?. I believe this was prescribed by default `settings.sh`.; https://github.com/google/deepvariant/blob/r0.8/settings.sh. Is there a good review stable release from which to try? Sorry. Really at a loss here, as tests pass.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514397821
https://github.com/google/deepvariant/issues/199#issuecomment-514397821:273,Testability,test,tests,273,"`[bazel release 0.21.0]` -- should I try a more recent version?. I believe this was prescribed by default `settings.sh`.; https://github.com/google/deepvariant/blob/r0.8/settings.sh. Is there a good review stable release from which to try? Sorry. Really at a loss here, as tests pass.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514397821
https://github.com/google/deepvariant/issues/199#issuecomment-514398912:143,Availability,down,downgrade,143,"I've also tried on Ubuntu 18.04 and getting similar problems. Have you tried these test on systems other than 16.04, or should I really try to downgrade to that exact version? Thanks!. `Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.15.0-54-generic x86_64)`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514398912
https://github.com/google/deepvariant/issues/199#issuecomment-514398912:83,Testability,test,test,83,"I've also tried on Ubuntu 18.04 and getting similar problems. Have you tried these test on systems other than 16.04, or should I really try to downgrade to that exact version? Thanks!. `Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.15.0-54-generic x86_64)`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514398912
https://github.com/google/deepvariant/issues/199#issuecomment-514424604:101,Availability,error,error,101,"We're consulting with our teammate @ThomasColthurst to see if he has anything to add here about your error message. In the past, I did try building with both Ubuntu 14 and 18 and didn't encounter such issue, but I have not done so recently. I'll try soon. And bazel - I think 0.21 is the right version we're using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514424604
https://github.com/google/deepvariant/issues/199#issuecomment-514424604:107,Integrability,message,message,107,"We're consulting with our teammate @ThomasColthurst to see if he has anything to add here about your error message. In the past, I did try building with both Ubuntu 14 and 18 and didn't encounter such issue, but I have not done so recently. I'll try soon. And bazel - I think 0.21 is the right version we're using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514424604
https://github.com/google/deepvariant/issues/199#issuecomment-514472365:644,Deployability,release,release,644,"Quick answer: Can you try `./build_release_binaries.sh` instead of `./build_and_test.sh` in your steps above?; Then it should work.; The issue you're encountering has nothing to do with Ubuntu 18. ----. More details:. Earlier today, @akolesnikov and I were just wondering why our internal tests didn't capture this.; We have daily tests that run scripts like this:; https://github.com/google/deepvariant/blob/r0.8/scripts/run_wes_case_study_binaries.sh. After checking what that script was doing, I found out that if you run `scripts/run_wes_case_study_binaries.sh`, it'll work on both Ubuntu 16 and 18. We'll fix build_and_test.sh in the next release. Thanks for reporting! ; If using `build_release_binaries.sh` still doesn't work for you, feel free to reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514472365
https://github.com/google/deepvariant/issues/199#issuecomment-514472365:289,Testability,test,tests,289,"Quick answer: Can you try `./build_release_binaries.sh` instead of `./build_and_test.sh` in your steps above?; Then it should work.; The issue you're encountering has nothing to do with Ubuntu 18. ----. More details:. Earlier today, @akolesnikov and I were just wondering why our internal tests didn't capture this.; We have daily tests that run scripts like this:; https://github.com/google/deepvariant/blob/r0.8/scripts/run_wes_case_study_binaries.sh. After checking what that script was doing, I found out that if you run `scripts/run_wes_case_study_binaries.sh`, it'll work on both Ubuntu 16 and 18. We'll fix build_and_test.sh in the next release. Thanks for reporting! ; If using `build_release_binaries.sh` still doesn't work for you, feel free to reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514472365
https://github.com/google/deepvariant/issues/199#issuecomment-514472365:331,Testability,test,tests,331,"Quick answer: Can you try `./build_release_binaries.sh` instead of `./build_and_test.sh` in your steps above?; Then it should work.; The issue you're encountering has nothing to do with Ubuntu 18. ----. More details:. Earlier today, @akolesnikov and I were just wondering why our internal tests didn't capture this.; We have daily tests that run scripts like this:; https://github.com/google/deepvariant/blob/r0.8/scripts/run_wes_case_study_binaries.sh. After checking what that script was doing, I found out that if you run `scripts/run_wes_case_study_binaries.sh`, it'll work on both Ubuntu 16 and 18. We'll fix build_and_test.sh in the next release. Thanks for reporting! ; If using `build_release_binaries.sh` still doesn't work for you, feel free to reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-514472365
https://github.com/google/deepvariant/issues/199#issuecomment-517213905:369,Testability,test,testdata,369,"yes @pichuan scripts/run_wes_case_study_binaries.sh does complete with those warnings, just wondering about why the output is binary when the postprocess command is invoked with tfrecord in name [examples.tfrecord.zip](https://github.com/google/deepvariant/files/3456033/examples.tfrecord.zip). `python ./bazel-bin/deepvariant/postprocess_variants.zip --ref quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --infile quickstart-output/examples.tfrecord.cvo.gz --outfile quickstart-output/examples.tfrecord.vcf`. than without [examples.zip](https://github.com/google/deepvariant/files/3456034/examples.zip). `python ./bazel-bin/deepvariant/postprocess_variants.zip --ref quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --infile quickstart-output/examples.tfrecord.cvo.gz --outfile quickstart-output/examples.vcf`. and what does that binary file represent?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-517213905
https://github.com/google/deepvariant/issues/199#issuecomment-517213905:682,Testability,test,testdata,682,"yes @pichuan scripts/run_wes_case_study_binaries.sh does complete with those warnings, just wondering about why the output is binary when the postprocess command is invoked with tfrecord in name [examples.tfrecord.zip](https://github.com/google/deepvariant/files/3456033/examples.tfrecord.zip). `python ./bazel-bin/deepvariant/postprocess_variants.zip --ref quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --infile quickstart-output/examples.tfrecord.cvo.gz --outfile quickstart-output/examples.tfrecord.vcf`. than without [examples.zip](https://github.com/google/deepvariant/files/3456034/examples.zip). `python ./bazel-bin/deepvariant/postprocess_variants.zip --ref quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --infile quickstart-output/examples.tfrecord.cvo.gz --outfile quickstart-output/examples.vcf`. and what does that binary file represent?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-517213905
https://github.com/google/deepvariant/issues/199#issuecomment-570115804:968,Availability,error,error,968,"Hi @kokyriakidis ,; in this thread (when it was still r0.8), running `././build-prereq.sh && ./build_release_binaries.sh` fixed the original user's question. In the latest release (r0.9), it should work even if you run `build_and_test.sh` instead of `build_release_binaries.sh`. The main trick was this `fix_zip_rule` that makes sure the symbolic links are correct:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/build_release_binaries.sh#L41. In the issue that you reported in bcbio/bcbio-nextgen#3048, there is one more layer because you're using bioconda. I'm surprised that your issue was actually with v0.9.0. Given that we actually fixed `build_and_test.sh` in the latest version. @kokyriakidis Two questions for you:; 1. With bioconda, were you able to run with v0.8.0 like @chapmanb suggested?; 1. Did you actually try building DeepVariant binaries on your own? If you tried, did that work for you or did you get the same error?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-570115804
https://github.com/google/deepvariant/issues/199#issuecomment-570115804:172,Deployability,release,release,172,"Hi @kokyriakidis ,; in this thread (when it was still r0.8), running `././build-prereq.sh && ./build_release_binaries.sh` fixed the original user's question. In the latest release (r0.9), it should work even if you run `build_and_test.sh` instead of `build_release_binaries.sh`. The main trick was this `fix_zip_rule` that makes sure the symbolic links are correct:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/build_release_binaries.sh#L41. In the issue that you reported in bcbio/bcbio-nextgen#3048, there is one more layer because you're using bioconda. I'm surprised that your issue was actually with v0.9.0. Given that we actually fixed `build_and_test.sh` in the latest version. @kokyriakidis Two questions for you:; 1. With bioconda, were you able to run with v0.8.0 like @chapmanb suggested?; 1. Did you actually try building DeepVariant binaries on your own? If you tried, did that work for you or did you get the same error?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/199#issuecomment-570115804
https://github.com/google/deepvariant/issues/200#issuecomment-513411694:529,Performance,perform,perform,529,"Hi @fengcong3 . DeepVariant has been applied to plant species. In the case of rice, there was good evidence of high accuracy superior [some results in this blog](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So in cases with polyploid organisms (which includes wheat), it is not yet clear how DeepVariant will perform. However, I am also not sure how other variant callers perform on polyploid samples. If you (or anyone) knows current polyploid callers for wheat, I would like to know and to run benchmarks between the two. Finally, it is possible to train DeepVariant models for a specific genome. We have a previous example of this in mosquitos [see our blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). We hope to explore training a general plant model or a general non-human model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/200#issuecomment-513411694
https://github.com/google/deepvariant/issues/200#issuecomment-513411694:592,Performance,perform,perform,592,"Hi @fengcong3 . DeepVariant has been applied to plant species. In the case of rice, there was good evidence of high accuracy superior [some results in this blog](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So in cases with polyploid organisms (which includes wheat), it is not yet clear how DeepVariant will perform. However, I am also not sure how other variant callers perform on polyploid samples. If you (or anyone) knows current polyploid callers for wheat, I would like to know and to run benchmarks between the two. Finally, it is possible to train DeepVariant models for a specific genome. We have a previous example of this in mosquitos [see our blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). We hope to explore training a general plant model or a general non-human model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/200#issuecomment-513411694
https://github.com/google/deepvariant/issues/200#issuecomment-513411694:716,Testability,benchmark,benchmarks,716,"Hi @fengcong3 . DeepVariant has been applied to plant species. In the case of rice, there was good evidence of high accuracy superior [some results in this blog](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So in cases with polyploid organisms (which includes wheat), it is not yet clear how DeepVariant will perform. However, I am also not sure how other variant callers perform on polyploid samples. If you (or anyone) knows current polyploid callers for wheat, I would like to know and to run benchmarks between the two. Finally, it is possible to train DeepVariant models for a specific genome. We have a previous example of this in mosquitos [see our blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). We hope to explore training a general plant model or a general non-human model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/200#issuecomment-513411694
https://github.com/google/deepvariant/issues/200#issuecomment-513411694:502,Usability,clear,clear,502,"Hi @fengcong3 . DeepVariant has been applied to plant species. In the case of rice, there was good evidence of high accuracy superior [some results in this blog](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So in cases with polyploid organisms (which includes wheat), it is not yet clear how DeepVariant will perform. However, I am also not sure how other variant callers perform on polyploid samples. If you (or anyone) knows current polyploid callers for wheat, I would like to know and to run benchmarks between the two. Finally, it is possible to train DeepVariant models for a specific genome. We have a previous example of this in mosquitos [see our blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). We hope to explore training a general plant model or a general non-human model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/200#issuecomment-513411694
https://github.com/google/deepvariant/issues/201#issuecomment-516839518:368,Availability,robust,robust,368,"Hi @anands-repo . Thank you for running the evaluation! The work detailed in the blog and in the publication (https://www.biorxiv.org/content/biorxiv/early/2019/01/23/519025.full.pdf) represent our initial efforts to train a model on PacBio data. Subsequent to that publication, PacBio generated more extensive training Sequel II data. We used this data to build more robust training datasets across a diverse range of coverage, insert sizes, and machine runs. This is the model which has been released in version 0.8.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/201#issuecomment-516839518
https://github.com/google/deepvariant/issues/201#issuecomment-516839518:494,Deployability,release,released,494,"Hi @anands-repo . Thank you for running the evaluation! The work detailed in the blog and in the publication (https://www.biorxiv.org/content/biorxiv/early/2019/01/23/519025.full.pdf) represent our initial efforts to train a model on PacBio data. Subsequent to that publication, PacBio generated more extensive training Sequel II data. We used this data to build more robust training datasets across a diverse range of coverage, insert sizes, and machine runs. This is the model which has been released in version 0.8.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/201#issuecomment-516839518
https://github.com/google/deepvariant/issues/201#issuecomment-516853711:71,Availability,avail,available,71,Thanks for the clarification. Is the new PacBio training data publicly available? I apologize if this is described somewhere and I didn't find it.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/201#issuecomment-516853711
https://github.com/google/deepvariant/issues/201#issuecomment-517517472:46,Availability,avail,available,46,The full content of that data is not publicly available. There are additional datasets available at the Genome in a Bottle FTP. These include:. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_SequelII_CCS_11kb/. which represents the BAM file used in our case study evaluation. This data is used for training (excluding chr20 for evaluation). And HG005 - ; ; ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/ChineseTrio/HG005_NA24631_son/PacBio_SequelII_CCS_11kb/. And HG001/NA12878 -. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/PacBio_SequelII_CCS_11kb/. The above files for HG001 and HG005 are not used for training or evaluation of DeepVariant at this time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/201#issuecomment-517517472
https://github.com/google/deepvariant/issues/201#issuecomment-517517472:87,Availability,avail,available,87,The full content of that data is not publicly available. There are additional datasets available at the Genome in a Bottle FTP. These include:. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_SequelII_CCS_11kb/. which represents the BAM file used in our case study evaluation. This data is used for training (excluding chr20 for evaluation). And HG005 - ; ; ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/ChineseTrio/HG005_NA24631_son/PacBio_SequelII_CCS_11kb/. And HG001/NA12878 -. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/PacBio_SequelII_CCS_11kb/. The above files for HG001 and HG005 are not used for training or evaluation of DeepVariant at this time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/201#issuecomment-517517472
https://github.com/google/deepvariant/issues/202#issuecomment-517123277:698,Testability,log,logic,698,"Hi @aderzelle ; First of all, when I first think of ""deterministic"" behavior, I usually think about whether the exact same input yields the same results. Which we've made effort to make sure that running on the same input on the same machine should have deterministic behavior. If you have noticed non-deterministic behavior on the same sample, please do let us know. That said, you're absolutely right. These two represent the exact same event, but simple comparison scripts will not understand those events are in fact identical. ```; 90123; TGGGT; T--GTTC <-- Sample 1; TGTTC <-- Sample 2; ```. I think other users have reported to us in the past. So far we haven't looked closely into the code logic yet, because we mostly rely on tools such as `hap.py` for normalizing these during evaluation. So, this is a known behavior but haven't been investigated yet. And I do agree that it'll be good if we can behavior more consistently. . I will file an internal issue to track this (if we don't have one already). I can't guarantee when it'll be addressed though. If you have more suggestions on whether one representation might be better than the other, let me know. I'm closing this issue, but feel free to add your thoughts here. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/202#issuecomment-517123277
https://github.com/google/deepvariant/issues/202#issuecomment-517123277:450,Usability,simpl,simple,450,"Hi @aderzelle ; First of all, when I first think of ""deterministic"" behavior, I usually think about whether the exact same input yields the same results. Which we've made effort to make sure that running on the same input on the same machine should have deterministic behavior. If you have noticed non-deterministic behavior on the same sample, please do let us know. That said, you're absolutely right. These two represent the exact same event, but simple comparison scripts will not understand those events are in fact identical. ```; 90123; TGGGT; T--GTTC <-- Sample 1; TGTTC <-- Sample 2; ```. I think other users have reported to us in the past. So far we haven't looked closely into the code logic yet, because we mostly rely on tools such as `hap.py` for normalizing these during evaluation. So, this is a known behavior but haven't been investigated yet. And I do agree that it'll be good if we can behavior more consistently. . I will file an internal issue to track this (if we don't have one already). I can't guarantee when it'll be addressed though. If you have more suggestions on whether one representation might be better than the other, let me know. I'm closing this issue, but feel free to add your thoughts here. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/202#issuecomment-517123277
https://github.com/google/deepvariant/issues/203#issuecomment-517389773:27,Usability,learn,learning,27,I'm not sure I see how the learning rate or batch_size would affect those metrics -- could you provide more information on your setup? And the two cases you are seeing -- are those randomly occurring or have you pinpointed why it flips one way or the other?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203#issuecomment-517389773
https://github.com/google/deepvariant/issues/203#issuecomment-518462111:661,Availability,checkpoint,checkpoints,661,"Hi @melkerdawy , thanks for reporting this issue.; In general it is difficult for us (as the developers of DeepVariant) to give advice on the training behavior of other people's data. I can, however, from a perspective of a fellow ML researcher, ask a few more questions here to see if we can help you make progress:. (1) You said ""Regardless of how low we set the learning rate or the batch size or saving the intervals, the value of either the values of (TNs/All) and (FNs/All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero."" --> The wording you have is confusing. I believe what you see here means that the `model_eval` code takes the checkpoints generated by `model_train`, and evaluated on the tuning data you've generated. And, based on the labeled tuning data, if the model thinks there are either no TNs(True Negatives) or FNs (False Negatives) --> this seems to indicate that it's likely at this point, the model might just don't call any negatives at all. But it's just my guess based on what you observe. (2) Before we even dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111
https://github.com/google/deepvariant/issues/203#issuecomment-518462111:2952,Availability,error,error,2952," dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidates that feed into the classifier), and many many other details that are specific to your data. Which is why I said our team cannot help debug the details of your case. But hopefully by examining your own distribution, you can first see if the training (and tuning) data makes sense or not. If the data has very skewed distribution, there are also other techniques that the ML community uses to improve the accuracy. But I won't be able to get into that. It's also not what DeepVariant designed for.; DeepVariant is wrapped around TensorFlow, which is a much more general purpose ML tool. If there are functionalities that we don't provide, please also look into TensorFlow to see if they have something useful for you. I'm closing this issue now because this is not really a DeepVariant issue. But if you believe this actually reveals some bugs in our codebase, I'm happy to discuss further if you can show a reproducible example that demonstrate the error.; Closing this issue now. But happy to follow up on this issue if you have more thoughts/questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111
https://github.com/google/deepvariant/issues/203#issuecomment-518462111:1464,Deployability,release,release,1464,"All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero."" --> The wording you have is confusing. I believe what you see here means that the `model_eval` code takes the checkpoints generated by `model_train`, and evaluated on the tuning data you've generated. And, based on the labeled tuning data, if the model thinks there are either no TNs(True Negatives) or FNs (False Negatives) --> this seems to indicate that it's likely at this point, the model might just don't call any negatives at all. But it's just my guess based on what you observe. (2) Before we even dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidates that feed into the classifier), and many many other details that are specific to your data. Which is why I said our team cannot help debug the details of your case. But hopefully by examining your own distribution, you can first see if the training (and tuning) data makes sense or not. If the data has very skewed distribution, there are also other techniques that the ML community uses to improve the accuracy. But I won't be able to get into that. It's also not what D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111
https://github.com/google/deepvariant/issues/203#issuecomment-518462111:1833,Integrability,depend,depends,1833,"call any negatives at all. But it's just my guess based on what you observe. (2) Before we even dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidates that feed into the classifier), and many many other details that are specific to your data. Which is why I said our team cannot help debug the details of your case. But hopefully by examining your own distribution, you can first see if the training (and tuning) data makes sense or not. If the data has very skewed distribution, there are also other techniques that the ML community uses to improve the accuracy. But I won't be able to get into that. It's also not what DeepVariant designed for.; DeepVariant is wrapped around TensorFlow, which is a much more general purpose ML tool. If there are functionalities that we don't provide, please also look into TensorFlow to see if they have something useful for you. I'm closing this issue now because this is not really a DeepVariant issue. But if you believe this actually reveals some bugs in our codebase, I'm happy to discuss further if you can show a reproducible example that demonstrate the error.; Cl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111
https://github.com/google/deepvariant/issues/203#issuecomment-518462111:2516,Integrability,wrap,wrapped,2516," dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidates that feed into the classifier), and many many other details that are specific to your data. Which is why I said our team cannot help debug the details of your case. But hopefully by examining your own distribution, you can first see if the training (and tuning) data makes sense or not. If the data has very skewed distribution, there are also other techniques that the ML community uses to improve the accuracy. But I won't be able to get into that. It's also not what DeepVariant designed for.; DeepVariant is wrapped around TensorFlow, which is a much more general purpose ML tool. If there are functionalities that we don't provide, please also look into TensorFlow to see if they have something useful for you. I'm closing this issue now because this is not really a DeepVariant issue. But if you believe this actually reveals some bugs in our codebase, I'm happy to discuss further if you can show a reproducible example that demonstrate the error.; Closing this issue now. But happy to follow up on this issue if you have more thoughts/questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111
https://github.com/google/deepvariant/issues/203#issuecomment-518462111:365,Usability,learn,learning,365,"Hi @melkerdawy , thanks for reporting this issue.; In general it is difficult for us (as the developers of DeepVariant) to give advice on the training behavior of other people's data. I can, however, from a perspective of a fellow ML researcher, ask a few more questions here to see if we can help you make progress:. (1) You said ""Regardless of how low we set the learning rate or the batch size or saving the intervals, the value of either the values of (TNs/All) and (FNs/All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero."" --> The wording you have is confusing. I believe what you see here means that the `model_eval` code takes the checkpoints generated by `model_train`, and evaluated on the tuning data you've generated. And, based on the labeled tuning data, if the model thinks there are either no TNs(True Negatives) or FNs (False Negatives) --> this seems to indicate that it's likely at this point, the model might just don't call any negatives at all. But it's just my guess based on what you observe. (2) Before we even dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111
https://github.com/google/deepvariant/issues/204#issuecomment-518518311:759,Deployability,release,release,759,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. We’re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently we’re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311
https://github.com/google/deepvariant/issues/204#issuecomment-518518311:1003,Deployability,release,released,1003,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. We’re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently we’re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311
https://github.com/google/deepvariant/issues/204#issuecomment-518518311:728,Energy Efficiency,reduce,reduce,728,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. We’re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently we’re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311
https://github.com/google/deepvariant/issues/204#issuecomment-518518311:250,Performance,perform,perform,250,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. We’re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently we’re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311
https://github.com/google/deepvariant/issues/204#issuecomment-518518311:916,Performance,optimiz,optimizing,916,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. We’re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently we’re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311
https://github.com/google/deepvariant/issues/204#issuecomment-518518311:457,Usability,learn,learns,457,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. We’re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently we’re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311
https://github.com/google/deepvariant/issues/207#issuecomment-523147155:336,Availability,error,error,336,@obsh regenerating the `.bai` index file should fix the first warning. The `Unrecognized SAM header type` warning should not cause the job to fail. . Some questions for you:; * What is the command you used to run `make_examples`? Any additional output would also be helpful.; * Is task 8 the only one that failed?; * (Unrelated to this error) Which model do you plan to use for this data?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-523147155
https://github.com/google/deepvariant/issues/207#issuecomment-523380440:30,Deployability,pipeline,pipeline,30,"Hi @gunjanbaid,; I'm starting pipeline with a GCP runner like this:; ```; MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard; IMAGE_VERSION=0.6.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; DOCKER_IMAGE_GPU=gcr.io/deepvariant-docker/deepvariant_gpu:""${IMAGE_VERSION}"". ./opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ""${PROJECT_ID}"" \; --zones ""${ZONES}"" \; --docker_image ""${DOCKER_IMAGE}"" \; --docker_image_gpu ""${DOCKER_IMAGE_GPU}"" \; --gpu \; --outfile ""${OUTPUT_BUCKET}""/""${OUTPUT_FILE_NAME}"" \; --staging ""${OUTPUT_BUCKET}""/""${STAGING_FOLDER_NAME}"" \; --model ""${MODEL}"" \; --ref ""${INPUT_REF}"" \; --bam ""${INPUT_BAM}"" \; --shards 512 \; --make_examples_workers 16 \; --make_examples_cores_per_worker 10 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 16 \; --call_variants_cores_per_worker 8 \; --call_variants_ram_per_worker_gb 30 \; --call_variants_disk_per_worker_gb 50; ```; I've checked logs - a bunch of tasks failed (8, 32, 67, 105, 192, 231, 261, 293, 358).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-523380440
https://github.com/google/deepvariant/issues/207#issuecomment-523380440:1064,Testability,log,logs,1064,"Hi @gunjanbaid,; I'm starting pipeline with a GCP runner like this:; ```; MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard; IMAGE_VERSION=0.6.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; DOCKER_IMAGE_GPU=gcr.io/deepvariant-docker/deepvariant_gpu:""${IMAGE_VERSION}"". ./opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ""${PROJECT_ID}"" \; --zones ""${ZONES}"" \; --docker_image ""${DOCKER_IMAGE}"" \; --docker_image_gpu ""${DOCKER_IMAGE_GPU}"" \; --gpu \; --outfile ""${OUTPUT_BUCKET}""/""${OUTPUT_FILE_NAME}"" \; --staging ""${OUTPUT_BUCKET}""/""${STAGING_FOLDER_NAME}"" \; --model ""${MODEL}"" \; --ref ""${INPUT_REF}"" \; --bam ""${INPUT_BAM}"" \; --shards 512 \; --make_examples_workers 16 \; --make_examples_cores_per_worker 10 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 16 \; --call_variants_cores_per_worker 8 \; --call_variants_ram_per_worker_gb 30 \; --call_variants_disk_per_worker_gb 50; ```; I've checked logs - a bunch of tasks failed (8, 32, 67, 105, 192, 231, 261, 293, 358).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-523380440
https://github.com/google/deepvariant/issues/207#issuecomment-524686835:50,Availability,failure,failure,50,"Hi @obsh, I am still not sure what is causing the failure. I noticed that you are using an older release (v0.6.1), so I would suggest switching to the latest v0.8.0 release. I'm not sure that this will address the failures, but I would recommend it regardless since we have updated the code and models. . As a sanity check, you could try running the command above with different data. [This page](https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration) contains an example using Whole Exome Sequencing (WES) data. If the problem is fixed, perhaps there is something unexpected about your data, and we can further investigate. We also recommend that you try directly running DeepVariant through the Docker image, which may provide additional error messages that are helpful for debugging. You can refer to [this document](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) for an example. Please let us know if you run into the issue through this setup. One thing to note: our models have been trained using human data, and may not necessarily generalize well across species. We have trained a model for mosquito data, and while doing so, we noticed that retraining using data from this species significantly improved performance (more details in [this post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)). The mosquito model is not public, but we are happy to share it with you. Feel free to email me at gunjanbaid@google.com for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-524686835
https://github.com/google/deepvariant/issues/207#issuecomment-524686835:214,Availability,failure,failures,214,"Hi @obsh, I am still not sure what is causing the failure. I noticed that you are using an older release (v0.6.1), so I would suggest switching to the latest v0.8.0 release. I'm not sure that this will address the failures, but I would recommend it regardless since we have updated the code and models. . As a sanity check, you could try running the command above with different data. [This page](https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration) contains an example using Whole Exome Sequencing (WES) data. If the problem is fixed, perhaps there is something unexpected about your data, and we can further investigate. We also recommend that you try directly running DeepVariant through the Docker image, which may provide additional error messages that are helpful for debugging. You can refer to [this document](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) for an example. Please let us know if you run into the issue through this setup. One thing to note: our models have been trained using human data, and may not necessarily generalize well across species. We have trained a model for mosquito data, and while doing so, we noticed that retraining using data from this species significantly improved performance (more details in [this post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)). The mosquito model is not public, but we are happy to share it with you. Feel free to email me at gunjanbaid@google.com for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-524686835
https://github.com/google/deepvariant/issues/207#issuecomment-524686835:783,Availability,error,error,783,"Hi @obsh, I am still not sure what is causing the failure. I noticed that you are using an older release (v0.6.1), so I would suggest switching to the latest v0.8.0 release. I'm not sure that this will address the failures, but I would recommend it regardless since we have updated the code and models. . As a sanity check, you could try running the command above with different data. [This page](https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration) contains an example using Whole Exome Sequencing (WES) data. If the problem is fixed, perhaps there is something unexpected about your data, and we can further investigate. We also recommend that you try directly running DeepVariant through the Docker image, which may provide additional error messages that are helpful for debugging. You can refer to [this document](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) for an example. Please let us know if you run into the issue through this setup. One thing to note: our models have been trained using human data, and may not necessarily generalize well across species. We have trained a model for mosquito data, and while doing so, we noticed that retraining using data from this species significantly improved performance (more details in [this post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)). The mosquito model is not public, but we are happy to share it with you. Feel free to email me at gunjanbaid@google.com for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-524686835
https://github.com/google/deepvariant/issues/207#issuecomment-524686835:97,Deployability,release,release,97,"Hi @obsh, I am still not sure what is causing the failure. I noticed that you are using an older release (v0.6.1), so I would suggest switching to the latest v0.8.0 release. I'm not sure that this will address the failures, but I would recommend it regardless since we have updated the code and models. . As a sanity check, you could try running the command above with different data. [This page](https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration) contains an example using Whole Exome Sequencing (WES) data. If the problem is fixed, perhaps there is something unexpected about your data, and we can further investigate. We also recommend that you try directly running DeepVariant through the Docker image, which may provide additional error messages that are helpful for debugging. You can refer to [this document](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) for an example. Please let us know if you run into the issue through this setup. One thing to note: our models have been trained using human data, and may not necessarily generalize well across species. We have trained a model for mosquito data, and while doing so, we noticed that retraining using data from this species significantly improved performance (more details in [this post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)). The mosquito model is not public, but we are happy to share it with you. Feel free to email me at gunjanbaid@google.com for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-524686835
https://github.com/google/deepvariant/issues/207#issuecomment-524686835:165,Deployability,release,release,165,"Hi @obsh, I am still not sure what is causing the failure. I noticed that you are using an older release (v0.6.1), so I would suggest switching to the latest v0.8.0 release. I'm not sure that this will address the failures, but I would recommend it regardless since we have updated the code and models. . As a sanity check, you could try running the command above with different data. [This page](https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration) contains an example using Whole Exome Sequencing (WES) data. If the problem is fixed, perhaps there is something unexpected about your data, and we can further investigate. We also recommend that you try directly running DeepVariant through the Docker image, which may provide additional error messages that are helpful for debugging. You can refer to [this document](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) for an example. Please let us know if you run into the issue through this setup. One thing to note: our models have been trained using human data, and may not necessarily generalize well across species. We have trained a model for mosquito data, and while doing so, we noticed that retraining using data from this species significantly improved performance (more details in [this post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)). The mosquito model is not public, but we are happy to share it with you. Feel free to email me at gunjanbaid@google.com for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-524686835
https://github.com/google/deepvariant/issues/207#issuecomment-524686835:274,Deployability,update,updated,274,"Hi @obsh, I am still not sure what is causing the failure. I noticed that you are using an older release (v0.6.1), so I would suggest switching to the latest v0.8.0 release. I'm not sure that this will address the failures, but I would recommend it regardless since we have updated the code and models. . As a sanity check, you could try running the command above with different data. [This page](https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration) contains an example using Whole Exome Sequencing (WES) data. If the problem is fixed, perhaps there is something unexpected about your data, and we can further investigate. We also recommend that you try directly running DeepVariant through the Docker image, which may provide additional error messages that are helpful for debugging. You can refer to [this document](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) for an example. Please let us know if you run into the issue through this setup. One thing to note: our models have been trained using human data, and may not necessarily generalize well across species. We have trained a model for mosquito data, and while doing so, we noticed that retraining using data from this species significantly improved performance (more details in [this post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)). The mosquito model is not public, but we are happy to share it with you. Feel free to email me at gunjanbaid@google.com for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-524686835
https://github.com/google/deepvariant/issues/207#issuecomment-524686835:789,Integrability,message,messages,789,"Hi @obsh, I am still not sure what is causing the failure. I noticed that you are using an older release (v0.6.1), so I would suggest switching to the latest v0.8.0 release. I'm not sure that this will address the failures, but I would recommend it regardless since we have updated the code and models. . As a sanity check, you could try running the command above with different data. [This page](https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration) contains an example using Whole Exome Sequencing (WES) data. If the problem is fixed, perhaps there is something unexpected about your data, and we can further investigate. We also recommend that you try directly running DeepVariant through the Docker image, which may provide additional error messages that are helpful for debugging. You can refer to [this document](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) for an example. Please let us know if you run into the issue through this setup. One thing to note: our models have been trained using human data, and may not necessarily generalize well across species. We have trained a model for mosquito data, and while doing so, we noticed that retraining using data from this species significantly improved performance (more details in [this post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)). The mosquito model is not public, but we are happy to share it with you. Feel free to email me at gunjanbaid@google.com for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-524686835
https://github.com/google/deepvariant/issues/207#issuecomment-524686835:1289,Performance,perform,performance,1289,"Hi @obsh, I am still not sure what is causing the failure. I noticed that you are using an older release (v0.6.1), so I would suggest switching to the latest v0.8.0 release. I'm not sure that this will address the failures, but I would recommend it regardless since we have updated the code and models. . As a sanity check, you could try running the command above with different data. [This page](https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration) contains an example using Whole Exome Sequencing (WES) data. If the problem is fixed, perhaps there is something unexpected about your data, and we can further investigate. We also recommend that you try directly running DeepVariant through the Docker image, which may provide additional error messages that are helpful for debugging. You can refer to [this document](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) for an example. Please let us know if you run into the issue through this setup. One thing to note: our models have been trained using human data, and may not necessarily generalize well across species. We have trained a model for mosquito data, and while doing so, we noticed that retraining using data from this species significantly improved performance (more details in [this post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)). The mosquito model is not public, but we are happy to share it with you. Feel free to email me at gunjanbaid@google.com for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-524686835
https://github.com/google/deepvariant/issues/207#issuecomment-524686835:310,Safety,sanity check,sanity check,310,"Hi @obsh, I am still not sure what is causing the failure. I noticed that you are using an older release (v0.6.1), so I would suggest switching to the latest v0.8.0 release. I'm not sure that this will address the failures, but I would recommend it regardless since we have updated the code and models. . As a sanity check, you could try running the command above with different data. [This page](https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration) contains an example using Whole Exome Sequencing (WES) data. If the problem is fixed, perhaps there is something unexpected about your data, and we can further investigate. We also recommend that you try directly running DeepVariant through the Docker image, which may provide additional error messages that are helpful for debugging. You can refer to [this document](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) for an example. Please let us know if you run into the issue through this setup. One thing to note: our models have been trained using human data, and may not necessarily generalize well across species. We have trained a model for mosquito data, and while doing so, we noticed that retraining using data from this species significantly improved performance (more details in [this post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)). The mosquito model is not public, but we are happy to share it with you. Feel free to email me at gunjanbaid@google.com for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-524686835
https://github.com/google/deepvariant/issues/207#issuecomment-527322608:47,Deployability,release,release,47,"Hi @gunjanbaid, thank you! I'll try the v0.8.0 release and other recommendations. I’m working on a cannabis variants project with a Googler @allenday.; We consulted @AndrewCarroll previously, from his preliminary analysis - seems that default DeepVariant model should work fine with our data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-527322608
https://github.com/google/deepvariant/issues/207#issuecomment-527996299:100,Availability,error,errors,100,"@obsh sounds good. I'll close this issue for now, but feel free to reopen if you run into any other errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/207#issuecomment-527996299
https://github.com/google/deepvariant/issues/208#issuecomment-523135630:29,Performance,perform,perform,29,"PacBio reads are too long to perform a local assembly on them, therefore --norealign_reads flag has to be added.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/208#issuecomment-523135630
https://github.com/google/deepvariant/issues/209#issuecomment-524318838:61,Availability,down,download,61,"Hello,; it's ok for me to share, where should I send you the download link?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-524318838
https://github.com/google/deepvariant/issues/209#issuecomment-531590836:696,Deployability,release,release,696,"Hi @Lenbok . Thank you for the note, with the links from @aderzelle, I was able to pull in the file and visualize this event. . I think what is happening is that there are variants that can be represented in an internally consistent way at two different sets of positions. I think that DeepVariant reassembly is generating these two sets of candidates. The neural net always sees positions reassembled in the context of that particular position, so there looks to be evidence for support for each when inspected relative to the reference. We've had some internal discussions about how to improve candidate haplotype assignment for reads, but it will likely take some time to implement, test, and release. Thank you for highlighting this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-531590836
https://github.com/google/deepvariant/issues/209#issuecomment-531590836:686,Testability,test,test,686,"Hi @Lenbok . Thank you for the note, with the links from @aderzelle, I was able to pull in the file and visualize this event. . I think what is happening is that there are variants that can be represented in an internally consistent way at two different sets of positions. I think that DeepVariant reassembly is generating these two sets of candidates. The neural net always sees positions reassembled in the context of that particular position, so there looks to be evidence for support for each when inspected relative to the reference. We've had some internal discussions about how to improve candidate haplotype assignment for reads, but it will likely take some time to implement, test, and release. Thank you for highlighting this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-531590836
https://github.com/google/deepvariant/issues/209#issuecomment-547735605:110,Deployability,update,update,110,"Hi @aderzelle , we're continuing to look into this issue. I'm leaving this open for now, and will give you an update later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-547735605
https://github.com/google/deepvariant/issues/209#issuecomment-549049881:35,Deployability,update,update,35,"Hi @pichuan ; thanks a lot for the update. No worries ... in the meanwhile I am working on improving the genome assembly, it's a very complex one (in short, I haven't moved on forgetting about deepvariants ;) ).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-549049881
https://github.com/google/deepvariant/issues/209#issuecomment-553647872:1171,Availability,avail,available,1171,"Hi @aderzelle . Today we released DeepVariant v0.9, which contains several changes to code and training models. As part of this release, we have introduced changes which fix the issue for the BAM snippets presented, and which we think will generally fix the issue that you observed in other cases. To briefly summarize what we believe to be the cause - in candidate generation, a de Bruijn graph of variant and reference haplotypes is constructed. In rare cases, some graph paths are created in which local connections are valid, but no individual read supports the entire path. In your case, this caused two similar representations to generate candidates at different positions, each of which could be locally supported. In our fix, we require at least some support for the constructed graph of the candidate haplotype. We also noticed a separate fix that resolves your case. Specifically, your case was sensitive to the kmer length used to construct the graph. By default, this is 10, but we noticed that increasing to 15 also resolved your issue. We think this may reflect local repetitiveness. We have exposed this parameter in make_examples as: --dbg_min_k. This is available when running make_examples directly, but not in the Docker image. Since the issue should be resolved in v0.9 without this change, this is mostly for your information if you want to experiment with other tweaks. We would be interested to hear your feedback confirming this case is resolved in v0.9. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-553647872
https://github.com/google/deepvariant/issues/209#issuecomment-553647872:25,Deployability,release,released,25,"Hi @aderzelle . Today we released DeepVariant v0.9, which contains several changes to code and training models. As part of this release, we have introduced changes which fix the issue for the BAM snippets presented, and which we think will generally fix the issue that you observed in other cases. To briefly summarize what we believe to be the cause - in candidate generation, a de Bruijn graph of variant and reference haplotypes is constructed. In rare cases, some graph paths are created in which local connections are valid, but no individual read supports the entire path. In your case, this caused two similar representations to generate candidates at different positions, each of which could be locally supported. In our fix, we require at least some support for the constructed graph of the candidate haplotype. We also noticed a separate fix that resolves your case. Specifically, your case was sensitive to the kmer length used to construct the graph. By default, this is 10, but we noticed that increasing to 15 also resolved your issue. We think this may reflect local repetitiveness. We have exposed this parameter in make_examples as: --dbg_min_k. This is available when running make_examples directly, but not in the Docker image. Since the issue should be resolved in v0.9 without this change, this is mostly for your information if you want to experiment with other tweaks. We would be interested to hear your feedback confirming this case is resolved in v0.9. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-553647872
https://github.com/google/deepvariant/issues/209#issuecomment-553647872:128,Deployability,release,release,128,"Hi @aderzelle . Today we released DeepVariant v0.9, which contains several changes to code and training models. As part of this release, we have introduced changes which fix the issue for the BAM snippets presented, and which we think will generally fix the issue that you observed in other cases. To briefly summarize what we believe to be the cause - in candidate generation, a de Bruijn graph of variant and reference haplotypes is constructed. In rare cases, some graph paths are created in which local connections are valid, but no individual read supports the entire path. In your case, this caused two similar representations to generate candidates at different positions, each of which could be locally supported. In our fix, we require at least some support for the constructed graph of the candidate haplotype. We also noticed a separate fix that resolves your case. Specifically, your case was sensitive to the kmer length used to construct the graph. By default, this is 10, but we noticed that increasing to 15 also resolved your issue. We think this may reflect local repetitiveness. We have exposed this parameter in make_examples as: --dbg_min_k. This is available when running make_examples directly, but not in the Docker image. Since the issue should be resolved in v0.9 without this change, this is mostly for your information if you want to experiment with other tweaks. We would be interested to hear your feedback confirming this case is resolved in v0.9. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-553647872
https://github.com/google/deepvariant/issues/209#issuecomment-553647872:1106,Security,expose,exposed,1106,"Hi @aderzelle . Today we released DeepVariant v0.9, which contains several changes to code and training models. As part of this release, we have introduced changes which fix the issue for the BAM snippets presented, and which we think will generally fix the issue that you observed in other cases. To briefly summarize what we believe to be the cause - in candidate generation, a de Bruijn graph of variant and reference haplotypes is constructed. In rare cases, some graph paths are created in which local connections are valid, but no individual read supports the entire path. In your case, this caused two similar representations to generate candidates at different positions, each of which could be locally supported. In our fix, we require at least some support for the constructed graph of the candidate haplotype. We also noticed a separate fix that resolves your case. Specifically, your case was sensitive to the kmer length used to construct the graph. By default, this is 10, but we noticed that increasing to 15 also resolved your issue. We think this may reflect local repetitiveness. We have exposed this parameter in make_examples as: --dbg_min_k. This is available when running make_examples directly, but not in the Docker image. Since the issue should be resolved in v0.9 without this change, this is mostly for your information if you want to experiment with other tweaks. We would be interested to hear your feedback confirming this case is resolved in v0.9. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-553647872
https://github.com/google/deepvariant/issues/209#issuecomment-553647872:1428,Usability,feedback,feedback,1428,"Hi @aderzelle . Today we released DeepVariant v0.9, which contains several changes to code and training models. As part of this release, we have introduced changes which fix the issue for the BAM snippets presented, and which we think will generally fix the issue that you observed in other cases. To briefly summarize what we believe to be the cause - in candidate generation, a de Bruijn graph of variant and reference haplotypes is constructed. In rare cases, some graph paths are created in which local connections are valid, but no individual read supports the entire path. In your case, this caused two similar representations to generate candidates at different positions, each of which could be locally supported. In our fix, we require at least some support for the constructed graph of the candidate haplotype. We also noticed a separate fix that resolves your case. Specifically, your case was sensitive to the kmer length used to construct the graph. By default, this is 10, but we noticed that increasing to 15 also resolved your issue. We think this may reflect local repetitiveness. We have exposed this parameter in make_examples as: --dbg_min_k. This is available when running make_examples directly, but not in the Docker image. Since the issue should be resolved in v0.9 without this change, this is mostly for your information if you want to experiment with other tweaks. We would be interested to hear your feedback confirming this case is resolved in v0.9. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/209#issuecomment-553647872
https://github.com/google/deepvariant/issues/210#issuecomment-524688432:139,Deployability,pipeline,pipeline,139,"Hi @case3526, we recommend running DeepVariant v0.8.0 using the Docker image or prebuilt binaries, instead of the `gcp_deepvariant_runner` pipeline. Here are links to case studies that show how you can run using Docker or binaries. Note: we recommend running the binaries on an Ubuntu 16.04 machine. * [DeepVariant quickstart with Docker](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md); * [Script to run WGS case study using binaries](https://github.com/google/deepvariant/blob/r0.8/scripts/run_wgs_case_study_binaries.sh)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/210#issuecomment-524688432
https://github.com/google/deepvariant/issues/212#issuecomment-526443293:17,Deployability,update,update,17,"@hamidqaedi Just update the architecture to `--copt=-march=native` in [settings.sh](https://github.com/google/deepvariant/blob/r0.8/settings.sh#L102), and use the same option when compiling Tensorflow from source and you should be fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/212#issuecomment-526443293
https://github.com/google/deepvariant/issues/213#issuecomment-527641443:225,Availability,avail,available,225,"It is an interesting point about homopolymer reads that they shouldn't be considered at all. I will think about it further. My thoughts were that the CNN probably filters out such information, and no additional gain would be available by removing those reads. Also, an upstream heterozygous SNV might disambiguate the indel allele correctly through realignment. Thanks for confirming that partial insertions are considered and not discarded. However, there are still cases where a site has two alternative insertion alleles (say, CT, and CTCTTT; both alleles are supported by reads covering both the left and right positions of the insertion), where it may not be possible to determine which allele a partially overlapping read supports (the read terminates with an insertion of CT). But this may be an extremely rare occurrence and one that is not considered relevant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/213#issuecomment-527641443
https://github.com/google/deepvariant/issues/213#issuecomment-527648978:55,Modifiability,layers,layers,55,"Resulting image that is fed into CNN contains multiple layers. Two of those are 'Read supports allele', and 'Read supports ref'. By removing an ambiguous read we may slightly improve the accuracy (in theory). Yes, your last example is difficult to make it right and we don't have a near future plans to address those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/213#issuecomment-527648978
https://github.com/google/deepvariant/issues/214#issuecomment-530561319:8,Deployability,release,released,8,We just released a new docker image located at `gcr.io/cloud-lifesciences/gcp-deepvariant-runner`. Please let us know if you still observe gcsfuse issue using the latest release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214#issuecomment-530561319
https://github.com/google/deepvariant/issues/214#issuecomment-530561319:170,Deployability,release,release,170,We just released a new docker image located at `gcr.io/cloud-lifesciences/gcp-deepvariant-runner`. Please let us know if you still observe gcsfuse issue using the latest release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/214#issuecomment-530561319
https://github.com/google/deepvariant/issues/217#issuecomment-530580878:454,Testability,test,test,454,"Hi Hamid,. **Short answer**: There is no pre-built docker that would work on an old hardware (with no AVX support). **Long answer**: It may be possible to run DeepVariant with custom TensorFlow library (this is what I found online https://stackoverflow.com/questions/53723217/is-there-a-version-of-tensorflow-not-compiled-for-avx-instructions). Then you may follow directions [here](https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-build-test.md) to manually build DeepVariant and then follow instructions to run DeepVariant without docker [here](https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md). To use custom version of TensorFlow you would need to modify ./run-prereq.sh. Unfortunately there is no guaranty this will work. . Another option is to try running DeepVariant Quick Study on Google Cloud Platform. It is fairly straight forward to set up a project and start creating virtual machines on Google Cloud. You may start from [here](https://cloud.google.com/compute/docs/instances/create-start-instance).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/217#issuecomment-530580878
https://github.com/google/deepvariant/issues/219#issuecomment-531389119:17,Availability,error,error,17,"Hi PlatonB,. The error you get is ""Could not open /home/platon/test/seq1.bam""; Did you forget to add ""-v"" flag to your docker run command to mount your input directory? Take a look at DeepVariant documentation [here](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for the example on how to run DeepVariant from docker image.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/219#issuecomment-531389119
https://github.com/google/deepvariant/issues/219#issuecomment-531389119:63,Testability,test,test,63,"Hi PlatonB,. The error you get is ""Could not open /home/platon/test/seq1.bam""; Did you forget to add ""-v"" flag to your docker run command to mount your input directory? Take a look at DeepVariant documentation [here](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) for the example on how to run DeepVariant from docker image.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/219#issuecomment-531389119
https://github.com/google/deepvariant/issues/220#issuecomment-532442476:83,Availability,avail,available,83,"Hi @bopohdr . I cannot speak to the dataset referenced in this paper, as it is not available, and I am not sure how merging would have been performed, and the paper references an earlier DeepVariant version. However, a higher de novo rate of DeepVariant relative to other callers is not something that we observe in the investigations we have conducted. I am attaching (if they will upload) two VCF files for the Oslo University HG002-HG003-HG004 trio available in Genome in a Bottle FTP. The DeepVariant trio is called with DeepVariant exome model and merged with GLnexus. The GATK4 trio is called with HaplotypeCaller and merged with GenotypeGVCFs. In these trios, DeepVariant has 87 de novo calls where the child is not 0/0 but both parents are 0/0; The GATK4 trio has 270 de novo cases with the same criteria. We have a set of documentation for best practices in merging a trio that we are hoping to release in the near future. If you would like me to share that with you privately now, you can reach out directly at awcarroll@google.com. Thanks,; Andrew. [deepvariant.cohort.vcf.gz](https://github.com/google/deepvariant/files/3623783/deepvariant.cohort.vcf.gz); [gatk4.cohort.vcf.gz](https://github.com/google/deepvariant/files/3623784/gatk4.cohort.vcf.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/220#issuecomment-532442476
https://github.com/google/deepvariant/issues/220#issuecomment-532442476:452,Availability,avail,available,452,"Hi @bopohdr . I cannot speak to the dataset referenced in this paper, as it is not available, and I am not sure how merging would have been performed, and the paper references an earlier DeepVariant version. However, a higher de novo rate of DeepVariant relative to other callers is not something that we observe in the investigations we have conducted. I am attaching (if they will upload) two VCF files for the Oslo University HG002-HG003-HG004 trio available in Genome in a Bottle FTP. The DeepVariant trio is called with DeepVariant exome model and merged with GLnexus. The GATK4 trio is called with HaplotypeCaller and merged with GenotypeGVCFs. In these trios, DeepVariant has 87 de novo calls where the child is not 0/0 but both parents are 0/0; The GATK4 trio has 270 de novo cases with the same criteria. We have a set of documentation for best practices in merging a trio that we are hoping to release in the near future. If you would like me to share that with you privately now, you can reach out directly at awcarroll@google.com. Thanks,; Andrew. [deepvariant.cohort.vcf.gz](https://github.com/google/deepvariant/files/3623783/deepvariant.cohort.vcf.gz); [gatk4.cohort.vcf.gz](https://github.com/google/deepvariant/files/3623784/gatk4.cohort.vcf.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/220#issuecomment-532442476
https://github.com/google/deepvariant/issues/220#issuecomment-532442476:904,Deployability,release,release,904,"Hi @bopohdr . I cannot speak to the dataset referenced in this paper, as it is not available, and I am not sure how merging would have been performed, and the paper references an earlier DeepVariant version. However, a higher de novo rate of DeepVariant relative to other callers is not something that we observe in the investigations we have conducted. I am attaching (if they will upload) two VCF files for the Oslo University HG002-HG003-HG004 trio available in Genome in a Bottle FTP. The DeepVariant trio is called with DeepVariant exome model and merged with GLnexus. The GATK4 trio is called with HaplotypeCaller and merged with GenotypeGVCFs. In these trios, DeepVariant has 87 de novo calls where the child is not 0/0 but both parents are 0/0; The GATK4 trio has 270 de novo cases with the same criteria. We have a set of documentation for best practices in merging a trio that we are hoping to release in the near future. If you would like me to share that with you privately now, you can reach out directly at awcarroll@google.com. Thanks,; Andrew. [deepvariant.cohort.vcf.gz](https://github.com/google/deepvariant/files/3623783/deepvariant.cohort.vcf.gz); [gatk4.cohort.vcf.gz](https://github.com/google/deepvariant/files/3623784/gatk4.cohort.vcf.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/220#issuecomment-532442476
https://github.com/google/deepvariant/issues/220#issuecomment-532442476:140,Performance,perform,performed,140,"Hi @bopohdr . I cannot speak to the dataset referenced in this paper, as it is not available, and I am not sure how merging would have been performed, and the paper references an earlier DeepVariant version. However, a higher de novo rate of DeepVariant relative to other callers is not something that we observe in the investigations we have conducted. I am attaching (if they will upload) two VCF files for the Oslo University HG002-HG003-HG004 trio available in Genome in a Bottle FTP. The DeepVariant trio is called with DeepVariant exome model and merged with GLnexus. The GATK4 trio is called with HaplotypeCaller and merged with GenotypeGVCFs. In these trios, DeepVariant has 87 de novo calls where the child is not 0/0 but both parents are 0/0; The GATK4 trio has 270 de novo cases with the same criteria. We have a set of documentation for best practices in merging a trio that we are hoping to release in the near future. If you would like me to share that with you privately now, you can reach out directly at awcarroll@google.com. Thanks,; Andrew. [deepvariant.cohort.vcf.gz](https://github.com/google/deepvariant/files/3623783/deepvariant.cohort.vcf.gz); [gatk4.cohort.vcf.gz](https://github.com/google/deepvariant/files/3623784/gatk4.cohort.vcf.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/220#issuecomment-532442476
https://github.com/google/deepvariant/issues/222#issuecomment-534797540:179,Availability,avail,available,179,"Alternatively, you can use [DeepVarianatRunner](https://cloud.google.com/genomics/docs/tutorials/deepvariant#run_deepvariant). It will run all 3 stages and final VCF file will be available at the output bucket. Note that if you want gVCF output you need to also set the [corresponding flag](https://cloud.google.com/genomics/docs/tutorials/deepvariant#genomic_vcf_gvcf_configuration). Similarly if your index files does not follow standard naming convention (such as `.bam.bai`) you need to set them using their flags.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-534797540
https://github.com/google/deepvariant/issues/222#issuecomment-535102260:346,Deployability,release,release,346,"Hi @anitagh ; From my experience it's not too uncommon for BAM files in the wild to have either no sample name in the BAM file, or multiple sample names (which will crash DeepVariant right now). We can easily add `--sample_name` to the run_deepvariant.py script, so you can still run that script once. We'll do this so it'll come out in the next release. So far, we want users to explicitly add this --sample_name this flag because we want to make sure users are aware that their BAM file has more than one (or 0) sample names. But it seems like all the cases I've seen so far, none of them actually intended for them to be different sample names anyway. So I might consider just removing this constraint and just make it a warning message instead. Either way, in our next release, you should be expecting to have `--sample_name` flag in the run_deepvariant.py script which you can do in one step. Thanks for reporting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535102260
https://github.com/google/deepvariant/issues/222#issuecomment-535102260:773,Deployability,release,release,773,"Hi @anitagh ; From my experience it's not too uncommon for BAM files in the wild to have either no sample name in the BAM file, or multiple sample names (which will crash DeepVariant right now). We can easily add `--sample_name` to the run_deepvariant.py script, so you can still run that script once. We'll do this so it'll come out in the next release. So far, we want users to explicitly add this --sample_name this flag because we want to make sure users are aware that their BAM file has more than one (or 0) sample names. But it seems like all the cases I've seen so far, none of them actually intended for them to be different sample names anyway. So I might consider just removing this constraint and just make it a warning message instead. Either way, in our next release, you should be expecting to have `--sample_name` flag in the run_deepvariant.py script which you can do in one step. Thanks for reporting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535102260
https://github.com/google/deepvariant/issues/222#issuecomment-535102260:732,Integrability,message,message,732,"Hi @anitagh ; From my experience it's not too uncommon for BAM files in the wild to have either no sample name in the BAM file, or multiple sample names (which will crash DeepVariant right now). We can easily add `--sample_name` to the run_deepvariant.py script, so you can still run that script once. We'll do this so it'll come out in the next release. So far, we want users to explicitly add this --sample_name this flag because we want to make sure users are aware that their BAM file has more than one (or 0) sample names. But it seems like all the cases I've seen so far, none of them actually intended for them to be different sample names anyway. So I might consider just removing this constraint and just make it a warning message instead. Either way, in our next release, you should be expecting to have `--sample_name` flag in the run_deepvariant.py script which you can do in one step. Thanks for reporting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535102260
https://github.com/google/deepvariant/issues/222#issuecomment-535311937:258,Availability,error,error,258,"Hi @anitagh and @PlatonB , ; to give you an update on this issue, we have made a change internally that:; 1) Added `--sample_name` to run_deepvariant.py; 2) When input BAM files has no sample names or more than one sample names, instead of crashing with the error message you reported, we now use a default string as the sample name (or pick one from the multiple names) and prints out a warning. This behavior should be less cumbersome to our users, and shouldn't cause any issues for most use cases. Our team is working towards a next release. Once the release is out, I can post another update to this issue to let you know. For now, please bear with us and use the solution in https://github.com/google/deepvariant/issues/222#issuecomment-534768468. We'll keep you posted!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535311937
https://github.com/google/deepvariant/issues/222#issuecomment-535311937:44,Deployability,update,update,44,"Hi @anitagh and @PlatonB , ; to give you an update on this issue, we have made a change internally that:; 1) Added `--sample_name` to run_deepvariant.py; 2) When input BAM files has no sample names or more than one sample names, instead of crashing with the error message you reported, we now use a default string as the sample name (or pick one from the multiple names) and prints out a warning. This behavior should be less cumbersome to our users, and shouldn't cause any issues for most use cases. Our team is working towards a next release. Once the release is out, I can post another update to this issue to let you know. For now, please bear with us and use the solution in https://github.com/google/deepvariant/issues/222#issuecomment-534768468. We'll keep you posted!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535311937
https://github.com/google/deepvariant/issues/222#issuecomment-535311937:537,Deployability,release,release,537,"Hi @anitagh and @PlatonB , ; to give you an update on this issue, we have made a change internally that:; 1) Added `--sample_name` to run_deepvariant.py; 2) When input BAM files has no sample names or more than one sample names, instead of crashing with the error message you reported, we now use a default string as the sample name (or pick one from the multiple names) and prints out a warning. This behavior should be less cumbersome to our users, and shouldn't cause any issues for most use cases. Our team is working towards a next release. Once the release is out, I can post another update to this issue to let you know. For now, please bear with us and use the solution in https://github.com/google/deepvariant/issues/222#issuecomment-534768468. We'll keep you posted!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535311937
https://github.com/google/deepvariant/issues/222#issuecomment-535311937:555,Deployability,release,release,555,"Hi @anitagh and @PlatonB , ; to give you an update on this issue, we have made a change internally that:; 1) Added `--sample_name` to run_deepvariant.py; 2) When input BAM files has no sample names or more than one sample names, instead of crashing with the error message you reported, we now use a default string as the sample name (or pick one from the multiple names) and prints out a warning. This behavior should be less cumbersome to our users, and shouldn't cause any issues for most use cases. Our team is working towards a next release. Once the release is out, I can post another update to this issue to let you know. For now, please bear with us and use the solution in https://github.com/google/deepvariant/issues/222#issuecomment-534768468. We'll keep you posted!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535311937
https://github.com/google/deepvariant/issues/222#issuecomment-535311937:590,Deployability,update,update,590,"Hi @anitagh and @PlatonB , ; to give you an update on this issue, we have made a change internally that:; 1) Added `--sample_name` to run_deepvariant.py; 2) When input BAM files has no sample names or more than one sample names, instead of crashing with the error message you reported, we now use a default string as the sample name (or pick one from the multiple names) and prints out a warning. This behavior should be less cumbersome to our users, and shouldn't cause any issues for most use cases. Our team is working towards a next release. Once the release is out, I can post another update to this issue to let you know. For now, please bear with us and use the solution in https://github.com/google/deepvariant/issues/222#issuecomment-534768468. We'll keep you posted!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535311937
https://github.com/google/deepvariant/issues/222#issuecomment-535311937:264,Integrability,message,message,264,"Hi @anitagh and @PlatonB , ; to give you an update on this issue, we have made a change internally that:; 1) Added `--sample_name` to run_deepvariant.py; 2) When input BAM files has no sample names or more than one sample names, instead of crashing with the error message you reported, we now use a default string as the sample name (or pick one from the multiple names) and prints out a warning. This behavior should be less cumbersome to our users, and shouldn't cause any issues for most use cases. Our team is working towards a next release. Once the release is out, I can post another update to this issue to let you know. For now, please bear with us and use the solution in https://github.com/google/deepvariant/issues/222#issuecomment-534768468. We'll keep you posted!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535311937
https://github.com/google/deepvariant/issues/222#issuecomment-535373169:564,Deployability,release,release,564,"Thanks for the quick response. ; I checked my bam file and see only one sample name in the header as shown below. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 ./references/bwa_index/RHA.fa ./SNP_Control_Fastq/SNP-Cnt-5p_S6_L001_R1_001.fastq.gz ./SNP_Control_Fastq/SNP-Cnt-5p_S6_L001_R2_001.fastq.gz. I also check the content of the bam file and all records contain the same sample name. > samtools view SNP-Cnt-5p_S6_L001_001_paired.bam | cut -f 3 | sort | uniq -c; 418776 RHA. Do you know when the next release is out? I have to complete my analysis within a week. I guess for now I should figure out how to call the three steps separately. . Thanks again, ; Azita",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535373169
https://github.com/google/deepvariant/issues/222#issuecomment-535654563:19,Deployability,release,release,19,"Hi @anitagh ,; the release won't come out within a week. Sorry for the inconvenience. For now you'll have to run make_examples separately and add the `--sample_name` flag. Another way to fix this is to make sure your BAM file header has one SM tag in it. If your BAM file is not very big, using `samtools reheader` to add a proper SM tag might also be a reasonable solution for now. If you're using GCP, you can try out the Google Cloud version that @samanvp pointed to in earlier comments. (If you have any feedback on that tool, please report to https://github.com/googlegenomics/gcp-deepvariant-runner/issues.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535654563
https://github.com/google/deepvariant/issues/222#issuecomment-535654563:508,Usability,feedback,feedback,508,"Hi @anitagh ,; the release won't come out within a week. Sorry for the inconvenience. For now you'll have to run make_examples separately and add the `--sample_name` flag. Another way to fix this is to make sure your BAM file header has one SM tag in it. If your BAM file is not very big, using `samtools reheader` to add a proper SM tag might also be a reasonable solution for now. If you're using GCP, you can try out the Google Cloud version that @samanvp pointed to in earlier comments. (If you have any feedback on that tool, please report to https://github.com/googlegenomics/gcp-deepvariant-runner/issues.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-535654563
https://github.com/google/deepvariant/issues/222#issuecomment-536285730:1270,Availability,checkpoint,checkpoint,1270,"Thanks @samanvp - unfortunately, because of confidentially issues, we won't be able to use GCP . @pichuan - I managed to run the three scripts but am seeing a surprising result. We have sequenced a panel of control amplicons with spiked inputs containing SNP at position 41. I aligned the reads to the ref using bwa mem and filtered paired reads. Then I ran GATK Mutect2, Illumina Pisces, and Deepvariants to confirm the presence of SNP at position 41. Below is my result. . ![image](https://user-images.githubusercontent.com/24441131/65831442-65545a00-e287-11e9-8c55-8541a8ef3fce.png). I'm not sure what mistake I have made. Below is the set of commands I used to generate the vcf files from the 10% spiked-in sample. BIN_VERSION=""0.8.0""; docker run \; -v ""${HOME_DIR}"":""/home"" \; -v ""${TMP_DIR}"":""/tmpdir"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode=calling \; --sample_name=RHA \; --examples=/tmpdir/make_examples.tfrecord.gz \; --ref=/home/RHA.fa \; --reads=/home/SNP-Cnt-10p_S7_L001_001.bam \; --gvcf=/tmpdir/gvcf.tfrecord.gztest.gv. docker run \; -v ""${HOME_DIR}"":""/home"" \; -v ""${TMP_DIR}"":""/tmpdir"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --checkpoint=/opt/models/wgs/model.ckpt \; --examples=/tmpdir/make_examples.tfrecord.gz \; --outfile=/tmpdir/call_variants_output.tfrecord.gz. docker run \; -v ""${HOME_DIR}"":""/home"" \; -v ""${TMP_DIR}"":""/tmpdir"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/postprocess_variants \; --ref=/home/RHA.fa \; --infile=/tmpdir/call_variants_output.tfrecord.gz \; --nonvariant_site_tfrecord_path=/tmpdir/gvcf.tfrecord.gztest.gv \; --outfile=/home/out.vcf \; --gvcf_outfile=/home/out.gvcf. Thanks for all your help; Azita",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-536285730
https://github.com/google/deepvariant/issues/222#issuecomment-536285730:44,Security,confidential,confidentially,44,"Thanks @samanvp - unfortunately, because of confidentially issues, we won't be able to use GCP . @pichuan - I managed to run the three scripts but am seeing a surprising result. We have sequenced a panel of control amplicons with spiked inputs containing SNP at position 41. I aligned the reads to the ref using bwa mem and filtered paired reads. Then I ran GATK Mutect2, Illumina Pisces, and Deepvariants to confirm the presence of SNP at position 41. Below is my result. . ![image](https://user-images.githubusercontent.com/24441131/65831442-65545a00-e287-11e9-8c55-8541a8ef3fce.png). I'm not sure what mistake I have made. Below is the set of commands I used to generate the vcf files from the 10% spiked-in sample. BIN_VERSION=""0.8.0""; docker run \; -v ""${HOME_DIR}"":""/home"" \; -v ""${TMP_DIR}"":""/tmpdir"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode=calling \; --sample_name=RHA \; --examples=/tmpdir/make_examples.tfrecord.gz \; --ref=/home/RHA.fa \; --reads=/home/SNP-Cnt-10p_S7_L001_001.bam \; --gvcf=/tmpdir/gvcf.tfrecord.gztest.gv. docker run \; -v ""${HOME_DIR}"":""/home"" \; -v ""${TMP_DIR}"":""/tmpdir"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --checkpoint=/opt/models/wgs/model.ckpt \; --examples=/tmpdir/make_examples.tfrecord.gz \; --outfile=/tmpdir/call_variants_output.tfrecord.gz. docker run \; -v ""${HOME_DIR}"":""/home"" \; -v ""${TMP_DIR}"":""/tmpdir"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/postprocess_variants \; --ref=/home/RHA.fa \; --infile=/tmpdir/call_variants_output.tfrecord.gz \; --nonvariant_site_tfrecord_path=/tmpdir/gvcf.tfrecord.gztest.gv \; --outfile=/home/out.vcf \; --gvcf_outfile=/home/out.gvcf. Thanks for all your help; Azita",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-536285730
https://github.com/google/deepvariant/issues/222#issuecomment-536320209:757,Availability,avail,available,757,"Hi @anitagh . Thank you for putting effort into detailed analyses. However, there is a key mistake. DeepVariant is not a somatic caller. It is not designed to detect subclonal variants. In the same way that for GATK you would not use HaplotypeCaller, you would use a different tool, Mutect2, you would not use DeepVariant for this problem. Before DeepVariant's neural net, there is a human-written candidate generation component which finds candidate positions. There is a threshold for this to even nominate a candidate for later classification. This is set to 12% (based on tuning for germline calling), so we would not expect that DeepVariant would nominate a 10% mix as candidates. . We do have a somatic calling tool in early access that we are making available to trusted partners. If you would be interested in that method, you can email me (awcarroll@google.com). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-536320209
https://github.com/google/deepvariant/issues/222#issuecomment-536320209:159,Safety,detect,detect,159,"Hi @anitagh . Thank you for putting effort into detailed analyses. However, there is a key mistake. DeepVariant is not a somatic caller. It is not designed to detect subclonal variants. In the same way that for GATK you would not use HaplotypeCaller, you would use a different tool, Mutect2, you would not use DeepVariant for this problem. Before DeepVariant's neural net, there is a human-written candidate generation component which finds candidate positions. There is a threshold for this to even nominate a candidate for later classification. This is set to 12% (based on tuning for germline calling), so we would not expect that DeepVariant would nominate a 10% mix as candidates. . We do have a somatic calling tool in early access that we are making available to trusted partners. If you would be interested in that method, you can email me (awcarroll@google.com). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-536320209
https://github.com/google/deepvariant/issues/222#issuecomment-536320209:731,Security,access,access,731,"Hi @anitagh . Thank you for putting effort into detailed analyses. However, there is a key mistake. DeepVariant is not a somatic caller. It is not designed to detect subclonal variants. In the same way that for GATK you would not use HaplotypeCaller, you would use a different tool, Mutect2, you would not use DeepVariant for this problem. Before DeepVariant's neural net, there is a human-written candidate generation component which finds candidate positions. There is a threshold for this to even nominate a candidate for later classification. This is set to 12% (based on tuning for germline calling), so we would not expect that DeepVariant would nominate a 10% mix as candidates. . We do have a somatic calling tool in early access that we are making available to trusted partners. If you would be interested in that method, you can email me (awcarroll@google.com). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-536320209
https://github.com/google/deepvariant/issues/222#issuecomment-553655187:263,Availability,robust,robust,263,"Hi @anitagh , I want to give you an update that after today's release (v0.9.0), you can now use the `--sample_name` flag with run_deepvariant.py:; https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L89. And, we also made the behavior more robust so that even with multiple or no sample names, we'll try to assign a reasonable default, and proceed with a warning (but without crashing). If you have more questions please feel free to follow up here, or file new issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-553655187
https://github.com/google/deepvariant/issues/222#issuecomment-553655187:36,Deployability,update,update,36,"Hi @anitagh , I want to give you an update that after today's release (v0.9.0), you can now use the `--sample_name` flag with run_deepvariant.py:; https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L89. And, we also made the behavior more robust so that even with multiple or no sample names, we'll try to assign a reasonable default, and proceed with a warning (but without crashing). If you have more questions please feel free to follow up here, or file new issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-553655187
https://github.com/google/deepvariant/issues/222#issuecomment-553655187:62,Deployability,release,release,62,"Hi @anitagh , I want to give you an update that after today's release (v0.9.0), you can now use the `--sample_name` flag with run_deepvariant.py:; https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L89. And, we also made the behavior more robust so that even with multiple or no sample names, we'll try to assign a reasonable default, and proceed with a warning (but without crashing). If you have more questions please feel free to follow up here, or file new issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/222#issuecomment-553655187
https://github.com/google/deepvariant/issues/224#issuecomment-540821357:175,Availability,avail,available,175,"Hi @anands-repo . The major factor was the length of chromosomes, selecting sizes that would allow enough data for comparison of tune and test while still maximizing the data available for training. The tuning and test chromosomes should also be roughly representative of the overall genome content. There are some chromosomes that should probably be avoided for tune or test choices - specifically ones that have regions quite different from the rest of the genome. Chromosome 6 is an example, since it contains the MHC regions. So it is mostly arbitrary, though other methods such as Clairvoyante seem to have adopted the convention as well. It is nice for the choices to align, since it allows comparisons with the same regions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/224#issuecomment-540821357
https://github.com/google/deepvariant/issues/224#issuecomment-540821357:129,Performance,tune,tune,129,"Hi @anands-repo . The major factor was the length of chromosomes, selecting sizes that would allow enough data for comparison of tune and test while still maximizing the data available for training. The tuning and test chromosomes should also be roughly representative of the overall genome content. There are some chromosomes that should probably be avoided for tune or test choices - specifically ones that have regions quite different from the rest of the genome. Chromosome 6 is an example, since it contains the MHC regions. So it is mostly arbitrary, though other methods such as Clairvoyante seem to have adopted the convention as well. It is nice for the choices to align, since it allows comparisons with the same regions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/224#issuecomment-540821357
https://github.com/google/deepvariant/issues/224#issuecomment-540821357:363,Performance,tune,tune,363,"Hi @anands-repo . The major factor was the length of chromosomes, selecting sizes that would allow enough data for comparison of tune and test while still maximizing the data available for training. The tuning and test chromosomes should also be roughly representative of the overall genome content. There are some chromosomes that should probably be avoided for tune or test choices - specifically ones that have regions quite different from the rest of the genome. Chromosome 6 is an example, since it contains the MHC regions. So it is mostly arbitrary, though other methods such as Clairvoyante seem to have adopted the convention as well. It is nice for the choices to align, since it allows comparisons with the same regions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/224#issuecomment-540821357
https://github.com/google/deepvariant/issues/224#issuecomment-540821357:351,Safety,avoid,avoided,351,"Hi @anands-repo . The major factor was the length of chromosomes, selecting sizes that would allow enough data for comparison of tune and test while still maximizing the data available for training. The tuning and test chromosomes should also be roughly representative of the overall genome content. There are some chromosomes that should probably be avoided for tune or test choices - specifically ones that have regions quite different from the rest of the genome. Chromosome 6 is an example, since it contains the MHC regions. So it is mostly arbitrary, though other methods such as Clairvoyante seem to have adopted the convention as well. It is nice for the choices to align, since it allows comparisons with the same regions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/224#issuecomment-540821357
https://github.com/google/deepvariant/issues/224#issuecomment-540821357:138,Testability,test,test,138,"Hi @anands-repo . The major factor was the length of chromosomes, selecting sizes that would allow enough data for comparison of tune and test while still maximizing the data available for training. The tuning and test chromosomes should also be roughly representative of the overall genome content. There are some chromosomes that should probably be avoided for tune or test choices - specifically ones that have regions quite different from the rest of the genome. Chromosome 6 is an example, since it contains the MHC regions. So it is mostly arbitrary, though other methods such as Clairvoyante seem to have adopted the convention as well. It is nice for the choices to align, since it allows comparisons with the same regions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/224#issuecomment-540821357
https://github.com/google/deepvariant/issues/224#issuecomment-540821357:214,Testability,test,test,214,"Hi @anands-repo . The major factor was the length of chromosomes, selecting sizes that would allow enough data for comparison of tune and test while still maximizing the data available for training. The tuning and test chromosomes should also be roughly representative of the overall genome content. There are some chromosomes that should probably be avoided for tune or test choices - specifically ones that have regions quite different from the rest of the genome. Chromosome 6 is an example, since it contains the MHC regions. So it is mostly arbitrary, though other methods such as Clairvoyante seem to have adopted the convention as well. It is nice for the choices to align, since it allows comparisons with the same regions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/224#issuecomment-540821357
https://github.com/google/deepvariant/issues/224#issuecomment-540821357:371,Testability,test,test,371,"Hi @anands-repo . The major factor was the length of chromosomes, selecting sizes that would allow enough data for comparison of tune and test while still maximizing the data available for training. The tuning and test chromosomes should also be roughly representative of the overall genome content. There are some chromosomes that should probably be avoided for tune or test choices - specifically ones that have regions quite different from the rest of the genome. Chromosome 6 is an example, since it contains the MHC regions. So it is mostly arbitrary, though other methods such as Clairvoyante seem to have adopted the convention as well. It is nice for the choices to align, since it allows comparisons with the same regions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/224#issuecomment-540821357
https://github.com/google/deepvariant/issues/227#issuecomment-542798708:153,Safety,predict,prediction,153,"Hi @jackycsie, there is currently no way to run `call_variants` on multiple GPUs. The codebase uses the TensorFlow Estimator API, which does not support prediction using multiple GPUs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/227#issuecomment-542798708
https://github.com/google/deepvariant/issues/228#issuecomment-547966813:91,Availability,down,download,91,"Thanks for the question and sorry about these issues. It seems like there was some kind of download issue with the deepvariant conda package in your environment. The 0.8.0 package has been around ~6 months and not changed recently, so I'm guessing maybe you got a partial download or other issue. If you remove your local download (` /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2`) and re-run, does it work cleanly? Is there anything about your network or other messages that might indicate an issue that could help us debug more? Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-547966813
https://github.com/google/deepvariant/issues/228#issuecomment-547966813:272,Availability,down,download,272,"Thanks for the question and sorry about these issues. It seems like there was some kind of download issue with the deepvariant conda package in your environment. The 0.8.0 package has been around ~6 months and not changed recently, so I'm guessing maybe you got a partial download or other issue. If you remove your local download (` /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2`) and re-run, does it work cleanly? Is there anything about your network or other messages that might indicate an issue that could help us debug more? Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-547966813
https://github.com/google/deepvariant/issues/228#issuecomment-547966813:322,Availability,down,download,322,"Thanks for the question and sorry about these issues. It seems like there was some kind of download issue with the deepvariant conda package in your environment. The 0.8.0 package has been around ~6 months and not changed recently, so I'm guessing maybe you got a partial download or other issue. If you remove your local download (` /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2`) and re-run, does it work cleanly? Is there anything about your network or other messages that might indicate an issue that could help us debug more? Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-547966813
https://github.com/google/deepvariant/issues/228#issuecomment-547966813:484,Integrability,message,messages,484,"Thanks for the question and sorry about these issues. It seems like there was some kind of download issue with the deepvariant conda package in your environment. The 0.8.0 package has been around ~6 months and not changed recently, so I'm guessing maybe you got a partial download or other issue. If you remove your local download (` /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2`) and re-run, does it work cleanly? Is there anything about your network or other messages that might indicate an issue that could help us debug more? Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-547966813
https://github.com/google/deepvariant/issues/228#issuecomment-549130970:20,Availability,error,error,20,"Hi, ; I got another error when I remove my local download ( /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2) and re-run as follows. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; ERROR conda.core.link:_execute(700): An error occurred while installing package 'bioconda::deepvariant-0.8.0-py27h7333d49_0'.; Rolling back transaction: done. LinkError: post-link script failed for package bioconda::deepvariant-0.8.0-py27h7333d49_0; location of failed script: /home/ydliu/anaconda3/envs/py2.7/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ==> script output <==; stdout: ; stderr: INFO 1101 14:33:31.774699 util.py] Retrying request, attempt #1...; INFO 1101 14:34:33.764798 util.py] Retrying request, attempt #2...; INFO 1101 14:35:36.852041 util.py] Retrying request, attempt #3...; INFO 1101 14:36:43.774008 util.py] Retrying request, attempt #4...; INFO 1101 14:37:59.406010 util.py] Retrying request, attempt #5...; INFO 1101 14:39:31.468027 util.py] Retrying request, attempt #6...; INFO 1101 14:41:03.574773 util.py] Retrying request, attempt #7...; INFO 1101 14:42:35.649608 util.py] Retrying request, attempt #8...; INFO 1101 14:44:07.752737 util.py] Retrying request, attempt #9...; INFO 1101 14:45:39.836059 util.py] Retrying request, attempt #10...; INFO 1101 14:47:11.942440 util.py] Retrying request, attempt #11...; INFO 1101 14:48:43.990016 util.py] Retrying request, attempt #12...; INFO 1101 14:50:16.078940 util.py] Retrying request, attempt #13...; INFO 1101 14:51:48.200379 util.py] Retrying request, attempt #14...; INFO 1101 14:53:20.307419 util.py] Retrying request, attempt #15...; INFO 1101 14:54:52.388456 util.py] Retrying request, attempt #16...; INFO 1101 14:56:24.486618 util.py] Retrying request, attempt #17...; INFO 1101 14:57:56.579738 util.py] Retrying request, attempt #18...; INFO 1101 14:59:28.640781 util.py] Retrying request, attempt #19...; INFO 1101 15:01:00.705409 util.py] Retrying r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970
https://github.com/google/deepvariant/issues/228#issuecomment-549130970:49,Availability,down,download,49,"Hi, ; I got another error when I remove my local download ( /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2) and re-run as follows. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; ERROR conda.core.link:_execute(700): An error occurred while installing package 'bioconda::deepvariant-0.8.0-py27h7333d49_0'.; Rolling back transaction: done. LinkError: post-link script failed for package bioconda::deepvariant-0.8.0-py27h7333d49_0; location of failed script: /home/ydliu/anaconda3/envs/py2.7/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ==> script output <==; stdout: ; stderr: INFO 1101 14:33:31.774699 util.py] Retrying request, attempt #1...; INFO 1101 14:34:33.764798 util.py] Retrying request, attempt #2...; INFO 1101 14:35:36.852041 util.py] Retrying request, attempt #3...; INFO 1101 14:36:43.774008 util.py] Retrying request, attempt #4...; INFO 1101 14:37:59.406010 util.py] Retrying request, attempt #5...; INFO 1101 14:39:31.468027 util.py] Retrying request, attempt #6...; INFO 1101 14:41:03.574773 util.py] Retrying request, attempt #7...; INFO 1101 14:42:35.649608 util.py] Retrying request, attempt #8...; INFO 1101 14:44:07.752737 util.py] Retrying request, attempt #9...; INFO 1101 14:45:39.836059 util.py] Retrying request, attempt #10...; INFO 1101 14:47:11.942440 util.py] Retrying request, attempt #11...; INFO 1101 14:48:43.990016 util.py] Retrying request, attempt #12...; INFO 1101 14:50:16.078940 util.py] Retrying request, attempt #13...; INFO 1101 14:51:48.200379 util.py] Retrying request, attempt #14...; INFO 1101 14:53:20.307419 util.py] Retrying request, attempt #15...; INFO 1101 14:54:52.388456 util.py] Retrying request, attempt #16...; INFO 1101 14:56:24.486618 util.py] Retrying request, attempt #17...; INFO 1101 14:57:56.579738 util.py] Retrying request, attempt #18...; INFO 1101 14:59:28.640781 util.py] Retrying request, attempt #19...; INFO 1101 15:01:00.705409 util.py] Retrying r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970
https://github.com/google/deepvariant/issues/228#issuecomment-549130970:239,Availability,ERROR,ERROR,239,"Hi, ; I got another error when I remove my local download ( /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2) and re-run as follows. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; ERROR conda.core.link:_execute(700): An error occurred while installing package 'bioconda::deepvariant-0.8.0-py27h7333d49_0'.; Rolling back transaction: done. LinkError: post-link script failed for package bioconda::deepvariant-0.8.0-py27h7333d49_0; location of failed script: /home/ydliu/anaconda3/envs/py2.7/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ==> script output <==; stdout: ; stderr: INFO 1101 14:33:31.774699 util.py] Retrying request, attempt #1...; INFO 1101 14:34:33.764798 util.py] Retrying request, attempt #2...; INFO 1101 14:35:36.852041 util.py] Retrying request, attempt #3...; INFO 1101 14:36:43.774008 util.py] Retrying request, attempt #4...; INFO 1101 14:37:59.406010 util.py] Retrying request, attempt #5...; INFO 1101 14:39:31.468027 util.py] Retrying request, attempt #6...; INFO 1101 14:41:03.574773 util.py] Retrying request, attempt #7...; INFO 1101 14:42:35.649608 util.py] Retrying request, attempt #8...; INFO 1101 14:44:07.752737 util.py] Retrying request, attempt #9...; INFO 1101 14:45:39.836059 util.py] Retrying request, attempt #10...; INFO 1101 14:47:11.942440 util.py] Retrying request, attempt #11...; INFO 1101 14:48:43.990016 util.py] Retrying request, attempt #12...; INFO 1101 14:50:16.078940 util.py] Retrying request, attempt #13...; INFO 1101 14:51:48.200379 util.py] Retrying request, attempt #14...; INFO 1101 14:53:20.307419 util.py] Retrying request, attempt #15...; INFO 1101 14:54:52.388456 util.py] Retrying request, attempt #16...; INFO 1101 14:56:24.486618 util.py] Retrying request, attempt #17...; INFO 1101 14:57:56.579738 util.py] Retrying request, attempt #18...; INFO 1101 14:59:28.640781 util.py] Retrying request, attempt #19...; INFO 1101 15:01:00.705409 util.py] Retrying r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970
https://github.com/google/deepvariant/issues/228#issuecomment-549130970:279,Availability,error,error,279,"Hi, ; I got another error when I remove my local download ( /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2) and re-run as follows. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; ERROR conda.core.link:_execute(700): An error occurred while installing package 'bioconda::deepvariant-0.8.0-py27h7333d49_0'.; Rolling back transaction: done. LinkError: post-link script failed for package bioconda::deepvariant-0.8.0-py27h7333d49_0; location of failed script: /home/ydliu/anaconda3/envs/py2.7/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ==> script output <==; stdout: ; stderr: INFO 1101 14:33:31.774699 util.py] Retrying request, attempt #1...; INFO 1101 14:34:33.764798 util.py] Retrying request, attempt #2...; INFO 1101 14:35:36.852041 util.py] Retrying request, attempt #3...; INFO 1101 14:36:43.774008 util.py] Retrying request, attempt #4...; INFO 1101 14:37:59.406010 util.py] Retrying request, attempt #5...; INFO 1101 14:39:31.468027 util.py] Retrying request, attempt #6...; INFO 1101 14:41:03.574773 util.py] Retrying request, attempt #7...; INFO 1101 14:42:35.649608 util.py] Retrying request, attempt #8...; INFO 1101 14:44:07.752737 util.py] Retrying request, attempt #9...; INFO 1101 14:45:39.836059 util.py] Retrying request, attempt #10...; INFO 1101 14:47:11.942440 util.py] Retrying request, attempt #11...; INFO 1101 14:48:43.990016 util.py] Retrying request, attempt #12...; INFO 1101 14:50:16.078940 util.py] Retrying request, attempt #13...; INFO 1101 14:51:48.200379 util.py] Retrying request, attempt #14...; INFO 1101 14:53:20.307419 util.py] Retrying request, attempt #15...; INFO 1101 14:54:52.388456 util.py] Retrying request, attempt #16...; INFO 1101 14:56:24.486618 util.py] Retrying request, attempt #17...; INFO 1101 14:57:56.579738 util.py] Retrying request, attempt #18...; INFO 1101 14:59:28.640781 util.py] Retrying request, attempt #19...; INFO 1101 15:01:00.705409 util.py] Retrying r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970
https://github.com/google/deepvariant/issues/228#issuecomment-549130970:300,Deployability,install,installing,300,"Hi, ; I got another error when I remove my local download ( /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2) and re-run as follows. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; ERROR conda.core.link:_execute(700): An error occurred while installing package 'bioconda::deepvariant-0.8.0-py27h7333d49_0'.; Rolling back transaction: done. LinkError: post-link script failed for package bioconda::deepvariant-0.8.0-py27h7333d49_0; location of failed script: /home/ydliu/anaconda3/envs/py2.7/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ==> script output <==; stdout: ; stderr: INFO 1101 14:33:31.774699 util.py] Retrying request, attempt #1...; INFO 1101 14:34:33.764798 util.py] Retrying request, attempt #2...; INFO 1101 14:35:36.852041 util.py] Retrying request, attempt #3...; INFO 1101 14:36:43.774008 util.py] Retrying request, attempt #4...; INFO 1101 14:37:59.406010 util.py] Retrying request, attempt #5...; INFO 1101 14:39:31.468027 util.py] Retrying request, attempt #6...; INFO 1101 14:41:03.574773 util.py] Retrying request, attempt #7...; INFO 1101 14:42:35.649608 util.py] Retrying request, attempt #8...; INFO 1101 14:44:07.752737 util.py] Retrying request, attempt #9...; INFO 1101 14:45:39.836059 util.py] Retrying request, attempt #10...; INFO 1101 14:47:11.942440 util.py] Retrying request, attempt #11...; INFO 1101 14:48:43.990016 util.py] Retrying request, attempt #12...; INFO 1101 14:50:16.078940 util.py] Retrying request, attempt #13...; INFO 1101 14:51:48.200379 util.py] Retrying request, attempt #14...; INFO 1101 14:53:20.307419 util.py] Retrying request, attempt #15...; INFO 1101 14:54:52.388456 util.py] Retrying request, attempt #16...; INFO 1101 14:56:24.486618 util.py] Retrying request, attempt #17...; INFO 1101 14:57:56.579738 util.py] Retrying request, attempt #18...; INFO 1101 14:59:28.640781 util.py] Retrying request, attempt #19...; INFO 1101 15:01:00.705409 util.py] Retrying r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970
https://github.com/google/deepvariant/issues/228#issuecomment-549130970:366,Deployability,Rolling,Rolling,366,"Hi, ; I got another error when I remove my local download ( /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2) and re-run as follows. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; ERROR conda.core.link:_execute(700): An error occurred while installing package 'bioconda::deepvariant-0.8.0-py27h7333d49_0'.; Rolling back transaction: done. LinkError: post-link script failed for package bioconda::deepvariant-0.8.0-py27h7333d49_0; location of failed script: /home/ydliu/anaconda3/envs/py2.7/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ==> script output <==; stdout: ; stderr: INFO 1101 14:33:31.774699 util.py] Retrying request, attempt #1...; INFO 1101 14:34:33.764798 util.py] Retrying request, attempt #2...; INFO 1101 14:35:36.852041 util.py] Retrying request, attempt #3...; INFO 1101 14:36:43.774008 util.py] Retrying request, attempt #4...; INFO 1101 14:37:59.406010 util.py] Retrying request, attempt #5...; INFO 1101 14:39:31.468027 util.py] Retrying request, attempt #6...; INFO 1101 14:41:03.574773 util.py] Retrying request, attempt #7...; INFO 1101 14:42:35.649608 util.py] Retrying request, attempt #8...; INFO 1101 14:44:07.752737 util.py] Retrying request, attempt #9...; INFO 1101 14:45:39.836059 util.py] Retrying request, attempt #10...; INFO 1101 14:47:11.942440 util.py] Retrying request, attempt #11...; INFO 1101 14:48:43.990016 util.py] Retrying request, attempt #12...; INFO 1101 14:50:16.078940 util.py] Retrying request, attempt #13...; INFO 1101 14:51:48.200379 util.py] Retrying request, attempt #14...; INFO 1101 14:53:20.307419 util.py] Retrying request, attempt #15...; INFO 1101 14:54:52.388456 util.py] Retrying request, attempt #16...; INFO 1101 14:56:24.486618 util.py] Retrying request, attempt #17...; INFO 1101 14:57:56.579738 util.py] Retrying request, attempt #18...; INFO 1101 14:59:28.640781 util.py] Retrying request, attempt #19...; INFO 1101 15:01:00.705409 util.py] Retrying r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970
https://github.com/google/deepvariant/issues/228#issuecomment-549130970:591,Integrability,message,messages,591,"Hi, ; I got another error when I remove my local download ( /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2) and re-run as follows. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; ERROR conda.core.link:_execute(700): An error occurred while installing package 'bioconda::deepvariant-0.8.0-py27h7333d49_0'.; Rolling back transaction: done. LinkError: post-link script failed for package bioconda::deepvariant-0.8.0-py27h7333d49_0; location of failed script: /home/ydliu/anaconda3/envs/py2.7/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ==> script output <==; stdout: ; stderr: INFO 1101 14:33:31.774699 util.py] Retrying request, attempt #1...; INFO 1101 14:34:33.764798 util.py] Retrying request, attempt #2...; INFO 1101 14:35:36.852041 util.py] Retrying request, attempt #3...; INFO 1101 14:36:43.774008 util.py] Retrying request, attempt #4...; INFO 1101 14:37:59.406010 util.py] Retrying request, attempt #5...; INFO 1101 14:39:31.468027 util.py] Retrying request, attempt #6...; INFO 1101 14:41:03.574773 util.py] Retrying request, attempt #7...; INFO 1101 14:42:35.649608 util.py] Retrying request, attempt #8...; INFO 1101 14:44:07.752737 util.py] Retrying request, attempt #9...; INFO 1101 14:45:39.836059 util.py] Retrying request, attempt #10...; INFO 1101 14:47:11.942440 util.py] Retrying request, attempt #11...; INFO 1101 14:48:43.990016 util.py] Retrying request, attempt #12...; INFO 1101 14:50:16.078940 util.py] Retrying request, attempt #13...; INFO 1101 14:51:48.200379 util.py] Retrying request, attempt #14...; INFO 1101 14:53:20.307419 util.py] Retrying request, attempt #15...; INFO 1101 14:54:52.388456 util.py] Retrying request, attempt #16...; INFO 1101 14:56:24.486618 util.py] Retrying request, attempt #17...; INFO 1101 14:57:56.579738 util.py] Retrying request, attempt #18...; INFO 1101 14:59:28.640781 util.py] Retrying request, attempt #19...; INFO 1101 15:01:00.705409 util.py] Retrying r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970
https://github.com/google/deepvariant/issues/228#issuecomment-549130970:5746,Modifiability,config,config,5746,"ion.py"", line 432, in __iter__; for blr in self.blr_iter:; File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/plurality_checkable_iterator.py"", line 60, in _PopulateHead; e = self.base_iterator.next(); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/wildcard_iterator.py"", line 476, in IterAll; expand_top_level_buckets=expand_top_level_buckets):; File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/wildcard_iterator.py"", line 215, in __iter__; provider=self.wildcard_url.scheme, fields=listing_fields):; File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/gcs_json_api.py"", line 595, in ListObjects; global_params=global_params); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/third_party/storage_apitools/storage_v1_client.py"", line 1237, in List; config, request, global_params=global_params); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/base_api.py"", line 701, in _RunMethod; http, http_request, **opts); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 341, in MakeRequest; check_response_func=check_response_func); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 391, in _MakeRequestNoRetry; redirections=redirections, connection_type=connection_type); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1570, in request; (response, content) = self._request(conn, authority, uri, request_uri, method, body, headers, redirections, cachekey); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-clou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970
https://github.com/google/deepvariant/issues/228#issuecomment-549130970:6709,Performance,cache,cachekey,6709," line 595, in ListObjects; global_params=global_params); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/third_party/storage_apitools/storage_v1_client.py"", line 1237, in List; config, request, global_params=global_params); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/base_api.py"", line 701, in _RunMethod; http, http_request, **opts); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 341, in MakeRequest; check_response_func=check_response_func); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 391, in _MakeRequestNoRetry; redirections=redirections, connection_type=connection_type); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1570, in request; (response, content) = self._request(conn, authority, uri, request_uri, method, body, headers, redirections, cachekey); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1317, in _request; (response, content) = self._conn_request(conn, request_uri, method, body, headers); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1252, in _conn_request; conn.connect(); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1018, in connect; sock.connect((self.host, self.port)); File ""/home/ydliu/anaconda3/envs/py2.7/lib/python2.7/socket.py"", line 228, in meth; return getattr(self._sock,name)(*args); socket.timeout: timed out. return code: 1. ()",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970
https://github.com/google/deepvariant/issues/228#issuecomment-549130970:7485,Safety,timeout,timeout,7485," line 595, in ListObjects; global_params=global_params); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/third_party/storage_apitools/storage_v1_client.py"", line 1237, in List; config, request, global_params=global_params); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/base_api.py"", line 701, in _RunMethod; http, http_request, **opts); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 341, in MakeRequest; check_response_func=check_response_func); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 391, in _MakeRequestNoRetry; redirections=redirections, connection_type=connection_type); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1570, in request; (response, content) = self._request(conn, authority, uri, request_uri, method, body, headers, redirections, cachekey); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1317, in _request; (response, content) = self._conn_request(conn, request_uri, method, body, headers); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1252, in _conn_request; conn.connect(); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1018, in connect; sock.connect((self.host, self.port)); File ""/home/ydliu/anaconda3/envs/py2.7/lib/python2.7/socket.py"", line 228, in meth; return getattr(self._sock,name)(*args); socket.timeout: timed out. return code: 1. ()",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970
https://github.com/google/deepvariant/issues/228#issuecomment-549166457:45,Availability,error,error,45,"Thanks for following up and for the detailed error log. This looks again like a connectivity issue. The deepvariant recipe needs to pull down the training files from a Google Bucket and is timing out trying to connect. Either there was a general internet issue or there is something blocking access to the bucket. If you re-try multiple times, does it fail in the same way? If so, you may need to investigate if the machine can access Google buckets for download. Hope this helps with tracking down the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549166457
https://github.com/google/deepvariant/issues/228#issuecomment-549166457:137,Availability,down,down,137,"Thanks for following up and for the detailed error log. This looks again like a connectivity issue. The deepvariant recipe needs to pull down the training files from a Google Bucket and is timing out trying to connect. Either there was a general internet issue or there is something blocking access to the bucket. If you re-try multiple times, does it fail in the same way? If so, you may need to investigate if the machine can access Google buckets for download. Hope this helps with tracking down the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549166457
https://github.com/google/deepvariant/issues/228#issuecomment-549166457:454,Availability,down,download,454,"Thanks for following up and for the detailed error log. This looks again like a connectivity issue. The deepvariant recipe needs to pull down the training files from a Google Bucket and is timing out trying to connect. Either there was a general internet issue or there is something blocking access to the bucket. If you re-try multiple times, does it fail in the same way? If so, you may need to investigate if the machine can access Google buckets for download. Hope this helps with tracking down the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549166457
https://github.com/google/deepvariant/issues/228#issuecomment-549166457:494,Availability,down,down,494,"Thanks for following up and for the detailed error log. This looks again like a connectivity issue. The deepvariant recipe needs to pull down the training files from a Google Bucket and is timing out trying to connect. Either there was a general internet issue or there is something blocking access to the bucket. If you re-try multiple times, does it fail in the same way? If so, you may need to investigate if the machine can access Google buckets for download. Hope this helps with tracking down the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549166457
https://github.com/google/deepvariant/issues/228#issuecomment-549166457:292,Security,access,access,292,"Thanks for following up and for the detailed error log. This looks again like a connectivity issue. The deepvariant recipe needs to pull down the training files from a Google Bucket and is timing out trying to connect. Either there was a general internet issue or there is something blocking access to the bucket. If you re-try multiple times, does it fail in the same way? If so, you may need to investigate if the machine can access Google buckets for download. Hope this helps with tracking down the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549166457
https://github.com/google/deepvariant/issues/228#issuecomment-549166457:428,Security,access,access,428,"Thanks for following up and for the detailed error log. This looks again like a connectivity issue. The deepvariant recipe needs to pull down the training files from a Google Bucket and is timing out trying to connect. Either there was a general internet issue or there is something blocking access to the bucket. If you re-try multiple times, does it fail in the same way? If so, you may need to investigate if the machine can access Google buckets for download. Hope this helps with tracking down the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549166457
https://github.com/google/deepvariant/issues/228#issuecomment-549166457:51,Testability,log,log,51,"Thanks for following up and for the detailed error log. This looks again like a connectivity issue. The deepvariant recipe needs to pull down the training files from a Google Bucket and is timing out trying to connect. Either there was a general internet issue or there is something blocking access to the bucket. If you re-try multiple times, does it fail in the same way? If so, you may need to investigate if the machine can access Google buckets for download. Hope this helps with tracking down the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-549166457
https://github.com/google/deepvariant/issues/228#issuecomment-550114233:100,Availability,down,download,100,Thanks. ; I have solved this problem. The main reason is my machine can't access Google buckets for download as you say. I will close this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-550114233
https://github.com/google/deepvariant/issues/228#issuecomment-550114233:74,Security,access,access,74,Thanks. ; I have solved this problem. The main reason is my machine can't access Google buckets for download as you say. I will close this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-550114233
https://github.com/google/deepvariant/issues/228#issuecomment-614524936:163,Availability,error,errors,163,"> Hi,; > when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:; > ; > CondaError: Downloaded bytes did not match Content-Length; > url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > Content-Length: 229846992; > downloaded bytes: 217650750; > ; > Best. hey i have got the same error. can you pls help if it's solved by you ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-614524936
https://github.com/google/deepvariant/issues/228#issuecomment-614524936:217,Availability,Down,Downloaded,217,"> Hi,; > when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:; > ; > CondaError: Downloaded bytes did not match Content-Length; > url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > Content-Length: 229846992; > downloaded bytes: 217650750; > ; > Best. hey i have got the same error. can you pls help if it's solved by you ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-614524936
https://github.com/google/deepvariant/issues/228#issuecomment-614524936:473,Availability,down,downloaded,473,"> Hi,; > when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:; > ; > CondaError: Downloaded bytes did not match Content-Length; > url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > Content-Length: 229846992; > downloaded bytes: 217650750; > ; > Best. hey i have got the same error. can you pls help if it's solved by you ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-614524936
https://github.com/google/deepvariant/issues/228#issuecomment-614524936:538,Availability,error,error,538,"> Hi,; > when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:; > ; > CondaError: Downloaded bytes did not match Content-Length; > url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > Content-Length: 229846992; > downloaded bytes: 217650750; > ; > Best. hey i have got the same error. can you pls help if it's solved by you ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-614524936
https://github.com/google/deepvariant/issues/228#issuecomment-614524936:16,Deployability,install,install,16,"> Hi,; > when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:; > ; > CondaError: Downloaded bytes did not match Content-Length; > url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > Content-Length: 229846992; > downloaded bytes: 217650750; > ; > Best. hey i have got the same error. can you pls help if it's solved by you ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-614524936
https://github.com/google/deepvariant/issues/228#issuecomment-614524936:72,Deployability,install,install,72,"> Hi,; > when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:; > ; > CondaError: Downloaded bytes did not match Content-Length; > url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > Content-Length: 229846992; > downloaded bytes: 217650750; > ; > Best. hey i have got the same error. can you pls help if it's solved by you ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-614524936
https://github.com/google/deepvariant/issues/228#issuecomment-614524936:128,Deployability,install,installed,128,"> Hi,; > when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:; > ; > CondaError: Downloaded bytes did not match Content-Length; > url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > Content-Length: 229846992; > downloaded bytes: 217650750; > ; > Best. hey i have got the same error. can you pls help if it's solved by you ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-614524936
https://github.com/google/deepvariant/issues/228#issuecomment-614524936:184,Deployability,install,installation,184,"> Hi,; > when I install deepvariant by anaconda with the command ""conda install -c bioconda deepvariant"", version 0.8.0 will be installed, but I got the following errors at the end of installation:; > ; > CondaError: Downloaded bytes did not match Content-Length; > url: https://conda.anaconda.org/bioconda/linux-64/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > target_path: /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2; > Content-Length: 229846992; > downloaded bytes: 217650750; > ; > Best. hey i have got the same error. can you pls help if it's solved by you ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/228#issuecomment-614524936
https://github.com/google/deepvariant/issues/229#issuecomment-545946241:256,Usability,simpl,simply,256,"Thank you very much for your reply. However I have 2 questions regarding the example visualization. . 1. As I am using multiple cores, the example files are splitted in to ; examples.tfrecord-00000-of-00008.gz to examples.tfrecord-00007-of-00008.gz. can I simply use ""examples.tfrecord-00000-of-00008.gz"" as the source path? or do I have to combine the 8 examples file together first?. 2. I cannot run the program as stated in the notebook as the `label` is not one of the features in the example for deepvariant. Is the label in the notebook the same as `alt_allele_indices/encoded` in Deepvariant? If so, how can I extract the information from the feature? I have tried 'alt_allele_indices/encoded': tf.FixedLenFeature([], tf.string), but it just gives my random symbols.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/229#issuecomment-545946241
https://github.com/google/deepvariant/issues/230#issuecomment-546050008:187,Availability,down,downsampling,187,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008
https://github.com/google/deepvariant/issues/230#issuecomment-546050008:605,Availability,down,downsampling,605,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008
https://github.com/google/deepvariant/issues/230#issuecomment-546050008:718,Availability,down,downsample,718,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008
https://github.com/google/deepvariant/issues/230#issuecomment-546050008:1101,Availability,down,downsampling,1101,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008
https://github.com/google/deepvariant/issues/230#issuecomment-546050008:814,Modifiability,evolve,evolved,814,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008
https://github.com/google/deepvariant/issues/230#issuecomment-546050008:885,Modifiability,Extend,Extending,885,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008
https://github.com/google/deepvariant/issues/230#issuecomment-546050008:470,Performance,perform,perform,470,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008
https://github.com/google/deepvariant/issues/230#issuecomment-546050008:577,Usability,learn,learn,577,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008
https://github.com/google/deepvariant/issues/230#issuecomment-553653477:91,Energy Efficiency,reduce,reduces,91,"Hi @anands-repo . You are correct, subsampling is static. A nice effect of this is that it reduces the complexity in terms of training reproducibility. We have not deeply investigated whether dynamic resampling (making a different image per epoch) would benefit training. It's an interesting question. It could potentially reduce overfitting that might occur at the read level and therefore allow training to progress through more epochs before a model is selected. . I think it is unlikely that this would improve the current production training setup, but it is not impossible. For the WGS training curves, there is little overfitting apparent in training graphs over a large number of epochs. For the WES training curves, some overfitting is apparent, but we suspect this is less due to signal from the read level and more due to the smaller number of regions represented in the exome. This is one reason that we currently train the exome model by warmstarting from the WGS model. It is probably worth us taking a look at some point, but likely isn't the lowest hanging fruit for us to improve performance. Thank you for the suggestion and discussion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-553653477
https://github.com/google/deepvariant/issues/230#issuecomment-553653477:323,Energy Efficiency,reduce,reduce,323,"Hi @anands-repo . You are correct, subsampling is static. A nice effect of this is that it reduces the complexity in terms of training reproducibility. We have not deeply investigated whether dynamic resampling (making a different image per epoch) would benefit training. It's an interesting question. It could potentially reduce overfitting that might occur at the read level and therefore allow training to progress through more epochs before a model is selected. . I think it is unlikely that this would improve the current production training setup, but it is not impossible. For the WGS training curves, there is little overfitting apparent in training graphs over a large number of epochs. For the WES training curves, some overfitting is apparent, but we suspect this is less due to signal from the read level and more due to the smaller number of regions represented in the exome. This is one reason that we currently train the exome model by warmstarting from the WGS model. It is probably worth us taking a look at some point, but likely isn't the lowest hanging fruit for us to improve performance. Thank you for the suggestion and discussion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-553653477
https://github.com/google/deepvariant/issues/230#issuecomment-553653477:1097,Performance,perform,performance,1097,"Hi @anands-repo . You are correct, subsampling is static. A nice effect of this is that it reduces the complexity in terms of training reproducibility. We have not deeply investigated whether dynamic resampling (making a different image per epoch) would benefit training. It's an interesting question. It could potentially reduce overfitting that might occur at the read level and therefore allow training to progress through more epochs before a model is selected. . I think it is unlikely that this would improve the current production training setup, but it is not impossible. For the WGS training curves, there is little overfitting apparent in training graphs over a large number of epochs. For the WES training curves, some overfitting is apparent, but we suspect this is less due to signal from the read level and more due to the smaller number of regions represented in the exome. This is one reason that we currently train the exome model by warmstarting from the WGS model. It is probably worth us taking a look at some point, but likely isn't the lowest hanging fruit for us to improve performance. Thank you for the suggestion and discussion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-553653477
https://github.com/google/deepvariant/issues/230#issuecomment-553717173:775,Energy Efficiency,reduce,reduces,775,"@AndrewCarroll Thanks for sharing your thoughts. I understand regarding the complexities involved, and that this may not be a low-hanging fruit. Also, bigger gains may be had from better data I imagine. While GIAB and Platinum Genomes are great, it would be great if the Syndip dataset would be pulled into a similar effort such as GIAB. Regarding dynamic subsampling, there may also be a need to correct for some effects such as re-thresholding with the 0.12 allele fraction cut-off after subsampling etc. The effect of realignment at a lower coverage (coverage after subsampling) may come into picture as well though may be not that much. Dynamic subsampling could result in a different input data statistic overall. May be your original assessment that static subsampling reduces complexity is a good enough argument to just continue with it. Thanks for the discussion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/230#issuecomment-553717173
https://github.com/google/deepvariant/issues/231#issuecomment-548147127:170,Availability,error,error,170,"Hi @mano2991 , Two questions:; (1) Can you tell me what environment (e.g., OS, version) you're running this on? ; There are also a few unexpected warnings (like the ""GPG error"" ones) before the bazel error, which I don't see when I test in my run.; (2); Can you try rerunning with:; `bash -x build-prereq.sh` which should give you a bit more context on what was executed around the error?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231#issuecomment-548147127
https://github.com/google/deepvariant/issues/231#issuecomment-548147127:200,Availability,error,error,200,"Hi @mano2991 , Two questions:; (1) Can you tell me what environment (e.g., OS, version) you're running this on? ; There are also a few unexpected warnings (like the ""GPG error"" ones) before the bazel error, which I don't see when I test in my run.; (2); Can you try rerunning with:; `bash -x build-prereq.sh` which should give you a bit more context on what was executed around the error?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231#issuecomment-548147127
https://github.com/google/deepvariant/issues/231#issuecomment-548147127:382,Availability,error,error,382,"Hi @mano2991 , Two questions:; (1) Can you tell me what environment (e.g., OS, version) you're running this on? ; There are also a few unexpected warnings (like the ""GPG error"" ones) before the bazel error, which I don't see when I test in my run.; (2); Can you try rerunning with:; `bash -x build-prereq.sh` which should give you a bit more context on what was executed around the error?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231#issuecomment-548147127
https://github.com/google/deepvariant/issues/231#issuecomment-548147127:232,Testability,test,test,232,"Hi @mano2991 , Two questions:; (1) Can you tell me what environment (e.g., OS, version) you're running this on? ; There are also a few unexpected warnings (like the ""GPG error"" ones) before the bazel error, which I don't see when I test in my run.; (2); Can you try rerunning with:; `bash -x build-prereq.sh` which should give you a bit more context on what was executed around the error?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231#issuecomment-548147127
https://github.com/google/deepvariant/issues/231#issuecomment-554460778:100,Availability,error,error,100,"@mano2991 I'll close this issue for now, but feel free to reopen if you are still running into this error. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/231#issuecomment-554460778
https://github.com/google/deepvariant/issues/232#issuecomment-548613339:205,Availability,error,errors,205,"I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); (2) Is this reliably reproducible on the same input?. Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548613339
https://github.com/google/deepvariant/issues/232#issuecomment-548613339:432,Availability,reliab,reliably,432,"I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); (2) Is this reliably reproducible on the same input?. Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548613339
https://github.com/google/deepvariant/issues/232#issuecomment-548613339:545,Availability,error,error,545,"I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); (2) Is this reliably reproducible on the same input?. Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548613339
https://github.com/google/deepvariant/issues/232#issuecomment-548613339:351,Integrability,message,message,351,"I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); (2) Is this reliably reproducible on the same input?. Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548613339
https://github.com/google/deepvariant/issues/232#issuecomment-548613339:551,Integrability,message,messages,551,"I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); (2) Is this reliably reproducible on the same input?. Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548613339
https://github.com/google/deepvariant/issues/232#issuecomment-548709854:209,Availability,error,errors,209,"> I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; > Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; > (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); > (2) Is this reliably reproducible on the same input?; > ; > Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it. Dear pichuan,. It is reliably reproducible with the same input (4x) (16x CPU, 30GB RAM).; When I run on a server with 32x CPU and 60GB RAM I got similar error:. ```; I1030 20:48:33.115523 140410104575744 make_examples.py:1167] Found 487 candidate variants; I1030 20:48:33.116027 140410104575744 make_examples.py:1168] Created 499 examples; real	3m0.610s; user	74m46.176s; sys	3m15.360s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 63 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/dnanexus/ref.fa"" --reads ""/home/dnanexus/input.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --regions ""/home/dnanexus/regions.bed"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@64.gz"" --task {}' returned non-zero e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548709854
https://github.com/google/deepvariant/issues/232#issuecomment-548709854:440,Availability,reliab,reliably,440,"> I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; > Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; > (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); > (2) Is this reliably reproducible on the same input?; > ; > Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it. Dear pichuan,. It is reliably reproducible with the same input (4x) (16x CPU, 30GB RAM).; When I run on a server with 32x CPU and 60GB RAM I got similar error:. ```; I1030 20:48:33.115523 140410104575744 make_examples.py:1167] Found 487 candidate variants; I1030 20:48:33.116027 140410104575744 make_examples.py:1168] Created 499 examples; real	3m0.610s; user	74m46.176s; sys	3m15.360s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 63 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/dnanexus/ref.fa"" --reads ""/home/dnanexus/input.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --regions ""/home/dnanexus/regions.bed"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@64.gz"" --task {}' returned non-zero e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548709854
https://github.com/google/deepvariant/issues/232#issuecomment-548709854:559,Availability,error,error,559,"> I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; > Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; > (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); > (2) Is this reliably reproducible on the same input?; > ; > Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it. Dear pichuan,. It is reliably reproducible with the same input (4x) (16x CPU, 30GB RAM).; When I run on a server with 32x CPU and 60GB RAM I got similar error:. ```; I1030 20:48:33.115523 140410104575744 make_examples.py:1167] Found 487 candidate variants; I1030 20:48:33.116027 140410104575744 make_examples.py:1168] Created 499 examples; real	3m0.610s; user	74m46.176s; sys	3m15.360s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 63 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/dnanexus/ref.fa"" --reads ""/home/dnanexus/input.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --regions ""/home/dnanexus/regions.bed"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@64.gz"" --task {}' returned non-zero e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548709854
https://github.com/google/deepvariant/issues/232#issuecomment-548709854:674,Availability,reliab,reliably,674,"> I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; > Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; > (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); > (2) Is this reliably reproducible on the same input?; > ; > Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it. Dear pichuan,. It is reliably reproducible with the same input (4x) (16x CPU, 30GB RAM).; When I run on a server with 32x CPU and 60GB RAM I got similar error:. ```; I1030 20:48:33.115523 140410104575744 make_examples.py:1167] Found 487 candidate variants; I1030 20:48:33.116027 140410104575744 make_examples.py:1168] Created 499 examples; real	3m0.610s; user	74m46.176s; sys	3m15.360s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 63 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/dnanexus/ref.fa"" --reads ""/home/dnanexus/input.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --regions ""/home/dnanexus/regions.bed"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@64.gz"" --task {}' returned non-zero e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548709854
https://github.com/google/deepvariant/issues/232#issuecomment-548709854:806,Availability,error,error,806,"> I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; > Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; > (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); > (2) Is this reliably reproducible on the same input?; > ; > Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it. Dear pichuan,. It is reliably reproducible with the same input (4x) (16x CPU, 30GB RAM).; When I run on a server with 32x CPU and 60GB RAM I got similar error:. ```; I1030 20:48:33.115523 140410104575744 make_examples.py:1167] Found 487 candidate variants; I1030 20:48:33.116027 140410104575744 make_examples.py:1168] Created 499 examples; real	3m0.610s; user	74m46.176s; sys	3m15.360s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 63 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/dnanexus/ref.fa"" --reads ""/home/dnanexus/input.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --regions ""/home/dnanexus/regions.bed"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@64.gz"" --task {}' returned non-zero e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548709854
https://github.com/google/deepvariant/issues/232#issuecomment-548709854:357,Integrability,message,message,357,"> I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; > Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; > (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); > (2) Is this reliably reproducible on the same input?; > ; > Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it. Dear pichuan,. It is reliably reproducible with the same input (4x) (16x CPU, 30GB RAM).; When I run on a server with 32x CPU and 60GB RAM I got similar error:. ```; I1030 20:48:33.115523 140410104575744 make_examples.py:1167] Found 487 candidate variants; I1030 20:48:33.116027 140410104575744 make_examples.py:1168] Created 499 examples; real	3m0.610s; user	74m46.176s; sys	3m15.360s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 63 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/dnanexus/ref.fa"" --reads ""/home/dnanexus/input.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --regions ""/home/dnanexus/regions.bed"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@64.gz"" --task {}' returned non-zero e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548709854
https://github.com/google/deepvariant/issues/232#issuecomment-548709854:565,Integrability,message,messages,565,"> I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; > Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; > (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); > (2) Is this reliably reproducible on the same input?; > ; > Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it. Dear pichuan,. It is reliably reproducible with the same input (4x) (16x CPU, 30GB RAM).; When I run on a server with 32x CPU and 60GB RAM I got similar error:. ```; I1030 20:48:33.115523 140410104575744 make_examples.py:1167] Found 487 candidate variants; I1030 20:48:33.116027 140410104575744 make_examples.py:1168] Created 499 examples; real	3m0.610s; user	74m46.176s; sys	3m15.360s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 63 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/dnanexus/ref.fa"" --reads ""/home/dnanexus/input.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --regions ""/home/dnanexus/regions.bed"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@64.gz"" --task {}' returned non-zero e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548709854
https://github.com/google/deepvariant/issues/232#issuecomment-548948642:87,Availability,error,error,87,"Hi @bopohdr, if it's possible, can you send me to full log?. In order to experience an error, I tried our example in Quick Start, but I deliberately make the FASTA file incompatible with the BAM file by removing the `chr` prefix. The error I got looks like this:; ![error](https://user-images.githubusercontent.com/471813/68056060-6f66df80-fcaf-11e9-875b-c56db1303c40.png). Note that the bottom error message is similar to yours, but it's not the most informative one, because it was just telling you that the wrapper script [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.8/scripts/run_deepvariant.py) is unable to run make_examples successfully. I'm hoping that somewhere above this last error message, you might be able to see more what have gone wrong. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548948642
https://github.com/google/deepvariant/issues/232#issuecomment-548948642:234,Availability,error,error,234,"Hi @bopohdr, if it's possible, can you send me to full log?. In order to experience an error, I tried our example in Quick Start, but I deliberately make the FASTA file incompatible with the BAM file by removing the `chr` prefix. The error I got looks like this:; ![error](https://user-images.githubusercontent.com/471813/68056060-6f66df80-fcaf-11e9-875b-c56db1303c40.png). Note that the bottom error message is similar to yours, but it's not the most informative one, because it was just telling you that the wrapper script [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.8/scripts/run_deepvariant.py) is unable to run make_examples successfully. I'm hoping that somewhere above this last error message, you might be able to see more what have gone wrong. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548948642
https://github.com/google/deepvariant/issues/232#issuecomment-548948642:266,Availability,error,error,266,"Hi @bopohdr, if it's possible, can you send me to full log?. In order to experience an error, I tried our example in Quick Start, but I deliberately make the FASTA file incompatible with the BAM file by removing the `chr` prefix. The error I got looks like this:; ![error](https://user-images.githubusercontent.com/471813/68056060-6f66df80-fcaf-11e9-875b-c56db1303c40.png). Note that the bottom error message is similar to yours, but it's not the most informative one, because it was just telling you that the wrapper script [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.8/scripts/run_deepvariant.py) is unable to run make_examples successfully. I'm hoping that somewhere above this last error message, you might be able to see more what have gone wrong. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548948642
https://github.com/google/deepvariant/issues/232#issuecomment-548948642:395,Availability,error,error,395,"Hi @bopohdr, if it's possible, can you send me to full log?. In order to experience an error, I tried our example in Quick Start, but I deliberately make the FASTA file incompatible with the BAM file by removing the `chr` prefix. The error I got looks like this:; ![error](https://user-images.githubusercontent.com/471813/68056060-6f66df80-fcaf-11e9-875b-c56db1303c40.png). Note that the bottom error message is similar to yours, but it's not the most informative one, because it was just telling you that the wrapper script [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.8/scripts/run_deepvariant.py) is unable to run make_examples successfully. I'm hoping that somewhere above this last error message, you might be able to see more what have gone wrong. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548948642
https://github.com/google/deepvariant/issues/232#issuecomment-548948642:709,Availability,error,error,709,"Hi @bopohdr, if it's possible, can you send me to full log?. In order to experience an error, I tried our example in Quick Start, but I deliberately make the FASTA file incompatible with the BAM file by removing the `chr` prefix. The error I got looks like this:; ![error](https://user-images.githubusercontent.com/471813/68056060-6f66df80-fcaf-11e9-875b-c56db1303c40.png). Note that the bottom error message is similar to yours, but it's not the most informative one, because it was just telling you that the wrapper script [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.8/scripts/run_deepvariant.py) is unable to run make_examples successfully. I'm hoping that somewhere above this last error message, you might be able to see more what have gone wrong. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548948642
https://github.com/google/deepvariant/issues/232#issuecomment-548948642:401,Integrability,message,message,401,"Hi @bopohdr, if it's possible, can you send me to full log?. In order to experience an error, I tried our example in Quick Start, but I deliberately make the FASTA file incompatible with the BAM file by removing the `chr` prefix. The error I got looks like this:; ![error](https://user-images.githubusercontent.com/471813/68056060-6f66df80-fcaf-11e9-875b-c56db1303c40.png). Note that the bottom error message is similar to yours, but it's not the most informative one, because it was just telling you that the wrapper script [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.8/scripts/run_deepvariant.py) is unable to run make_examples successfully. I'm hoping that somewhere above this last error message, you might be able to see more what have gone wrong. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548948642
https://github.com/google/deepvariant/issues/232#issuecomment-548948642:510,Integrability,wrap,wrapper,510,"Hi @bopohdr, if it's possible, can you send me to full log?. In order to experience an error, I tried our example in Quick Start, but I deliberately make the FASTA file incompatible with the BAM file by removing the `chr` prefix. The error I got looks like this:; ![error](https://user-images.githubusercontent.com/471813/68056060-6f66df80-fcaf-11e9-875b-c56db1303c40.png). Note that the bottom error message is similar to yours, but it's not the most informative one, because it was just telling you that the wrapper script [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.8/scripts/run_deepvariant.py) is unable to run make_examples successfully. I'm hoping that somewhere above this last error message, you might be able to see more what have gone wrong. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548948642
https://github.com/google/deepvariant/issues/232#issuecomment-548948642:715,Integrability,message,message,715,"Hi @bopohdr, if it's possible, can you send me to full log?. In order to experience an error, I tried our example in Quick Start, but I deliberately make the FASTA file incompatible with the BAM file by removing the `chr` prefix. The error I got looks like this:; ![error](https://user-images.githubusercontent.com/471813/68056060-6f66df80-fcaf-11e9-875b-c56db1303c40.png). Note that the bottom error message is similar to yours, but it's not the most informative one, because it was just telling you that the wrapper script [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.8/scripts/run_deepvariant.py) is unable to run make_examples successfully. I'm hoping that somewhere above this last error message, you might be able to see more what have gone wrong. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548948642
https://github.com/google/deepvariant/issues/232#issuecomment-548948642:55,Testability,log,log,55,"Hi @bopohdr, if it's possible, can you send me to full log?. In order to experience an error, I tried our example in Quick Start, but I deliberately make the FASTA file incompatible with the BAM file by removing the `chr` prefix. The error I got looks like this:; ![error](https://user-images.githubusercontent.com/471813/68056060-6f66df80-fcaf-11e9-875b-c56db1303c40.png). Note that the bottom error message is similar to yours, but it's not the most informative one, because it was just telling you that the wrapper script [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.8/scripts/run_deepvariant.py) is unable to run make_examples successfully. I'm hoping that somewhere above this last error message, you might be able to see more what have gone wrong. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-548948642
https://github.com/google/deepvariant/issues/232#issuecomment-549153164:9,Testability,log,logs,9,"The full logs are attached for exit status 16 and 20: ; [Logs_deepvariant.txt](https://github.com/google/deepvariant/files/3801760/Logs_deepvariant.txt). I have found, that the Deepvariant works fine for the same .bam file, but with another target region .bed file (both are attached). Again, the target file that does not work for some samples, works fine for other samples. [Works_fine_Twist_Exome_Target.txt](https://github.com/google/deepvariant/files/3801765/Works_fine_Twist_Exome_Target.txt); [does_not_work_Trusight_one_bed.txt](https://github.com/google/deepvariant/files/3801766/does_not_work_Trusight_one_bed.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-549153164
https://github.com/google/deepvariant/issues/232#issuecomment-549225504:55,Availability,error,error,55,"Hi @bopohdr ; Thank you, the log is helpful.; From the error, specifically this line:; ```; ValueError: Failed precondition: Cannot query without an index; ```; It looks like maybe your BAM file doesn't have a corresponding *.bai file. Can you double check that your input BAM file has a correct *.bai file associated with it? Usually, if you have a BAM file named `foo.bam`, there should be a corresponding `foo.bam.bai` or `foo.bai` in the same directory. If it doesn't exist, please run:; `samtools index foo.bam` to generate an index file before you proceed. Let me know if this resolves your issue. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-549225504
https://github.com/google/deepvariant/issues/232#issuecomment-549225504:29,Testability,log,log,29,"Hi @bopohdr ; Thank you, the log is helpful.; From the error, specifically this line:; ```; ValueError: Failed precondition: Cannot query without an index; ```; It looks like maybe your BAM file doesn't have a corresponding *.bai file. Can you double check that your input BAM file has a correct *.bai file associated with it? Usually, if you have a BAM file named `foo.bam`, there should be a corresponding `foo.bam.bai` or `foo.bai` in the same directory. If it doesn't exist, please run:; `samtools index foo.bam` to generate an index file before you proceed. Let me know if this resolves your issue. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-549225504
https://github.com/google/deepvariant/issues/232#issuecomment-549458167:224,Availability,error,error,224,"Hi @pichuan ,; You are right, the problem is with the index file. ; samtools index is done for all samples before running deepvariant , however for some of the larger .bam files it did not produced index files and therefore error in deepvariant. . Thank you !",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-549458167
https://github.com/google/deepvariant/issues/232#issuecomment-596040813:190,Availability,error,errors,190,"Hi @sclan ; to give you an update, I made an internal fix to make sure [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py) can output more clear errors when they occur.; The new code is not on GitHub yet, but will come out in the next release. Specifically, I changed the main function to be: ; ```; def main(_):; check_or_create_intermediate_results_dir(FLAGS.intermediate_results_dir); check_flags(). commands = create_all_commands(); for command in commands:; print('\n***** Running the command:*****\n{}\n'.format(command)); try:; subprocess.check_call(command, shell=True, executable='/bin/bash'); except subprocess.CalledProcessError as e:; logging.info(e.output); raise; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-596040813
https://github.com/google/deepvariant/issues/232#issuecomment-596040813:27,Deployability,update,update,27,"Hi @sclan ; to give you an update, I made an internal fix to make sure [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py) can output more clear errors when they occur.; The new code is not on GitHub yet, but will come out in the next release. Specifically, I changed the main function to be: ; ```; def main(_):; check_or_create_intermediate_results_dir(FLAGS.intermediate_results_dir); check_flags(). commands = create_all_commands(); for command in commands:; print('\n***** Running the command:*****\n{}\n'.format(command)); try:; subprocess.check_call(command, shell=True, executable='/bin/bash'); except subprocess.CalledProcessError as e:; logging.info(e.output); raise; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-596040813
https://github.com/google/deepvariant/issues/232#issuecomment-596040813:280,Deployability,release,release,280,"Hi @sclan ; to give you an update, I made an internal fix to make sure [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py) can output more clear errors when they occur.; The new code is not on GitHub yet, but will come out in the next release. Specifically, I changed the main function to be: ; ```; def main(_):; check_or_create_intermediate_results_dir(FLAGS.intermediate_results_dir); check_flags(). commands = create_all_commands(); for command in commands:; print('\n***** Running the command:*****\n{}\n'.format(command)); try:; subprocess.check_call(command, shell=True, executable='/bin/bash'); except subprocess.CalledProcessError as e:; logging.info(e.output); raise; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-596040813
https://github.com/google/deepvariant/issues/232#issuecomment-596040813:692,Testability,log,logging,692,"Hi @sclan ; to give you an update, I made an internal fix to make sure [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py) can output more clear errors when they occur.; The new code is not on GitHub yet, but will come out in the next release. Specifically, I changed the main function to be: ; ```; def main(_):; check_or_create_intermediate_results_dir(FLAGS.intermediate_results_dir); check_flags(). commands = create_all_commands(); for command in commands:; print('\n***** Running the command:*****\n{}\n'.format(command)); try:; subprocess.check_call(command, shell=True, executable='/bin/bash'); except subprocess.CalledProcessError as e:; logging.info(e.output); raise; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-596040813
https://github.com/google/deepvariant/issues/232#issuecomment-596040813:184,Usability,clear,clear,184,"Hi @sclan ; to give you an update, I made an internal fix to make sure [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py) can output more clear errors when they occur.; The new code is not on GitHub yet, but will come out in the next release. Specifically, I changed the main function to be: ; ```; def main(_):; check_or_create_intermediate_results_dir(FLAGS.intermediate_results_dir); check_flags(). commands = create_all_commands(); for command in commands:; print('\n***** Running the command:*****\n{}\n'.format(command)); try:; subprocess.check_call(command, shell=True, executable='/bin/bash'); except subprocess.CalledProcessError as e:; logging.info(e.output); raise; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/232#issuecomment-596040813
https://github.com/google/deepvariant/issues/233#issuecomment-550436768:1226,Deployability,update,updated,1226,"Hi @ydLiu-HIT . Thank you for your question. The answer to this is complicated. It looks like the region that these elements is in is a LINE element - long regions with multiple copies through the genome that have high sequence similarity to each other. Because of the high sequence similarity, reads to line elements can map to other parts of the genome, and they are generally very difficult regions to call correctly. We've seen the behavior in DeepVariant not calling variants that are near other variants and in regions with two (or more) variant-rich haplotypes. We think that one of the reasons for this is that DeepVariant has learned that these regions represent uncaptured segmental duplication and LINE elements, which are often labelled as not variant in the more comprehensive genome in a bottle truth set. . Whether these positions represent true variants at that position, or sequences from a similar LINE element elsewhere is difficult to say. Since this is HG002, if this is within the confident regions, you can see whether Genome in a Bottle indicates them to be true variants. However, Genome in a Bottle has some more recent corrections to variants in/near LINE elements, so it may be better to check the updated (though still beta) [truth set](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4beta_SmallVariantDraftBenchmark_07192019/). DeepVariant will output every candidate considered, so if you want to find positions that are called in this way, looking for 0/0 or ./. calls with more than 35% ALT support and within 100bp of 2 or more candidate variants may be able to pull out many of these examples. . The other option to pull out examples like this would be to intersect with a LINE element annotation track from UCSC. Please let us know if there is something unclear about this answer. This is a rather complicated concept and explanation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/233#issuecomment-550436768
https://github.com/google/deepvariant/issues/233#issuecomment-550436768:635,Usability,learn,learned,635,"Hi @ydLiu-HIT . Thank you for your question. The answer to this is complicated. It looks like the region that these elements is in is a LINE element - long regions with multiple copies through the genome that have high sequence similarity to each other. Because of the high sequence similarity, reads to line elements can map to other parts of the genome, and they are generally very difficult regions to call correctly. We've seen the behavior in DeepVariant not calling variants that are near other variants and in regions with two (or more) variant-rich haplotypes. We think that one of the reasons for this is that DeepVariant has learned that these regions represent uncaptured segmental duplication and LINE elements, which are often labelled as not variant in the more comprehensive genome in a bottle truth set. . Whether these positions represent true variants at that position, or sequences from a similar LINE element elsewhere is difficult to say. Since this is HG002, if this is within the confident regions, you can see whether Genome in a Bottle indicates them to be true variants. However, Genome in a Bottle has some more recent corrections to variants in/near LINE elements, so it may be better to check the updated (though still beta) [truth set](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4beta_SmallVariantDraftBenchmark_07192019/). DeepVariant will output every candidate considered, so if you want to find positions that are called in this way, looking for 0/0 or ./. calls with more than 35% ALT support and within 100bp of 2 or more candidate variants may be able to pull out many of these examples. . The other option to pull out examples like this would be to intersect with a LINE element annotation track from UCSC. Please let us know if there is something unclear about this answer. This is a rather complicated concept and explanation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/233#issuecomment-550436768
https://github.com/google/deepvariant/pull/234#issuecomment-554897702:898,Security,authoriz,authorized,898,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F234) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/234#issuecomment-554897702
https://github.com/google/deepvariant/pull/234#issuecomment-554897702:966,Security,authoriz,authorized,966,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F234) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/234#issuecomment-554897702
https://github.com/google/deepvariant/pull/234#issuecomment-554897702:1239,Security,authoriz,authorized,1239,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F234) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/234#issuecomment-554897702
https://github.com/google/deepvariant/pull/234#issuecomment-554897702:1534,Security,authoriz,authorized,1534,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F234) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/234#issuecomment-554897702
https://github.com/google/deepvariant/issues/235#issuecomment-557203919:66,Deployability,update,updated,66,"I'm going to close this issue, but please let us know whether the updated Docker image works for you. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/235#issuecomment-557203919
https://github.com/google/deepvariant/issues/235#issuecomment-557652463:15,Deployability,update,updated,15,"Hi Maria,. The updated Docker image made the module work perfectly. I appreciate the quick fix and response. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/235#issuecomment-557652463
https://github.com/google/deepvariant/issues/236#issuecomment-557101397:218,Availability,error,error,218,"Thank you for reporting the issue.; Can you tell us more about what environment you're building it in? (OS version, gcc version); I tried build_and_test.sh just now, and didn't see any issue. I'd like to reproduce the error so we can help fix it for your setting. For example, here is my version:; ```; pichuan@pichuan-build:~$ uname -a; Linux pichuan-build 4.15.0-1049-gcp #52-Ubuntu SMP Fri Nov 8 10:30:54 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux; pichuan@pichuan-build:~/deepvariant$ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.6 LTS; Release: 16.04; Codename: xenial; ```. Then I clone our repo:; ```; pichuan@pichuan-build:~$ git clone https://github.com/google/deepvariant.git; ```; Confirmed it's on r0.9:; ```; pichuan@pichuan-build:~$ cd deepvariant/; pichuan@pichuan-build:~/deepvariant$ git branch; * r0.9; ```; And then I build:; ```; pichuan@pichuan-build:~/deepvariant$ ./build-prereq.sh && ./build_and_test.sh ; ```; This completed without an error. I checked my gcc version:; ```; pichuan@pichuan-build:~/deepvariant$ gcc --version; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609; Copyright (C) 2015 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-557101397
https://github.com/google/deepvariant/issues/236#issuecomment-557101397:521,Availability,avail,available,521,"Thank you for reporting the issue.; Can you tell us more about what environment you're building it in? (OS version, gcc version); I tried build_and_test.sh just now, and didn't see any issue. I'd like to reproduce the error so we can help fix it for your setting. For example, here is my version:; ```; pichuan@pichuan-build:~$ uname -a; Linux pichuan-build 4.15.0-1049-gcp #52-Ubuntu SMP Fri Nov 8 10:30:54 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux; pichuan@pichuan-build:~/deepvariant$ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.6 LTS; Release: 16.04; Codename: xenial; ```. Then I clone our repo:; ```; pichuan@pichuan-build:~$ git clone https://github.com/google/deepvariant.git; ```; Confirmed it's on r0.9:; ```; pichuan@pichuan-build:~$ cd deepvariant/; pichuan@pichuan-build:~/deepvariant$ git branch; * r0.9; ```; And then I build:; ```; pichuan@pichuan-build:~/deepvariant$ ./build-prereq.sh && ./build_and_test.sh ; ```; This completed without an error. I checked my gcc version:; ```; pichuan@pichuan-build:~/deepvariant$ gcc --version; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609; Copyright (C) 2015 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-557101397
https://github.com/google/deepvariant/issues/236#issuecomment-557101397:1010,Availability,error,error,1010,"Thank you for reporting the issue.; Can you tell us more about what environment you're building it in? (OS version, gcc version); I tried build_and_test.sh just now, and didn't see any issue. I'd like to reproduce the error so we can help fix it for your setting. For example, here is my version:; ```; pichuan@pichuan-build:~$ uname -a; Linux pichuan-build 4.15.0-1049-gcp #52-Ubuntu SMP Fri Nov 8 10:30:54 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux; pichuan@pichuan-build:~/deepvariant$ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.6 LTS; Release: 16.04; Codename: xenial; ```. Then I clone our repo:; ```; pichuan@pichuan-build:~$ git clone https://github.com/google/deepvariant.git; ```; Confirmed it's on r0.9:; ```; pichuan@pichuan-build:~$ cd deepvariant/; pichuan@pichuan-build:~/deepvariant$ git branch; * r0.9; ```; And then I build:; ```; pichuan@pichuan-build:~/deepvariant$ ./build-prereq.sh && ./build_and_test.sh ; ```; This completed without an error. I checked my gcc version:; ```; pichuan@pichuan-build:~/deepvariant$ gcc --version; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609; Copyright (C) 2015 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-557101397
https://github.com/google/deepvariant/issues/236#issuecomment-557101397:590,Deployability,Release,Release,590,"Thank you for reporting the issue.; Can you tell us more about what environment you're building it in? (OS version, gcc version); I tried build_and_test.sh just now, and didn't see any issue. I'd like to reproduce the error so we can help fix it for your setting. For example, here is my version:; ```; pichuan@pichuan-build:~$ uname -a; Linux pichuan-build 4.15.0-1049-gcp #52-Ubuntu SMP Fri Nov 8 10:30:54 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux; pichuan@pichuan-build:~/deepvariant$ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.6 LTS; Release: 16.04; Codename: xenial; ```. Then I clone our repo:; ```; pichuan@pichuan-build:~$ git clone https://github.com/google/deepvariant.git; ```; Confirmed it's on r0.9:; ```; pichuan@pichuan-build:~$ cd deepvariant/; pichuan@pichuan-build:~/deepvariant$ git branch; * r0.9; ```; And then I build:; ```; pichuan@pichuan-build:~/deepvariant$ ./build-prereq.sh && ./build_and_test.sh ; ```; This completed without an error. I checked my gcc version:; ```; pichuan@pichuan-build:~/deepvariant$ gcc --version; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609; Copyright (C) 2015 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-557101397
https://github.com/google/deepvariant/issues/236#issuecomment-557117908:283,Availability,avail,available,283,"Thank you for answer.; We are using Ubuntu 14.04 for our images (yes, still).; ```; # uname -a; Linux 417d805a5037 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux; ```; ```; root@417d805a5037:/outputs# lsb_release -a; No LSB modules are available. ; Distributor ID: Ubuntu ; Description: Ubuntu 14.04.6 LTS ; Release: 14.04 ; Codename: trusty ; ```; ```; root@417d805a5037:/outputs# gcc --version ; gcc (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4 ; Copyright (C) 2013 Free Software Foundation, Inc. ; This is free software; see the source for copying conditions. There is NO ; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; ```. Maybe we need newer `gcc`?. And we don't clone repos - we download release archives.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-557117908
https://github.com/google/deepvariant/issues/236#issuecomment-557117908:757,Availability,down,download,757,"Thank you for answer.; We are using Ubuntu 14.04 for our images (yes, still).; ```; # uname -a; Linux 417d805a5037 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux; ```; ```; root@417d805a5037:/outputs# lsb_release -a; No LSB modules are available. ; Distributor ID: Ubuntu ; Description: Ubuntu 14.04.6 LTS ; Release: 14.04 ; Codename: trusty ; ```; ```; root@417d805a5037:/outputs# gcc --version ; gcc (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4 ; Copyright (C) 2013 Free Software Foundation, Inc. ; This is free software; see the source for copying conditions. There is NO ; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; ```. Maybe we need newer `gcc`?. And we don't clone repos - we download release archives.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-557117908
https://github.com/google/deepvariant/issues/236#issuecomment-557117908:355,Deployability,Release,Release,355,"Thank you for answer.; We are using Ubuntu 14.04 for our images (yes, still).; ```; # uname -a; Linux 417d805a5037 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux; ```; ```; root@417d805a5037:/outputs# lsb_release -a; No LSB modules are available. ; Distributor ID: Ubuntu ; Description: Ubuntu 14.04.6 LTS ; Release: 14.04 ; Codename: trusty ; ```; ```; root@417d805a5037:/outputs# gcc --version ; gcc (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4 ; Copyright (C) 2013 Free Software Foundation, Inc. ; This is free software; see the source for copying conditions. There is NO ; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; ```. Maybe we need newer `gcc`?. And we don't clone repos - we download release archives.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-557117908
https://github.com/google/deepvariant/issues/236#issuecomment-557117908:766,Deployability,release,release,766,"Thank you for answer.; We are using Ubuntu 14.04 for our images (yes, still).; ```; # uname -a; Linux 417d805a5037 3.10.0-1062.4.1.el7.x86_64 #1 SMP Fri Oct 18 17:15:30 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux; ```; ```; root@417d805a5037:/outputs# lsb_release -a; No LSB modules are available. ; Distributor ID: Ubuntu ; Description: Ubuntu 14.04.6 LTS ; Release: 14.04 ; Codename: trusty ; ```; ```; root@417d805a5037:/outputs# gcc --version ; gcc (Ubuntu 4.8.4-2ubuntu1~14.04.4) 4.8.4 ; Copyright (C) 2013 Free Software Foundation, Inc. ; This is free software; see the source for copying conditions. There is NO ; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; ```. Maybe we need newer `gcc`?. And we don't clone repos - we download release archives.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-557117908
https://github.com/google/deepvariant/issues/236#issuecomment-557244618:188,Deployability,update,update,188,"@Stikus Hi, I was able to reproduce the issue on Ubuntu 14.; We don't officially support building on Ubuntu 14, but I'll give it a try and see if I'm able to get it to work. I can give an update later. One question for you - Can you tell us the reason why you're not using our Docker image, and instead building your own binaries? It'll be useful for us to understand what our users need so we can prioritize the right things. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-557244618
https://github.com/google/deepvariant/issues/236#issuecomment-557265475:404,Deployability,release,release,404,"Hi @Stikus , ; actually , it seems like simply removing the line; ```; #include <optional>; ```; will build. From the code, we're using ""optional"" from tensorflow::gtl::optional. So we don't really need the #include here. I have confirmed that removing this line builds on Ubuntu14.04. Please give that a try. If it doesn't work, let me know. I will make an internal fix, which will come out in the next release. For now, please make a local edit before you build.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-557265475
https://github.com/google/deepvariant/issues/236#issuecomment-557265475:40,Usability,simpl,simply,40,"Hi @Stikus , ; actually , it seems like simply removing the line; ```; #include <optional>; ```; will build. From the code, we're using ""optional"" from tensorflow::gtl::optional. So we don't really need the #include here. I have confirmed that removing this line builds on Ubuntu14.04. Please give that a try. If it doesn't work, let me know. I will make an internal fix, which will come out in the next release. For now, please make a local edit before you build.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-557265475
https://github.com/google/deepvariant/issues/236#issuecomment-558062729:46,Testability,test,tests,46,"Thank you for help, with your tweak build and tests passed.; But I get different results on test data - described [here](https://github.com/google/deepvariant/issues/239#issuecomment-558061938).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-558062729
https://github.com/google/deepvariant/issues/236#issuecomment-558062729:92,Testability,test,test,92,"Thank you for help, with your tweak build and tests passed.; But I get different results on test data - described [here](https://github.com/google/deepvariant/issues/239#issuecomment-558061938).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/236#issuecomment-558062729
https://github.com/google/deepvariant/issues/238#issuecomment-557229093:355,Deployability,release,release,355,"Hi @segoerge . Thank you for your question. It is a good observation that better support for MNP would be helpful. We have discussed this internally to a limited extent, but have not yet mapped out what would be involved to make the change. I couldn't give a timeframe for this yet (or give an indication as to whether this could be something in the next release). Your feedback is appreciated, as it helps us understand the needs of the user community. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/238#issuecomment-557229093
https://github.com/google/deepvariant/issues/238#issuecomment-557229093:370,Usability,feedback,feedback,370,"Hi @segoerge . Thank you for your question. It is a good observation that better support for MNP would be helpful. We have discussed this internally to a limited extent, but have not yet mapped out what would be involved to make the change. I couldn't give a timeframe for this yet (or give an indication as to whether this could be something in the next release). Your feedback is appreciated, as it helps us understand the needs of the user community. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/238#issuecomment-557229093
https://github.com/google/deepvariant/issues/238#issuecomment-1700653315:344,Availability,down,downstream,344,"HI!; any update on this topic?. We called multiple genomes recently and found a first example of 4 adjacent base substitution AACT => GGTC being called 4 separate SNVs. The support information is very similar for the 4 calls and I wonder why deepvariant did not call them as one single MNP. This makes that the annotation and effect prediction downstream are wrong. Thanks for your support. ![failed_MNP-call](https://github.com/google/deepvariant/assets/858516/a02c3b5a-0362-4030-be8a-8b3c49129a8f). here is the extract of the gVCF at that location for one sample. ```; chrNN 51225801 . T <*> 0 . END=51225807 GT:GQ:MIN_DP:PL 0/0:50:27:0,81,809; chrNN 51225808 . A G,<*> 30.5 PASS . GT:GQ:DP:AD:VAF:PL 0/1:30:26:17,9,0:0.346154,0:30,0,37,990,990,990; chrNN 51225809 . A G,<*> 31.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:26:17,9,0:0.346154,0:31,0,38,990,990,990; chrNN 51225810 . C T,<*> 31.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:26:17,9,0:0.346154,0:31,0,39,990,990,990; chrNN 51225811 . T C,<*> 32.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:28:17,9,0:0.321429,0:32,0,37,990,990,990; chrNN 51225812 . C <*> 0 . END=51225881 GT:GQ:MIN_DP:PL 0/0:48:26:0,87,869; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/238#issuecomment-1700653315
https://github.com/google/deepvariant/issues/238#issuecomment-1700653315:9,Deployability,update,update,9,"HI!; any update on this topic?. We called multiple genomes recently and found a first example of 4 adjacent base substitution AACT => GGTC being called 4 separate SNVs. The support information is very similar for the 4 calls and I wonder why deepvariant did not call them as one single MNP. This makes that the annotation and effect prediction downstream are wrong. Thanks for your support. ![failed_MNP-call](https://github.com/google/deepvariant/assets/858516/a02c3b5a-0362-4030-be8a-8b3c49129a8f). here is the extract of the gVCF at that location for one sample. ```; chrNN 51225801 . T <*> 0 . END=51225807 GT:GQ:MIN_DP:PL 0/0:50:27:0,81,809; chrNN 51225808 . A G,<*> 30.5 PASS . GT:GQ:DP:AD:VAF:PL 0/1:30:26:17,9,0:0.346154,0:30,0,37,990,990,990; chrNN 51225809 . A G,<*> 31.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:26:17,9,0:0.346154,0:31,0,38,990,990,990; chrNN 51225810 . C T,<*> 31.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:26:17,9,0:0.346154,0:31,0,39,990,990,990; chrNN 51225811 . T C,<*> 32.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:28:17,9,0:0.321429,0:32,0,37,990,990,990; chrNN 51225812 . C <*> 0 . END=51225881 GT:GQ:MIN_DP:PL 0/0:48:26:0,87,869; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/238#issuecomment-1700653315
https://github.com/google/deepvariant/issues/238#issuecomment-1700653315:333,Safety,predict,prediction,333,"HI!; any update on this topic?. We called multiple genomes recently and found a first example of 4 adjacent base substitution AACT => GGTC being called 4 separate SNVs. The support information is very similar for the 4 calls and I wonder why deepvariant did not call them as one single MNP. This makes that the annotation and effect prediction downstream are wrong. Thanks for your support. ![failed_MNP-call](https://github.com/google/deepvariant/assets/858516/a02c3b5a-0362-4030-be8a-8b3c49129a8f). here is the extract of the gVCF at that location for one sample. ```; chrNN 51225801 . T <*> 0 . END=51225807 GT:GQ:MIN_DP:PL 0/0:50:27:0,81,809; chrNN 51225808 . A G,<*> 30.5 PASS . GT:GQ:DP:AD:VAF:PL 0/1:30:26:17,9,0:0.346154,0:30,0,37,990,990,990; chrNN 51225809 . A G,<*> 31.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:26:17,9,0:0.346154,0:31,0,38,990,990,990; chrNN 51225810 . C T,<*> 31.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:26:17,9,0:0.346154,0:31,0,39,990,990,990; chrNN 51225811 . T C,<*> 32.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:28:17,9,0:0.321429,0:32,0,37,990,990,990; chrNN 51225812 . C <*> 0 . END=51225881 GT:GQ:MIN_DP:PL 0/0:48:26:0,87,869; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/238#issuecomment-1700653315
https://github.com/google/deepvariant/issues/238#issuecomment-1702210635:100,Deployability,update,update,100,"Hi @splaisan . We are currently working on a more complete approach for MNPs, but we don't have any update at this time and it won't be in the very next release. Hopefully sometime after that we will have results for MNPs, but I can't give a specific timeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/238#issuecomment-1702210635
https://github.com/google/deepvariant/issues/238#issuecomment-1702210635:153,Deployability,release,release,153,"Hi @splaisan . We are currently working on a more complete approach for MNPs, but we don't have any update at this time and it won't be in the very next release. Hopefully sometime after that we will have results for MNPs, but I can't give a specific timeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/238#issuecomment-1702210635
https://github.com/google/deepvariant/issues/238#issuecomment-1703129794:744,Performance,perform,performs,744,"Hi @splaisan,. That is a very complex experimental setup. I initially proposed freebayes + DV in order to merge and observe them together, for making proper selection in study-specific regions as a follow-up among the two results. Of course it would easier to write a program to compare between the two, filtered on thresholds appropriate to your design. In any case, even though [freebayes has gVCF output](https://github.com/freebayes/freebayes/blob/master/README.md#usage), let's ignore that for now and look closer at the GLnexus algorithm - as it gives nice insights about how variants are processed from gVCF files. Thus when it generates the pVCF, unified variant sites are collected from variants spanning overlapping regions. Since it performs $`\arg \max_{g} Pr(Genotype = g) Pr(Data | Genotype = g)`$, having multiple possible MNPs in the unified site (across gVCFs) dilutes the probability of each genotype, and thus having them as individual SNPs ensures the GQ and PL are maximally contributed to for each base. This is also because for such unified sites, it considers all discovered QC-filtered alleles greedily in descending order of frequency, and might be exacerbated with rare alleles when calling sites. Therefore given the above, utilizing the combined output of the VCF for post-processing into MNPs might be optimal. Ideally, having phasing information would also provide more confidence in the quality of MNPs from combined adjacent SNPs belonging to a haplotype. If possible, limiting via a BED file to regions that are more relevant for your study - especially under heavy multi-threaded conditions - might shorted the analysis time. With many threads you might be thrashing, thus limiting your performance by spending more time context-switching. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/238#issuecomment-1703129794
https://github.com/google/deepvariant/issues/238#issuecomment-1703129794:1600,Performance,multi-thread,multi-threaded,1600,"Hi @splaisan,. That is a very complex experimental setup. I initially proposed freebayes + DV in order to merge and observe them together, for making proper selection in study-specific regions as a follow-up among the two results. Of course it would easier to write a program to compare between the two, filtered on thresholds appropriate to your design. In any case, even though [freebayes has gVCF output](https://github.com/freebayes/freebayes/blob/master/README.md#usage), let's ignore that for now and look closer at the GLnexus algorithm - as it gives nice insights about how variants are processed from gVCF files. Thus when it generates the pVCF, unified variant sites are collected from variants spanning overlapping regions. Since it performs $`\arg \max_{g} Pr(Genotype = g) Pr(Data | Genotype = g)`$, having multiple possible MNPs in the unified site (across gVCFs) dilutes the probability of each genotype, and thus having them as individual SNPs ensures the GQ and PL are maximally contributed to for each base. This is also because for such unified sites, it considers all discovered QC-filtered alleles greedily in descending order of frequency, and might be exacerbated with rare alleles when calling sites. Therefore given the above, utilizing the combined output of the VCF for post-processing into MNPs might be optimal. Ideally, having phasing information would also provide more confidence in the quality of MNPs from combined adjacent SNPs belonging to a haplotype. If possible, limiting via a BED file to regions that are more relevant for your study - especially under heavy multi-threaded conditions - might shorted the analysis time. With many threads you might be thrashing, thus limiting your performance by spending more time context-switching. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/238#issuecomment-1703129794
https://github.com/google/deepvariant/issues/238#issuecomment-1703129794:1722,Performance,perform,performance,1722,"Hi @splaisan,. That is a very complex experimental setup. I initially proposed freebayes + DV in order to merge and observe them together, for making proper selection in study-specific regions as a follow-up among the two results. Of course it would easier to write a program to compare between the two, filtered on thresholds appropriate to your design. In any case, even though [freebayes has gVCF output](https://github.com/freebayes/freebayes/blob/master/README.md#usage), let's ignore that for now and look closer at the GLnexus algorithm - as it gives nice insights about how variants are processed from gVCF files. Thus when it generates the pVCF, unified variant sites are collected from variants spanning overlapping regions. Since it performs $`\arg \max_{g} Pr(Genotype = g) Pr(Data | Genotype = g)`$, having multiple possible MNPs in the unified site (across gVCFs) dilutes the probability of each genotype, and thus having them as individual SNPs ensures the GQ and PL are maximally contributed to for each base. This is also because for such unified sites, it considers all discovered QC-filtered alleles greedily in descending order of frequency, and might be exacerbated with rare alleles when calling sites. Therefore given the above, utilizing the combined output of the VCF for post-processing into MNPs might be optimal. Ideally, having phasing information would also provide more confidence in the quality of MNPs from combined adjacent SNPs belonging to a haplotype. If possible, limiting via a BED file to regions that are more relevant for your study - especially under heavy multi-threaded conditions - might shorted the analysis time. With many threads you might be thrashing, thus limiting your performance by spending more time context-switching. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/238#issuecomment-1703129794
https://github.com/google/deepvariant/issues/239#issuecomment-558061938:77,Testability,test,test,77,"After updating to v0.9.0 we have different results:. <details>; <summary>Our test VCF (DeepVariant v0.8.0)</summary>. ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; ##FILTER=<ID=LowQual,Description=""Confidence in this variant being real is below calling threshold."">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End position (for use with symbolic alleles)"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">; ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">; ##contig=<ID=chr20,length=63025520>; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NA12878; chr20	9999996	.	A	ACT	44.4	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:41:44:0,44:1:44,44,0; chr20	10000117	.	C	T	36.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:34:55:25,30:0.545455:36,0,37; chr20	10000211	.	C	T	34.1	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:34:59:30,29:0.491525:34,0,51; chr20	10000439	.	T	G	40	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:39:72:0,72:1:40,45,0; chr20	10000598	.	T	A	62.2	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:54:46:0,46:1:62,55,0; chr20	10000694	.	G	A	35.1	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:35:48:26,22:0.458333:35,0,47; chr20	10000758	.	T	A	69.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:56:56:0,56:1:69,55,0; chr20	10001019	.	T	G	2.1	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:4:44:31,13:0.295455:0,2,34; chr20	10001298	.	T	A	54.7	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:49:43:0,43:1:54,50,0; chr20	10001436	.	A	AAGGCT	38.7	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:38:39:2,35:0.897436:38,47,0; chr20	100014",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/239#issuecomment-558061938
https://github.com/google/deepvariant/issues/239#issuecomment-558061938:23915,Testability,test,test,23915,"333:0,3,33; chr20	10099055	.	T	C	0	RefCall	.	GT:GQ:DP:AD:VAF:PL	0/0:28:42:28,14:0.333333:0,28,55; chr20	10099079	.	C	T	0.1	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:16:38:23,15:0.394737:0,15,48; chr20	10099111	.	T	TTTTGTTTG	1.9	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:4:49:33,15:0.306122:0,2,35; chr20	10099140	.	G	T	0.1	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:16:52:28,24:0.461538:0,15,46; chr20	10099190	.	G	T	9.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:9:47:20,27:0.574468:8,0,42; chr20	10099220	.	A	G	37.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:37:40:21,19:0.475:37,0,51; chr20	10099250	.	G	A	32.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:32:39:21,18:0.461538:32,0,46; chr20	10099535	.	G	A	45.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:46:58:32,26:0.448276:45,0,57; chr20	10099565	.	C	T	42.1	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:42:56:33,23:0.410714:42,0,52; chr20	10099755	.	C	T	24.8	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:25:50:25,25:0.5:24,0,47; chr20	10099832	.	A	G	29.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:29:53:31,22:0.415094:29,0,50; ```; </details>. <details>; <summary>Our test VCF (DeepVariant v0.9.0)</summary>; ; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; ##FILTER=<ID=LowQual,Description=""Confidence in this variant being real is below calling threshold."">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End position (for use with symbolic alleles)"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">; ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">; #",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/239#issuecomment-558061938
https://github.com/google/deepvariant/issues/240#issuecomment-558701300:407,Performance,perform,perform,407,"Hi @aardes, we have reported [this runtime](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md#runtime) for running DeepVariant WGS on a GPU. The specs for that GPU are listed at the bottom of the [page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform). I'm not sure how your specific GPU type would perform, but those numbers should give you a point of comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/240#issuecomment-558701300
https://github.com/google/deepvariant/issues/241#issuecomment-559228691:119,Performance,perform,performs,119,"Hi @mikael-christensen . For somatic calling, we have a version of DeepVariant designed to call somatic variants. This performs well in simulated benchmarks and in benchmarks with COLO829. However, we want to get more feedback from external groups about its performance across real data for more cancer types before we feel comfortable generally releasing it. The time to train DeepVariant internally is a function of the amount and prioritization of training jobs on internal infrastructure. Generally it takes a few days to train the WGS model and is shorter for the WES and PacBio models. Training uses TPUs. I am not sure of the exact resource requirements in the production runs. However, external groups who have successfully trained DeepVariant have generally done so with much less data than what we train for in production, and often warmstart from one of our production models and re-train for their data. External groups have shown improvements for their workflows using GPUs and (I believe) on the order of a day or less of re-training time. The candidates created by the very sensitive caller are not genotyped, so comparing accuracy of the sensitive caller in the same way isn't quite possible. DeepVariant's VCF output will write every position which it generated a candidate for. For a 50x PCR-Free Illumina HG002 run, DeepVariant produced:. 0/0 calls - 934,097 ; ./. calls - 150,238; 0/1 calls - 2,793,521; 1/1 calls - 1,851,566; 1/2 calls - 78,194. This means that for about 19% of the positions which the sensitive caller proposes, DeepVariant does not believe there is sufficient evidence for a variant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/241#issuecomment-559228691
https://github.com/google/deepvariant/issues/241#issuecomment-559228691:258,Performance,perform,performance,258,"Hi @mikael-christensen . For somatic calling, we have a version of DeepVariant designed to call somatic variants. This performs well in simulated benchmarks and in benchmarks with COLO829. However, we want to get more feedback from external groups about its performance across real data for more cancer types before we feel comfortable generally releasing it. The time to train DeepVariant internally is a function of the amount and prioritization of training jobs on internal infrastructure. Generally it takes a few days to train the WGS model and is shorter for the WES and PacBio models. Training uses TPUs. I am not sure of the exact resource requirements in the production runs. However, external groups who have successfully trained DeepVariant have generally done so with much less data than what we train for in production, and often warmstart from one of our production models and re-train for their data. External groups have shown improvements for their workflows using GPUs and (I believe) on the order of a day or less of re-training time. The candidates created by the very sensitive caller are not genotyped, so comparing accuracy of the sensitive caller in the same way isn't quite possible. DeepVariant's VCF output will write every position which it generated a candidate for. For a 50x PCR-Free Illumina HG002 run, DeepVariant produced:. 0/0 calls - 934,097 ; ./. calls - 150,238; 0/1 calls - 2,793,521; 1/1 calls - 1,851,566; 1/2 calls - 78,194. This means that for about 19% of the positions which the sensitive caller proposes, DeepVariant does not believe there is sufficient evidence for a variant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/241#issuecomment-559228691
https://github.com/google/deepvariant/issues/241#issuecomment-559228691:146,Testability,benchmark,benchmarks,146,"Hi @mikael-christensen . For somatic calling, we have a version of DeepVariant designed to call somatic variants. This performs well in simulated benchmarks and in benchmarks with COLO829. However, we want to get more feedback from external groups about its performance across real data for more cancer types before we feel comfortable generally releasing it. The time to train DeepVariant internally is a function of the amount and prioritization of training jobs on internal infrastructure. Generally it takes a few days to train the WGS model and is shorter for the WES and PacBio models. Training uses TPUs. I am not sure of the exact resource requirements in the production runs. However, external groups who have successfully trained DeepVariant have generally done so with much less data than what we train for in production, and often warmstart from one of our production models and re-train for their data. External groups have shown improvements for their workflows using GPUs and (I believe) on the order of a day or less of re-training time. The candidates created by the very sensitive caller are not genotyped, so comparing accuracy of the sensitive caller in the same way isn't quite possible. DeepVariant's VCF output will write every position which it generated a candidate for. For a 50x PCR-Free Illumina HG002 run, DeepVariant produced:. 0/0 calls - 934,097 ; ./. calls - 150,238; 0/1 calls - 2,793,521; 1/1 calls - 1,851,566; 1/2 calls - 78,194. This means that for about 19% of the positions which the sensitive caller proposes, DeepVariant does not believe there is sufficient evidence for a variant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/241#issuecomment-559228691
https://github.com/google/deepvariant/issues/241#issuecomment-559228691:164,Testability,benchmark,benchmarks,164,"Hi @mikael-christensen . For somatic calling, we have a version of DeepVariant designed to call somatic variants. This performs well in simulated benchmarks and in benchmarks with COLO829. However, we want to get more feedback from external groups about its performance across real data for more cancer types before we feel comfortable generally releasing it. The time to train DeepVariant internally is a function of the amount and prioritization of training jobs on internal infrastructure. Generally it takes a few days to train the WGS model and is shorter for the WES and PacBio models. Training uses TPUs. I am not sure of the exact resource requirements in the production runs. However, external groups who have successfully trained DeepVariant have generally done so with much less data than what we train for in production, and often warmstart from one of our production models and re-train for their data. External groups have shown improvements for their workflows using GPUs and (I believe) on the order of a day or less of re-training time. The candidates created by the very sensitive caller are not genotyped, so comparing accuracy of the sensitive caller in the same way isn't quite possible. DeepVariant's VCF output will write every position which it generated a candidate for. For a 50x PCR-Free Illumina HG002 run, DeepVariant produced:. 0/0 calls - 934,097 ; ./. calls - 150,238; 0/1 calls - 2,793,521; 1/1 calls - 1,851,566; 1/2 calls - 78,194. This means that for about 19% of the positions which the sensitive caller proposes, DeepVariant does not believe there is sufficient evidence for a variant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/241#issuecomment-559228691
https://github.com/google/deepvariant/issues/241#issuecomment-559228691:218,Usability,feedback,feedback,218,"Hi @mikael-christensen . For somatic calling, we have a version of DeepVariant designed to call somatic variants. This performs well in simulated benchmarks and in benchmarks with COLO829. However, we want to get more feedback from external groups about its performance across real data for more cancer types before we feel comfortable generally releasing it. The time to train DeepVariant internally is a function of the amount and prioritization of training jobs on internal infrastructure. Generally it takes a few days to train the WGS model and is shorter for the WES and PacBio models. Training uses TPUs. I am not sure of the exact resource requirements in the production runs. However, external groups who have successfully trained DeepVariant have generally done so with much less data than what we train for in production, and often warmstart from one of our production models and re-train for their data. External groups have shown improvements for their workflows using GPUs and (I believe) on the order of a day or less of re-training time. The candidates created by the very sensitive caller are not genotyped, so comparing accuracy of the sensitive caller in the same way isn't quite possible. DeepVariant's VCF output will write every position which it generated a candidate for. For a 50x PCR-Free Illumina HG002 run, DeepVariant produced:. 0/0 calls - 934,097 ; ./. calls - 150,238; 0/1 calls - 2,793,521; 1/1 calls - 1,851,566; 1/2 calls - 78,194. This means that for about 19% of the positions which the sensitive caller proposes, DeepVariant does not believe there is sufficient evidence for a variant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/241#issuecomment-559228691
https://github.com/google/deepvariant/issues/241#issuecomment-1478100032:107,Testability,benchmark,benchmarks,107,"@AndrewCarroll , what data was used for training the somatic caller? And how do you generate the simulated benchmarks? Could you point me to more information on this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/241#issuecomment-1478100032
https://github.com/google/deepvariant/issues/242#issuecomment-561296792:100,Deployability,release,release,100,Thanks @ptrebert .; I added this as a future improvement that we'll work on and include in the next release. I'll leave this open.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/242#issuecomment-561296792
https://github.com/google/deepvariant/issues/242#issuecomment-596040916:123,Deployability,release,release,123,"Hi @ptrebert ; Thanks for the suggestion. I have fixed this in our internal version, and the fix will come out in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/242#issuecomment-596040916
https://github.com/google/deepvariant/issues/243#issuecomment-561996442:3248,Availability,error,errors,3248,"do docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. ## GPU image; ```; VERSION=0.9.0-gpu; sudo nvidia-docker pull google/deepvariant:${VERSION}; sudo nvidia-docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant_gpu:latest; sudo nvidia-docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo nvidia-docker push localhost:5000/deepvariant_gpu:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant_gpu:latest; ```. Running through Quick Start just to make sure nothing wrong:; ```; singularity -s exec --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. Both CPU and GPU versions finished on Quick Start data without errors. ## Misc; For installing nvidia-docker and singularity, you can also refer to https://github.com/google/deepvariant/blob/r0.9/scripts/install_nvidia_docker.sh; and; https://github.com/google/deepvariant/blob/r0.9/scripts/install_singularity.sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442
https://github.com/google/deepvariant/issues/243#issuecomment-561996442:198,Deployability,release,release,198,"Hi @jguhlin ,; thanks for the feedback, and for letting us know that our users are still interested in Singularity images.; I have been a bit hesitated to make Singularity images part of our formal release process, mostly because the additional quality control burden. ; But, in the future we'll consider building these *.simg files and just distribute them. The steps below I used were documented here:; https://github.com/google/deepvariant/issues/132#issuecomment-482430728. I have the detailed commands that I used for my conversion, and I copied the output *.simg files here:. ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Or you can find them in the browser here:; https://console.cloud.google.com/storage/browser/deepvariant/singularity_images/. I was able to test both CPU and GPU version on the Quick Start data (see below). Can you see if if my `deepvariant-0.9.0-gpu.simg` file works for you?. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU (v0.9.0). If you don't have singularity on your computer, install it first:; https://sylabs.io/docs/. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; VERSION=0.9.0; sudo apt -y update && sudo apt-get install -y docker.io; sudo docker pull google/deepvariant:${VERSION}; sudo docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442
https://github.com/google/deepvariant/issues/243#issuecomment-561996442:1138,Deployability,install,install,1138," been a bit hesitated to make Singularity images part of our formal release process, mostly because the additional quality control burden. ; But, in the future we'll consider building these *.simg files and just distribute them. The steps below I used were documented here:; https://github.com/google/deepvariant/issues/132#issuecomment-482430728. I have the detailed commands that I used for my conversion, and I copied the output *.simg files here:. ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Or you can find them in the browser here:; https://console.cloud.google.com/storage/browser/deepvariant/singularity_images/. I was able to test both CPU and GPU version on the Quick Start data (see below). Can you see if if my `deepvariant-0.9.0-gpu.simg` file works for you?. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU (v0.9.0). If you don't have singularity on your computer, install it first:; https://sylabs.io/docs/. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; VERSION=0.9.0; sudo apt -y update && sudo apt-get install -y docker.io; sudo docker pull google/deepvariant:${VERSION}; sudo docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442
https://github.com/google/deepvariant/issues/243#issuecomment-561996442:1322,Deployability,update,update,1322,"d just distribute them. The steps below I used were documented here:; https://github.com/google/deepvariant/issues/132#issuecomment-482430728. I have the detailed commands that I used for my conversion, and I copied the output *.simg files here:. ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Or you can find them in the browser here:; https://console.cloud.google.com/storage/browser/deepvariant/singularity_images/. I was able to test both CPU and GPU version on the Quick Start data (see below). Can you see if if my `deepvariant-0.9.0-gpu.simg` file works for you?. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU (v0.9.0). If you don't have singularity on your computer, install it first:; https://sylabs.io/docs/. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; VERSION=0.9.0; sudo apt -y update && sudo apt-get install -y docker.io; sudo docker pull google/deepvariant:${VERSION}; sudo docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. ## GPU image; ```; VERSION=0.9.0-gpu; sudo n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442
https://github.com/google/deepvariant/issues/243#issuecomment-561996442:1345,Deployability,install,install,1345,"d just distribute them. The steps below I used were documented here:; https://github.com/google/deepvariant/issues/132#issuecomment-482430728. I have the detailed commands that I used for my conversion, and I copied the output *.simg files here:. ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Or you can find them in the browser here:; https://console.cloud.google.com/storage/browser/deepvariant/singularity_images/. I was able to test both CPU and GPU version on the Quick Start data (see below). Can you see if if my `deepvariant-0.9.0-gpu.simg` file works for you?. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU (v0.9.0). If you don't have singularity on your computer, install it first:; https://sylabs.io/docs/. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; VERSION=0.9.0; sudo apt -y update && sudo apt-get install -y docker.io; sudo docker pull google/deepvariant:${VERSION}; sudo docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. ## GPU image; ```; VERSION=0.9.0-gpu; sudo n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442
https://github.com/google/deepvariant/issues/243#issuecomment-561996442:3269,Deployability,install,installing,3269,"do docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. ## GPU image; ```; VERSION=0.9.0-gpu; sudo nvidia-docker pull google/deepvariant:${VERSION}; sudo nvidia-docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant_gpu:latest; sudo nvidia-docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo nvidia-docker push localhost:5000/deepvariant_gpu:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant_gpu:latest; ```. Running through Quick Start just to make sure nothing wrong:; ```; singularity -s exec --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. Both CPU and GPU versions finished on Quick Start data without errors. ## Misc; For installing nvidia-docker and singularity, you can also refer to https://github.com/google/deepvariant/blob/r0.9/scripts/install_nvidia_docker.sh; and; https://github.com/google/deepvariant/blob/r0.9/scripts/install_singularity.sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442
https://github.com/google/deepvariant/issues/243#issuecomment-561996442:855,Testability,test,test,855,"Hi @jguhlin ,; thanks for the feedback, and for letting us know that our users are still interested in Singularity images.; I have been a bit hesitated to make Singularity images part of our formal release process, mostly because the additional quality control burden. ; But, in the future we'll consider building these *.simg files and just distribute them. The steps below I used were documented here:; https://github.com/google/deepvariant/issues/132#issuecomment-482430728. I have the detailed commands that I used for my conversion, and I copied the output *.simg files here:. ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Or you can find them in the browser here:; https://console.cloud.google.com/storage/browser/deepvariant/singularity_images/. I was able to test both CPU and GPU version on the Quick Start data (see below). Can you see if if my `deepvariant-0.9.0-gpu.simg` file works for you?. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU (v0.9.0). If you don't have singularity on your computer, install it first:; https://sylabs.io/docs/. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; VERSION=0.9.0; sudo apt -y update && sudo apt-get install -y docker.io; sudo docker pull google/deepvariant:${VERSION}; sudo docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442
https://github.com/google/deepvariant/issues/243#issuecomment-561996442:30,Usability,feedback,feedback,30,"Hi @jguhlin ,; thanks for the feedback, and for letting us know that our users are still interested in Singularity images.; I have been a bit hesitated to make Singularity images part of our formal release process, mostly because the additional quality control burden. ; But, in the future we'll consider building these *.simg files and just distribute them. The steps below I used were documented here:; https://github.com/google/deepvariant/issues/132#issuecomment-482430728. I have the detailed commands that I used for my conversion, and I copied the output *.simg files here:. ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Or you can find them in the browser here:; https://console.cloud.google.com/storage/browser/deepvariant/singularity_images/. I was able to test both CPU and GPU version on the Quick Start data (see below). Can you see if if my `deepvariant-0.9.0-gpu.simg` file works for you?. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU (v0.9.0). If you don't have singularity on your computer, install it first:; https://sylabs.io/docs/. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; VERSION=0.9.0; sudo apt -y update && sudo apt-get install -y docker.io; sudo docker pull google/deepvariant:${VERSION}; sudo docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442
https://github.com/google/deepvariant/issues/243#issuecomment-562108502:352,Availability,error,errors,352,"I strongly concur. Singularity images would be nice. Using Docker has already given me multiple diseases. It's such a hassle and from what other colleagues tell me, I am not the only one. ; Note that there is nothing wrong with the deepvariant image, it's the Docker process that can cause many problems running, pulling images, restarting, generating errors, etc ... ; Like this issue, unresolved since 2017: ; https://github.com/docker/for-win/issues/813. Dark Souls bosses are easier to take down than pulling a docker image on some systems. ; So to be clear again, nothing wrong at all with deepvariant, which I love more and more btw, but having something else than Docker would be indeed really great.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-562108502
https://github.com/google/deepvariant/issues/243#issuecomment-562108502:495,Availability,down,down,495,"I strongly concur. Singularity images would be nice. Using Docker has already given me multiple diseases. It's such a hassle and from what other colleagues tell me, I am not the only one. ; Note that there is nothing wrong with the deepvariant image, it's the Docker process that can cause many problems running, pulling images, restarting, generating errors, etc ... ; Like this issue, unresolved since 2017: ; https://github.com/docker/for-win/issues/813. Dark Souls bosses are easier to take down than pulling a docker image on some systems. ; So to be clear again, nothing wrong at all with deepvariant, which I love more and more btw, but having something else than Docker would be indeed really great.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-562108502
https://github.com/google/deepvariant/issues/243#issuecomment-562108502:556,Usability,clear,clear,556,"I strongly concur. Singularity images would be nice. Using Docker has already given me multiple diseases. It's such a hassle and from what other colleagues tell me, I am not the only one. ; Note that there is nothing wrong with the deepvariant image, it's the Docker process that can cause many problems running, pulling images, restarting, generating errors, etc ... ; Like this issue, unresolved since 2017: ; https://github.com/docker/for-win/issues/813. Dark Souls bosses are easier to take down than pulling a docker image on some systems. ; So to be clear again, nothing wrong at all with deepvariant, which I love more and more btw, but having something else than Docker would be indeed really great.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-562108502
https://github.com/google/deepvariant/issues/243#issuecomment-562237254:32,Usability,feedback,feedback,32,"Hi @aderzelle ; thanks for your feedback. If you have a chance to try out the two images I shared:; ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Please let me know whether they work for you or not. If you see any issues, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-562237254
https://github.com/google/deepvariant/issues/243#issuecomment-565122777:30,Deployability,release,release,30,Thanks @jguhlin . In the next release we'll plan to include the converted version in the same GCS bucket.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-565122777
https://github.com/google/deepvariant/issues/243#issuecomment-579406829:163,Availability,error,errors,163,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-579406829
https://github.com/google/deepvariant/issues/243#issuecomment-579406829:1025,Energy Efficiency,Power,Power,1025,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-579406829
https://github.com/google/deepvariant/issues/243#issuecomment-579406829:64,Testability,test,tested,64,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-579406829
https://github.com/google/deepvariant/issues/243#issuecomment-579406829:106,Testability,test,test,106,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-579406829
https://github.com/google/deepvariant/issues/243#issuecomment-579406829:481,Testability,test,test,481,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-579406829
https://github.com/google/deepvariant/issues/243#issuecomment-579406829:503,Testability,test,testdata,503,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-579406829
https://github.com/google/deepvariant/issues/243#issuecomment-579406829:575,Testability,test,test,575,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-579406829
https://github.com/google/deepvariant/issues/243#issuecomment-579406829:597,Testability,test,testdata,597,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-579406829
https://github.com/google/deepvariant/issues/243#issuecomment-579406829:1039,Testability,log,login,1039,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/243#issuecomment-579406829
https://github.com/google/deepvariant/issues/247#issuecomment-563063086:658,Availability,down,downstream,658,"This question is complex enough that it cuts to some of the fundamental issues of VCF. I expect that across the community, opinions will differ as to whether this represents troubling inconsistency as opposed to a strange edge case. The heart of this question is whether people will tend to interpret a 0/1 as ""I have evidence for a reference allele and evidence for the 1st alternate allele"" or as ""I have evidence for one copy of the ALT allele but for one non-REF position"". This could be fully expressed with REF: CATTA || ALT: C, CAGTA || GT: 1/2. I expect that the majority of programs don't do this sort of normalization, and I expect the majority of downstream tools operate in a way that doesn't assume that the two lines you post introduce inconsistency (that is most probably assume ""REF"" more like ""Evidence for at least one copy that is non-ALT, other lines may clarify if an additional non-REF allele is present). However, if you want to dig into this question further, I would encourage you to ask on one of the GA4GH forums about variant representations. I don't consider myself knowledgable enough to give definitive statements on edge cases like this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/247#issuecomment-563063086
https://github.com/google/deepvariant/issues/247#issuecomment-563285549:176,Availability,down,downstream,176,"I guess the core question for the variant caller is, (whether or not fully normalized as you have written) whether the raw calls can be interpreted without contradictions by a downstream method (notwithstanding the VCF standard). Considering GT=0 to be ""non-ALT"" instead of ""REF allele"" for such call clusters does seem to work in these cases (though I am not aware of a standardization of such an interpretation). The code I have referenced from DeepVariant above seems to try to filter out cases that do not make sense for such clusters of calls, so a downstream method that assumes GT=0 to imply ""non-ALT"" might work smoothly almost all the time (except perhaps when the algorithm aborts). I also didn't have an idea as to whether, as a method developer, you considered these to be corner cases, and your comment above has provided an answer to that. I myself do not have any measurements on how prevalent these types of cases are, but I encountered some of these cases recently. I will make another post here if I learn anything further about this, but I understand the point-of-view behind the original piece of code that I posted. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/247#issuecomment-563285549
https://github.com/google/deepvariant/issues/247#issuecomment-563285549:554,Availability,down,downstream,554,"I guess the core question for the variant caller is, (whether or not fully normalized as you have written) whether the raw calls can be interpreted without contradictions by a downstream method (notwithstanding the VCF standard). Considering GT=0 to be ""non-ALT"" instead of ""REF allele"" for such call clusters does seem to work in these cases (though I am not aware of a standardization of such an interpretation). The code I have referenced from DeepVariant above seems to try to filter out cases that do not make sense for such clusters of calls, so a downstream method that assumes GT=0 to imply ""non-ALT"" might work smoothly almost all the time (except perhaps when the algorithm aborts). I also didn't have an idea as to whether, as a method developer, you considered these to be corner cases, and your comment above has provided an answer to that. I myself do not have any measurements on how prevalent these types of cases are, but I encountered some of these cases recently. I will make another post here if I learn anything further about this, but I understand the point-of-view behind the original piece of code that I posted. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/247#issuecomment-563285549
https://github.com/google/deepvariant/issues/247#issuecomment-563285549:684,Safety,abort,aborts,684,"I guess the core question for the variant caller is, (whether or not fully normalized as you have written) whether the raw calls can be interpreted without contradictions by a downstream method (notwithstanding the VCF standard). Considering GT=0 to be ""non-ALT"" instead of ""REF allele"" for such call clusters does seem to work in these cases (though I am not aware of a standardization of such an interpretation). The code I have referenced from DeepVariant above seems to try to filter out cases that do not make sense for such clusters of calls, so a downstream method that assumes GT=0 to imply ""non-ALT"" might work smoothly almost all the time (except perhaps when the algorithm aborts). I also didn't have an idea as to whether, as a method developer, you considered these to be corner cases, and your comment above has provided an answer to that. I myself do not have any measurements on how prevalent these types of cases are, but I encountered some of these cases recently. I will make another post here if I learn anything further about this, but I understand the point-of-view behind the original piece of code that I posted. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/247#issuecomment-563285549
https://github.com/google/deepvariant/issues/247#issuecomment-563285549:1018,Usability,learn,learn,1018,"I guess the core question for the variant caller is, (whether or not fully normalized as you have written) whether the raw calls can be interpreted without contradictions by a downstream method (notwithstanding the VCF standard). Considering GT=0 to be ""non-ALT"" instead of ""REF allele"" for such call clusters does seem to work in these cases (though I am not aware of a standardization of such an interpretation). The code I have referenced from DeepVariant above seems to try to filter out cases that do not make sense for such clusters of calls, so a downstream method that assumes GT=0 to imply ""non-ALT"" might work smoothly almost all the time (except perhaps when the algorithm aborts). I also didn't have an idea as to whether, as a method developer, you considered these to be corner cases, and your comment above has provided an answer to that. I myself do not have any measurements on how prevalent these types of cases are, but I encountered some of these cases recently. I will make another post here if I learn anything further about this, but I understand the point-of-view behind the original piece of code that I posted. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/247#issuecomment-563285549
https://github.com/google/deepvariant/issues/247#issuecomment-564333529:726,Energy Efficiency,reduce,reduce,726,"Hi @anands-repo,. Thanks for the question. You are correct that in these cases we are semantically treating GT=0 to be ""non-ALT""; the goal of the haplotypes.py library is to try to resolve cases where the raw calls indicate more than two alternate alleles at a single base pair. This can occur because DeepVariant processes each candidate variant independently, and sometimes candidates overlap the same stretch of the reference genome. As you observed in the code, the haplotypes.py library is even still just a ""best effort"" and there are cases where > 2 alternate alleles will still be emitted by the algorithm. Regarding your prevalence comment, empirically on the Genome in a Bottle HG002 sample we observed this library reduce the number of regions with > 2 alternate alleles from ~3000 to ~300 when it was first introduced. (The numbers may be slightly different now due to subsequent improvements in variant discovery and calling). regards,; Cory",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/247#issuecomment-564333529
https://github.com/google/deepvariant/issues/247#issuecomment-564413332:115,Safety,predict,predicted,115,"@cmclean Thanks for the additional explanation. So the cases to be filtered out are those with more than 2 alleles predicted per reference position. Also, 300 out of millions of variants does seem to indicate that these cases are really rare after the use of this functionality. @AndrewCarroll @cmclean Thank you for your patience and the helpful explanations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/247#issuecomment-564413332
https://github.com/google/deepvariant/issues/248#issuecomment-566700041:8,Deployability,update,updated,8,"We have updated the documentation to mention that AVX instructions are needed. These changes will come out with the next release. Thank you for the feedback!. Edit: we specifically mention this requirement again in the quickstart documentation, in addition to the page linked below by @pgrosu. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248#issuecomment-566700041
https://github.com/google/deepvariant/issues/248#issuecomment-566700041:121,Deployability,release,release,121,"We have updated the documentation to mention that AVX instructions are needed. These changes will come out with the next release. Thank you for the feedback!. Edit: we specifically mention this requirement again in the quickstart documentation, in addition to the page linked below by @pgrosu. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248#issuecomment-566700041
https://github.com/google/deepvariant/issues/248#issuecomment-566700041:148,Usability,feedback,feedback,148,"We have updated the documentation to mention that AVX instructions are needed. These changes will come out with the next release. Thank you for the feedback!. Edit: we specifically mention this requirement again in the quickstart documentation, in addition to the page linked below by @pgrosu. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/248#issuecomment-566700041
https://github.com/google/deepvariant/issues/249#issuecomment-563510580:146,Availability,failure,failure,146,"Hi se2cheeese,. It looks like one of the shards failed during make_examples stage.; In order to diagnose the problem we need to see logs with the failure. You may run it without shards (remove --num_shards flag). It should be pretty fast for 100K bases. ; Then attach the entire output so that we could look at it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-563510580
https://github.com/google/deepvariant/issues/249#issuecomment-563510580:132,Testability,log,logs,132,"Hi se2cheeese,. It looks like one of the shards failed during make_examples stage.; In order to diagnose the problem we need to see logs with the failure. You may run it without shards (remove --num_shards flag). It should be pretty fast for 100K bases. ; Then attach the entire output so that we could look at it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-563510580
https://github.com/google/deepvariant/issues/249#issuecomment-565441661:2987,Availability,checkpoint,checkpoint,2987,"sing CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2019-12-13 13:07:00.397901: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728; I1213 13:07:01.327224 139885760198400 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1213 13:07:01.414663 139885760198400 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1213 13:07:02.310832 139885760198400 make_examples.py:1363] Task 0: 0 candidates (0 examples) [1.93s elapsed]; I1213 13:07:05.401592 139885760198400 make_examples.py:1380] Found 28 candidate variants; I1213 13:07:05.402002 139885760198400 make_examples.py:1381] Created 28 examples. real	0m10.204s; user	0m5.490s; sys	0m3.310s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I1213 13:07:08.439639 140638419556096 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 13:07:08.488881: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 13:07:08.491470: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x58612b0 executing computations on platform Host. Devices:; 2019-12-13 13:07:08.491562: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 13:07:08.495160: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 13:07:08.525229 140638419556096 modeling.py:560] Initializing model with random parameters; W1213 13:07:08.527353 140638419556096 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpr4M5u5; I1213 13:07:08.528274 140638419556096 estimator.py:201] Using config: {'_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-565441661
https://github.com/google/deepvariant/issues/249#issuecomment-565441661:29945,Availability,error,error,29945,"5993887488 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; I1213 13:11:54.929332 140405993887488 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00009-of-00010.gz; I1213 13:11:54.929711 140405993887488 make_examples.py:1334] Writing gvcf records to /tmp/deepvariant_tmp_output/gvcf.tfrecord-00009-of-00010.gz; I1213 13:11:54.940470 140405993887488 make_examples.py:905] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2019-12-13 13:11:54.943603: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728. real	0m33.273s; user	0m30.820s; sys	1m32.400s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 9 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/genome.fa"" --reads ""/input/HC3-BC_RG_bwa.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@10.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@10.gz"" --regions ""20:10,000,000-10,100,000"" --task {}' returned non-zero exit status 1; ```. The same error develops with your WES-case-study sample file/script. It would be great to solve this problem. ; Thank you in advance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-565441661
https://github.com/google/deepvariant/issues/249#issuecomment-565441661:7867,Integrability,depend,depend,7867,"ing:; Use standard file APIs to check for files with this prefix.; I1213 13:07:13.621948 140638419556096 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I1213 13:07:15.663943 140638419556096 session_manager.py:491] Running local_init_op.; I1213 13:07:15.711944 140638419556096 session_manager.py:493] Done running local_init_op.; I1213 13:07:16.176234 140638419556096 modeling.py:410] Reloading EMA...; I1213 13:07:16.177736 140638419556096 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I1213 13:07:20.278557 140638419556096 call_variants.py:399] Processed 1 examples in 1 batches [1174.939 sec per 100]; I1213 13:07:20.328917 140638419556096 call_variants.py:401] Done evaluating variants. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. real	0m15.024s; user	0m13.890s; sys	0m3.410s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/genome.fa"" --infile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2019-12-13 13:07:22.565874: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz; 2019-12-13 13:07:22.566377: I deepvariant/postprocess_variants.cc:97] Done reading: /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. #entries in single_site_calls = 28; 2019-12-13 13:07:22.566443: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 28; 2019-12-13 13:07:22.566459: I deepvariant/postprocess_variants.cc:103] Start SortSingleSiteCalls; 2019-12-13 13:07:22.566492: I deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-565441661
https://github.com/google/deepvariant/issues/249#issuecomment-565441661:3989,Modifiability,config,config,3989,"s.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 13:07:08.488881: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 13:07:08.491470: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x58612b0 executing computations on platform Host. Devices:; 2019-12-13 13:07:08.491562: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 13:07:08.495160: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 13:07:08.525229 140638419556096 modeling.py:560] Initializing model with random parameters; W1213 13:07:08.527353 140638419556096 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpr4M5u5; I1213 13:07:08.528274 140638419556096 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe88a5b6190>, '_model_dir': '/tmp/tmpr4M5u5', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}; I1213 13:07:08.528713 140638419556096 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz; W1213 13:07:08.533111 140638419556096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-565441661
https://github.com/google/deepvariant/issues/249#issuecomment-565441661:3650,Performance,Tune,Tune,3650," make_examples.py:1381] Created 28 examples. real	0m10.204s; user	0m5.490s; sys	0m3.310s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I1213 13:07:08.439639 140638419556096 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 13:07:08.488881: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 13:07:08.491470: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x58612b0 executing computations on platform Host. Devices:; 2019-12-13 13:07:08.491562: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 13:07:08.495160: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 13:07:08.525229 140638419556096 modeling.py:560] Initializing model with random parameters; W1213 13:07:08.527353 140638419556096 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpr4M5u5; I1213 13:07:08.528274 140638419556096 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe88a5b6190>, '_model_dir': '/tmp/tmpr4M5u5', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-565441661
https://github.com/google/deepvariant/issues/249#issuecomment-565441661:3699,Performance,perform,performance,3699," make_examples.py:1381] Created 28 examples. real	0m10.204s; user	0m5.490s; sys	0m3.310s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I1213 13:07:08.439639 140638419556096 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 13:07:08.488881: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 13:07:08.491470: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x58612b0 executing computations on platform Host. Devices:; 2019-12-13 13:07:08.491562: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 13:07:08.495160: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 13:07:08.525229 140638419556096 modeling.py:560] Initializing model with random parameters; W1213 13:07:08.527353 140638419556096 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpr4M5u5; I1213 13:07:08.528274 140638419556096 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe88a5b6190>, '_model_dir': '/tmp/tmpr4M5u5', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-565441661
https://github.com/google/deepvariant/issues/249#issuecomment-565657419:116,Availability,error,error,116,"Dear akolesikov,. --regions ""20:10,000,000-10,100,000"" was ok without shards,; however, for WES.bed I got a similar error as below. . ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I1213 19:19:36.445342 140624564107008 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 19:19:36.497919: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 19:19:36.499703: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x52dd110 executing computations on platform Host. Devices:; 2019-12-13 19:19:36.499773: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 19:19:36.503204: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 19:19:36.524235 140624564107008 modeling.py:560] Initializing model with random parameters; W1213 19:19:36.526154 140624564107008 estimator.py:1760] Using temporary folder as model directory: /tmp/tmp6q1g_L; I1213 19:19:36.526648 140624564107008 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe550820190>, '_model_dir': '/tmp/tmp6q1g_L', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distrib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-565657419
https://github.com/google/deepvariant/issues/249#issuecomment-565657419:356,Availability,checkpoint,checkpoint,356,"Dear akolesikov,. --regions ""20:10,000,000-10,100,000"" was ok without shards,; however, for WES.bed I got a similar error as below. . ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I1213 19:19:36.445342 140624564107008 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 19:19:36.497919: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 19:19:36.499703: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x52dd110 executing computations on platform Host. Devices:; 2019-12-13 19:19:36.499773: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 19:19:36.503204: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 19:19:36.524235 140624564107008 modeling.py:560] Initializing model with random parameters; W1213 19:19:36.526154 140624564107008 estimator.py:1760] Using temporary folder as model directory: /tmp/tmp6q1g_L; I1213 19:19:36.526648 140624564107008 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe550820190>, '_model_dir': '/tmp/tmp6q1g_L', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distrib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-565657419
https://github.com/google/deepvariant/issues/249#issuecomment-565657419:5621,Availability,checkpoint,checkpoint,5621,"058 140624564107008 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.; Instructions for updating:; Use standard file APIs to check for files with this prefix.; I1213 19:19:41.240454 140624564107008 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I1213 19:19:42.471465 140624564107008 session_manager.py:491] Running local_init_op.; I1213 19:19:42.514822 140624564107008 session_manager.py:493] Done running local_init_op.; I1213 19:19:42.911899 140624564107008 modeling.py:410] Reloading EMA...; I1213 19:19:42.913168 140624564107008 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt. real	0m35.093s; user	0m23.000s; sys	2m0.390s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt""' returned non-zero exit status 247; ```; I'm attaching the code and the entire output. . Thanks a lot. ; [WES_20191214_samplenoshards.txt](https://github.com/google/deepvariant/files/3963193/WES_20191214_samplenoshards.txt). [error_output.txt](https://github.com/google/deepvariant/files/3963194/error_output.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-565657419
https://github.com/google/deepvariant/issues/249#issuecomment-565657419:1358,Modifiability,config,config,1358,"s.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 19:19:36.497919: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 19:19:36.499703: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x52dd110 executing computations on platform Host. Devices:; 2019-12-13 19:19:36.499773: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 19:19:36.503204: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 19:19:36.524235 140624564107008 modeling.py:560] Initializing model with random parameters; W1213 19:19:36.526154 140624564107008 estimator.py:1760] Using temporary folder as model directory: /tmp/tmp6q1g_L; I1213 19:19:36.526648 140624564107008 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe550820190>, '_model_dir': '/tmp/tmp6q1g_L', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}; I1213 19:19:36.526952 140624564107008 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz; W1213 19:19:36.531224 140624564107008 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-565657419
https://github.com/google/deepvariant/issues/249#issuecomment-565657419:1019,Performance,Tune,Tune,1019,",000"" was ok without shards,; however, for WES.bed I got a similar error as below. . ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I1213 19:19:36.445342 140624564107008 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 19:19:36.497919: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 19:19:36.499703: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x52dd110 executing computations on platform Host. Devices:; 2019-12-13 19:19:36.499773: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 19:19:36.503204: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 19:19:36.524235 140624564107008 modeling.py:560] Initializing model with random parameters; W1213 19:19:36.526154 140624564107008 estimator.py:1760] Using temporary folder as model directory: /tmp/tmp6q1g_L; I1213 19:19:36.526648 140624564107008 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe550820190>, '_model_dir': '/tmp/tmp6q1g_L', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-565657419
https://github.com/google/deepvariant/issues/249#issuecomment-565657419:1068,Performance,perform,performance,1068,",000"" was ok without shards,; however, for WES.bed I got a similar error as below. . ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I1213 19:19:36.445342 140624564107008 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 19:19:36.497919: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 19:19:36.499703: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x52dd110 executing computations on platform Host. Devices:; 2019-12-13 19:19:36.499773: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 19:19:36.503204: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 19:19:36.524235 140624564107008 modeling.py:560] Initializing model with random parameters; W1213 19:19:36.526154 140624564107008 estimator.py:1760] Using temporary folder as model directory: /tmp/tmp6q1g_L; I1213 19:19:36.526648 140624564107008 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe550820190>, '_model_dir': '/tmp/tmp6q1g_L', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-565657419
https://github.com/google/deepvariant/issues/249#issuecomment-573900023:299,Energy Efficiency,reduce,reduce,299,"I tried and was unsuccessful to reproduce the problem with the case study input data. I could try to reproduce the problem with your data if you can attach your BAM (just the 100,000 bases of it) and the reference.; As a work around you could run without --num_shards flag, or you could also try to reduce the number of shards.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-573900023
https://github.com/google/deepvariant/issues/249#issuecomment-586409546:75,Deployability,update,update,75,"Hi @se2cheeese ; I will close this issue for now because there's no reason update from you. But please feel free to follow up with more questions, either reopening this issue, or another one. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/249#issuecomment-586409546
https://github.com/google/deepvariant/issues/251#issuecomment-563508582:76,Energy Efficiency,reduce,reduced,76,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251#issuecomment-563508582
https://github.com/google/deepvariant/issues/251#issuecomment-563508582:84,Usability,learn,learning,84,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251#issuecomment-563508582
https://github.com/google/deepvariant/issues/251#issuecomment-563508582:307,Usability,learn,learn,307,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251#issuecomment-563508582
https://github.com/google/deepvariant/issues/251#issuecomment-566180701:243,Performance,perform,perform,243,"@jguhlin glad to hear that training curves look reasonable! I want to mention that DV is currently written to be a diploid variant caller. In case you are retraining with data from polyploid organisms, it is not yet clear how DeepVariant will perform. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251#issuecomment-566180701
https://github.com/google/deepvariant/issues/251#issuecomment-566180701:216,Usability,clear,clear,216,"@jguhlin glad to hear that training curves look reasonable! I want to mention that DV is currently written to be a diploid variant caller. In case you are retraining with data from polyploid organisms, it is not yet clear how DeepVariant will perform. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/251#issuecomment-566180701
https://github.com/google/deepvariant/issues/252#issuecomment-566176982:16,Availability,error,error,16,"Looking at this error message, I wonder if it's related to Python3 vs Python2.; Currently DeepVariant still only supports Python2. Adding @chapmanb (who maintains DeepVariant on bioconda ) , do you think there might be other issues going on here? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566176982
https://github.com/google/deepvariant/issues/252#issuecomment-566176982:22,Integrability,message,message,22,"Looking at this error message, I wonder if it's related to Python3 vs Python2.; Currently DeepVariant still only supports Python2. Adding @chapmanb (who maintains DeepVariant on bioconda ) , do you think there might be other issues going on here? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566176982
https://github.com/google/deepvariant/issues/252#issuecomment-566199242:150,Deployability,install,install,150,"Thanks for the question. I'm agreed with Pi-Chuan, trying in a python 2.7 conda environment should hopefully resolve these issues. Hope that gets the install figured out.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566199242
https://github.com/google/deepvariant/issues/252#issuecomment-566427577:79,Deployability,install,installed,79,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:; ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge; _tflow_select: 2.1.0-gpu ; absl-py: 0.8.1-py27_0 conda-forge; astor: 0.7.1-py_0 conda-forge; backports: 1.0-py_2 conda-forge; backports.weakref: 1.0.post1-py27_1000 conda-forge; boost: 1.70.0-py27h9de70de_1 conda-forge; boost-cpp: 1.70.0-h8e57a91_2 conda-forge; bzip2: 1.0.8-h516909a_2 conda-forge; c-ares: 1.15.0-h516909a_1001 conda-forge; ca-certificates: 2019.11.28-hecc5488_0 conda-forge; certifi: 2019.11.28-py27_0 conda-forge; cffi: 1.13.2-py27h8022711_0 conda-forge; chardet: 3.0.4-py27_1003 conda-forge; cliff: 2.15.0-py27_0 conda-forge; cmd2: 0.8.6-py27_0 conda-forge; contextlib2: 0.6.0-py_0 conda-forge; crcmod: 1.7-py27_1002 conda-forge; cryptography: 2.8-py27h72c5cf5_1 conda-forge; cudatoolkit: 9.2-0 ; cudnn: 7.6.4-cuda9.2_0 ; cupti: 9.2.148-0 ; curl: 7.65.3-hf8cf82a_0 conda-forge; enum34: 1.1.6-py27_1002 conda-forge; funcsigs: 1.0.2-py_3 conda-forge; futures: 3.3.0-py27_0 conda-forge; gast: 0.3.2-py_0 conda-forge; google-cloud-sdk: 166.0.0-py27_0 bioconda ; grpcio: 1.23.0-py27he9ae1f9_0 conda-forge; h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge; hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge; htslib: 1.9-h244ad75_9 bioconda ; httplib2: 0.14.0-py27_0 conda-forge; icu: 64.2-he1b5a44_1 conda-forge; idna: 2.8-py27_1000 conda-forge; intervaltree: 3.0.2-py_0 conda-forge; ipaddress: 1.0.23-py_0 conda-forge; keras-applications: 1.0.8-py_1 conda-forge; keras-preprocessing: 1.1.0-py_0 conda-forge; krb5: 1.16.4-h2fd8d38_0 conda-forge; libblas: 3.8.0-14_openblas conda-forge; libcblas: 3.8.0-14_openblas conda-forge; libcurl: 7.65.3-hda55be3_0 conda-forge; libdeflate: 1.3-h516909a_0 conda-forge; libedit: 3.1.20170329-hf8c457e_1001 conda-forge; libffi: 3.2.1-he1b5a44_1006 conda-forge; libgcc-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566427577
https://github.com/google/deepvariant/issues/252#issuecomment-566427577:4257,Deployability,install,installed,4257,"; mock: 3.0.5-py27_0 conda-forge; ncurses: 6.1-hf484d3e_1002 conda-forge; numpy: 1.14.6-py27h95a1406_1201 conda-forge; oauth2client: 1.5.2-py27_0 bioconda ; openjdk: 8.0.192-h14c3975_1003 conda-forge; openssl: 1.1.1d-h516909a_0 conda-forge; parallel: 20160622-1 bioconda ; pbr: 5.4.2-py_0 conda-forge; perl: 5.26.2-h516909a_1006 conda-forge; perl-threaded: 5.26.0-0 bioconda ; pip: 19.3.1-py27_0 conda-forge; prettytable: 0.7.2-py_3 conda-forge; protobuf: 3.11.1-py27he1b5a44_0 conda-forge; psutil: 5.6.7-py27h516909a_0 conda-forge; pyasn1: 0.4.8-py_0 conda-forge; pyasn1-modules: 0.2.7-py_0 conda-forge; pycparser: 2.19-py27_1 conda-forge; pyopenssl: 19.1.0-py27_0 conda-forge; pyparsing: 2.4.5-py_0 conda-forge; pyperclip: 1.7.0-py_0 conda-forge; pysocks: 1.7.0-py27_0 conda-forge; python: 2.7.15-h5a48372_1009 conda-forge; pyyaml: 5.2-py27h516909a_0 conda-forge; readline: 8.0-hf8c457e_0 conda-forge; requests: 2.22.0-py27_1 conda-forge; rsa: 3.1.4-py27_0 bioconda ; scipy: 1.2.1-py27h921218d_2 conda-forge; setuptools: 42.0.2-py27_0 conda-forge; six: 1.13.0-py27_0 conda-forge; sortedcontainers: 2.1.0-py_0 conda-forge; sqlite: 3.30.1-hcee41ef_0 conda-forge; stevedore: 1.30.1-py_0 conda-forge; subprocess32: 3.5.4-py27h516909a_0 conda-forge; tensorboard: 1.12.0-py27_1000 conda-forge; tensorflow: 1.12.0-gpu_py27h2a0f108_0 ; tensorflow-base: 1.12.0-gpu_py27had579c0_0 ; tensorflow-estimator: 1.13.0-py_0 ; termcolor: 1.1.0-py_2 conda-forge; tk: 8.6.10-hed695b0_0 conda-forge; traceback2: 1.4.0-py27_0 conda-forge; unicodecsv: 0.14.1-py_1 conda-forge; unittest2: 1.1.0-py_0 conda-forge; urllib3: 1.25.7-py27_0 conda-forge; wcwidth: 0.1.7-py_1 conda-forge; werkzeug: 0.16.0-py_0 conda-forge; wheel: 0.33.6-py27_0 conda-forge; xz: 5.2.4-h14c3975_1001 conda-forge; yaml: 0.2.2-h516909a_1 conda-forge; zlib: 1.2.11-h516909a_1006 conda-forge; ```; I've also installed one package at a time, and then installed the deepvariant package with the comman ```conda install -c bioconda deepvariant```. Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566427577
https://github.com/google/deepvariant/issues/252#issuecomment-566427577:4299,Deployability,install,installed,4299,"; mock: 3.0.5-py27_0 conda-forge; ncurses: 6.1-hf484d3e_1002 conda-forge; numpy: 1.14.6-py27h95a1406_1201 conda-forge; oauth2client: 1.5.2-py27_0 bioconda ; openjdk: 8.0.192-h14c3975_1003 conda-forge; openssl: 1.1.1d-h516909a_0 conda-forge; parallel: 20160622-1 bioconda ; pbr: 5.4.2-py_0 conda-forge; perl: 5.26.2-h516909a_1006 conda-forge; perl-threaded: 5.26.0-0 bioconda ; pip: 19.3.1-py27_0 conda-forge; prettytable: 0.7.2-py_3 conda-forge; protobuf: 3.11.1-py27he1b5a44_0 conda-forge; psutil: 5.6.7-py27h516909a_0 conda-forge; pyasn1: 0.4.8-py_0 conda-forge; pyasn1-modules: 0.2.7-py_0 conda-forge; pycparser: 2.19-py27_1 conda-forge; pyopenssl: 19.1.0-py27_0 conda-forge; pyparsing: 2.4.5-py_0 conda-forge; pyperclip: 1.7.0-py_0 conda-forge; pysocks: 1.7.0-py27_0 conda-forge; python: 2.7.15-h5a48372_1009 conda-forge; pyyaml: 5.2-py27h516909a_0 conda-forge; readline: 8.0-hf8c457e_0 conda-forge; requests: 2.22.0-py27_1 conda-forge; rsa: 3.1.4-py27_0 bioconda ; scipy: 1.2.1-py27h921218d_2 conda-forge; setuptools: 42.0.2-py27_0 conda-forge; six: 1.13.0-py27_0 conda-forge; sortedcontainers: 2.1.0-py_0 conda-forge; sqlite: 3.30.1-hcee41ef_0 conda-forge; stevedore: 1.30.1-py_0 conda-forge; subprocess32: 3.5.4-py27h516909a_0 conda-forge; tensorboard: 1.12.0-py27_1000 conda-forge; tensorflow: 1.12.0-gpu_py27h2a0f108_0 ; tensorflow-base: 1.12.0-gpu_py27had579c0_0 ; tensorflow-estimator: 1.13.0-py_0 ; termcolor: 1.1.0-py_2 conda-forge; tk: 8.6.10-hed695b0_0 conda-forge; traceback2: 1.4.0-py27_0 conda-forge; unicodecsv: 0.14.1-py_1 conda-forge; unittest2: 1.1.0-py_0 conda-forge; urllib3: 1.25.7-py27_0 conda-forge; wcwidth: 0.1.7-py_1 conda-forge; werkzeug: 0.16.0-py_0 conda-forge; wheel: 0.33.6-py27_0 conda-forge; xz: 5.2.4-h14c3975_1001 conda-forge; yaml: 0.2.2-h516909a_1 conda-forge; zlib: 1.2.11-h516909a_1006 conda-forge; ```; I've also installed one package at a time, and then installed the deepvariant package with the comman ```conda install -c bioconda deepvariant```. Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566427577
https://github.com/google/deepvariant/issues/252#issuecomment-566427577:4358,Deployability,install,install,4358,"; mock: 3.0.5-py27_0 conda-forge; ncurses: 6.1-hf484d3e_1002 conda-forge; numpy: 1.14.6-py27h95a1406_1201 conda-forge; oauth2client: 1.5.2-py27_0 bioconda ; openjdk: 8.0.192-h14c3975_1003 conda-forge; openssl: 1.1.1d-h516909a_0 conda-forge; parallel: 20160622-1 bioconda ; pbr: 5.4.2-py_0 conda-forge; perl: 5.26.2-h516909a_1006 conda-forge; perl-threaded: 5.26.0-0 bioconda ; pip: 19.3.1-py27_0 conda-forge; prettytable: 0.7.2-py_3 conda-forge; protobuf: 3.11.1-py27he1b5a44_0 conda-forge; psutil: 5.6.7-py27h516909a_0 conda-forge; pyasn1: 0.4.8-py_0 conda-forge; pyasn1-modules: 0.2.7-py_0 conda-forge; pycparser: 2.19-py27_1 conda-forge; pyopenssl: 19.1.0-py27_0 conda-forge; pyparsing: 2.4.5-py_0 conda-forge; pyperclip: 1.7.0-py_0 conda-forge; pysocks: 1.7.0-py27_0 conda-forge; python: 2.7.15-h5a48372_1009 conda-forge; pyyaml: 5.2-py27h516909a_0 conda-forge; readline: 8.0-hf8c457e_0 conda-forge; requests: 2.22.0-py27_1 conda-forge; rsa: 3.1.4-py27_0 bioconda ; scipy: 1.2.1-py27h921218d_2 conda-forge; setuptools: 42.0.2-py27_0 conda-forge; six: 1.13.0-py27_0 conda-forge; sortedcontainers: 2.1.0-py_0 conda-forge; sqlite: 3.30.1-hcee41ef_0 conda-forge; stevedore: 1.30.1-py_0 conda-forge; subprocess32: 3.5.4-py27h516909a_0 conda-forge; tensorboard: 1.12.0-py27_1000 conda-forge; tensorflow: 1.12.0-gpu_py27h2a0f108_0 ; tensorflow-base: 1.12.0-gpu_py27had579c0_0 ; tensorflow-estimator: 1.13.0-py_0 ; termcolor: 1.1.0-py_2 conda-forge; tk: 8.6.10-hed695b0_0 conda-forge; traceback2: 1.4.0-py27_0 conda-forge; unicodecsv: 0.14.1-py_1 conda-forge; unittest2: 1.1.0-py_0 conda-forge; urllib3: 1.25.7-py27_0 conda-forge; wcwidth: 0.1.7-py_1 conda-forge; werkzeug: 0.16.0-py_0 conda-forge; wheel: 0.33.6-py27_0 conda-forge; xz: 5.2.4-h14c3975_1001 conda-forge; yaml: 0.2.2-h516909a_1 conda-forge; zlib: 1.2.11-h516909a_1006 conda-forge; ```; I've also installed one package at a time, and then installed the deepvariant package with the comman ```conda install -c bioconda deepvariant```. Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566427577
https://github.com/google/deepvariant/issues/252#issuecomment-566427577:633,Security,certificate,certificates,633,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:; ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge; _tflow_select: 2.1.0-gpu ; absl-py: 0.8.1-py27_0 conda-forge; astor: 0.7.1-py_0 conda-forge; backports: 1.0-py_2 conda-forge; backports.weakref: 1.0.post1-py27_1000 conda-forge; boost: 1.70.0-py27h9de70de_1 conda-forge; boost-cpp: 1.70.0-h8e57a91_2 conda-forge; bzip2: 1.0.8-h516909a_2 conda-forge; c-ares: 1.15.0-h516909a_1001 conda-forge; ca-certificates: 2019.11.28-hecc5488_0 conda-forge; certifi: 2019.11.28-py27_0 conda-forge; cffi: 1.13.2-py27h8022711_0 conda-forge; chardet: 3.0.4-py27_1003 conda-forge; cliff: 2.15.0-py27_0 conda-forge; cmd2: 0.8.6-py27_0 conda-forge; contextlib2: 0.6.0-py_0 conda-forge; crcmod: 1.7-py27_1002 conda-forge; cryptography: 2.8-py27h72c5cf5_1 conda-forge; cudatoolkit: 9.2-0 ; cudnn: 7.6.4-cuda9.2_0 ; cupti: 9.2.148-0 ; curl: 7.65.3-hf8cf82a_0 conda-forge; enum34: 1.1.6-py27_1002 conda-forge; funcsigs: 1.0.2-py_3 conda-forge; futures: 3.3.0-py27_0 conda-forge; gast: 0.3.2-py_0 conda-forge; google-cloud-sdk: 166.0.0-py27_0 bioconda ; grpcio: 1.23.0-py27he9ae1f9_0 conda-forge; h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge; hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge; htslib: 1.9-h244ad75_9 bioconda ; httplib2: 0.14.0-py27_0 conda-forge; icu: 64.2-he1b5a44_1 conda-forge; idna: 2.8-py27_1000 conda-forge; intervaltree: 3.0.2-py_0 conda-forge; ipaddress: 1.0.23-py_0 conda-forge; keras-applications: 1.0.8-py_1 conda-forge; keras-preprocessing: 1.1.0-py_0 conda-forge; krb5: 1.16.4-h2fd8d38_0 conda-forge; libblas: 3.8.0-14_openblas conda-forge; libcblas: 3.8.0-14_openblas conda-forge; libcurl: 7.65.3-hda55be3_0 conda-forge; libdeflate: 1.3-h516909a_0 conda-forge; libedit: 3.1.20170329-hf8c457e_1001 conda-forge; libffi: 3.2.1-he1b5a44_1006 conda-forge; libgcc-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566427577
https://github.com/google/deepvariant/issues/252#issuecomment-566427577:2402,Testability,mock,mock,2402,a-forge; htslib: 1.9-h244ad75_9 bioconda ; httplib2: 0.14.0-py27_0 conda-forge; icu: 64.2-he1b5a44_1 conda-forge; idna: 2.8-py27_1000 conda-forge; intervaltree: 3.0.2-py_0 conda-forge; ipaddress: 1.0.23-py_0 conda-forge; keras-applications: 1.0.8-py_1 conda-forge; keras-preprocessing: 1.1.0-py_0 conda-forge; krb5: 1.16.4-h2fd8d38_0 conda-forge; libblas: 3.8.0-14_openblas conda-forge; libcblas: 3.8.0-14_openblas conda-forge; libcurl: 7.65.3-hda55be3_0 conda-forge; libdeflate: 1.3-h516909a_0 conda-forge; libedit: 3.1.20170329-hf8c457e_1001 conda-forge; libffi: 3.2.1-he1b5a44_1006 conda-forge; libgcc-ng: 9.2.0-hdf63c60_0 conda-forge; libgfortran-ng: 7.3.0-hdf63c60_2 conda-forge; liblapack: 3.8.0-14_openblas conda-forge; libopenblas: 0.3.7-h5ec1e0e_5 conda-forge; libpng: 1.6.37-hed695b0_0 conda-forge; libprotobuf: 3.11.1-h8b12597_0 conda-forge; libssh2: 1.8.2-h22169c7_2 conda-forge; libstdcxx-ng: 9.2.0-hdf63c60_0 conda-forge; linecache2: 1.0.0-py_1 conda-forge; markdown: 3.1.1-py_0 conda-forge; mock: 3.0.5-py27_0 conda-forge; ncurses: 6.1-hf484d3e_1002 conda-forge; numpy: 1.14.6-py27h95a1406_1201 conda-forge; oauth2client: 1.5.2-py27_0 bioconda ; openjdk: 8.0.192-h14c3975_1003 conda-forge; openssl: 1.1.1d-h516909a_0 conda-forge; parallel: 20160622-1 bioconda ; pbr: 5.4.2-py_0 conda-forge; perl: 5.26.2-h516909a_1006 conda-forge; perl-threaded: 5.26.0-0 bioconda ; pip: 19.3.1-py27_0 conda-forge; prettytable: 0.7.2-py_3 conda-forge; protobuf: 3.11.1-py27he1b5a44_0 conda-forge; psutil: 5.6.7-py27h516909a_0 conda-forge; pyasn1: 0.4.8-py_0 conda-forge; pyasn1-modules: 0.2.7-py_0 conda-forge; pycparser: 2.19-py27_1 conda-forge; pyopenssl: 19.1.0-py27_0 conda-forge; pyparsing: 2.4.5-py_0 conda-forge; pyperclip: 1.7.0-py_0 conda-forge; pysocks: 1.7.0-py27_0 conda-forge; python: 2.7.15-h5a48372_1009 conda-forge; pyyaml: 5.2-py27h516909a_0 conda-forge; readline: 8.0-hf8c457e_0 conda-forge; requests: 2.22.0-py27_1 conda-forge; rsa: 3.1.4-py27_0 bioconda ; scipy: 1.2.1-py27h921218d_,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566427577
https://github.com/google/deepvariant/issues/252#issuecomment-566496481:119,Availability,error,error,119,"Andrea;; Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:; ```; conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'; ```; If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566496481
https://github.com/google/deepvariant/issues/252#issuecomment-566496481:596,Availability,error,errors,596,"Andrea;; Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:; ```; conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'; ```; If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566496481
https://github.com/google/deepvariant/issues/252#issuecomment-566496481:646,Availability,down,downloading,646,"Andrea;; Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:; ```; conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'; ```; If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566496481
https://github.com/google/deepvariant/issues/252#issuecomment-566496481:151,Deployability,install,installed,151,"Andrea;; Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:; ```; conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'; ```; If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566496481
https://github.com/google/deepvariant/issues/252#issuecomment-566496481:314,Deployability,install,installing,314,"Andrea;; Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:; ```; conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'; ```; If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566496481
https://github.com/google/deepvariant/issues/252#issuecomment-566496481:125,Integrability,message,messages,125,"Andrea;; Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:; ```; conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'; ```; If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566496481
https://github.com/google/deepvariant/issues/252#issuecomment-566496481:370,Safety,avoid,avoid,370,"Andrea;; Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:; ```; conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'; ```; If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566496481
https://github.com/google/deepvariant/issues/252#issuecomment-566564983:330,Availability,error,error,330,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; ```; conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; ```; Everything is installed correctly. However, when I try to run it I get the following error:; ```; python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>; import tensorflow as tf; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>; from google.protobuf import descriptor as _descriptor; File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>; from google.protobuf.pyext import _message; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so); ```; I've tried to install an updated version of the library using ; ```conda install librosa```; but it didn't work. Any suggestion?. Thank you again for your support,; Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566564983
https://github.com/google/deepvariant/issues/252#issuecomment-566564983:259,Deployability,install,installed,259,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; ```; conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; ```; Everything is installed correctly. However, when I try to run it I get the following error:; ```; python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>; import tensorflow as tf; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>; from google.protobuf import descriptor as _descriptor; File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>; from google.protobuf.pyext import _message; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so); ```; I've tried to install an updated version of the library using ; ```conda install librosa```; but it didn't work. Any suggestion?. Thank you again for your support,; Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566564983
https://github.com/google/deepvariant/issues/252#issuecomment-566564983:1588,Deployability,install,install,1588,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; ```; conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; ```; Everything is installed correctly. However, when I try to run it I get the following error:; ```; python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>; import tensorflow as tf; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>; from google.protobuf import descriptor as _descriptor; File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>; from google.protobuf.pyext import _message; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so); ```; I've tried to install an updated version of the library using ; ```conda install librosa```; but it didn't work. Any suggestion?. Thank you again for your support,; Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566564983
https://github.com/google/deepvariant/issues/252#issuecomment-566564983:1599,Deployability,update,updated,1599,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; ```; conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; ```; Everything is installed correctly. However, when I try to run it I get the following error:; ```; python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>; import tensorflow as tf; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>; from google.protobuf import descriptor as _descriptor; File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>; from google.protobuf.pyext import _message; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so); ```; I've tried to install an updated version of the library using ; ```conda install librosa```; but it didn't work. Any suggestion?. Thank you again for your support,; Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566564983
https://github.com/google/deepvariant/issues/252#issuecomment-566564983:1647,Deployability,install,install,1647,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; ```; conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; ```; Everything is installed correctly. However, when I try to run it I get the following error:; ```; python /PATH/TO/Andrea/myanaconda/deepvariant/share/deepvariant-0.9.0-0/binaries/DeepVariant/0.9.0/DeepVariant-0.9.0/call_variants.zip --help; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lazCGY/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 47, in <module>; import tensorflow as tf; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 59, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/PATH/TO/Andrea/myanaconda/deepvariant/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>; from google.protobuf import descriptor as _descriptor; File ""/tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 47, in <module>; from google.protobuf.pyext import _message; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_lazCGY/runfiles/protobuf_archive/python/google/protobuf/pyext/_message.so); ```; I've tried to install an updated version of the library using ; ```conda install librosa```; but it didn't work. Any suggestion?. Thank you again for your support,; Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566564983
https://github.com/google/deepvariant/issues/252#issuecomment-566566093:259,Deployability,install,installed,259,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; ```; conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; ```; Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; I'm not sure I've been able to find it. . Thank you again for your support,; Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566566093
https://github.com/google/deepvariant/issues/252#issuecomment-566566093:319,Deployability,install,installed,319,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; ```; conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; ```; Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; I'm not sure I've been able to find it. . Thank you again for your support,; Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566566093
https://github.com/google/deepvariant/issues/252#issuecomment-566566093:291,Usability,guid,guide,291,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; ```; conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; ```; Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; I'm not sure I've been able to find it. . Thank you again for your support,; Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-566566093
https://github.com/google/deepvariant/issues/252#issuecomment-567477006:33,Availability,error,error,33,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it?. > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; > ; > ```; > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; > ```; > ; > Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; > I'm not sure I've been able to find it.; > ; > Thank you again for your support,; > Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-567477006
https://github.com/google/deepvariant/issues/252#issuecomment-567477006:103,Deployability,install,installation,103,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it?. > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; > ; > ```; > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; > ```; > ; > Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; > I'm not sure I've been able to find it.; > ; > Thank you again for your support,; > Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-567477006
https://github.com/google/deepvariant/issues/252#issuecomment-567477006:427,Deployability,install,installed,427,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it?. > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; > ; > ```; > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; > ```; > ; > Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; > I'm not sure I've been able to find it.; > ; > Thank you again for your support,; > Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-567477006
https://github.com/google/deepvariant/issues/252#issuecomment-567477006:487,Deployability,install,installed,487,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it?. > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; > ; > ```; > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; > ```; > ; > Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; > I'm not sure I've been able to find it.; > ; > Thank you again for your support,; > Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-567477006
https://github.com/google/deepvariant/issues/252#issuecomment-567477006:459,Usability,guid,guide,459,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it?. > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; > ; > ```; > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; > ```; > ; > Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; > I'm not sure I've been able to find it.; > ; > Thank you again for your support,; > Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-567477006
https://github.com/google/deepvariant/issues/252#issuecomment-568129639:1087,Availability,error,error,1087,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```; $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0; binaries models ; ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```; $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D; eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>; ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D; eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions!. @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-568129639
https://github.com/google/deepvariant/issues/252#issuecomment-568129639:19,Deployability,install,installing,19,"@RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment. ```; $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0; binaries models ; ```. The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:. ```; $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D; eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>; ```. Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D; eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions!. @prabal97 could you share the error you are seeing?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-568129639
https://github.com/google/deepvariant/issues/252#issuecomment-568349835:1131,Availability,error,error,1131,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment.; > ; > ```; > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0; > binaries models ; > ```; > ; > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:; > ; > ```; > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D; > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>; > ```; > ; > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions!; > ; > @prabal97 could you share the error you are seeing?. I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-568349835
https://github.com/google/deepvariant/issues/252#issuecomment-568349835:1181,Availability,error,error,1181,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment.; > ; > ```; > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0; > binaries models ; > ```; > ; > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:; > ; > ```; > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D; > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>; > ```; > ; > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions!; > ; > @prabal97 could you share the error you are seeing?. I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-568349835
https://github.com/google/deepvariant/issues/252#issuecomment-568349835:21,Deployability,install,installing,21,"> @RenzoTale88 after installing through bioconda, the binaries and models for DeepVariant will be located under the following path, where $ENV_NAME is the name of the conda environment.; > ; > ```; > $ ls miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0; > binaries models ; > ```; > ; > The binaries can be run as shown in [this script](https://github.com/google/deepvariant/blob/r0.9/scripts/run_wgs_case_study_binaries.sh), which goes through the [WGS case study (Docker instructions)](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md). For example, you can run:; > ; > ```; > $ python miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D; > eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip <flags>; > ```; > ; > Note, you may have to run `chmod +x miniconda2/envs/${ENV_NAME}/share/deepvariant-0.9.0-0/binaries/D eepVariant/0.9.0/DeepVariant-0.9.0/make_examples.zip` prior to running the command above. This will apply to other binaries as well (`call_variants`, `postprocess_variants`, etc.). Let me know if you have any other questions!; > ; > @prabal97 could you share the error you are seeing?. I am getting the following error even after using the binaries in the mentioned directories . ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-568349835
https://github.com/google/deepvariant/issues/252#issuecomment-568430043:12,Availability,error,error,12,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-568430043
https://github.com/google/deepvariant/issues/252#issuecomment-568430043:389,Security,access,access,389,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-568430043
https://github.com/google/deepvariant/issues/252#issuecomment-568717631:118,Deployability,update,updates,118,"Thank you for your answer, unfortunately is a central cluster system, and I have not the options to make large system updates. If in the future I have the need to use deepvariant I will consider a cloud solution. Thank you again for your support, . Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-568717631
https://github.com/google/deepvariant/issues/252#issuecomment-585527464:92,Deployability,release,release,92,"@serge2016 @PlatonB Thank you for raising this issue again. We're now aiming for an earlier release, in March. I'll also update to the other issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-585527464
https://github.com/google/deepvariant/issues/252#issuecomment-585527464:121,Deployability,update,update,121,"@serge2016 @PlatonB Thank you for raising this issue again. We're now aiming for an earlier release, in March. I'll also update to the other issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/252#issuecomment-585527464
https://github.com/google/deepvariant/issues/253#issuecomment-567578821:307,Availability,down,downstream,307,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253#issuecomment-567578821
https://github.com/google/deepvariant/issues/253#issuecomment-567578821:151,Deployability,release,release,151,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253#issuecomment-567578821
https://github.com/google/deepvariant/issues/253#issuecomment-567578821:382,Energy Efficiency,monitor,monitor,382,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253#issuecomment-567578821
https://github.com/google/deepvariant/issues/253#issuecomment-567578821:353,Testability,log,logging,353,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/253#issuecomment-567578821
https://github.com/google/deepvariant/issues/254#issuecomment-567577141:24,Integrability,message,messages,24,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```; I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT; ```. I do notice a possible issue with your Docker command, pasted below. ```; sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4; ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: ; `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254#issuecomment-567577141
https://github.com/google/deepvariant/issues/254#issuecomment-567577141:106,Testability,log,logging,106,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```; I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT; ```. I do notice a possible issue with your Docker command, pasted below. ```; sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4; ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: ; `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/254#issuecomment-567577141
https://github.com/google/deepvariant/issues/255#issuecomment-568131537:159,Integrability,wrap,wrapper,159,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:; ; intervaltree==2.1.0. This is the submit script without the SLURM commands:; ```; export BIN_VERSION=""0.9.0""; export BASE=""${PWD}/deepvariant-run""; export INPUT_DIR=""${BASE}/input""; export REF=""hs37d5.fa.gz""; export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam""; export OUTPUT_DIR=""${BASE}/output""; export DATA_DIR=""${INPUT_DIR}/data""; export OUTPUT_VCF=""HG002.output.vcf.gz""; export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}""; gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/; module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \; --reads=""${DATA}/input/data/${BAM}"" \; 	 --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255#issuecomment-568131537
https://github.com/google/deepvariant/issues/255#issuecomment-568131537:985,Performance,perform,performance-testdata,985,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:; ; intervaltree==2.1.0. This is the submit script without the SLURM commands:; ```; export BIN_VERSION=""0.9.0""; export BASE=""${PWD}/deepvariant-run""; export INPUT_DIR=""${BASE}/input""; export REF=""hs37d5.fa.gz""; export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam""; export OUTPUT_DIR=""${BASE}/output""; export DATA_DIR=""${INPUT_DIR}/data""; export OUTPUT_VCF=""HG002.output.vcf.gz""; export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}""; gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/; module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \; --reads=""${DATA}/input/data/${BAM}"" \; 	 --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255#issuecomment-568131537
https://github.com/google/deepvariant/issues/255#issuecomment-568131537:1057,Performance,perform,performance-testdata,1057,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:; ; intervaltree==2.1.0. This is the submit script without the SLURM commands:; ```; export BIN_VERSION=""0.9.0""; export BASE=""${PWD}/deepvariant-run""; export INPUT_DIR=""${BASE}/input""; export REF=""hs37d5.fa.gz""; export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam""; export OUTPUT_DIR=""${BASE}/output""; export DATA_DIR=""${INPUT_DIR}/data""; export OUTPUT_VCF=""HG002.output.vcf.gz""; export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}""; gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/; module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \; --reads=""${DATA}/input/data/${BAM}"" \; 	 --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255#issuecomment-568131537
https://github.com/google/deepvariant/issues/255#issuecomment-568131537:1162,Performance,load,load,1162,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:; ; intervaltree==2.1.0. This is the submit script without the SLURM commands:; ```; export BIN_VERSION=""0.9.0""; export BASE=""${PWD}/deepvariant-run""; export INPUT_DIR=""${BASE}/input""; export REF=""hs37d5.fa.gz""; export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam""; export OUTPUT_DIR=""${BASE}/output""; export DATA_DIR=""${INPUT_DIR}/data""; export OUTPUT_VCF=""HG002.output.vcf.gz""; export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}""; gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/; module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \; --reads=""${DATA}/input/data/${BAM}"" \; 	 --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255#issuecomment-568131537
https://github.com/google/deepvariant/issues/255#issuecomment-568131537:997,Testability,test,testdata,997,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:; ; intervaltree==2.1.0. This is the submit script without the SLURM commands:; ```; export BIN_VERSION=""0.9.0""; export BASE=""${PWD}/deepvariant-run""; export INPUT_DIR=""${BASE}/input""; export REF=""hs37d5.fa.gz""; export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam""; export OUTPUT_DIR=""${BASE}/output""; export DATA_DIR=""${INPUT_DIR}/data""; export OUTPUT_VCF=""HG002.output.vcf.gz""; export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}""; gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/; module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \; --reads=""${DATA}/input/data/${BAM}"" \; 	 --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255#issuecomment-568131537
https://github.com/google/deepvariant/issues/255#issuecomment-568131537:1069,Testability,test,testdata,1069,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:; ; intervaltree==2.1.0. This is the submit script without the SLURM commands:; ```; export BIN_VERSION=""0.9.0""; export BASE=""${PWD}/deepvariant-run""; export INPUT_DIR=""${BASE}/input""; export REF=""hs37d5.fa.gz""; export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam""; export OUTPUT_DIR=""${BASE}/output""; export DATA_DIR=""${INPUT_DIR}/data""; export OUTPUT_VCF=""HG002.output.vcf.gz""; export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}""; gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/; module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \; --reads=""${DATA}/input/data/${BAM}"" \; 	 --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255#issuecomment-568131537
https://github.com/google/deepvariant/issues/255#issuecomment-568962849:30,Deployability,install,installing,30,Hi @coneheadusa ; Can you try installing intervaltree==3.0.2?; This is what we required in the run-prereq.sh script:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/run-prereq.sh#L89,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255#issuecomment-568962849
https://github.com/google/deepvariant/issues/255#issuecomment-570610250:215,Testability,test,test,215,Thank you for providing the run-prereq.sh information. I found a 2.1.0 version of intervaltree in ${HOME}/.local/lib/python2.7/site-packages that was overriding the site-packages directory within the container. The test ran successfully once I eliminated the conflict.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/255#issuecomment-570610250
https://github.com/google/deepvariant/issues/256#issuecomment-568660419:937,Deployability,release,release,937,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256#issuecomment-568660419
https://github.com/google/deepvariant/issues/256#issuecomment-568660419:1113,Deployability,release,released,1113,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256#issuecomment-568660419
https://github.com/google/deepvariant/issues/256#issuecomment-568660419:1173,Deployability,release,release,1173,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256#issuecomment-568660419
https://github.com/google/deepvariant/issues/256#issuecomment-568660419:1237,Deployability,release,released,1237,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256#issuecomment-568660419
https://github.com/google/deepvariant/issues/256#issuecomment-568660419:538,Testability,log,logic,538,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256#issuecomment-568660419
https://github.com/google/deepvariant/issues/256#issuecomment-568660419:762,Testability,log,logic,762,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256#issuecomment-568660419
https://github.com/google/deepvariant/issues/256#issuecomment-568660419:225,Usability,simpl,simple,225,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256#issuecomment-568660419
https://github.com/google/deepvariant/issues/256#issuecomment-568673024:308,Performance,optimiz,optimizing,308,"I thought the ""very sensitive"" has something to do with the indel threshold, but the result turned out to be more exciting!; I just wondered if it's possible for deepvariant to generate candidates according to an existing VCF, and now you have been working for it.; It would be a great feature for uses like optimizing deepvariant models with on-the-flow data, or ensemble calling in somatic-variant detecting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256#issuecomment-568673024
https://github.com/google/deepvariant/issues/256#issuecomment-568673024:400,Safety,detect,detecting,400,"I thought the ""very sensitive"" has something to do with the indel threshold, but the result turned out to be more exciting!; I just wondered if it's possible for deepvariant to generate candidates according to an existing VCF, and now you have been working for it.; It would be a great feature for uses like optimizing deepvariant models with on-the-flow data, or ensemble calling in somatic-variant detecting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/256#issuecomment-568673024
https://github.com/google/deepvariant/issues/257#issuecomment-569115062:321,Performance,perform,performance,321,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257#issuecomment-569115062
https://github.com/google/deepvariant/issues/257#issuecomment-569126095:170,Usability,feedback,feedback,170,"Thank you! ; My experiment was designed for the reference to be as close as possible to the studied populations so that shouldn't be an issue. Thanks for the always fast feedback.; I read in the blog post. > Of the 94,554 Mendelian violations where the child is HOM_REF, **only 17,475 (18%) of those have the HOM_REF call based just upon reference and non-reference read counts, the remaining 82% had the HOM_REF call produced by the CNN**. This seemed suspicious, so we investigated the allele depth fractions for each of HOM_REF, HETEROZYGOUS, and HOM_ALT calls in all three individuals. That's interesting because I also get quite a non trivial number of HOM_REF calls which, just based on the biology and specifics of my experiment (I prefer to remain vague about that publicly) is highly suspicious. In fact, all new homozygous variants are suspicious in my experiment. . I have highlighted a sentence in boldface, simply: how did you do that? How do you know that deepvariant made the call based on non-reference/reference read ratio or that it made the call based on its CNN interpretation? I thought the CNN was used for all calls? Or I am missing something obvious here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257#issuecomment-569126095
https://github.com/google/deepvariant/issues/257#issuecomment-569126095:920,Usability,simpl,simply,920,"Thank you! ; My experiment was designed for the reference to be as close as possible to the studied populations so that shouldn't be an issue. Thanks for the always fast feedback.; I read in the blog post. > Of the 94,554 Mendelian violations where the child is HOM_REF, **only 17,475 (18%) of those have the HOM_REF call based just upon reference and non-reference read counts, the remaining 82% had the HOM_REF call produced by the CNN**. This seemed suspicious, so we investigated the allele depth fractions for each of HOM_REF, HETEROZYGOUS, and HOM_ALT calls in all three individuals. That's interesting because I also get quite a non trivial number of HOM_REF calls which, just based on the biology and specifics of my experiment (I prefer to remain vague about that publicly) is highly suspicious. In fact, all new homozygous variants are suspicious in my experiment. . I have highlighted a sentence in boldface, simply: how did you do that? How do you know that deepvariant made the call based on non-reference/reference read ratio or that it made the call based on its CNN interpretation? I thought the CNN was used for all calls? Or I am missing something obvious here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257#issuecomment-569126095
https://github.com/google/deepvariant/issues/257#issuecomment-569131100:533,Deployability,release,release,533,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257#issuecomment-569131100
https://github.com/google/deepvariant/issues/257#issuecomment-569131100:998,Integrability,depend,depending,998,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257#issuecomment-569131100
https://github.com/google/deepvariant/issues/257#issuecomment-569131100:902,Safety,avoid,avoiding,902,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257#issuecomment-569131100
https://github.com/google/deepvariant/issues/257#issuecomment-569150582:629,Testability,test,tested,629,"Great, glad you found the report useful!. If you need the numbers from the visual report, you can extract them by clicking the button on the top right of the report and in the dropdown selecting ""View Source"". This gives the JSON that in vega-lite produces those plots, and it has the summarized data inside it. These are all summarized data though, since the raw data is the VCF itself. . Also note that you can run our DeepVariant visual report with VCFs from other callers if you want to compare. Other VCFs may be missing some of the information used to make a few of the plots, but otherwise it should work fine -- I stress tested the visual report a lot to make sure of that. Let me know if you try it whether that works for you. And good luck with your research!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257#issuecomment-569150582
https://github.com/google/deepvariant/issues/257#issuecomment-577247342:1319,Availability,error,errors,1319," the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position.; > ; > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); > ; > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? ; Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257#issuecomment-577247342
https://github.com/google/deepvariant/issues/257#issuecomment-577247342:1337,Availability,error,errors,1337," the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position.; > ; > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); > ; > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? ; Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population and the direct descendant, with absolutely no reason to believe the descendant might exhibit new mutations (except for the ones that might randomly occur but those should be negligible as it did not really have the time to diverge much, they are separated by a few generations max'). The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257#issuecomment-577247342
https://github.com/google/deepvariant/issues/257#issuecomment-577247342:547,Deployability,release,release,547,"> Hi @aderzelle; > ; > I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position.; > ; > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); > ; > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? ; Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257#issuecomment-577247342
https://github.com/google/deepvariant/issues/257#issuecomment-577247342:1018,Integrability,depend,depending,1018,"> Hi @aderzelle; > ; > I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position.; > ; > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); > ; > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? ; Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257#issuecomment-577247342
https://github.com/google/deepvariant/issues/257#issuecomment-577247342:922,Safety,avoid,avoiding,922,"> Hi @aderzelle; > ; > I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position.; > ; > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); > ; > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? ; Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/257#issuecomment-577247342
https://github.com/google/deepvariant/issues/258#issuecomment-572913563:33,Deployability,release,release,33,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258#issuecomment-572913563
https://github.com/google/deepvariant/issues/258#issuecomment-572913563:104,Deployability,release,release,104,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258#issuecomment-572913563
https://github.com/google/deepvariant/issues/258#issuecomment-572913563:158,Deployability,update,updates,158,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258#issuecomment-572913563
https://github.com/google/deepvariant/issues/258#issuecomment-572913563:298,Integrability,depend,dependencies,298,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258#issuecomment-572913563
https://github.com/google/deepvariant/issues/258#issuecomment-572937313:56,Deployability,release,release,56,Hi @AndrewCarroll . Is there an estimated date for next release?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258#issuecomment-572937313
https://github.com/google/deepvariant/issues/258#issuecomment-573807042:32,Deployability,release,release,32,Estimated timeline for the next release is Q2 2020.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258#issuecomment-573807042
https://github.com/google/deepvariant/issues/258#issuecomment-585527577:30,Deployability,update,update,30,"Hi @llllaaaa , to give you an update:; We're now aiming for an earlier release in March.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258#issuecomment-585527577
https://github.com/google/deepvariant/issues/258#issuecomment-585527577:71,Deployability,release,release,71,"Hi @llllaaaa , to give you an update:; We're now aiming for an earlier release in March.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/258#issuecomment-585527577
https://github.com/google/deepvariant/issues/259#issuecomment-573804485:31,Deployability,update,update,31,"Hi Sergey,. We are planning to update Bazel to the latest version for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/259#issuecomment-573804485
https://github.com/google/deepvariant/issues/259#issuecomment-573804485:79,Deployability,release,release,79,"Hi Sergey,. We are planning to update Bazel to the latest version for the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/259#issuecomment-573804485
https://github.com/google/deepvariant/issues/260#issuecomment-573487475:1595,Integrability,message,message,1595,"If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later.; > ; > thanks; > ; > Andy; > ; > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > ; > p.p.s. Is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475
https://github.com/google/deepvariant/issues/260#issuecomment-573487475:1187,Testability,log,logs,1187,"put. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines.; Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from?; If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475
https://github.com/google/deepvariant/issues/260#issuecomment-573487475:1263,Testability,log,logs,1263,"s like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines.; Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from?; If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later.; > ; > thanks; > ; > An",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475
https://github.com/google/deepvariant/issues/260#issuecomment-573487475:1591,Testability,log,log,1591,"If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later.; > ; > thanks; > ; > Andy; > ; > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > ; > p.p.s. Is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475
https://github.com/google/deepvariant/issues/260#issuecomment-573487475:1708,Testability,log,logs,1708,"w I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later.; > ; > thanks; > ; > Andy; > ; > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > ; > p.p.s. Is there a better place to ask questions like this?. This is a good place to ask :); It's a public forum, so our team and everyone in the community can ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475
https://github.com/google/deepvariant/issues/260#issuecomment-573487475:1790,Testability,log,log,1790,"r job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later.; > ; > thanks; > ; > Andy; > ; > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > ; > p.p.s. Is there a better place to ask questions like this?. This is a good place to ask :); It's a public forum, so our team and everyone in the community can see and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475
https://github.com/google/deepvariant/issues/260#issuecomment-573487475:433,Usability,guid,guidelines,433,"> 1. Any idea how I might estimate the expected run time?. The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines.; Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from?; If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475
https://github.com/google/deepvariant/issues/260#issuecomment-573841969:599,Integrability,message,message,599,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from?. The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com); > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!; > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not; > ; > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > . region: oregen; m5dn.8xlarge; 32 CPU; 2 x 600GB SSD; Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-573841969
https://github.com/google/deepvariant/issues/260#issuecomment-573841969:595,Testability,log,log,595,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from?. The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com); > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!; > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not; > ; > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > . region: oregen; m5dn.8xlarge; 32 CPU; 2 x 600GB SSD; Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-573841969
https://github.com/google/deepvariant/issues/260#issuecomment-573841969:1249,Usability,Learn,Learning,1249,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from?. The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com); > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!; > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not; > ; > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > . region: oregen; m5dn.8xlarge; 32 CPU; 2 x 600GB SSD; Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-573841969
https://github.com/google/deepvariant/issues/260#issuecomment-581727444:0,Deployability,Update,Update,0,Update: we discussed offline. Using `--regions` helped.; I'll close this bug.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/260#issuecomment-581727444
https://github.com/google/deepvariant/issues/262#issuecomment-574898473:83,Availability,echo,echo,83,What is the value of your ${INPUT_DIR} and ${OUTPUT_DIR} ? What is the output of ; echo $INPUT_DIR,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262#issuecomment-574898473
https://github.com/google/deepvariant/issues/262#issuecomment-574900293:90,Availability,echo,echo,90,"```; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output""; ```. `echo $INPUT_DIR`; returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:; `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262#issuecomment-574900293
https://github.com/google/deepvariant/issues/262#issuecomment-574900293:280,Availability,error,error,280,"```; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output""; ```. `echo $INPUT_DIR`; returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:; `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262#issuecomment-574900293
https://github.com/google/deepvariant/issues/262#issuecomment-574900293:314,Availability,Error,Error,314,"```; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output""; ```. `echo $INPUT_DIR`; returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:; `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262#issuecomment-574900293
https://github.com/google/deepvariant/issues/262#issuecomment-574900293:34,Testability,test,testdata,34,"```; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output""; ```. `echo $INPUT_DIR`; returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:; `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262#issuecomment-574900293
https://github.com/google/deepvariant/issues/262#issuecomment-574900293:157,Testability,test,testdata,157,"```; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output""; ```. `echo $INPUT_DIR`; returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:; `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/262#issuecomment-574900293
https://github.com/google/deepvariant/issues/263#issuecomment-577940658:127,Availability,error,error,127,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:; ```; WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl; curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}""; pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263#issuecomment-577940658
https://github.com/google/deepvariant/issues/263#issuecomment-577940658:85,Deployability,install,installation,85,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:; ```; WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl; curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}""; pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263#issuecomment-577940658
https://github.com/google/deepvariant/issues/263#issuecomment-577940658:545,Deployability,install,install,545,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:; ```; WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl; curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}""; pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263#issuecomment-577940658
https://github.com/google/deepvariant/issues/263#issuecomment-577940658:572,Deployability,upgrade,upgrade,572,"We now have an custom wheel for `intel-tensorflow==1.13.1` which fixes the above the installation issue. If you are seeing the error above, please make the following changes in [`run-prereq.sh`](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh). Replace [line 146](https://github.com/google/deepvariant/blob/r0.9/run-prereq.sh#L146) with the following:; ```; WHEEL_NAME=tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl; curl ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-1.13.1-py27/${WHEEL_NAME}"" > ""/tmp/${WHEEL_NAME}""; pip install ""${PIP_ARGS[@]}"" --upgrade ""/tmp/${WHEEL_NAME}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/263#issuecomment-577940658
https://github.com/google/deepvariant/issues/265#issuecomment-580549772:1347,Availability,Error,Errors,1347," whether this affects singularity); 1.17.5; 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU?. I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`; # Pull Singularity images; INPUT_DIR='singularity'; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images""; # Non-gpu image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg; # GPU image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors: ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772
https://github.com/google/deepvariant/issues/265#issuecomment-580549772:2106,Availability,Error,Errors,2106,"; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors: ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); Fi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772
https://github.com/google/deepvariant/issues/265#issuecomment-580549772:4154,Availability,error,errors,4154,"nsorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): ; `# Pull the deep variant docker image.; sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; ​. # Test running docker interactively w/Singularity. ; BIN_VERSION=""0.9.0""; singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; `; I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772
https://github.com/google/deepvariant/issues/265#issuecomment-580549772:936,Testability,Test,Test,936,"Hi @pichuan,; Thanks for getting back to me! Any suggestions would be great. . 1. Your OS version.; NAME=""CentOS Linux""; VERSION=""7 (Core)""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""7""; PRETTY_NAME=""CentOS Linux 7 (Core)""; ANSI_COLOR=""0;31"". 2. Your Singularity version.; singularity version 3.4.1-1.2.el7; 3. Your numpy version (I'm not sure whether this affects singularity); 1.17.5; 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU?. I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`; # Pull Singularity images; INPUT_DIR='singularity'; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images""; # Non-gpu image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg; # GPU image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772
https://github.com/google/deepvariant/issues/265#issuecomment-580549772:979,Testability,test,test,979,"ing back to me! Any suggestions would be great. . 1. Your OS version.; NAME=""CentOS Linux""; VERSION=""7 (Core)""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""7""; PRETTY_NAME=""CentOS Linux 7 (Core)""; ANSI_COLOR=""0;31"". 2. Your Singularity version.; singularity version 3.4.1-1.2.el7; 3. Your numpy version (I'm not sure whether this affects singularity); 1.17.5; 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU?. I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`; # Pull Singularity images; INPUT_DIR='singularity'; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images""; # Non-gpu image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg; # GPU image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772
https://github.com/google/deepvariant/issues/265#issuecomment-580549772:1687,Testability,Test,Test,1687,"TP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images""; # Non-gpu image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg; # GPU image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors: ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772
https://github.com/google/deepvariant/issues/265#issuecomment-580549772:1734,Testability,test,test,1734,"age; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg; # GPU image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors: ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772
https://github.com/google/deepvariant/issues/265#issuecomment-580549772:3678,Testability,Test,Test,3678,"nsorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): ; `# Pull the deep variant docker image.; sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; ​. # Test running docker interactively w/Singularity. ; BIN_VERSION=""0.9.0""; singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; `; I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772
https://github.com/google/deepvariant/issues/265#issuecomment-580549772:3798,Testability,test,test,3798,"nsorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): ; `# Pull the deep variant docker image.; sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; ​. # Test running docker interactively w/Singularity. ; BIN_VERSION=""0.9.0""; singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; `; I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772
https://github.com/google/deepvariant/issues/265#issuecomment-580549772:4171,Testability,test,test,4171,"nsorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): ; `# Pull the deep variant docker image.; sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; ​. # Test running docker interactively w/Singularity. ; BIN_VERSION=""0.9.0""; singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; `; I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772
https://github.com/google/deepvariant/issues/265#issuecomment-581773805:111,Deployability,release,release,111,"Hi @ksw9 ,; It seems like you're using Numpy 1.17.5, which requires Python 3.5-3.8:; https://numpy.org/devdocs/release/1.17.5-notes.html; Currently, DeepVariant is still supporting only Python2. We plan to make a release with Python3 support in Q2. If you're running with docker, I don't expect that use case to be affected though. If you're seeing issues with running strictly with docker, let me know. I'm going to close this issue for now, given that the issue (Python2 vs 3) is a known issue that we'll address in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265#issuecomment-581773805
https://github.com/google/deepvariant/issues/265#issuecomment-581773805:213,Deployability,release,release,213,"Hi @ksw9 ,; It seems like you're using Numpy 1.17.5, which requires Python 3.5-3.8:; https://numpy.org/devdocs/release/1.17.5-notes.html; Currently, DeepVariant is still supporting only Python2. We plan to make a release with Python3 support in Q2. If you're running with docker, I don't expect that use case to be affected though. If you're seeing issues with running strictly with docker, let me know. I'm going to close this issue for now, given that the issue (Python2 vs 3) is a known issue that we'll address in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265#issuecomment-581773805
https://github.com/google/deepvariant/issues/265#issuecomment-581773805:527,Deployability,release,release,527,"Hi @ksw9 ,; It seems like you're using Numpy 1.17.5, which requires Python 3.5-3.8:; https://numpy.org/devdocs/release/1.17.5-notes.html; Currently, DeepVariant is still supporting only Python2. We plan to make a release with Python3 support in Q2. If you're running with docker, I don't expect that use case to be affected though. If you're seeing issues with running strictly with docker, let me know. I'm going to close this issue for now, given that the issue (Python2 vs 3) is a known issue that we'll address in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/265#issuecomment-581773805
https://github.com/google/deepvariant/issues/266#issuecomment-580713806:1488,Availability,avail,available,1488,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266#issuecomment-580713806
https://github.com/google/deepvariant/issues/266#issuecomment-580713806:556,Modifiability,enhance,enhanced,556,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266#issuecomment-580713806
https://github.com/google/deepvariant/issues/266#issuecomment-580713806:158,Usability,learn,learned,158,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266#issuecomment-580713806
https://github.com/google/deepvariant/issues/266#issuecomment-580767025:800,Availability,error,errors,800,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). ; The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266#issuecomment-580767025
https://github.com/google/deepvariant/issues/266#issuecomment-580767025:970,Usability,simpl,simply,970,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). ; The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/266#issuecomment-580767025
https://github.com/google/deepvariant/issues/267#issuecomment-581533399:222,Modifiability,flexible,flexible,222,"The vcf_stats_report script can only use the data that is in the VCF file. Since this is built for DeepVariant VCF files, other callers may not include all the same metrics in their output VCFs. We made sure the script is flexible enough to finish creating a report despite this missing info, but the charts cannot be made if the data isn't there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/267#issuecomment-581533399
https://github.com/google/deepvariant/issues/268#issuecomment-586408803:817,Performance,load,loading,817,"Hi @aderzelle ,; the current design is that you do need to specify both the `model_type` and `customized_model` flag.; The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```; --model_type=WGS \; --customized_model=/input/your.model.ckpt \; ```. (or WES, they should be the same); Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268#issuecomment-586408803
https://github.com/google/deepvariant/issues/268#issuecomment-586584341:2304,Availability,checkpoint,checkpoint,2304,"on()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s; user	0m9.233s; sys	0m4.817s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the direc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268#issuecomment-586584341
https://github.com/google/deepvariant/issues/268#issuecomment-586584341:3187,Availability,checkpoint,checkpoint,3187,"ine 1268, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s; user	0m9.233s; sys	0m4.817s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```; #!/usr/bin/zsh; OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model""; mkdir -p ""${OUTPUT_DIR}""; INPUT_DIR=""${PWD}""; BIN_VERSION=""0.9.0""; N_SHARDS=20; LOG_DIR=""${OUTPUT_DIR}/logs"" ; mkdir -p ""${LOG_DIR}"" ; #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3); #for SAMPLE in ""${decade[@]}""; #do; # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz; #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268#issuecomment-586584341
https://github.com/google/deepvariant/issues/268#issuecomment-586584341:1925,Modifiability,config,config,1925,"riod_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s; user	0m9.233s; sys	0m4.817s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise Ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268#issuecomment-586584341
https://github.com/google/deepvariant/issues/268#issuecomment-586584341:1932,Modifiability,config,config,1932,"riod_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s; user	0m9.233s; sys	0m4.817s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise Ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268#issuecomment-586584341
https://github.com/google/deepvariant/issues/268#issuecomment-586584341:593,Safety,predict,prediction,593,"Hello, I tried but this returns ; ```. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268#issuecomment-586584341
https://github.com/google/deepvariant/issues/268#issuecomment-586584341:611,Safety,predict,predictions,611,"Hello, I tried but this returns ; ```. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268#issuecomment-586584341
https://github.com/google/deepvariant/issues/268#issuecomment-586584341:736,Safety,predict,predict,736,"Hello, I tried but this returns ; ```. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268#issuecomment-586584341
https://github.com/google/deepvariant/issues/268#issuecomment-586584341:3736,Testability,log,logs,3736,"bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```; #!/usr/bin/zsh; OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model""; mkdir -p ""${OUTPUT_DIR}""; INPUT_DIR=""${PWD}""; BIN_VERSION=""0.9.0""; N_SHARDS=20; LOG_DIR=""${OUTPUT_DIR}/logs"" ; mkdir -p ""${LOG_DIR}"" ; #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3); #for SAMPLE in ""${decade[@]}""; #do; # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz; #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log""; #done; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268#issuecomment-586584341
https://github.com/google/deepvariant/issues/268#issuecomment-586584341:4437,Testability,log,log,4437,"bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```; #!/usr/bin/zsh; OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model""; mkdir -p ""${OUTPUT_DIR}""; INPUT_DIR=""${PWD}""; BIN_VERSION=""0.9.0""; N_SHARDS=20; LOG_DIR=""${OUTPUT_DIR}/logs"" ; mkdir -p ""${LOG_DIR}"" ; #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3); #for SAMPLE in ""${decade[@]}""; #do; # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz; #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log""; #done; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/268#issuecomment-586584341
https://github.com/google/deepvariant/issues/270#issuecomment-587577840:24,Availability,error,error,24,Hi @situssog ; From the error message it seems like there's something wrong with the base quality scores in your BAM. Is there more information you can provide? Are you able to use the same BAM file with other tools that reads the base quality scores?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270#issuecomment-587577840
https://github.com/google/deepvariant/issues/270#issuecomment-587577840:30,Integrability,message,message,30,Hi @situssog ; From the error message it seems like there's something wrong with the base quality scores in your BAM. Is there more information you can provide? Are you able to use the same BAM file with other tools that reads the base quality scores?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270#issuecomment-587577840
https://github.com/google/deepvariant/issues/270#issuecomment-587936807:595,Safety,predict,predict,595,"Hi @situssog . To expand on Pi-Chuan's answer - DeepVariant will expect quality scores to be present in the BAM file (and to have the same length as the bases for each read). In order to run DeepVariant, you would need to add QUAL scores of all one value. It is unclear how DeepVariant's accuracy and behavior in variant calling would change as a result. . I believe that a value of ""!"" corresponds to the lowest base qual value. DeepVariant may be conservative in calling reads if Base Qualities are given that as the confidence. A value like ""@"" would be more balanced, but it is difficult to predict how DeepVariant will behave either way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270#issuecomment-587936807
https://github.com/google/deepvariant/issues/270#issuecomment-593729219:193,Usability,simpl,simply,193,"Hi @situssog ; Is there a specific reason why you have a fastA file and not a fastQ?; PacBio's tool (https://github.com/PacificBiosciences/bam2fastx) can generate either, so we would recommend simply generating the fastQ file to preserve the base quality scores, so then BWA-MEM, DeepVariant, and any other tools can use them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270#issuecomment-593729219
https://github.com/google/deepvariant/issues/270#issuecomment-594228762:21,Deployability,update,update,21,"Okay, thanks for the update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/270#issuecomment-594228762
https://github.com/google/deepvariant/issues/271#issuecomment-586402168:586,Modifiability,Config,ConfigProto,586,"Hi @aderzelle ; This is not unexpected:; the relationship between n_shards and how many processes is not 1:1 in the `call_variants` step. In `call_variants` step, the parallelism is decided by TensorFlow, which can be controlled by the `config_string` flag of call_variants. If you're using the run_deepvariant script, you should be able to add a flag like this:; `--call_variants_extra_args config_string=""gpu_options: {per_process_gpu_memory_fraction: 0.5}""` to specify your config_string. I think this is the options you have: https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConfigProto; (I can't find a better documentation, though. @gunjanbaid do you know?). @aderzelle - Let me know if this works. I usually don't specify this because the default setting works well for my own use case. It'll be great if you share what your use case is, and let us know whether you're successful by setting this flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271#issuecomment-586402168
https://github.com/google/deepvariant/issues/271#issuecomment-586441972:98,Modifiability,config,config,98,Hi @aderzelle ; The flag option I gave was just an example.; Maybe use_per_session_threads in the config proto can be relevant. I haven't used this option before so I can't be completely sure. ; Are you trying to limit to just one thread?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271#issuecomment-586441972
https://github.com/google/deepvariant/issues/271#issuecomment-586523576:59,Modifiability,config,config,59,"Got it. Thanks for the context! If you end up tweaking the config, let me know whether it works for you or not.; I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/271#issuecomment-586523576
https://github.com/google/deepvariant/issues/272#issuecomment-586520046:347,Integrability,depend,depends,347,"I ran make_examples with dad.sam.gz and recorded output of the realigner (upper part is original BAM and lower part show realigned reads). You are right Brent, realinged reads are mapped to support 1 del and 1 ins. But in the end the haplotype would be the same with 1 DEL and 1 INS or 3 SNPs, it is just a different representation. This behavior depends on penalty scores that we use for Smith–Waterman alignment. . ![image](https://user-images.githubusercontent.com/1168691/74576052-33921600-4f3e-11ea-8aef-1e354bf2b466.png)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-586520046
https://github.com/google/deepvariant/issues/272#issuecomment-586545236:296,Availability,down,downside,296,"hi, thanks for the replies. so `--make_examples_extra_args ""ws_use_window_selector_model=false""` (since I am using `run_deepvariant`) will accomplish the same as what @akolesnikov suggests with `--nows_use_window_selector_model` ?. and I don't see much documentation on that option, is there any downside to that? (I assume it just affects run time?)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-586545236
https://github.com/google/deepvariant/issues/272#issuecomment-586546504:765,Availability,down,downside,765,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. ; In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`.; We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:; https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-586546504
https://github.com/google/deepvariant/issues/272#issuecomment-586546504:1117,Deployability,release,released,1117,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. ; In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`.; We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:; https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-586546504
https://github.com/google/deepvariant/issues/272#issuecomment-586546504:1174,Deployability,release,releases,1174,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. ; In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`.; We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:; https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-586546504
https://github.com/google/deepvariant/issues/272#issuecomment-586546504:46,Integrability,wrap,wrapper,46,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. ; In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`.; We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:; https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-586546504
https://github.com/google/deepvariant/issues/272#issuecomment-586546504:122,Modifiability,flexible,flexible,122,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. ; In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`.; We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:; https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-586546504
https://github.com/google/deepvariant/issues/272#issuecomment-587538179:515,Deployability,release,released,515,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`; does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. ; A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-587538179
https://github.com/google/deepvariant/issues/272#issuecomment-587538179:129,Energy Efficiency,reduce,reduce,129,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`; does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. ; A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-587538179
https://github.com/google/deepvariant/issues/272#issuecomment-588045756:496,Deployability,release,release,496,"Hi @brentp . This is a very good question, but I am not sure I have a good answer for you. For you and @kokyriakidis the fact that window selector can cause inconsistency across a pedigree was not something we previously appreciated when considering the trade-offs between speed and accuracy. In the intermediate future, we hope to profile performance of make_examples to improve speed and at that time we will revisit the decision to enable this by default. This will not occur for the upcoming release, but possibly the following one. It is possible to create a BED file covering de novo sites and force window selector=false across all of them, this should be fast and it could be an exercise we would want to do on the 1KG pVCF. . We are also working on some more involved methods for calling in a trio, but that is also in the intermediate timescale.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-588045756
https://github.com/google/deepvariant/issues/272#issuecomment-588045756:340,Performance,perform,performance,340,"Hi @brentp . This is a very good question, but I am not sure I have a good answer for you. For you and @kokyriakidis the fact that window selector can cause inconsistency across a pedigree was not something we previously appreciated when considering the trade-offs between speed and accuracy. In the intermediate future, we hope to profile performance of make_examples to improve speed and at that time we will revisit the decision to enable this by default. This will not occur for the upcoming release, but possibly the following one. It is possible to create a BED file covering de novo sites and force window selector=false across all of them, this should be fast and it could be an exercise we would want to do on the 1KG pVCF. . We are also working on some more involved methods for calling in a trio, but that is also in the intermediate timescale.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-588045756
https://github.com/google/deepvariant/issues/272#issuecomment-592194090:697,Usability,simpl,simpler,697,"I can open a separate issue if it's helpful, but just a couple more things related to this... ; First, while the haplotype stuff like in the images above is mostly gone with `ws_use_window_selector_model=false`, I still see the problem in some false positive calls. Another thing that happens with things that DV calls de novos but obviously are not is that the kid will just meet some threshold and have a number of MQ ~40 reads with the de novo, where as the parent will have a number of reads with the allele that are MQ ~18 or lower. But the VCF reports AD[1] == 0 for many of these in the parent. If the count of low-quality alleles were reported in the sample fields in the VCF, it would be simpler to filter to make sure the allele was absent from the parent.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-592194090
https://github.com/google/deepvariant/issues/272#issuecomment-605207780:69,Deployability,release,released,69,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:; - Updated to Python3 and TensorFlow2; - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 ; If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-605207780
https://github.com/google/deepvariant/issues/272#issuecomment-605207780:255,Deployability,Update,Updated,255,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:; - Updated to Python3 and TensorFlow2; - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 ; If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-605207780
https://github.com/google/deepvariant/issues/272#issuecomment-605207780:378,Deployability,release,release,378,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:; - Updated to Python3 and TensorFlow2; - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 ; If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-605207780
https://github.com/google/deepvariant/issues/272#issuecomment-605207780:431,Deployability,release,releases,431,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:; - Updated to Python3 and TensorFlow2; - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 ; If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-605207780
https://github.com/google/deepvariant/issues/272#issuecomment-605207780:470,Usability,feedback,feedback,470,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:; - Updated to Python3 and TensorFlow2; - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 ; If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/272#issuecomment-605207780
https://github.com/google/deepvariant/issues/274#issuecomment-597715853:118,Availability,error,error,118,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-597715853
https://github.com/google/deepvariant/issues/274#issuecomment-597715853:1033,Availability,avail,available,1033,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-597715853
https://github.com/google/deepvariant/issues/274#issuecomment-597715853:457,Modifiability,variab,variable,457,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-597715853
https://github.com/google/deepvariant/issues/274#issuecomment-597715853:14,Testability,test,testing,14,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-597715853
https://github.com/google/deepvariant/issues/274#issuecomment-597978325:231,Testability,test,test,231,"Thanks @chrisfleisch for following up on this issue.; If I understand correctly, you're also talking specifically about the `call_variants` step, not the `make_examples` step, right?. I can try to see if I can get a AMD machine to test it out. I actually have not made any progress on this issue yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-597978325
https://github.com/google/deepvariant/issues/274#issuecomment-598179709:186,Availability,error,error,186,"I ran `/opt/deepvariant/bin/run_deepvariant` and I believe it ran out of threads during the `make_examples` step when I tried to use all the cores on the AMD machine. Here's more of the error. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable; > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > Traceback (most recent call last):; > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; > import numpy as np; > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>; > from . import core; > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>; > raise ImportError(msg); > ImportError:; > ; > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!; > ; > Importing the multiarray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
https://github.com/google/deepvariant/issues/274#issuecomment-598179709:2046,Availability,error,error,2046,"rray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; > ""__main__"", fname, loader, pkg_name); > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; > exec code in run_globals; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main; > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call; > return Popen(*popenargs, **kwargs).wait(); > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__; > errread, errwrite); > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child; > self.pid = os.fork(); > OSError: [Errno 11] Re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
https://github.com/google/deepvariant/issues/274#issuecomment-598179709:2179,Availability,error,error,2179," - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; > ""__main__"", fname, loader, pkg_name); > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; > exec code in run_globals; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main; > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call; > return Popen(*popenargs, **kwargs).wait(); > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__; > errread, errwrite); > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child; > self.pid = os.fork(); > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'; > ; > real 19m19.271s; > user 1084m5.580s; > sys 17m12.750s; > Traceback (most recent call last):; > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; > app.run(main); > File ""/usr/local/lib/python2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
https://github.com/google/deepvariant/issues/274#issuecomment-598179709:1303,Deployability,install,installed,1303,"eate failed for thread 63 of 64: Resource temporarily unavailable; > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > Traceback (most recent call last):; > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; > import numpy as np; > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>; > from . import core; > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>; > raise ImportError(msg); > ImportError:; > ; > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!; > ; > Importing the multiarray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
https://github.com/google/deepvariant/issues/274#issuecomment-598179709:1322,Deployability,install,installation,1322,"eate failed for thread 63 of 64: Resource temporarily unavailable; > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > Traceback (most recent call last):; > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; > import numpy as np; > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>; > from . import core; > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>; > raise ImportError(msg); > ImportError:; > ; > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!; > ; > Importing the multiarray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
https://github.com/google/deepvariant/issues/274#issuecomment-598179709:1810,Deployability,install,installed,1810,"rray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; > ""__main__"", fname, loader, pkg_name); > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; > exec code in run_globals; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main; > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call; > return Popen(*popenargs, **kwargs).wait(); > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__; > errread, errwrite); > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child; > self.pid = os.fork(); > OSError: [Errno 11] Re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
https://github.com/google/deepvariant/issues/274#issuecomment-598179709:1840,Deployability,install,installed,1840,"rray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; > ""__main__"", fname, loader, pkg_name); > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; > exec code in run_globals; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main; > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call; > return Popen(*popenargs, **kwargs).wait(); > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__; > errread, errwrite); > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child; > self.pid = os.fork(); > OSError: [Errno 11] Re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
https://github.com/google/deepvariant/issues/274#issuecomment-598179709:1940,Deployability,install,installed,1940,"rray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; > ""__main__"", fname, loader, pkg_name); > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; > exec code in run_globals; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main; > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call; > return Popen(*popenargs, **kwargs).wait(); > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__; > errread, errwrite); > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child; > self.pid = os.fork(); > OSError: [Errno 11] Re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
https://github.com/google/deepvariant/issues/274#issuecomment-598179709:2378,Performance,load,loader,2378,"; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; > ""__main__"", fname, loader, pkg_name); > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; > exec code in run_globals; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main; > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call; > return Popen(*popenargs, **kwargs).wait(); > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__; > errread, errwrite); > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child; > self.pid = os.fork(); > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'; > ; > real 19m19.271s; > user 1084m5.580s; > sys 17m12.750s; > Traceback (most recent call last):; > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; > app.run(main); > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; > _run_main(main, args); > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
https://github.com/google/deepvariant/issues/274#issuecomment-598179709:2024,Testability,log,log,2024,"rray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; > ""__main__"", fname, loader, pkg_name); > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; > exec code in run_globals; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main; > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call; > return Popen(*popenargs, **kwargs).wait(); > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__; > errread, errwrite); > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child; > self.pid = os.fork(); > OSError: [Errno 11] Re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
https://github.com/google/deepvariant/issues/274#issuecomment-598179709:1250,Usability,simpl,simply,1250,"eate failed for thread 63 of 64: Resource temporarily unavailable; > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > Traceback (most recent call last):; > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; > import numpy as np; > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>; > from . import core; > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>; > raise ImportError(msg); > ImportError:; > ; > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!; > ; > Importing the multiarray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709
https://github.com/google/deepvariant/issues/274#issuecomment-599870096:252,Availability,error,error,252,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step.; I'll post what I did below (which didn't reproduce your error).; But, maybe this Stackoverflow issue could be related?; https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable; Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you?. ---. For completeness, here is what I tried:. I got a AMD machine:. ```; gcloud compute instances create ""${USER}-amd"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \; --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \; --zone ""europe-west4-b""; ```. On that machine, I ran:; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 16; On-line CPU(s) list: 0-15; Thread(s) per core: 2; Core(s) per socket: 8; Socket(s): 1; NUMA node(s): 1; Vendor ID: AuthenticAMD; CPU family: 23; Model: 49; Model name: AMD EPYC 7B12; Stepping: 0; CPU MHz: 2249.998; BogoMIPS: 4499.99; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 512K; L3 cache: 16384K; NUMA node0 CPU(s): 0-15; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid; ```. Then, I ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-599870096
https://github.com/google/deepvariant/issues/274#issuecomment-599870096:1313,Performance,cache,cache,1313,"ry the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you?. ---. For completeness, here is what I tried:. I got a AMD machine:. ```; gcloud compute instances create ""${USER}-amd"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \; --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \; --zone ""europe-west4-b""; ```. On that machine, I ran:; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 16; On-line CPU(s) list: 0-15; Thread(s) per core: 2; Core(s) per socket: 8; Socket(s): 1; NUMA node(s): 1; Vendor ID: AuthenticAMD; CPU family: 23; Model: 49; Model name: AMD EPYC 7B12; Stepping: 0; CPU MHz: 2249.998; BogoMIPS: 4499.99; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 512K; L3 cache: 16384K; NUMA node0 CPU(s): 0-15; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid; ```. Then, I just directly run the WES case study with docker:; ```; $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x ; ```. Once make_examples started running, I used “top” to look at the jobs:; ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take close to 100% CPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-599870096
https://github.com/google/deepvariant/issues/274#issuecomment-599870096:1329,Performance,cache,cache,1329,"ry the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you?. ---. For completeness, here is what I tried:. I got a AMD machine:. ```; gcloud compute instances create ""${USER}-amd"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \; --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \; --zone ""europe-west4-b""; ```. On that machine, I ran:; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 16; On-line CPU(s) list: 0-15; Thread(s) per core: 2; Core(s) per socket: 8; Socket(s): 1; NUMA node(s): 1; Vendor ID: AuthenticAMD; CPU family: 23; Model: 49; Model name: AMD EPYC 7B12; Stepping: 0; CPU MHz: 2249.998; BogoMIPS: 4499.99; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 512K; L3 cache: 16384K; NUMA node0 CPU(s): 0-15; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid; ```. Then, I just directly run the WES case study with docker:; ```; $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x ; ```. Once make_examples started running, I used “top” to look at the jobs:; ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take close to 100% CPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-599870096
https://github.com/google/deepvariant/issues/274#issuecomment-599870096:1344,Performance,cache,cache,1344,"ry the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you?. ---. For completeness, here is what I tried:. I got a AMD machine:. ```; gcloud compute instances create ""${USER}-amd"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \; --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \; --zone ""europe-west4-b""; ```. On that machine, I ran:; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 16; On-line CPU(s) list: 0-15; Thread(s) per core: 2; Core(s) per socket: 8; Socket(s): 1; NUMA node(s): 1; Vendor ID: AuthenticAMD; CPU family: 23; Model: 49; Model name: AMD EPYC 7B12; Stepping: 0; CPU MHz: 2249.998; BogoMIPS: 4499.99; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 512K; L3 cache: 16384K; NUMA node0 CPU(s): 0-15; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid; ```. Then, I just directly run the WES case study with docker:; ```; $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x ; ```. Once make_examples started running, I used “top” to look at the jobs:; ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take close to 100% CPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-599870096
https://github.com/google/deepvariant/issues/274#issuecomment-599870096:1360,Performance,cache,cache,1360,"ry the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you?. ---. For completeness, here is what I tried:. I got a AMD machine:. ```; gcloud compute instances create ""${USER}-amd"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \; --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \; --zone ""europe-west4-b""; ```. On that machine, I ran:; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 16; On-line CPU(s) list: 0-15; Thread(s) per core: 2; Core(s) per socket: 8; Socket(s): 1; NUMA node(s): 1; Vendor ID: AuthenticAMD; CPU family: 23; Model: 49; Model name: AMD EPYC 7B12; Stepping: 0; CPU MHz: 2249.998; BogoMIPS: 4499.99; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 512K; L3 cache: 16384K; NUMA node0 CPU(s): 0-15; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid; ```. Then, I just directly run the WES case study with docker:; ```; $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x ; ```. Once make_examples started running, I used “top” to look at the jobs:; ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take close to 100% CPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-599870096
https://github.com/google/deepvariant/issues/274#issuecomment-599870096:120,Testability,test,test,120,"Hi @chrisfleisch , sorry it took me a while to find some time to try this. I just got an AMD machine on Google Cloud to test. But, I'm unable to reproduce your issue in the `make_examples` step.; I'll post what I did below (which didn't reproduce your error).; But, maybe this Stackoverflow issue could be related?; https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable; Specifically, can you try the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you?. ---. For completeness, here is what I tried:. I got a AMD machine:. ```; gcloud compute instances create ""${USER}-amd"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \; --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \; --zone ""europe-west4-b""; ```. On that machine, I ran:; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 16; On-line CPU(s) list: 0-15; Thread(s) per core: 2; Core(s) per socket: 8; Socket(s): 1; NUMA node(s): 1; Vendor ID: AuthenticAMD; CPU family: 23; Model: 49; Model name: AMD EPYC 7B12; Stepping: 0; CPU MHz: 2249.998; BogoMIPS: 4499.99; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 512K; L3 cache: 16384K; NUMA node0 CPU(s): 0-15; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid; ```. Then, I ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-599870096
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:3000,Availability,down,down,3000,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:1093,Energy Efficiency,monitor,monitor,1093,"tance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 1; > Vendor ID: GenuineIntel; > CPU family: 6; > Model: 85; > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz; > Stepping: 4; > CPU MHz: 1520.299; > BogoMIPS: 4999.99; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 32K; > L2 cache: 1024K; > L3 cache: 33792K; > NUMA node0 CPU(s): 0-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:126,Performance,perform,performance,126,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 1; > Vendor ID: GenuineIntel; > CPU family: 6; > Model: 85; > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz; > Stepping: 4; > CPU MHz: 1520.299; > BogoMIPS: 4999.99; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 32K; > L2 cache: 1024K; > L3 cache: 33792K; > NUMA node0 CPU(s): 0-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:747,Performance,cache,cache,747,"tance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 1; > Vendor ID: GenuineIntel; > CPU family: 6; > Model: 85; > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz; > Stepping: 4; > CPU MHz: 1520.299; > BogoMIPS: 4999.99; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 32K; > L2 cache: 1024K; > L3 cache: 33792K; > NUMA node0 CPU(s): 0-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:765,Performance,cache,cache,765,"tance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 1; > Vendor ID: GenuineIntel; > CPU family: 6; > Model: 85; > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz; > Stepping: 4; > CPU MHz: 1520.299; > BogoMIPS: 4999.99; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 32K; > L2 cache: 1024K; > L3 cache: 33792K; > NUMA node0 CPU(s): 0-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:782,Performance,cache,cache,782,"tance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 1; > Vendor ID: GenuineIntel; > CPU family: 6; > Model: 85; > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz; > Stepping: 4; > CPU MHz: 1520.299; > BogoMIPS: 4999.99; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 32K; > L2 cache: 1024K; > L3 cache: 33792K; > NUMA node0 CPU(s): 0-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:801,Performance,cache,cache,801,"tance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 1; > Vendor ID: GenuineIntel; > CPU family: 6; > Model: 85; > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz; > Stepping: 4; > CPU MHz: 1520.299; > BogoMIPS: 4999.99; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 32K; > L2 cache: 1024K; > L3 cache: 33792K; > NUMA node0 CPU(s): 0-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:1884,Performance,cache,cache,1884,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:1902,Performance,cache,cache,1902,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:1919,Performance,cache,cache,1919,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:1937,Performance,cache,cache,1937,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:2688,Performance,load,load,2688,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:2721,Performance,load,load,2721,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:2848,Performance,load,load,2848,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:2881,Performance,load,load,2881,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-603439134:192,Testability,test,testing,192,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 1; > Vendor ID: GenuineIntel; > CPU family: 6; > Model: 85; > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz; > Stepping: 4; > CPU MHz: 1520.299; > BogoMIPS: 4999.99; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 32K; > L2 cache: 1024K; > L3 cache: 33792K; > NUMA node0 CPU(s): 0-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134
https://github.com/google/deepvariant/issues/274#issuecomment-604195181:295,Energy Efficiency,power,power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-,295,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-604195181
https://github.com/google/deepvariant/issues/274#issuecomment-604195181:107,Performance,perform,performance,107,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-604195181
https://github.com/google/deepvariant/issues/274#issuecomment-604195181:162,Performance,perform,performance,162,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-604195181
https://github.com/google/deepvariant/issues/274#issuecomment-604195181:377,Performance,optimiz,optimizations,377,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-604195181
https://github.com/google/deepvariant/issues/274#issuecomment-604195181:486,Performance,optimiz,optimization,486,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/274#issuecomment-604195181
https://github.com/google/deepvariant/issues/275#issuecomment-587751831:27,Modifiability,extend,extend,27,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/275#issuecomment-587751831
https://github.com/google/deepvariant/issues/277#issuecomment-589689487:435,Deployability,continuous,continuous,435,"Hi @archanaraja . DeepVariant can run on PacBio HiFi (CCS) data (see this paper for details: https://www.nature.com/articles/s41587-019-0217-9 or biorxiv: https://www.biorxiv.org/content/10.1101/519025v1). To do this, you would run with the --model_type=PACBIO option (see the [Quickstart](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md). However, the current version of DeepVariant does not work with continuous long reads (CLR). . PacBio recommends 15x coverage of CCS data in their best practices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277#issuecomment-589689487
https://github.com/google/deepvariant/issues/277#issuecomment-594834868:136,Availability,error,error,136,"No. Currently PacBio HiFi/CCS data is the only long-read sequencing type supported. ONT reads, including PromethIon, have a much higher error rate (same for PacBio CLR) and would therefore need special treatment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/277#issuecomment-594834868
https://github.com/google/deepvariant/issues/278#issuecomment-591476042:3374,Usability,clear,clearly,3374,".g.vcf.gz|grep -C 3 ""10764356"" . chromosome_1	10764353	.	C	T,<*>	27.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:27:162:100,62,0:0.382716,0:27,0,51,990,990,990; chromosome_1	10764354	.	A	<*>	0	.	END=10764354	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999; chromosome_1	10764355	.	C	T,<*>	26.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:162:100,62,0:0.382716,0:26,0,60,990,990,990; chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990; chromosome_1	10764357	.	T	<*>	0	.	END=10764357	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999; chromosome_1	10764358	.	T	C,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:172:98,63,0:0.366279,0:33,0,69,990,990,990; chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:169:0,300,2999. ```; To me in the 3 samples it's the same site that is present. . Here another one (where the SNP is RefCall in one sample and PASS in another). ```; zgrep -w ""chromosome_2"" output.g.vcf.gz|grep -C 2 ""9780248""; chromosome_2	9780195	.	T	<*>	0	.	END=9780244	GT:GQ:MIN_DP:PL	0/0:50:294:0,300,2999; chromosome_2	9780245	.	GGT	G,<*>	37.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:294:161,131,0:0.445578,0:37,0,41,990,990,990; chromosome_2	9780248	.	A	<*>	0	.	END=9780249	GT:GQ:MIN_DP:PL	0/0:50:163:0,270,2939; chromosome_2	9780250	.	T	TTG,<*>	36.8	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:32:298:161,133,0:0.446309,0:36,0,34,990,990,990; chromosome_2	9780251	.	A	<*>	0	.	END=9780281	GT:GQ:MIN_DP:PL	0/0:50:272:0,300,2999; ```; In this second case, I don't understand why DeepVariant did not even consider there might be a variant there, as the bam clearly shows many reads mapping in that position with a variant site (it's the middle T flanked by 2 homozygous T sites). ![example2](https://user-images.githubusercontent.com/23341393/75358443-2ddbef00-58b3-11ea-9170-dd996a53386b.png). Now I know that calling SNP (in the sense of single nucleotide) variation in the vicinity of more complex events is known to be tricky, therefore this might not be an issue with DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278#issuecomment-591476042
https://github.com/google/deepvariant/issues/278#issuecomment-591490219:24,Integrability,message,messages,24,"Sorry for the string of messages, but a bit of background: my samples controlH is a direct descendant of the ""output"" one, it is a clonal organism so I am assuming all variants private to the controlH are false calls. By investigating them visually (eyeballing the bam file) it seems they are indeed false positives. What is interesting is that they all seem to be in the direct vicinity of an indel or string of mismatches. Is it a known problem with DeepVariant, miscalls/ inconsistencies between samples, for sites around indels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278#issuecomment-591490219
https://github.com/google/deepvariant/issues/278#issuecomment-592330459:400,Availability,down,downsampling,400,"Hi @aderzelle, in the case of position 10764356, the allele fraction is a bit lower in the case of the RefCall. A few other implementation details that might cause the differences between samples: DeepVariant randomly samples the reads as the pileup images generated can accommodate at most 100 reads. In the case of high coverage regions, the observed allele fraction can change as a result of this downsampling. Sampling for a particular sample is deterministic, but may happen differently across samples. Another source of difference between the three samples might be caused the realigner. DeepVariant runs a realignment that can be turned off by adding the flag `--norealign_reads` to the `make_examples` step. Turning the realigner off entirely will likely hurt overall accuracy, but for this example, it might be useful to see if that's affecting the results. Regions with many nearby variants do end up being challenging for the neural network to correctly classify. However, in the case of position 9780248, it is surprising that a candidate was not generated. Candidate generation should not be affected by the nearby variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/278#issuecomment-592330459
https://github.com/google/deepvariant/issues/279#issuecomment-591172632:171,Availability,down,down,171,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279#issuecomment-591172632
https://github.com/google/deepvariant/issues/279#issuecomment-591172632:397,Availability,down,down,397,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279#issuecomment-591172632
https://github.com/google/deepvariant/issues/279#issuecomment-591172632:415,Availability,error,errors,415,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279#issuecomment-591172632
https://github.com/google/deepvariant/issues/279#issuecomment-591172632:156,Testability,benchmark,benchmarks,156,"Hi @yassineS . Thank you for the question. In short, DeepVariant tends to call fewer variants as coverage drops (this is similar to other callers). We have benchmarks for down to 15x of Illumina WGS [at our ""20 is the new 30 blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/). This also breaks down the types of errors by class as coverage falls. I am not sure what the lower bound of coverage for using DeepVariant. At some point, imputation approaches will be required instead of variant calling ones. I would guess this is somewhere around 5x-8x. . I am curious how low you consider low coverage, so I understand your use case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279#issuecomment-591172632
https://github.com/google/deepvariant/issues/279#issuecomment-591260702:850,Availability,down,down,850,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes; at coverages between 0.1-1X. So data missingness is a reality we know how; to deal with, I just want to make sure that the calls we are making are; accurate. The second family of data I am working with are indigenous groups that are; at least diverged from the reference over 2000 generations ago. We observe; a strong bias towards the reference allele using GATK, but it'll be very; interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>; wrote:. > Hi @yassineS <https://github.com/yassineS>; >; > Thank you for the question. In short, DeepVariant tends to call fewer; > variants as coverage drops (this is similar to other callers). We have; > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog; > post; > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>.; > This also breaks down the types of errors by class as coverage falls.; >; > I am not sure what the lower bound of coverage for using DeepVariant. At; > some point, imputation approaches will be required instead of variant; > calling ones. I would guess this is somewhere around 5x-8x.; >; > I am curious how low you consider low coverage, so I understand your use; > case.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279#issuecomment-591260702
https://github.com/google/deepvariant/issues/279#issuecomment-591260702:1084,Availability,down,down,1084,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes; at coverages between 0.1-1X. So data missingness is a reality we know how; to deal with, I just want to make sure that the calls we are making are; accurate. The second family of data I am working with are indigenous groups that are; at least diverged from the reference over 2000 generations ago. We observe; a strong bias towards the reference allele using GATK, but it'll be very; interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>; wrote:. > Hi @yassineS <https://github.com/yassineS>; >; > Thank you for the question. In short, DeepVariant tends to call fewer; > variants as coverage drops (this is similar to other callers). We have; > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog; > post; > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>.; > This also breaks down the types of errors by class as coverage falls.; >; > I am not sure what the lower bound of coverage for using DeepVariant. At; > some point, imputation approaches will be required instead of variant; > calling ones. I would guess this is somewhere around 5x-8x.; >; > I am curious how low you consider low coverage, so I understand your use; > case.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279#issuecomment-591260702
https://github.com/google/deepvariant/issues/279#issuecomment-591260702:1102,Availability,error,errors,1102,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes; at coverages between 0.1-1X. So data missingness is a reality we know how; to deal with, I just want to make sure that the calls we are making are; accurate. The second family of data I am working with are indigenous groups that are; at least diverged from the reference over 2000 generations ago. We observe; a strong bias towards the reference allele using GATK, but it'll be very; interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>; wrote:. > Hi @yassineS <https://github.com/yassineS>; >; > Thank you for the question. In short, DeepVariant tends to call fewer; > variants as coverage drops (this is similar to other callers). We have; > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog; > post; > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>.; > This also breaks down the types of errors by class as coverage falls.; >; > I am not sure what the lower bound of coverage for using DeepVariant. At; > some point, imputation approaches will be required instead of variant; > calling ones. I would guess this is somewhere around 5x-8x.; >; > I am curious how low you consider low coverage, so I understand your use; > case.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279#issuecomment-591260702
https://github.com/google/deepvariant/issues/279#issuecomment-591260702:835,Testability,benchmark,benchmarks,835,"Hi Andy, thank you for your response. I am working with ancient DNA which in the vast majority of the cases comes; at coverages between 0.1-1X. So data missingness is a reality we know how; to deal with, I just want to make sure that the calls we are making are; accurate. The second family of data I am working with are indigenous groups that are; at least diverged from the reference over 2000 generations ago. We observe; a strong bias towards the reference allele using GATK, but it'll be very; interesting to see how DeepVariant behave in such cases. On Wed, 26 Feb 2020 at 11:17, Andrew Carroll <notifications@github.com>; wrote:. > Hi @yassineS <https://github.com/yassineS>; >; > Thank you for the question. In short, DeepVariant tends to call fewer; > variants as coverage drops (this is similar to other callers). We have; > benchmarks for down to 15x of Illumina WGS at our ""20 is the new 30 blog; > post; > <https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/>.; > This also breaks down the types of errors by class as coverage falls.; >; > I am not sure what the lower bound of coverage for using DeepVariant. At; > some point, imputation approaches will be required instead of variant; > calling ones. I would guess this is somewhere around 5x-8x.; >; > I am curious how low you consider low coverage, so I understand your use; > case.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/279?email_source=notifications&email_token=AANPQIK47CJ7LXIEDGA365DREW3YTA5CNFSM4K2FW2S2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEM6JIGA#issuecomment-591172632>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AANPQIIDTK3D5ON7PYU7BE3REW3YTANCNFSM4K2FW2SQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/279#issuecomment-591260702
https://github.com/google/deepvariant/issues/280#issuecomment-593558131:23,Performance,perform,performs,23,"@aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. . By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. `--emit_realigned_reads` - enables writing out of realigned reads; `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280#issuecomment-593558131
https://github.com/google/deepvariant/issues/280#issuecomment-593561282:32,Performance,perform,perform,32,"Thank you,. How exactly does it perform the realignment? How does it differ from, let's say, bwa mem?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280#issuecomment-593561282
https://github.com/google/deepvariant/issues/280#issuecomment-598767292:2028,Deployability,update,update,2028,"eads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data.; > ; > `--emit_realigned_reads` - enables writing out of realigned reads; > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```; docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics; ```. but runs into. ```; Traceback (most recent call last): ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> ; app.run(main) ; File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run ; _run_main(main, args) ; File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main ; sys.exit(main(argv)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main ; commands = create_all_commands() ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands ; sample_name=FLAGS.sample_name)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make_examples_command ; kwargs.update(_extra_args_to_dict(extra_args)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 135, in _extra_args_to_dict ; (flag_name, flag_value) = extra_arg.split('=') ; ValueError: need more than 1 value to unpack ; ```. EDIT: ah ok, the flag must explicitly be set to ""=true"" ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280#issuecomment-598767292
https://github.com/google/deepvariant/issues/280#issuecomment-598767292:768,Integrability,wrap,wrapper,768,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated.; > ; > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data.; > ; > `--emit_realigned_reads` - enables writing out of realigned reads; > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```; docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics; ```. but runs into. ```; Traceback (most recent call last): ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> ; app.run(main) ; File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run ; _run_main(main, args) ; File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main ; sys.exit(main(argv)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main ; commands = create_all_commands() ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands ; sample_name=FLAGS.sample_name)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280#issuecomment-598767292
https://github.com/google/deepvariant/issues/280#issuecomment-598767292:25,Performance,perform,performs,25,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated.; > ; > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data.; > ; > `--emit_realigned_reads` - enables writing out of realigned reads; > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```; docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics; ```. but runs into. ```; Traceback (most recent call last): ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> ; app.run(main) ; File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run ; _run_main(main, args) ; File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main ; sys.exit(main(argv)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main ; commands = create_all_commands() ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands ; sample_name=FLAGS.sample_name)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/280#issuecomment-598767292
https://github.com/google/deepvariant/issues/282#issuecomment-954067904:612,Availability,down,down,612,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written?. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282#issuecomment-954067904
https://github.com/google/deepvariant/issues/282#issuecomment-954067904:621,Availability,down,downstream,621,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written?. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282#issuecomment-954067904
https://github.com/google/deepvariant/issues/282#issuecomment-954067904:913,Deployability,configurat,configuration-and-analysis,913,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written?. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282#issuecomment-954067904
https://github.com/google/deepvariant/issues/282#issuecomment-954067904:913,Modifiability,config,configuration-and-analysis,913,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written?. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282#issuecomment-954067904
https://github.com/google/deepvariant/issues/282#issuecomment-954075521:197,Performance,concurren,concurrent,197,"p.s. My coworker let me know that the reason the first three records in your quoted gVCF are split into three rather than a single one is an implementation detail, where the genome is processed by concurrent processes operating on 1,000 bp chunks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282#issuecomment-954075521
https://github.com/google/deepvariant/issues/282#issuecomment-966408955:621,Availability,down,down,621,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position).; > ; > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/).; > ; > May I ask what is your use case that prefers every non-variant position to be written?; > ; > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,; David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282#issuecomment-966408955
https://github.com/google/deepvariant/issues/282#issuecomment-966408955:630,Availability,down,downstream,630,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position).; > ; > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/).; > ; > May I ask what is your use case that prefers every non-variant position to be written?; > ; > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,; David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282#issuecomment-966408955
https://github.com/google/deepvariant/issues/282#issuecomment-966408955:922,Deployability,configurat,configuration-and-analysis,922,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position).; > ; > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/).; > ; > May I ask what is your use case that prefers every non-variant position to be written?; > ; > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,; David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282#issuecomment-966408955
https://github.com/google/deepvariant/issues/282#issuecomment-966408955:922,Modifiability,config,configuration-and-analysis,922,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position).; > ; > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/).; > ; > May I ask what is your use case that prefers every non-variant position to be written?; > ; > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,; David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/282#issuecomment-966408955
https://github.com/google/deepvariant/issues/283#issuecomment-599859404:188,Energy Efficiency,adapt,adapted,188,"Hi @claudiologiudice ,; You can find the answers in this older thread: https://github.com/google/deepvariant/issues/115; Our answer would still be the same. I think some of our users have adapted DeepVariant to run on RNAseq data. If some of them would like to share here, that will be great. But we don't have an official solution right now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283#issuecomment-599859404
https://github.com/google/deepvariant/issues/283#issuecomment-599859404:188,Modifiability,adapt,adapted,188,"Hi @claudiologiudice ,; You can find the answers in this older thread: https://github.com/google/deepvariant/issues/115; Our answer would still be the same. I think some of our users have adapted DeepVariant to run on RNAseq data. If some of them would like to share here, that will be great. But we don't have an official solution right now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283#issuecomment-599859404
https://github.com/google/deepvariant/issues/283#issuecomment-1281193477:77,Deployability,release,released,77,"@claudiologiudice although this issue was closed some time ago, we have just released a new RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) for Illumina data. . Please take a look if you are still considering this and let us know if you have any feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283#issuecomment-1281193477
https://github.com/google/deepvariant/issues/283#issuecomment-1281193477:316,Usability,feedback,feedback,316,"@claudiologiudice although this issue was closed some time ago, we have just released a new RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) for Illumina data. . Please take a look if you are still considering this and let us know if you have any feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/283#issuecomment-1281193477
https://github.com/google/deepvariant/issues/285#issuecomment-602758544:47,Deployability,release,release,47,"Hi @matthdsm , we're actively working on a new release that will be using Python3.; We haven't updated out GitHub code, but you can find the Docker images on `google/deepvariant:0.10.0` already. If you have a chance to try it out, let us know if you encounter any issues. We'll have a release on GitHub soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/285#issuecomment-602758544
https://github.com/google/deepvariant/issues/285#issuecomment-602758544:95,Deployability,update,updated,95,"Hi @matthdsm , we're actively working on a new release that will be using Python3.; We haven't updated out GitHub code, but you can find the Docker images on `google/deepvariant:0.10.0` already. If you have a chance to try it out, let us know if you encounter any issues. We'll have a release on GitHub soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/285#issuecomment-602758544
https://github.com/google/deepvariant/issues/285#issuecomment-602758544:285,Deployability,release,release,285,"Hi @matthdsm , we're actively working on a new release that will be using Python3.; We haven't updated out GitHub code, but you can find the Docker images on `google/deepvariant:0.10.0` already. If you have a chance to try it out, let us know if you encounter any issues. We'll have a release on GitHub soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/285#issuecomment-602758544
https://github.com/google/deepvariant/issues/287#issuecomment-604129307:61,Deployability,release,release,61,"Hi @moldach ; Thank you for reporting this! For our upcoming release, I drafted a few new sections in the Quick Start already:; https://gist.github.com/pichuan/01f701573ba730e64354c534983f62c3#notes-on-singularity. I actually ended up just using `singularity pull`. (See the link above) Can you let me know if that works for you? If not, I can updated the doc to your version. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-604129307
https://github.com/google/deepvariant/issues/287#issuecomment-604129307:344,Deployability,update,updated,344,"Hi @moldach ; Thank you for reporting this! For our upcoming release, I drafted a few new sections in the Quick Start already:; https://gist.github.com/pichuan/01f701573ba730e64354c534983f62c3#notes-on-singularity. I actually ended up just using `singularity pull`. (See the link above) Can you let me know if that works for you? If not, I can updated the doc to your version. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-604129307
https://github.com/google/deepvariant/issues/287#issuecomment-604736676:53,Availability,error,error,53,"Pulling both the cpu/gpu versions works but I get an error when trying to run:. ```; [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO]**; FATAL: Unable to handle docker://google/deepvariant: uri: failed to get SHA of docker://google/deepvariant:: unable to parse image name docker://google/deepvariant:: invalid reference format; [moldach@cdr767 bin]$ --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; > --num_shards=1; bash: --ref=/ucsc.hg19.chr20.unittest.fasta: No such file or directory; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-604736676
https://github.com/google/deepvariant/issues/287#issuecomment-604760300:369,Modifiability,variab,variables,369,"Hi,; Thanks for trying the command. v0.10.0 is out today, and the Quick Start can be found here:; https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. First of all, clean up the example command a bit: Don't include the part ` **Replace this string with exactly one of the following [WGS,WES,PACBIO]**` in your command. And, the environment variables like ""${BIN_VERSION}"" and other variables will need to be set. Please refer to the documentation above to set it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-604760300
https://github.com/google/deepvariant/issues/287#issuecomment-604760300:411,Modifiability,variab,variables,411,"Hi,; Thanks for trying the command. v0.10.0 is out today, and the Quick Start can be found here:; https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. First of all, clean up the example command a bit: Don't include the part ` **Replace this string with exactly one of the following [WGS,WES,PACBIO]**` in your command. And, the environment variables like ""${BIN_VERSION}"" and other variables will need to be set. Please refer to the documentation above to set it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-604760300
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:88,Availability,error,error,88,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:; ## Set the environment; ```; [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0""; [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata""; [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output""; [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000; [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; > --num_shards=1; WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Paral",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:249,Availability,error,error,249,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:; ## Set the environment; ```; [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0""; [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata""; [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output""; [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000; [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; > --num_shards=1; WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Paral",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:4494,Availability,checkpoint,checkpoint,4494,"rence you passed in with --ref; 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728; I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]; I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants; I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s; user 0m5.478s; sys 0m3.350s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags.; 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz; 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:; 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version; 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_paralle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:14074,Availability,error,error,14074,"ine 1007, in main; FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \; File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer; exclude_header=exclude_header); File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__; writer_options); ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s; user 0m2.899s; sys 0m0.651s; I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1.; [moldach@cdr767 bin]$ ^C; [moldach@cdr767 bin]$ exit; exit; srun: error: cdr767: task 0: Exited with exit code 130; ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:2101,Energy Efficiency,Power,Power,2101,"E_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs; I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]; I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:74,Integrability,message,message,74,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:; ## Set the environment; ```; [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0""; [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata""; [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output""; [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000; [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; > --num_shards=1; WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Paral",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:26,Modifiability,variab,variables,26,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:; ## Set the environment; ```; [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0""; [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata""; [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output""; [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000; [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; > --num_shards=1; WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Paral",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:5822,Modifiability,config,config,5822,"tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:; 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version; 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance.; I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters; W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m; I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz; W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version.; Instructions for updati",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:7139,Modifiability,layers,layers,7139,"100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz; W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version.; Instructions for updating:; If using Keras pass *_constraint arguments to layers.; I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8; W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`.; I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48; W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:8896,Modifiability,layers,layers,8896,"45245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation.; I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn.; W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn.; I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized.; I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt; I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op.; I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p.; I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA...; I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt; I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]; I0327 13:32:39.551621 47138345245376",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:8903,Modifiability,layers,layers,8903,"45245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation.; I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn.; W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn.; I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized.; I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt; I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op.; I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p.; I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA...; I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt; I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]; I0327 13:32:39.551621 47138345245376",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:4725,Performance,optimiz,optimized,4725,"r; I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]; I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants; I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s; user 0m5.478s; sys 0m3.350s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags.; 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz; 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:; 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version; 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance.; I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters; W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m; I0327 13:32:13.751226 47138345245376 estimator.py:212] ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:4799,Performance,perform,performance,4799,"r; I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]; I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants; I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s; user 0m5.478s; sys 0m3.350s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags.; 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz; 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:; 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version; 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance.; I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters; W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m; I0327 13:32:13.751226 47138345245376 estimator.py:212] ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:5481,Performance,Tune,Tune,5481,"models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags.; 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz; 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:; 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version; 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance.; I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters; W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m; I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master':",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:5531,Performance,perform,performance,5531,"models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags.; 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz; 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:; 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version; 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance.; I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters; W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m; I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master':",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:8313,Performance,optimiz,optimizations,8313,"ogle_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`.; I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48; W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation.; I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn.; W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn.; I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized.; I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parame",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:381,Testability,test,testdata,381,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:; ## Set the environment; ```; [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0""; [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata""; [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output""; [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000; [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; > --num_shards=1; WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Paral",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:1611,Testability,test,testdata,1611,"ngularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; > --num_shards=1; WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:1694,Testability,test,testdata,1694," --model_type=WGS \; > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; > --num_shards=1; WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:2115,Testability,log,login,2115,"E_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs; I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]; I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:2647,Testability,test,testdata,2647,"r20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs; I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]; I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz; I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz; I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref; 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOC",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:2888,Testability,test,testdata,2888,",000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs; I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]; I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz; I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz; I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref; 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728; I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.389374 47175299967680 genomics_reader.py:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:3756,Testability,test,testdata,3756,"7680 make_examples.py:535] Preparing inputs; I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]; I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz; I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz; I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref; 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728; I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]; I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants; I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s; user 0m5.478s; sys 0m3.350s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with In",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:3920,Testability,test,testdata,3920,"S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]; I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz; I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz; I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref; 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728; I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]; I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants; I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s; user 0m5.478s; sys 0m3.350s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow wit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:10106,Testability,test,testdata,10106,"r updating:; Please use `layer.__call__` method instead.; I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn.; I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized.; I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt; I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op.; I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p.; I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA...; I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt; I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]; I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s; user 0m20.774s; sys 0m5.396s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz; 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86; 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86; 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls; 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls; I0327 13:32:43.6935",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-605306987:13660,Testability,test,testdata,13660,"ine 1007, in main; FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \; File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer; exclude_header=exclude_header); File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__; writer_options); ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s; user 0m2.899s; sys 0m0.651s; I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1.; [moldach@cdr767 bin]$ ^C; [moldach@cdr767 bin]$ exit; exit; srun: error: cdr767: task 0: Exited with exit code 130; ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987
https://github.com/google/deepvariant/issues/287#issuecomment-606340201:286,Availability,error,error,286,"Hi @moldach ; somehow your logs above have a lot of spaces in the middle, making it hard to read. It seems like the last step was looking for `/scratch/moldach/bin/quickstart-output/output.vcf.gz` but couldn't find it. . I'll walk through the steps again to see if I can reproduce your error, but meanwhile, can you try:. ```; singularity run -B $PWD,/usr/lib/locale/ \; ```. instead of . ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; ```. And see if it works for you? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-606340201
https://github.com/google/deepvariant/issues/287#issuecomment-606340201:27,Testability,log,logs,27,"Hi @moldach ; somehow your logs above have a lot of spaces in the middle, making it hard to read. It seems like the last step was looking for `/scratch/moldach/bin/quickstart-output/output.vcf.gz` but couldn't find it. . I'll walk through the steps again to see if I can reproduce your error, but meanwhile, can you try:. ```; singularity run -B $PWD,/usr/lib/locale/ \; ```. instead of . ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; ```. And see if it works for you? Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/287#issuecomment-606340201
https://github.com/google/deepvariant/issues/289#issuecomment-605288518:282,Availability,ping,ping,282,Hi @Stikus . You're right that we haven't rebuild the corresponding Ubuntu 18 version like the one in gs://deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz. I'll file an internal bug and get to this when I can. I think I probably have time in the next week. Feel free to ping in another week if I don't reply.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289#issuecomment-605288518
https://github.com/google/deepvariant/issues/289#issuecomment-605291997:10,Deployability,Update,Update,10,"@Stikus ; Update: @gunjanbaid will start woking on this, and we'll get back to you soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/289#issuecomment-605291997
https://github.com/google/deepvariant/issues/290#issuecomment-606102679:80,Deployability,release,released,80,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290#issuecomment-606102679
https://github.com/google/deepvariant/issues/290#issuecomment-606102679:188,Deployability,update,update,188,"Hi @meghanasp21 . Thank you for your question. The visualization capability was released with the v0.9 version of DeepVariant. In your run, you are using the v0.8 version. Are you able to update to v0.9? If so, the visualization outputs should be present. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290#issuecomment-606102679
https://github.com/google/deepvariant/issues/290#issuecomment-606155318:18,Availability,avail,available,18,"It should also be available in the v0.10.0 release. If not, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290#issuecomment-606155318
https://github.com/google/deepvariant/issues/290#issuecomment-606155318:43,Deployability,release,release,43,"It should also be available in the v0.10.0 release. If not, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290#issuecomment-606155318
https://github.com/google/deepvariant/issues/290#issuecomment-606166642:276,Availability,avail,available,276,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md; The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290#issuecomment-606166642
https://github.com/google/deepvariant/issues/290#issuecomment-606166642:350,Deployability,pipeline,pipeline,350,"In case you don't want to rerun DeepVariant with the newer version to get the report, you can use this script to generate the VCF stats report from an existing VCF file: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-vcf-stats-report.md; The script is only available in version 0.9+, but this way you won't have to rerun the whole pipeline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290#issuecomment-606166642
https://github.com/google/deepvariant/issues/290#issuecomment-697073304:27,Integrability,message,message,27,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0""; BASE=""${PWD}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""10consensus.fasta""; BAM=""268_041_m10.sorted.bam""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""M10.output.vcf.gz""; OUTPUT_GVCF=""M10.output.g.vcf.gz""; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290#issuecomment-697073304
https://github.com/google/deepvariant/issues/290#issuecomment-697073304:82,Testability,test,test,82,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0""; BASE=""${PWD}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""10consensus.fasta""; BAM=""268_041_m10.sorted.bam""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""M10.output.vcf.gz""; OUTPUT_GVCF=""M10.output.g.vcf.gz""; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290#issuecomment-697073304
https://github.com/google/deepvariant/issues/290#issuecomment-698670947:447,Security,secur,security,447,"I got it. Looking at the source code:. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>; <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-lite@3.4.0""></script>; <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-embed@4""></script>. unfortunately the ""https://storage.googleapis.com"" is blocked here for ""security reasons"" :( . I open in my mobile using external network and I can see the complete output. Thanks for the feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290#issuecomment-698670947
https://github.com/google/deepvariant/issues/290#issuecomment-698670947:563,Usability,feedback,feedback,563,"I got it. Looking at the source code:. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>; <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-lite@3.4.0""></script>; <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-embed@4""></script>. unfortunately the ""https://storage.googleapis.com"" is blocked here for ""security reasons"" :( . I open in my mobile using external network and I can see the complete output. Thanks for the feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/290#issuecomment-698670947
https://github.com/google/deepvariant/issues/291#issuecomment-607407000:1268,Deployability,configurat,configuration-file-for-each,1268,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU?. * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291#issuecomment-607407000
https://github.com/google/deepvariant/issues/291#issuecomment-607407000:840,Integrability,Depend,Depending,840,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU?. * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291#issuecomment-607407000
https://github.com/google/deepvariant/issues/291#issuecomment-607407000:1268,Modifiability,config,configuration-file-for-each,1268,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU?. * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291#issuecomment-607407000
https://github.com/google/deepvariant/issues/291#issuecomment-607407000:983,Usability,simpl,simplest,983,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU?. * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/291#issuecomment-607407000
https://github.com/google/deepvariant/issues/292#issuecomment-607119273:18,Availability,error,error,18,"Hi @moldach,. The error message indicates that the sequence contig names present in the reference genome don't match those in the BAM file. . It would be useful to know the names and lengths of the contigs in the BAM header. Are you able to provide the output of this command:. samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep \@SQ. Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-607119273
https://github.com/google/deepvariant/issues/292#issuecomment-607119273:24,Integrability,message,message,24,"Hi @moldach,. The error message indicates that the sequence contig names present in the reference genome don't match those in the BAM file. . It would be useful to know the names and lengths of the contigs in the BAM header. Are you able to provide the output of this command:. samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep \@SQ. Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-607119273
https://github.com/google/deepvariant/issues/292#issuecomment-608553986:483,Availability,error,error,483,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```; @SQ SN:I LN:15072434; @SQ SN:II LN:15279421; @SQ SN:III LN:13783801; @SQ SN:IV LN:17493829; @SQ SN:V LN:20924180; @SQ SN:X LN:17718942; @SQ SN:MtDNA LN:13794; ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```; ...; I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]; I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]; I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]; I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]; I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]; I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]; [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?); 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020; Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-608553986
https://github.com/google/deepvariant/issues/292#issuecomment-608553986:766,Availability,error,error,766,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```; @SQ SN:I LN:15072434; @SQ SN:II LN:15279421; @SQ SN:III LN:13783801; @SQ SN:IV LN:17493829; @SQ SN:V LN:20924180; @SQ SN:X LN:17718942; @SQ SN:MtDNA LN:13794; ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```; ...; I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]; I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]; I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]; I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]; I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]; I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]; [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?); 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020; Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-608553986
https://github.com/google/deepvariant/issues/292#issuecomment-608553986:1753,Availability,error,error,1753,"pvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```; ...; I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]; I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]; I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]; I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]; I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]; I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]; [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?); 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020; Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-608553986
https://github.com/google/deepvariant/issues/292#issuecomment-608553986:1760,Safety,Abort,Aborted,1760,"pvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```; ...; I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]; I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]; I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]; I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]; I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]; I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]; [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?); 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020; Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-608553986
https://github.com/google/deepvariant/issues/292#issuecomment-608553986:338,Usability,guid,guidance,338,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```; @SQ SN:I LN:15072434; @SQ SN:II LN:15279421; @SQ SN:III LN:13783801; @SQ SN:IV LN:17493829; @SQ SN:V LN:20924180; @SQ SN:X LN:17718942; @SQ SN:MtDNA LN:13794; ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```; ...; I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]; I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]; I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]; I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]; I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]; I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]; [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?); 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020; Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-608553986
https://github.com/google/deepvariant/issues/292#issuecomment-608968890:224,Availability,error,error,224,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-608968890
https://github.com/google/deepvariant/issues/292#issuecomment-608968890:306,Availability,error,error,306,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-608968890
https://github.com/google/deepvariant/issues/292#issuecomment-608968890:422,Modifiability,extend,extend,422,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-608968890
https://github.com/google/deepvariant/issues/292#issuecomment-608968890:657,Modifiability,extend,extending,657,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-608968890
https://github.com/google/deepvariant/issues/292#issuecomment-608968890:812,Safety,safe,safest,812,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-608968890
https://github.com/google/deepvariant/issues/292#issuecomment-610586044:43,Deployability,update,update,43,"@bilgehannevruz okay great, thanks for the update!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/292#issuecomment-610586044
https://github.com/google/deepvariant/issues/295#issuecomment-608975822:52,Testability,benchmark,benchmarked,52,"Hi @gevro,. Early in building the PacBio models, we benchmarked on both NGMLR and Minimap2/pbmm2 and found that they gave similar quality of variant calls. We train with data from Minimap2/pbmm2, and I would recommend using pbmm2 for the mapper.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/295#issuecomment-608975822
https://github.com/google/deepvariant/issues/296#issuecomment-609147540:416,Deployability,release,releases,416,"Hi @meghanasp21 . I suspect that what is occurring is that previous, no longer used Docker containers from DeepVariant runs are taking up space on your filesystem. Can you run . **docker system prune** (https://docs.docker.com/config/pruning/). This should clean up those images. I will also make a note for us to remove intermediate files from within the Docker image after runs (this would only come out in future releases) which should decrease the impact of previous Docker images on storage space. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-609147540
https://github.com/google/deepvariant/issues/296#issuecomment-609147540:227,Modifiability,config,config,227,"Hi @meghanasp21 . I suspect that what is occurring is that previous, no longer used Docker containers from DeepVariant runs are taking up space on your filesystem. Can you run . **docker system prune** (https://docs.docker.com/config/pruning/). This should clean up those images. I will also make a note for us to remove intermediate files from within the Docker image after runs (this would only come out in future releases) which should decrease the impact of previous Docker images on storage space. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-609147540
https://github.com/google/deepvariant/issues/296#issuecomment-609217252:271,Testability,log,logs,271,"Hi @meghanasp21 ; If you're running with Singularity, I suspect a temp directory has been created directly under your /tmp/. The line of code that created the tempdir is this line:. https://github.com/google/deepvariant/blob/r0.10/scripts/run_deepvariant.py#L234. In the logs as you run, it should give you the name of the corresponding directory under `/tmp/`. You can manually remove the directory after your run is finished.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-609217252
https://github.com/google/deepvariant/issues/296#issuecomment-766990007:107,Availability,error,error,107,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc.; Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-766990007
https://github.com/google/deepvariant/issues/296#issuecomment-766990007:113,Integrability,message,message,113,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc.; Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-766990007
https://github.com/google/deepvariant/issues/296#issuecomment-767207902:58,Availability,Error,Error,58,"singularity version 3.7.0-1.el8; CentOS 8. ```; parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s; user	0m2.686s; sys	0m4.872s; I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767207902
https://github.com/google/deepvariant/issues/296#issuecomment-767207902:154,Availability,Error,Error,154,"singularity version 3.7.0-1.el8; CentOS 8. ```; parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s; user	0m2.686s; sys	0m4.872s; I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767207902
https://github.com/google/deepvariant/issues/296#issuecomment-767207902:1129,Availability,error,error,1129,"singularity version 3.7.0-1.el8; CentOS 8. ```; parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real	24m13.204s; user	0m2.686s; sys	0m4.872s; I0125 08:09:01.783140 139960451901184 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); ```. Now, as suggested here (https://github.com/google/deepvariant/issues/400), if I set `--intermediate_results_dir tmp/`, it runs without error. But I cannot find where the intermediate files are.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767207902
https://github.com/google/deepvariant/issues/296#issuecomment-767223258:115,Availability,error,error,115,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767223258
https://github.com/google/deepvariant/issues/296#issuecomment-767223258:168,Availability,error,error,168,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767223258
https://github.com/google/deepvariant/issues/296#issuecomment-767223260:115,Availability,error,error,115,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767223260
https://github.com/google/deepvariant/issues/296#issuecomment-767223260:168,Availability,error,error,168,"Yes @pichuan, the intermediate output will be under `/DATA/transfer/deepvariant_tmp`. But the same `no space left` error will pop up. And if I set `tmp/`, then no this error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767223260
https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1841,Availability,down,downloaded,1841,",cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```; [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir""; call_variants_output.tfrecord.gz; gvcf.tfrecord-00000-of-00001.gz; make_examples.tfrecord-00000-of-00001.gz; ```. Next, I want to try running with `--inter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1170,Deployability,release,release,1170,"mp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1532,Deployability,install,installed,1532," are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1636,Deployability,install,installation,1636," this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```; [pi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1654,Deployability,install,installation-on-linux,1654,"below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```; [pichuan@pichuan-centos8 ~]$ l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
https://github.com/google/deepvariant/issues/296#issuecomment-767294612:119,Testability,test,tests,119,"Hi @simoncchu ,; I tried to run a small Quick Start using the same Singularity and OS version as you. ; With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
https://github.com/google/deepvariant/issues/296#issuecomment-767294612:728,Testability,test,test,728,"Hi @simoncchu ,; I tried to run a small Quick Start using the same Singularity and OS version as you. ; With the small tests, I'm not able to see the issue that you observed. And, when I ran with `--intermediate_results_dir tmp/`, I actually do see a ""tmp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1833,Testability,test,test,1833,"ER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```; [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir""; call_variants_output.tfrecord.gz; gvcf.tfrecord-00000-of-00001.gz; make_examples.tfrecord-00000-of-00001.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
https://github.com/google/deepvariant/issues/296#issuecomment-767294612:3046,Testability,test,testdata,3046,"://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```; [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir""; call_variants_output.tfrecord.gz; gvcf.tfrecord-00000-of-00001.gz; make_examples.tfrecord-00000-of-00001.gz; ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:; ```; [pichuan@pichuan-centos8 ~]$ ls -1; deepvariant.sif; quickstart-output; quickstart-testdata; ```. Then I ran:; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir tmp/ \; --num_shards=1; ```. Now I check the current directory again:; ```; [pichuan@pichuan-centos8 ~]$ ls -1; deepvariant.sif; quickstart-output; quickstart-testdata; tmp; ```. and I see the intermediate results in the directory:. ```; [pichuan@pichuan-centos8 ~]$ ls -1 tmp/; call_variants_output.tfrecord.gz; gvcf.tfrecord-00000-of-00001.gz; make_examples.tfrecord-00000-of-00001.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
https://github.com/google/deepvariant/issues/296#issuecomment-767294612:3650,Testability,test,testdata,3650,"://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```; [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir""; call_variants_output.tfrecord.gz; gvcf.tfrecord-00000-of-00001.gz; make_examples.tfrecord-00000-of-00001.gz; ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:; ```; [pichuan@pichuan-centos8 ~]$ ls -1; deepvariant.sif; quickstart-output; quickstart-testdata; ```. Then I ran:; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir tmp/ \; --num_shards=1; ```. Now I check the current directory again:; ```; [pichuan@pichuan-centos8 ~]$ ls -1; deepvariant.sif; quickstart-output; quickstart-testdata; tmp; ```. and I see the intermediate results in the directory:. ```; [pichuan@pichuan-centos8 ~]$ ls -1 tmp/; call_variants_output.tfrecord.gz; gvcf.tfrecord-00000-of-00001.gz; make_examples.tfrecord-00000-of-00001.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1613,Usability,guid,guides,1613,") that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1630,Usability,guid,guide,1630," this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```; [pi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612
https://github.com/google/deepvariant/issues/296#issuecomment-1624413952:346,Availability,down,downstream,346,"@splaisan The intermediate directory is mainly used to generate and store [`TFRecord`](https://www.tensorflow.org/tutorials/load_data/tfrecord) files that are used among the `make_examples`, `call_variants` and `postprocess_variants` steps (performed by the `run_deepvariant` command above) to eventually generate the VCF and gVCF files used for downstream analysis. So you don't need them after a successfully completed DeepVariant run, as most folks are only interested in the resulting VCF and gVCF files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-1624413952
https://github.com/google/deepvariant/issues/296#issuecomment-1624413952:241,Performance,perform,performed,241,"@splaisan The intermediate directory is mainly used to generate and store [`TFRecord`](https://www.tensorflow.org/tutorials/load_data/tfrecord) files that are used among the `make_examples`, `call_variants` and `postprocess_variants` steps (performed by the `run_deepvariant` command above) to eventually generate the VCF and gVCF files used for downstream analysis. So you don't need them after a successfully completed DeepVariant run, as most folks are only interested in the resulting VCF and gVCF files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-1624413952
https://github.com/google/deepvariant/issues/296#issuecomment-1625516054:551,Modifiability,layers,layers,551,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```; $ docker info | grep Root; Docker Root Dir: /var/lib/docker; $; ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-1625516054
https://github.com/google/deepvariant/issues/296#issuecomment-1625614434:202,Modifiability,config,config,202,"Hi @splaisan ,; like earlier comments mentioned, if you don't specify intermediate_results_dir, it won't be saved separately. A few more tricks for cleaning up after Docker:; 1. https://docs.docker.com/config/pruning/ has information about how to clean up for Docker.; 2. If you run Docker with `--rm` , I believe it'll also clean up after your run completes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/296#issuecomment-1625614434
https://github.com/google/deepvariant/issues/297#issuecomment-613950795:252,Usability,simpl,simply,252,"Hello again,. actually the evidence bam folder is borderline unusable, there are so many folders that the file explorer tries to commit suicide every time I attempt to navigate it, same for the ""built-in"" explorer of bam viewers, for example Tablet is simply unusable, it systematically crashes. . I don't know how feasible this is computationally but maybe it would be best if DeepVariant produced a single bam per sample. Or even a single bam per sample per chromosome. If the user wants to subsequently split the bam that's not too complicated (otherwise the bam files themselves are neat I am happy with the results ^^). . Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/297#issuecomment-613950795
https://github.com/google/deepvariant/issues/298#issuecomment-617918041:1029,Modifiability,extend,extend,1029,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so!. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/298#issuecomment-617918041
https://github.com/google/deepvariant/issues/299#issuecomment-617891319:30,Energy Efficiency,monitor,monitor,30,"(assigning to myself so I can monitor activity of this issue, and close after a week of inactivity)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/299#issuecomment-617891319
https://github.com/google/deepvariant/issues/300#issuecomment-617891031:203,Deployability,pipeline,pipeline,203,"Thanks for clarifying. I was just curious, because when I used GATK to call germline variants, it gave me 10x less, so I just was curious about the large discrepancy. But, perhaps this is more of a GATK pipeline issue. So, the ""PASS"" variants should be the final filtered germline variants?. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300#issuecomment-617891031
https://github.com/google/deepvariant/issues/300#issuecomment-617897260:803,Usability,learn,learning,803,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position.; You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf; ```; FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. ; ```; Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/300#issuecomment-617897260
https://github.com/google/deepvariant/issues/301#issuecomment-617907163:536,Deployability,install,installing,536,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301#issuecomment-617907163
https://github.com/google/deepvariant/issues/301#issuecomment-617907163:86,Integrability,message,message,86,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301#issuecomment-617907163
https://github.com/google/deepvariant/issues/301#issuecomment-617907163:197,Testability,log,logging,197,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301#issuecomment-617907163
https://github.com/google/deepvariant/issues/301#issuecomment-617907163:415,Testability,log,logging,415,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301#issuecomment-617907163
https://github.com/google/deepvariant/issues/301#issuecomment-617907163:372,Usability,clear,clear,372,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301#issuecomment-617907163
https://github.com/google/deepvariant/issues/301#issuecomment-618511477:116,Integrability,message,message,116,"Hi @forumsan, you are already getting the speedup from AVX2/AVX-512 if you are using Intel Skylake or later! :) The message you are seeing is an issue with logging in TensorFlow. Feel free to ignore that message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301#issuecomment-618511477
https://github.com/google/deepvariant/issues/301#issuecomment-618511477:204,Integrability,message,message,204,"Hi @forumsan, you are already getting the speedup from AVX2/AVX-512 if you are using Intel Skylake or later! :) The message you are seeing is an issue with logging in TensorFlow. Feel free to ignore that message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301#issuecomment-618511477
https://github.com/google/deepvariant/issues/301#issuecomment-618511477:156,Testability,log,logging,156,"Hi @forumsan, you are already getting the speedup from AVX2/AVX-512 if you are using Intel Skylake or later! :) The message you are seeing is an issue with logging in TensorFlow. Feel free to ignore that message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/301#issuecomment-618511477
https://github.com/google/deepvariant/issues/302#issuecomment-620808470:443,Usability,feedback,feedback,443,"Hi @tetsuro90 [this documentation](https://github.com/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs) has more information about the representation and the ""half-calls"" specifically. It involves some gnarly issues with overlapping variants in VCF for which there isn't a lot of standardization across tools unfortunately. [This issue](https://github.com/dnanexus-rnd/GLnexus/issues/210) also discusses some potential future developments. Any feedback there is welcome. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/302#issuecomment-620808470
https://github.com/google/deepvariant/issues/304#issuecomment-620382628:155,Testability,log,log,155,"Hi @MorganHow ,; What is the command you used? I assume you're using the latest `0.10.0` version?. The latest v0.10.0 is built with Python 3.6. ; From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share?. Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-620382628
https://github.com/google/deepvariant/issues/304#issuecomment-620382628:304,Testability,log,logs,304,"Hi @MorganHow ,; What is the command you used? I assume you're using the latest `0.10.0` version?. The latest v0.10.0 is built with Python 3.6. ; From the log, it seems like make_examples finish running, but not that many examples were made. However, I don't expect call_variants to fail. Are there more logs that you can share?. Or, can you share the OS version, and command that you're using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-620382628
https://github.com/google/deepvariant/issues/304#issuecomment-620560432:577,Availability,error,errors,577,"The command I ran was on version 0.10.0; `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287); Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help!; [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-620560432
https://github.com/google/deepvariant/issues/304#issuecomment-620560432:629,Testability,log,log,629,"The command I ran was on version 0.10.0; `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287); Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help!; [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-620560432
https://github.com/google/deepvariant/issues/304#issuecomment-620560432:702,Testability,log,log,702,"The command I ran was on version 0.10.0; `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287); Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help!; [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-620560432
https://github.com/google/deepvariant/issues/304#issuecomment-620713923:88,Deployability,update,update,88,"I see!; @MorganHow , unfortunately DeepVariant isn't designed to run on MacOS. ; We can update our [prerequisites](https://github.com/google/deepvariant#prerequisites) to be a bit more specific. We have not tried it on MacOS because even if you can get a small example to run, it won't be feasible to run on a real sample. @MorganHow Do you have another Linux machines you can try this? Thanks for reporting!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-620713923
https://github.com/google/deepvariant/issues/304#issuecomment-620715339:177,Availability,error,error,177,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-620715339
https://github.com/google/deepvariant/issues/304#issuecomment-620715339:136,Deployability,update,update,136,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-620715339
https://github.com/google/deepvariant/issues/304#issuecomment-620715339:10,Security,access,access,10,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-620715339
https://github.com/google/deepvariant/issues/304#issuecomment-642781261:98,Availability,error,error,98,"@ptrebert Thank you for reporting. ; Can you open another GitHub issue, with the command and full error (of the 2 jobs that failed), so we can help specifically with your issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-642781261
https://github.com/google/deepvariant/issues/304#issuecomment-642917555:130,Security,access,access,130,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-642917555
https://github.com/google/deepvariant/issues/304#issuecomment-643144376:63,Availability,avail,available,63,"@pgrosu ; A substantial increase (almost double) of the memory available to the DeepVariant cluster jobs worked, both jobs now succeeded. Thanks for your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-643144376
https://github.com/google/deepvariant/issues/304#issuecomment-643473196:154,Integrability,inject,injects,154,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-643473196
https://github.com/google/deepvariant/issues/304#issuecomment-643473196:167,Modifiability,layers,layers,167,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-643473196
https://github.com/google/deepvariant/issues/304#issuecomment-643473196:154,Security,inject,injects,154,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-643473196
https://github.com/google/deepvariant/issues/304#issuecomment-643473196:200,Security,expose,exposed,200,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/304#issuecomment-643473196
https://github.com/google/deepvariant/issues/305#issuecomment-620645675:590,Testability,log,log,590,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot!; [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-620645675
https://github.com/google/deepvariant/issues/305#issuecomment-620645675:640,Testability,log,logs,640,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot!; [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-620645675
https://github.com/google/deepvariant/issues/305#issuecomment-620645675:729,Testability,log,logs,729,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot!; [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-620645675
https://github.com/google/deepvariant/issues/305#issuecomment-620732836:703,Availability,error,errors,703,"Oh, sorry I couldn't see that you already had one `--regions` in there.; I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in.; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"".; ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-620732836
https://github.com/google/deepvariant/issues/305#issuecomment-620732836:738,Availability,failure,failure,738,"Oh, sorry I couldn't see that you already had one `--regions` in there.; I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in.; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"".; ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-620732836
https://github.com/google/deepvariant/issues/305#issuecomment-620732836:124,Testability,log,logs,124,"Oh, sorry I couldn't see that you already had one `--regions` in there.; I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in.; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"".; ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-620732836
https://github.com/google/deepvariant/issues/305#issuecomment-620759329:509,Deployability,update,update,509,"I was able to reproduce the issue you noticed when running the docker build. The issue looks to be that the quotes are used up and not passed on to the make_examples command intact. I was able to get it working by doing this: `--regions=""'3:178936057-178936106 3:178952054-178952106'""`. This uses single quotes encased by double quotes, while the other way around did not work for some reason. Does that work for you?. Thanks for bringing this up. I will make a note of this, so we can find a fix or at least update the documentation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-620759329
https://github.com/google/deepvariant/issues/305#issuecomment-620845064:148,Availability,avail,available,148,"Thank a lot for your reply. The 'single quotes encased by double quotes' works for this case, but when I added more regions, it couldn't give me an available output and showed an exit status 247.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-620845064
https://github.com/google/deepvariant/issues/305#issuecomment-620891124:57,Availability,error,error,57,"Interesting. Could you share the command you ran and the error message?. On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'; > works for this case, but when I added more regions, it couldn't give me an; > available output and showed an exit status 247.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-620891124
https://github.com/google/deepvariant/issues/305#issuecomment-620891124:307,Availability,avail,available,307,"Interesting. Could you share the command you ran and the error message?. On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'; > works for this case, but when I added more regions, it couldn't give me an; > available output and showed an exit status 247.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-620891124
https://github.com/google/deepvariant/issues/305#issuecomment-620891124:63,Integrability,message,message,63,"Interesting. Could you share the command you ran and the error message?. On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'; > works for this case, but when I added more regions, it couldn't give me an; > available output and showed an exit status 247.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-620891124
https://github.com/google/deepvariant/issues/305#issuecomment-623005616:60,Availability,error,error,60,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1.; For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-623005616
https://github.com/google/deepvariant/issues/305#issuecomment-623005616:287,Availability,avail,available,287,"Hi, thanks for your reply. When I retried it, it showed the error with 'subprocess.CalledProcessError: Command 'time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling' and --task {}' returned non-zero exit status 1.; For the codes I used, is it available to send to your email? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-623005616
https://github.com/google/deepvariant/issues/305#issuecomment-623601298:25,Availability,error,error,25,"There should be a longer error trace coming from DeepVariant itself, which will be more informative than just the exit status code. Can you share that? If you need to, you can email me at marianattestad@google.com.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/305#issuecomment-623601298
https://github.com/google/deepvariant/issues/306#issuecomment-620378545:493,Deployability,release,release,493,"Hi @jumpyknight . You are correct, and this description shows a good amount of thought and analysis about what DeepVariant sees. The full insert information is not present, meaning that for insertions (especially long ones or at multi-allelic sites), the supports variant channel becomes very important. . We have made a similar observation and are currently working on an improvement that will allow capturing more information about these insertions. I hope that this will go out in the next release version of DeepVariant. I believe that the base quality present at these insertion positions reflects the base quality of the base overlapping the reference base just prior to the insertion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/306#issuecomment-620378545
https://github.com/google/deepvariant/issues/307#issuecomment-628230170:1040,Availability,error,errors,1040,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. ; Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:; BIN_VERSION=""0.10.0""; BASE=""${HOME}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""hg19.fa""; BAM=""2009617.cram""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""2009617.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; Here is the command to execute deepvariant:; sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc); and here is the new errors I'm seeing:; ValueError: Failed precondition: Cannot query without an index; parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples; --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --exampl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307#issuecomment-628230170
https://github.com/google/deepvariant/issues/307#issuecomment-628230170:1192,Availability,error,errors,1192,"d version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:; BIN_VERSION=""0.10.0""; BASE=""${HOME}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""hg19.fa""; BAM=""2009617.cram""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""2009617.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; Here is the command to execute deepvariant:; sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc); and here is the new errors I'm seeing:; ValueError: Failed precondition: Cannot query without an index; parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples; --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --examples ""/tmp/tmp1cb8k6ly/make_examples.tfrecord@1; 6.gz"" --gvcf ""/tmp/tmp1cb8k6ly/gvcf.tfrecord@16.gz"" --task {}' returned non-zero exit status 1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307#issuecomment-628230170
https://github.com/google/deepvariant/issues/307#issuecomment-628230170:175,Deployability,update,updated,175,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. ; Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:; BIN_VERSION=""0.10.0""; BASE=""${HOME}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""hg19.fa""; BAM=""2009617.cram""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""2009617.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; Here is the command to execute deepvariant:; sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc); and here is the new errors I'm seeing:; ValueError: Failed precondition: Cannot query without an index; parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples; --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --exampl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307#issuecomment-628230170
https://github.com/google/deepvariant/issues/307#issuecomment-628230170:221,Deployability,update,updated,221,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. ; Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:; BIN_VERSION=""0.10.0""; BASE=""${HOME}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""hg19.fa""; BAM=""2009617.cram""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""2009617.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; Here is the command to execute deepvariant:; sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc); and here is the new errors I'm seeing:; ValueError: Failed precondition: Cannot query without an index; parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples; --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --exampl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307#issuecomment-628230170
https://github.com/google/deepvariant/issues/307#issuecomment-628230170:321,Modifiability,variab,variables,321,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. ; Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:; BIN_VERSION=""0.10.0""; BASE=""${HOME}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""hg19.fa""; BAM=""2009617.cram""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""2009617.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; Here is the command to execute deepvariant:; sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc); and here is the new errors I'm seeing:; ValueError: Failed precondition: Cannot query without an index; parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples; --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --exampl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307#issuecomment-628230170
https://github.com/google/deepvariant/issues/307#issuecomment-628240247:4,Availability,error,error,4,"The error message says ""Cannot query without an index"", so I'm guessing your CRAM file might be missing its index too. See: http://www.htslib.org/doc/samtools-index.html; Again see the [documentation](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307#issuecomment-628240247
https://github.com/google/deepvariant/issues/307#issuecomment-628240247:10,Integrability,message,message,10,"The error message says ""Cannot query without an index"", so I'm guessing your CRAM file might be missing its index too. See: http://www.htslib.org/doc/samtools-index.html; Again see the [documentation](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/307#issuecomment-628240247
https://github.com/google/deepvariant/issues/308#issuecomment-628304654:432,Availability,avail,available,432,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:; 1. No, this is a limit within Inception V3 that DeepVariant uses.; 2. We don't have any other training tutorials for other systems.; 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research.; If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-628304654
https://github.com/google/deepvariant/issues/308#issuecomment-628304654:112,Energy Efficiency,adapt,adapt,112,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:; 1. No, this is a limit within Inception V3 that DeepVariant uses.; 2. We don't have any other training tutorials for other systems.; 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research.; If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-628304654
https://github.com/google/deepvariant/issues/308#issuecomment-628304654:112,Modifiability,adapt,adapt,112,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:; 1. No, this is a limit within Inception V3 that DeepVariant uses.; 2. We don't have any other training tutorials for other systems.; 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research.; If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-628304654
https://github.com/google/deepvariant/issues/308#issuecomment-628304654:871,Usability,guid,guide,871,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:; 1. No, this is a limit within Inception V3 that DeepVariant uses.; 2. We don't have any other training tutorials for other systems.; 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research.; If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-628304654
https://github.com/google/deepvariant/issues/308#issuecomment-629860393:277,Availability,error,error,277,"Hi Maria,; Thanks a lot for your help!; [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error.; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo nvidia-docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1; Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'.; The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-629860393
https://github.com/google/deepvariant/issues/308#issuecomment-629860393:991,Availability,error,error,991,"Hi Maria,; Thanks a lot for your help!; [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error.; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo nvidia-docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1; Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'.; The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-629860393
https://github.com/google/deepvariant/issues/308#issuecomment-629860393:53,Testability,log,logs,53,"Hi Maria,; Thanks a lot for your help!; [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error.; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo nvidia-docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1; Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'.; The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-629860393
https://github.com/google/deepvariant/issues/308#issuecomment-629860393:142,Testability,log,logs,142,"Hi Maria,; Thanks a lot for your help!; [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error.; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo nvidia-docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1; Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'.; The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-629860393
https://github.com/google/deepvariant/issues/308#issuecomment-629860393:358,Testability,log,log,358,"Hi Maria,; Thanks a lot for your help!; [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error.; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo nvidia-docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1; Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'.; The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-629860393
https://github.com/google/deepvariant/issues/308#issuecomment-629860393:815,Testability,log,log,815,"Hi Maria,; Thanks a lot for your help!; [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error.; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo nvidia-docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1; Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'.; The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-629860393
https://github.com/google/deepvariant/issues/308#issuecomment-629860393:1053,Testability,log,log,1053,"Hi Maria,; Thanks a lot for your help!; [problematic logs with training.docx](https://github.com/google/deepvariant/files/4641018/problematic.logs.with.training.docx). Now I am setting Google Cloud SDK in our Linux system, but whenever I run the following command, there is an error.; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo nvidia-docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1; Since there are no GPUs in our server, I changed the ""nvidia-docker"" to ""docker"", and the ""${BIN_VERSION}-gpu"" to ""${BIN_VERSION}"", but it still didn't work with an error that 'The index file is older than the data file'.; The log is attached here and I wonder whether the training mode could work without a GPU machine. Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-629860393
https://github.com/google/deepvariant/issues/308#issuecomment-631608785:284,Availability,error,error,284,"Hi @WeiweiBian ,; first of all, the ""The index file is older than the data file"" warning is usually something to do with your input data files (BAM, VCF, or FASTA). It's usually just a warning and wouldn't affect the run. You should still be able to run the training. Looking at your error, it seems to me that you might not using the correct REF file that has the `chr` prefix. On my side, I will check to confirm that our training case study actually works as intended. But, specifically for your use case --; To re-iterate what @MariaNattestad has mentioned, the current DeepVariant setup is not designed for your use case, and our team won't really have the bandwidth to support a very different use case like yours right now. Please consider using another tool that is designed for your use case. If you plan to use TensorFlow (which is what we built on) for your own use case and explore the possibilities, that is great! Based on my experience, training with just CPUs would really slow for the training step (model_train and model_eval steps), I have not even attempted that myself to a real training run using only CPUs. I do not recommend it. I will close this issue given that your use case is not a feature that our team will have bandwidth to support officially. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/308#issuecomment-631608785
https://github.com/google/deepvariant/issues/309#issuecomment-631261750:7,Availability,error,error,7,Second error (num_shards) has been resolved,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/309#issuecomment-631261750
https://github.com/google/deepvariant/issues/310#issuecomment-636143865:4,Availability,error,error,4,The error messages sounds like it's not seeing your fasta index file. Since you are utilizing environment variables I would suggest verifying that those are accurate and that both your fasta and it's corresponding index file are present in that same directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310#issuecomment-636143865
https://github.com/google/deepvariant/issues/310#issuecomment-636143865:10,Integrability,message,messages,10,The error messages sounds like it's not seeing your fasta index file. Since you are utilizing environment variables I would suggest verifying that those are accurate and that both your fasta and it's corresponding index file are present in that same directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310#issuecomment-636143865
https://github.com/google/deepvariant/issues/310#issuecomment-636143865:106,Modifiability,variab,variables,106,The error messages sounds like it's not seeing your fasta index file. Since you are utilizing environment variables I would suggest verifying that those are accurate and that both your fasta and it's corresponding index file are present in that same directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310#issuecomment-636143865
https://github.com/google/deepvariant/issues/310#issuecomment-637449360:67,Availability,avail,available,67,@pichuan @Roj4ck thanks for your quick reply. However the index is available in the same directory. I also tried removing it and recreating it with the samtools command. This didn't resolve the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310#issuecomment-637449360
https://github.com/google/deepvariant/issues/310#issuecomment-637679152:89,Usability,clear,clear,89,"Hi @colsen ; thanks for confirming that the file was there. At this point I don't have a clear next guess on what could have been wrong. But can you check these two things:. 1. Confirm that docker see this file:; Run a similar command:. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" ls /input/hg19.fasta*. This can confirm that docker actually sees the file. (should have both hg19.fasta and hg19.fasta.fai). 2. Run another program to make sure the the fasta file is well-formed. Given that you have tried samtools faidx it without issue, I think this is probably not the issue, but might still be good to check. @Roj4ck if you have more suggestions on this, please feel free to chime in!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310#issuecomment-637679152
https://github.com/google/deepvariant/issues/310#issuecomment-637685215:195,Modifiability,variab,variables,195,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:; BIN_VERSION=""0.10.0""; BASE=""${HOME}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""hg19.fa""; BAM=""2009617s.cram""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""2009617s.vcf.gz""; OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310#issuecomment-637685215
https://github.com/google/deepvariant/issues/310#issuecomment-637685215:221,Modifiability,variab,variables,221,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:; BIN_VERSION=""0.10.0""; BASE=""${HOME}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""hg19.fa""; BAM=""2009617s.cram""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""2009617s.vcf.gz""; OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310#issuecomment-637685215
https://github.com/google/deepvariant/issues/310#issuecomment-637685215:932,Modifiability,variab,variables,932,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:; BIN_VERSION=""0.10.0""; BASE=""${HOME}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""hg19.fa""; BAM=""2009617s.cram""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""2009617s.vcf.gz""; OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310#issuecomment-637685215
https://github.com/google/deepvariant/issues/310#issuecomment-637685215:1033,Modifiability,variab,variables,1033,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:; BIN_VERSION=""0.10.0""; BASE=""${HOME}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""hg19.fa""; BAM=""2009617s.cram""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""2009617s.vcf.gz""; OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/310#issuecomment-637685215
https://github.com/google/deepvariant/issues/311#issuecomment-636996628:1043,Availability,avail,available,1043,"Thank you for the quick response!; I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.); What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) ; what depths - 152x mean depth of coverage with a quality threshold of 98.6; what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents); Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311#issuecomment-636996628
https://github.com/google/deepvariant/issues/311#issuecomment-636996628:192,Performance,perform,performed,192,"Thank you for the quick response!; I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.); What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) ; what depths - 152x mean depth of coverage with a quality threshold of 98.6; what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents); Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311#issuecomment-636996628
https://github.com/google/deepvariant/issues/311#issuecomment-636996628:171,Testability,test,testing,171,"Thank you for the quick response!; I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.); What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) ; what depths - 152x mean depth of coverage with a quality threshold of 98.6; what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents); Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311#issuecomment-636996628
https://github.com/google/deepvariant/issues/311#issuecomment-636996628:206,Testability,test,tests,206,"Thank you for the quick response!; I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.); What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) ; what depths - 152x mean depth of coverage with a quality threshold of 98.6; what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents); Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311#issuecomment-636996628
https://github.com/google/deepvariant/issues/311#issuecomment-636996628:284,Testability,Test,Testing,284,"Thank you for the quick response!; I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.); What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) ; what depths - 152x mean depth of coverage with a quality threshold of 98.6; what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents); Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311#issuecomment-636996628
https://github.com/google/deepvariant/issues/311#issuecomment-637057495:632,Deployability,install,installer,632,"Hi @Roj4ck I think it cloud be helpful to add the flag `-o output_rtg_annotated.vcf.gz` flag when running `rtg mendelian`, and to manually inspect the output by `zless output_rtg_annotated.vcf.gz`. This file should have a specific annotation for calls with indeterminate consistency status. As @pichuan mentioned above they're most likely due to incomplete genotypes like `./.`. You can also manually inspect a few number of calls violating Mendelian consistency and try to find patterns among them. You can find more instructions about the RTG Tools in their documentation: https://cdn.rawgit.com/RealTimeGenomics/rtg-tools/master/installer/resources/tools/RTGOperationsManual/rtg_command_reference.html#mendelian. It'll be also useful to know how many samples are included in the cohort VCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311#issuecomment-637057495
https://github.com/google/deepvariant/issues/311#issuecomment-637256372:54,Usability,simpl,simple,54,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2115432]; 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls; 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617]; 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls; 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617, 2115432]; 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls; 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/311#issuecomment-637256372
https://github.com/google/deepvariant/issues/312#issuecomment-637264990:445,Performance,tune,tune,445,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)); 2. Shuffling is needed for tune and train data, and not needed for validation data.; 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. ; 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-637264990
https://github.com/google/deepvariant/issues/312#issuecomment-637264990:151,Safety,avoid,avoid,151,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)); 2. Shuffling is needed for tune and train data, and not needed for validation data.; 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. ; 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-637264990
https://github.com/google/deepvariant/issues/312#issuecomment-637264990:485,Security,validat,validation,485,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)); 2. Shuffling is needed for tune and train data, and not needed for validation data.; 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. ; 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-637264990
https://github.com/google/deepvariant/issues/312#issuecomment-637523488:135,Security,validat,validation,135,"Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. Very much looking forward to reading your comments on warmstarting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-637523488
https://github.com/google/deepvariant/issues/312#issuecomment-637523488:51,Usability,clear,clearer,51,"Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. Very much looking forward to reading your comments on warmstarting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-637523488
https://github.com/google/deepvariant/issues/312#issuecomment-638521636:254,Deployability,release,release,254,"Hi @mpinese ,; I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:; For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638521636
https://github.com/google/deepvariant/issues/312#issuecomment-638521636:1834,Modifiability,evolve,evolve,1834,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:; For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638521636
https://github.com/google/deepvariant/issues/312#issuecomment-638521636:533,Testability,log,logic,533,"Hi @mpinese ,; I'm commenting here for your question 4. Internally for warmstarting, we use the same flag (`--start_from_checkpoint`) in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md to train our WES and PacBio release models. You refer to: https://github.com/google/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:; For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638521636
https://github.com/google/deepvariant/issues/312#issuecomment-638521636:1779,Usability,intuit,intuition,1779,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:; For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638521636
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1179,Availability,checkpoint,checkpoint,1179,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1529,Deployability,update,update,1529,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1553,Deployability,release,release,1553,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:486,Safety,safe,safer,486,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:137,Security,validat,validation,137,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:266,Security,validat,validation,266,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1057,Security,Validat,Validation,1057,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1414,Security,validat,validation,1414,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1655,Security,validat,validation,1655,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1986,Security,validat,validation,1986,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1687,Testability,test,test,1687,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1845,Testability,test,test,1845,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:53,Usability,clear,clearer,53,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:937,Usability,learn,learns,937,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1022,Usability,learn,learning,1022,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1133,Usability,learn,learns,1133,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1263,Usability,learn,learning,1263,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1751,Usability,learn,learning,1751,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1799,Usability,learn,learning,1799,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733
https://github.com/google/deepvariant/issues/314#issuecomment-637733785:155,Availability,avail,available,155,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314#issuecomment-637733785
https://github.com/google/deepvariant/issues/314#issuecomment-637733785:94,Integrability,depend,dependency,94,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314#issuecomment-637733785
https://github.com/google/deepvariant/issues/314#issuecomment-637888585:78,Availability,echo,echo,78,@chapmanb ; Thank you for your reply!. I checked the container PATH.; ```; ~$ echo $PATH; /opt/conda/envs/deepvariant/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin; ```. and I tried to find out if unzip exists in the bin directory.; ```; ~$ ls -l /opt/conda/envs/deepvariant/bin/; .; .; .; -rwxrwxr-x 2 root root 22480 Dec 11 2018 toe*; -rwxrwxr-x 2 root root 22512 Dec 11 2018 tput*; -rwxrwxr-x 2 root root 30680 Dec 11 2018 tset*; lrwxrwxrwx 1 root root 3 Jun 3 00:24 unlz4 -> lz4*; lrwxrwxrwx 1 root root 2 Jun 3 00:24 unlzma -> xz*; -rwxrwxr-x 2 root root 238086 May 18 15:34 unpack200*; lrwxrwxrwx 1 root root 2 Jun 3 00:24 unxz -> xz*; lrwxrwxrwx 1 root root 4 Jun 3 00:24 unzstd -> zstd*; -rwxrwxr-x 2 root root 25904 Dec 18 17:04 uuclient*; -rwxr-xr-x 1 root root 236 Jun 3 00:24 wheel*; lrwxrwxrwx 1 root root 7 Jun 3 00:24 wish -> wish8.6*; .; .; .; ```. The unzip itself doesn't seem to be installed during `conda install` .; Is this a problem that only happens to me?. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314#issuecomment-637888585
https://github.com/google/deepvariant/issues/314#issuecomment-637888585:955,Deployability,install,installed,955,@chapmanb ; Thank you for your reply!. I checked the container PATH.; ```; ~$ echo $PATH; /opt/conda/envs/deepvariant/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin; ```. and I tried to find out if unzip exists in the bin directory.; ```; ~$ ls -l /opt/conda/envs/deepvariant/bin/; .; .; .; -rwxrwxr-x 2 root root 22480 Dec 11 2018 toe*; -rwxrwxr-x 2 root root 22512 Dec 11 2018 tput*; -rwxrwxr-x 2 root root 30680 Dec 11 2018 tset*; lrwxrwxrwx 1 root root 3 Jun 3 00:24 unlz4 -> lz4*; lrwxrwxrwx 1 root root 2 Jun 3 00:24 unlzma -> xz*; -rwxrwxr-x 2 root root 238086 May 18 15:34 unpack200*; lrwxrwxrwx 1 root root 2 Jun 3 00:24 unxz -> xz*; lrwxrwxrwx 1 root root 4 Jun 3 00:24 unzstd -> zstd*; -rwxrwxr-x 2 root root 25904 Dec 18 17:04 uuclient*; -rwxr-xr-x 1 root root 236 Jun 3 00:24 wheel*; lrwxrwxrwx 1 root root 7 Jun 3 00:24 wish -> wish8.6*; .; .; .; ```. The unzip itself doesn't seem to be installed during `conda install` .; Is this a problem that only happens to me?. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314#issuecomment-637888585
https://github.com/google/deepvariant/issues/314#issuecomment-637888585:979,Deployability,install,install,979,@chapmanb ; Thank you for your reply!. I checked the container PATH.; ```; ~$ echo $PATH; /opt/conda/envs/deepvariant/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin; ```. and I tried to find out if unzip exists in the bin directory.; ```; ~$ ls -l /opt/conda/envs/deepvariant/bin/; .; .; .; -rwxrwxr-x 2 root root 22480 Dec 11 2018 toe*; -rwxrwxr-x 2 root root 22512 Dec 11 2018 tput*; -rwxrwxr-x 2 root root 30680 Dec 11 2018 tset*; lrwxrwxrwx 1 root root 3 Jun 3 00:24 unlz4 -> lz4*; lrwxrwxrwx 1 root root 2 Jun 3 00:24 unlzma -> xz*; -rwxrwxr-x 2 root root 238086 May 18 15:34 unpack200*; lrwxrwxrwx 1 root root 2 Jun 3 00:24 unxz -> xz*; lrwxrwxrwx 1 root root 4 Jun 3 00:24 unzstd -> zstd*; -rwxrwxr-x 2 root root 25904 Dec 18 17:04 uuclient*; -rwxr-xr-x 1 root root 236 Jun 3 00:24 wheel*; lrwxrwxrwx 1 root root 7 Jun 3 00:24 wish -> wish8.6*; .; .; .; ```. The unzip itself doesn't seem to be installed during `conda install` .; Is this a problem that only happens to me?. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314#issuecomment-637888585
https://github.com/google/deepvariant/issues/314#issuecomment-638091561:478,Deployability,install,install,478,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314#issuecomment-638091561
https://github.com/google/deepvariant/issues/314#issuecomment-638091561:189,Integrability,depend,dependencies,189,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314#issuecomment-638091561
https://github.com/google/deepvariant/issues/314#issuecomment-638091561:607,Integrability,depend,dependencies,607,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314#issuecomment-638091561
https://github.com/google/deepvariant/issues/314#issuecomment-638091561:405,Safety,avoid,avoid,405,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314#issuecomment-638091561
https://github.com/google/deepvariant/issues/314#issuecomment-638091561:541,Safety,avoid,avoid,541,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314#issuecomment-638091561
https://github.com/google/deepvariant/issues/314#issuecomment-638128799:177,Deployability,install,installing,177,"@chapmanb ; Thank you for your reply. ; I have seen the conda recipi. It is indeed strange. Maybe there's something wrong with my running environment.; OK, I'll deal with it by installing `unzip` directly in `apt` or `conda`.; Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/314#issuecomment-638128799
https://github.com/google/deepvariant/issues/315#issuecomment-638394420:96,Testability,log,logs,96,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command.; ``` ; /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315#issuecomment-638394420
https://github.com/google/deepvariant/issues/315#issuecomment-638394420:139,Testability,log,logtostderr,139,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command.; ``` ; /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315#issuecomment-638394420
https://github.com/google/deepvariant/issues/315#issuecomment-638394420:161,Testability,log,logs,161,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command.; ``` ; /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315#issuecomment-638394420
https://github.com/google/deepvariant/issues/315#issuecomment-638394420:582,Testability,log,logtostderr,582,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command.; ``` ; /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315#issuecomment-638394420
https://github.com/google/deepvariant/issues/315#issuecomment-638428497:68,Availability,error,errors,68,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/input/ilAriAges1.fasta.bgz"" \; --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \; --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \; --norealign_reads \; --vsc_min_fraction_indels ""0.12"" \; --task 54 --logtostderr; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315#issuecomment-638428497
https://github.com/google/deepvariant/issues/315#issuecomment-638428497:196,Integrability,message,message,196,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/input/ilAriAges1.fasta.bgz"" \; --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \; --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \; --norealign_reads \; --vsc_min_fraction_indels ""0.12"" \; --task 54 --logtostderr; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315#issuecomment-638428497
https://github.com/google/deepvariant/issues/315#issuecomment-638428497:793,Testability,log,logtostderr,793,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/input/ilAriAges1.fasta.bgz"" \; --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \; --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \; --norealign_reads \; --vsc_min_fraction_indels ""0.12"" \; --task 54 --logtostderr; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315#issuecomment-638428497
https://github.com/google/deepvariant/issues/315#issuecomment-640852141:71,Usability,guid,guidance,71,"hello @akolesnikov,. I was wondering if you might have some additional guidance towards running deepvariant. I have started running deepvariant successfully in another server, but I would like to be run the process in parallel in multiple servers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/315#issuecomment-640852141
https://github.com/google/deepvariant/issues/316#issuecomment-641651605:41,Availability,avail,available,41,"Hi @tetsuro90, `run_deepvariant` is only available for Docker and not for Conda. You can reference [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md) for the flags needed to write out GVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/316#issuecomment-641651605
https://github.com/google/deepvariant/issues/317#issuecomment-644965403:249,Performance,perform,performs,249,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB.; Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-644965403
https://github.com/google/deepvariant/issues/317#issuecomment-644965403:799,Performance,perform,performed,799,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB.; Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-644965403
https://github.com/google/deepvariant/issues/317#issuecomment-644965403:183,Usability,Learn,Learning,183,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB.; Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-644965403
https://github.com/google/deepvariant/issues/317#issuecomment-644965403:318,Usability,learn,learns,318,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB.; Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-644965403
https://github.com/google/deepvariant/issues/317#issuecomment-644965403:537,Usability,simpl,simple,537,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB.; Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-644965403
https://github.com/google/deepvariant/issues/317#issuecomment-644965403:685,Usability,learn,learn,685,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB.; Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-644965403
https://github.com/google/deepvariant/issues/317#issuecomment-645258677:495,Availability,error,errors,495,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously.; However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not.; Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors.; I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions?. On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-645258677
https://github.com/google/deepvariant/issues/317#issuecomment-645258677:587,Availability,error,errors,587,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously.; However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not.; Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors.; I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions?. On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-645258677
https://github.com/google/deepvariant/issues/317#issuecomment-645258677:667,Safety,detect,detected,667,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously.; However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not.; Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors.; I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions?. On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-645258677
https://github.com/google/deepvariant/issues/317#issuecomment-645804554:278,Availability,error,errors,278,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-645804554
https://github.com/google/deepvariant/issues/317#issuecomment-645804554:782,Availability,error,errors,782,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-645804554
https://github.com/google/deepvariant/issues/317#issuecomment-645804554:917,Availability,error,errors,917,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-645804554
https://github.com/google/deepvariant/issues/317#issuecomment-645804554:1023,Availability,error,errors,1023,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-645804554
https://github.com/google/deepvariant/issues/317#issuecomment-645804554:510,Integrability,contract,contraction,510,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/317#issuecomment-645804554
https://github.com/google/deepvariant/issues/318#issuecomment-645470394:701,Availability,down,downstream,701,"Hi @dhwani2410 . The VCF DeepVariant generates does contain hom-ref calls, which are labeled with ""RefCall"" instead of ""PASS"" in the FILTER column. These are the positions that DeepVariant evaluated candidate variants at (due to signal in some reads) but found to be hom-ref. The gVCF includes many more positions that were not evaluated and is only needed for specific applications. It uses blocks to keep the file size reasonably short. For more information on the gVCF format, see our documentation page on that topic: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md. From your question, it sounds like the VCF file has the information you need and works with the downstream tools you mentioned, so you may not need the gVCF file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318#issuecomment-645470394
https://github.com/google/deepvariant/issues/318#issuecomment-645516372:186,Modifiability,rewrite,rewrite,186,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md; @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp?. Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not; Just like GATK has BP resolution option when we run the variant caller. 2) ; #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318#issuecomment-645516372
https://github.com/google/deepvariant/issues/318#issuecomment-645516372:162,Usability,clear,clear,162,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md; @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp?. Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not; Just like GATK has BP resolution option when we run the variant caller. 2) ; #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318#issuecomment-645516372
https://github.com/google/deepvariant/issues/318#issuecomment-645810441:83,Availability,avail,available,83,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318#issuecomment-645810441
https://github.com/google/deepvariant/issues/318#issuecomment-645810441:1021,Usability,learn,learn,1021,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318#issuecomment-645810441
https://github.com/google/deepvariant/issues/318#issuecomment-645810441:1295,Usability,learn,learned,1295,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318#issuecomment-645810441
https://github.com/google/deepvariant/issues/318#issuecomment-645829089:194,Usability,clear,clear,194,"@AndrewCarroll Thanks for your response and the VCF example I have shared is my own data and not public data. Could you please refer to which part of my query you have answered? Maybe, I am not clear with your replies and which part does it address?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318#issuecomment-645829089
https://github.com/google/deepvariant/issues/318#issuecomment-646486563:362,Availability,down,downstream,362,"@AndrewCarroll ; _I think this request corresponds to wanting gVCF output. When you use the --output_gvcf option. This will write reference ranges for regions of the genome that did not produce any candidates for variant calling. I did not answer this question in the prior response._. I agree GVCF gives more information, but I need to process the VCF file and downstream processing tools accepts only VCF format. Just like in GATK we can get information for all bases using ""-ERC BP resolution"", is it possible here in deep variant to get information of all variant as well as non-variant sites in VCF format",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/318#issuecomment-646486563
https://github.com/google/deepvariant/issues/320#issuecomment-647629571:264,Performance,perform,performance,264,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:; 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) ; 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:; 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using.; 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail.; 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/320#issuecomment-647629571
https://github.com/google/deepvariant/issues/321#issuecomment-655676506:88,Usability,guid,guide,88,"Hi Sebastian, . That output doesn't look right. Did you follow [DeepVariant quick start guide](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md#notes-on-gpu-image) ?. Could you please provide the exact commands that you run?. Thank you; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-655676506
https://github.com/google/deepvariant/issues/321#issuecomment-656000987:897,Deployability,install,installed,897,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-656000987
https://github.com/google/deepvariant/issues/321#issuecomment-656000987:1013,Testability,log,logs,1013,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-656000987
https://github.com/google/deepvariant/issues/321#issuecomment-656000987:553,Usability,guid,guide,553,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-656000987
https://github.com/google/deepvariant/issues/321#issuecomment-671016803:376,Availability,mainten,maintenance-policy,376,"Hi @segoerge ; Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine; ```; gcloud compute instances create ""${USER}-debian-10-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image=debian-10-buster-v20200618 \; --image-project=debian-cloud \; --machine-type n1-standard-16 \; --zone ""us-west1-b""; ```. ## On the machine, install driver and docker; I did:; ```; curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x; ```; to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:; ```; pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown.; ERRO[0000] error waiting for container: context canceled ; ```; I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-671016803
https://github.com/google/deepvariant/issues/321#issuecomment-671016803:1170,Availability,Error,Error,1170,"Hi @segoerge ; Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine; ```; gcloud compute instances create ""${USER}-debian-10-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image=debian-10-buster-v20200618 \; --image-project=debian-cloud \; --machine-type n1-standard-16 \; --zone ""us-west1-b""; ```. ## On the machine, install driver and docker; I did:; ```; curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x; ```; to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:; ```; pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown.; ERRO[0000] error waiting for container: context canceled ; ```; I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-671016803
https://github.com/google/deepvariant/issues/321#issuecomment-671016803:1387,Availability,error,error,1387,"Hi @segoerge ; Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine; ```; gcloud compute instances create ""${USER}-debian-10-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image=debian-10-buster-v20200618 \; --image-project=debian-cloud \; --machine-type n1-standard-16 \; --zone ""us-west1-b""; ```. ## On the machine, install driver and docker; I did:; ```; curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x; ```; to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:; ```; pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown.; ERRO[0000] error waiting for container: context canceled ; ```; I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-671016803
https://github.com/google/deepvariant/issues/321#issuecomment-671016803:1477,Availability,error,error,1477,"Hi @segoerge ; Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine; ```; gcloud compute instances create ""${USER}-debian-10-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image=debian-10-buster-v20200618 \; --image-project=debian-cloud \; --machine-type n1-standard-16 \; --zone ""us-west1-b""; ```. ## On the machine, install driver and docker; I did:; ```; curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x; ```; to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:; ```; pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown.; ERRO[0000] error waiting for container: context canceled ; ```; I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-671016803
https://github.com/google/deepvariant/issues/321#issuecomment-671016803:1491,Availability,error,error,1491,"Hi @segoerge ; Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine; ```; gcloud compute instances create ""${USER}-debian-10-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image=debian-10-buster-v20200618 \; --image-project=debian-cloud \; --machine-type n1-standard-16 \; --zone ""us-west1-b""; ```. ## On the machine, install driver and docker; I did:; ```; curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x; ```; to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:; ```; pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown.; ERRO[0000] error waiting for container: context canceled ; ```; I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-671016803
https://github.com/google/deepvariant/issues/321#issuecomment-671016803:1558,Availability,error,error,1558,"Hi @segoerge ; Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine; ```; gcloud compute instances create ""${USER}-debian-10-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image=debian-10-buster-v20200618 \; --image-project=debian-cloud \; --machine-type n1-standard-16 \; --zone ""us-west1-b""; ```. ## On the machine, install driver and docker; I did:; ```; curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x; ```; to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:; ```; pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown.; ERRO[0000] error waiting for container: context canceled ; ```; I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-671016803
https://github.com/google/deepvariant/issues/321#issuecomment-671016803:606,Deployability,install,install,606,"Hi @segoerge ; Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine; ```; gcloud compute instances create ""${USER}-debian-10-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image=debian-10-buster-v20200618 \; --image-project=debian-cloud \; --machine-type n1-standard-16 \; --zone ""us-west1-b""; ```. ## On the machine, install driver and docker; I did:; ```; curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x; ```; to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:; ```; pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown.; ERRO[0000] error waiting for container: context canceled ; ```; I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-671016803
https://github.com/google/deepvariant/issues/321#issuecomment-671016803:825,Deployability,install,install,825,"Hi @segoerge ; Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine; ```; gcloud compute instances create ""${USER}-debian-10-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image=debian-10-buster-v20200618 \; --image-project=debian-cloud \; --machine-type n1-standard-16 \; --zone ""us-west1-b""; ```. ## On the machine, install driver and docker; I did:; ```; curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x; ```; to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:; ```; pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown.; ERRO[0000] error waiting for container: context canceled ; ```; I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-671016803
https://github.com/google/deepvariant/issues/321#issuecomment-671016803:1767,Testability,test,test,1767,"Hi @segoerge ; Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine; ```; gcloud compute instances create ""${USER}-debian-10-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image=debian-10-buster-v20200618 \; --image-project=debian-cloud \; --machine-type n1-standard-16 \; --zone ""us-west1-b""; ```. ## On the machine, install driver and docker; I did:; ```; curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x; ```; to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:; ```; pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi; docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown.; ERRO[0000] error waiting for container: context canceled ; ```; I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-671016803
https://github.com/google/deepvariant/issues/321#issuecomment-674744222:517,Availability,down,downgrade,517,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-674744222
https://github.com/google/deepvariant/issues/321#issuecomment-674744222:213,Deployability,install,install,213,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-674744222
https://github.com/google/deepvariant/issues/321#issuecomment-674744222:315,Deployability,install,installed,315,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-674744222
https://github.com/google/deepvariant/issues/321#issuecomment-674744222:303,Safety,detect,detect,303,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-674744222
https://github.com/google/deepvariant/issues/321#issuecomment-674744222:69,Testability,test,test,69,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-674744222
https://github.com/google/deepvariant/issues/321#issuecomment-675317201:261,Availability,error,error,261,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences!. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-675317201
https://github.com/google/deepvariant/issues/321#issuecomment-675317201:131,Deployability,configurat,configurations,131,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences!. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-675317201
https://github.com/google/deepvariant/issues/321#issuecomment-675317201:158,Deployability,install,installations,158,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences!. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-675317201
https://github.com/google/deepvariant/issues/321#issuecomment-675317201:131,Modifiability,config,configurations,131,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences!. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321#issuecomment-675317201
https://github.com/google/deepvariant/issues/324#issuecomment-658908897:244,Availability,avail,available,244,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324#issuecomment-658908897
https://github.com/google/deepvariant/issues/324#issuecomment-658908897:308,Deployability,release,release,308,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324#issuecomment-658908897
https://github.com/google/deepvariant/issues/324#issuecomment-659250952:112,Testability,benchmark,benchmarks,112,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324#issuecomment-659250952
https://github.com/google/deepvariant/issues/325#issuecomment-658938973:53,Testability,test,testdata,53,"Hi @abrozzi, a quick check: where is the `quickstart-testdata` directory located? The Docker command you are using expects it to be under `/root`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325#issuecomment-658938973
https://github.com/google/deepvariant/issues/325#issuecomment-659076993:316,Availability,error,error,316,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well.; * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```; sudo docker run \; -v ""/root/quickstart-testdata"":""/input"" \; -v ""/root/quickstart-output"":""/output"" \; google/deepvariant:latest \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \; --examples ""/output/make_examples.tfrecord@1.gz"" \; --gvcf ""/output/gvcf.tfrecord@1.gz"" \; --regions ""chr20:10,000,000-10,010,000""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325#issuecomment-659076993
https://github.com/google/deepvariant/issues/325#issuecomment-659076993:322,Integrability,message,message,322,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well.; * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```; sudo docker run \; -v ""/root/quickstart-testdata"":""/input"" \; -v ""/root/quickstart-output"":""/output"" \; google/deepvariant:latest \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \; --examples ""/output/make_examples.tfrecord@1.gz"" \; --gvcf ""/output/gvcf.tfrecord@1.gz"" \; --regions ""chr20:10,000,000-10,010,000""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325#issuecomment-659076993
https://github.com/google/deepvariant/issues/325#issuecomment-659076993:376,Testability,test,testdata,376,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well.; * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```; sudo docker run \; -v ""/root/quickstart-testdata"":""/input"" \; -v ""/root/quickstart-output"":""/output"" \; google/deepvariant:latest \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \; --examples ""/output/make_examples.tfrecord@1.gz"" \; --gvcf ""/output/gvcf.tfrecord@1.gz"" \; --regions ""chr20:10,000,000-10,010,000""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325#issuecomment-659076993
https://github.com/google/deepvariant/issues/325#issuecomment-659193187:221,Availability,error,error,221,"Hi Gunjan,; and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....).; Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```.; It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,; -A",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325#issuecomment-659193187
https://github.com/google/deepvariant/issues/325#issuecomment-659193187:76,Security,access,accessible,76,"Hi Gunjan,; and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....).; Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```.; It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,; -A",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325#issuecomment-659193187
https://github.com/google/deepvariant/issues/325#issuecomment-659193187:248,Usability,clear,clear,248,"Hi Gunjan,; and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....).; Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```.; It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,; -A",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325#issuecomment-659193187
https://github.com/google/deepvariant/issues/326#issuecomment-659742784:1039,Availability,down,download,1039,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326#issuecomment-659742784
https://github.com/google/deepvariant/issues/326#issuecomment-659742784:794,Deployability,Configurat,Configuration,794,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326#issuecomment-659742784
https://github.com/google/deepvariant/issues/326#issuecomment-659742784:137,Modifiability,config,config,137,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326#issuecomment-659742784
https://github.com/google/deepvariant/issues/326#issuecomment-659742784:166,Modifiability,config,config,166,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326#issuecomment-659742784
https://github.com/google/deepvariant/issues/326#issuecomment-659742784:794,Modifiability,Config,Configuration,794,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326#issuecomment-659742784
https://github.com/google/deepvariant/issues/326#issuecomment-659742784:1144,Modifiability,config,config,1144,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326#issuecomment-659742784
https://github.com/google/deepvariant/issues/326#issuecomment-660563647:198,Integrability,Depend,Depending,198,"We can be (that would imply `min_AQ1 > min_AQ2`), but based on our investigation the current settings were chosen to give the best overall quality for variety of cohort sizes and sequencing depths. Depending on how the cohort VCF is used, one may want to apply additional filters to the cohort, which can either be applied _after_ merging using a standard VCF modification tool (e.g. bcftools), or by changing the merging settings directly in the .yml file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326#issuecomment-660563647
https://github.com/google/deepvariant/issues/326#issuecomment-661427604:224,Availability,down,downloading,224,"FYI I submitted a pull request to GLnexus repo for the ""nomod"" preset for merging (no filters or genotype revision). https://github.com/dnanexus-rnd/GLnexus/pull/229 If this is accepted, you'll be able to try it out without downloading an external .yml file. @aderzelle Please let me know if you have any questions/comments related to this issue. If not, please feel free to close it :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326#issuecomment-661427604
https://github.com/google/deepvariant/issues/326#issuecomment-663874710:254,Availability,down,downloading,254,"> FYI I submitted a pull request to GLnexus repo for the ""nomod"" preset for merging (no filters or genotype revision). [dnanexus-rnd/GLnexus#229](https://github.com/dnanexus-rnd/GLnexus/pull/229) If this is accepted, you'll be able to try it out without downloading an external .yml file.; > ; > @aderzelle Please let me know if you have any questions/comments related to this issue. If not, please feel free to close it :). Thanks for your detail explanation! I think I have all I need ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326#issuecomment-663874710
https://github.com/google/deepvariant/issues/328#issuecomment-663252998:824,Availability,error,error-correction,824,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; ```; import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); shape_map_for_layers = reader.get_variable_to_shape_map(); print(shape_map_for_layers); ```; I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663252998
https://github.com/google/deepvariant/issues/328#issuecomment-663252998:58,Modifiability,layers,layers,58,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; ```; import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); shape_map_for_layers = reader.get_variable_to_shape_map(); print(shape_map_for_layers); ```; I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663252998
https://github.com/google/deepvariant/issues/328#issuecomment-663252998:461,Testability,test,tested,461,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; ```; import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); shape_map_for_layers = reader.get_variable_to_shape_map(); print(shape_map_for_layers); ```; I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663252998
https://github.com/google/deepvariant/issues/328#issuecomment-663252998:981,Usability,learn,learning,981,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; ```; import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); shape_map_for_layers = reader.get_variable_to_shape_map(); print(shape_map_for_layers); ```; I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663252998
https://github.com/google/deepvariant/issues/328#issuecomment-663306398:872,Availability,error,error-correction,872,"> Hi @X1angyang; > ; > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; > ; > ```; > import tensorflow as tf; > ; > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; > checkpoint_path = '/tmp/model.ckpt'; > ; > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); > shape_map_for_layers = reader.get_variable_to_shape_map(); > print(shape_map_for_layers); > ```; > ; > I just tested that in Colab (https://colab.research.google.com/).; > ; > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus.; > ; > I hope that helps!; > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663306398
https://github.com/google/deepvariant/issues/328#issuecomment-663306398:65,Modifiability,layers,layers,65,"> Hi @X1angyang; > ; > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; > ; > ```; > import tensorflow as tf; > ; > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; > checkpoint_path = '/tmp/model.ckpt'; > ; > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); > shape_map_for_layers = reader.get_variable_to_shape_map(); > print(shape_map_for_layers); > ```; > ; > I just tested that in Colab (https://colab.research.google.com/).; > ; > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus.; > ; > I hope that helps!; > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663306398
https://github.com/google/deepvariant/issues/328#issuecomment-663306398:502,Testability,test,tested,502,"> Hi @X1angyang; > ; > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; > ; > ```; > import tensorflow as tf; > ; > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; > checkpoint_path = '/tmp/model.ckpt'; > ; > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); > shape_map_for_layers = reader.get_variable_to_shape_map(); > print(shape_map_for_layers); > ```; > ; > I just tested that in Colab (https://colab.research.google.com/).; > ; > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus.; > ; > I hope that helps!; > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663306398
https://github.com/google/deepvariant/issues/328#issuecomment-663306398:1031,Usability,learn,learning,1031,"> Hi @X1angyang; > ; > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; > ; > ```; > import tensorflow as tf; > ; > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; > checkpoint_path = '/tmp/model.ckpt'; > ; > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); > shape_map_for_layers = reader.get_variable_to_shape_map(); > print(shape_map_for_layers); > ```; > ; > I just tested that in Colab (https://colab.research.google.com/).; > ; > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus.; > ; > I hope that helps!; > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328#issuecomment-663306398
https://github.com/google/deepvariant/issues/329#issuecomment-663696939:65,Availability,error,error,65,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329#issuecomment-663696939
https://github.com/google/deepvariant/issues/329#issuecomment-663696939:140,Availability,error,errors,140,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329#issuecomment-663696939
https://github.com/google/deepvariant/issues/329#issuecomment-663696939:235,Modifiability,variab,variability,235,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329#issuecomment-663696939
https://github.com/google/deepvariant/issues/331#issuecomment-667352024:96,Deployability,release,release,96,I should also mention that DeepVariant has had 6 (or more) channels since the first open source release (v0.4). The type of image mentioned in the 2018 paper was only used in the very first version of DeepVariant before the software was rewritten for open-source.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/331#issuecomment-667352024
https://github.com/google/deepvariant/issues/332#issuecomment-669330605:135,Deployability,release,release,135,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release.; After our next release, your VCF output will have a line that looks like this:; ```; ##DeepVariant_version=1.0.0; ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:; ```; docker run google/deepvariant:""${BIN_VERSION}"" --version; ```; to output the current version?. I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332#issuecomment-669330605
https://github.com/google/deepvariant/issues/332#issuecomment-669330605:160,Deployability,release,release,160,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release.; After our next release, your VCF output will have a line that looks like this:; ```; ##DeepVariant_version=1.0.0; ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:; ```; docker run google/deepvariant:""${BIN_VERSION}"" --version; ```; to output the current version?. I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332#issuecomment-669330605
https://github.com/google/deepvariant/issues/332#issuecomment-669331139:50,Deployability,release,release,50,I filed an issue internally to track. In our code release you should have both the version in VCF and the `--version` flag.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332#issuecomment-669331139
https://github.com/google/deepvariant/issues/333#issuecomment-671441195:398,Deployability,release,release,398,"Hi @marchoeppner ; Thank you for reporting this. Currently, MT is excluded in this code:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/exclude_contigs.py. We actually got many requests that they want to be able to call MT. So, we have already made the internal change to remove MT and chrM from that `exclude_contigs.py` file already. In our next release, you'll be able to call MT by default!. For the current codebase, if you want to call it, you'll need to remove it from code and build your own binaries. Sorry that we don't have a flag for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333#issuecomment-671441195
https://github.com/google/deepvariant/issues/333#issuecomment-695166040:34,Deployability,update,update,34,"Hi @marchoeppner ; To give you an update, this file now doesn't have MT https://github.com/google/deepvariant/blob/r1.0/deepvariant/exclude_contigs.py, which means you should be able to call MT now with version 1.0.0. It's removed in this commit: https://github.com/google/deepvariant/commit/7f4f2885059f7efebf83a098f187e32f87b0836a.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333#issuecomment-695166040
https://github.com/google/deepvariant/issues/334#issuecomment-673773009:308,Deployability,release,release,308,"Thanks for the suggestion @arostamianfar . Sounds like a good suggestion and should be easy to do : adding a flag to postprocess_variants. If `sample_name` is specified for run_deepvariant.py, we'll use it for both. . I'll file an internal issue to track this, and we should be able to have this in our next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334#issuecomment-673773009
https://github.com/google/deepvariant/issues/334#issuecomment-675909928:76,Availability,avail,available,76,The functionality has been implemented in the internal codebase and will be available in the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334#issuecomment-675909928
https://github.com/google/deepvariant/issues/334#issuecomment-675909928:98,Deployability,release,release,98,The functionality has been implemented in the internal codebase and will be available in the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334#issuecomment-675909928
https://github.com/google/deepvariant/issues/334#issuecomment-678050738:1491,Deployability,release,release,1491,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1 \; --sample_name=FOOBAR; ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```; $ zcat quickstart-output/output.vcf.gz | grep FOOBAR; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR; ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334#issuecomment-678050738
https://github.com/google/deepvariant/issues/334#issuecomment-679467513:254,Deployability,release,release,254,"Ah yes, thanks @arostamianfar for providing the `--regions ""chr20:1,000-10,000""` example and I can now reproduce the issue. It seems like my previous internal ""fix"" hasn't really fixed it, so I'll send in another fix, which will come out in the upcoming release :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334#issuecomment-679467513
https://github.com/google/deepvariant/issues/335#issuecomment-674131304:213,Security,access,access,213,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335#issuecomment-674131304
https://github.com/google/deepvariant/issues/335#issuecomment-674131304:407,Security,access,access,407,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335#issuecomment-674131304
https://github.com/google/deepvariant/issues/335#issuecomment-674188410:96,Deployability,install,install,96,That works for me (Ubuntu 18.04.1-based OS with Docker 19.03.6): https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335#issuecomment-674188410
https://github.com/google/deepvariant/issues/337#issuecomment-679213268:246,Security,access,accessed,246,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference?. Thanks!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337#issuecomment-679213268
https://github.com/google/deepvariant/issues/338#issuecomment-679221703:288,Availability,down,downsampled,288,"Since you already have 200X coverage of WGS data, the best quality variant calls would probably come from just taking the WGS data and running with that alone with `--model_type=WGS`. This is because the pileup images only fit up to 95 reads for each variant, so the 200X will already be downsampled during runtime to fit. The WES data will have some inherent biases due to the target enrichment process, so combining the datasets would carry its own potential issues. Luckily you don't need that since you have plenty of WGS data. Also I just want to check that you are only doing germline variant calling, since in my experience people with high coverage are often looking for low-frequency variants. DeepVariant is built and trained for germline variant calling only, and it can only report heterozygous, homozygous reference, and homozygous alternate genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-679221703
https://github.com/google/deepvariant/issues/338#issuecomment-680771609:324,Availability,down,downsampled,324,"Hi Maria,; thanks for your answer. I will thus not merge WGS and WES data, but analyze them separately instead. This way I can compare and have a look at the potential issues you mention. ; And thanks for checking: I am indeed looking at germline variant calling. A question regarding the pileup images you mention that are downsampled during runtime to 95x:; Here (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md#updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint) it says: ""_The biggest change was to move away from RGB-encoded (3-channel) pileup images and instead represent the aligned read data using a multi-channel tensor data layout. We currently represent the data as a 6-channel raw tensor in which we encode: ..._"" Is the downsampling to 95x still applied in the new design?. thanks, Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-680771609
https://github.com/google/deepvariant/issues/338#issuecomment-680771609:790,Availability,down,downsampling,790,"Hi Maria,; thanks for your answer. I will thus not merge WGS and WES data, but analyze them separately instead. This way I can compare and have a look at the potential issues you mention. ; And thanks for checking: I am indeed looking at germline variant calling. A question regarding the pileup images you mention that are downsampled during runtime to 95x:; Here (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md#updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint) it says: ""_The biggest change was to move away from RGB-encoded (3-channel) pileup images and instead represent the aligned read data using a multi-channel tensor data layout. We currently represent the data as a 6-channel raw tensor in which we encode: ..._"" Is the downsampling to 95x still applied in the new design?. thanks, Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-680771609
https://github.com/google/deepvariant/issues/338#issuecomment-680771609:443,Deployability,update,updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint,443,"Hi Maria,; thanks for your answer. I will thus not merge WGS and WES data, but analyze them separately instead. This way I can compare and have a look at the potential issues you mention. ; And thanks for checking: I am indeed looking at germline variant calling. A question regarding the pileup images you mention that are downsampled during runtime to 95x:; Here (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md#updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint) it says: ""_The biggest change was to move away from RGB-encoded (3-channel) pileup images and instead represent the aligned read data using a multi-channel tensor data layout. We currently represent the data as a 6-channel raw tensor in which we encode: ..._"" Is the downsampling to 95x still applied in the new design?. thanks, Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-680771609
https://github.com/google/deepvariant/issues/338#issuecomment-680960164:197,Availability,down,downsampling,197,"Sounds good!; Yes, we still call it a pileup ""image"", it is just in the form of a TensorFlow tensor with 6 channels instead of an RGB image, which was limited to 3 color channels plus opacity. The downsampling to 95x is still applied and has been in all versions of DeepVariant. This blog post gives more detail on how the pileup images are structured: https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-680960164
https://github.com/google/deepvariant/issues/338#issuecomment-681126260:260,Deployability,release,released,260,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:; Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:; In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m); * Accuracy (on case study chr20); - Indel F1: 0.983872 —> 0.982728; - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:; 1. Amplicon will likely use the height=100; 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-681126260
https://github.com/google/deepvariant/issues/338#issuecomment-681126260:579,Deployability,release,release,579,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:; Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:; In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m); * Accuracy (on case study chr20); - Indel F1: 0.983872 —> 0.982728; - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:; 1. Amplicon will likely use the height=100; 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-681126260
https://github.com/google/deepvariant/issues/338#issuecomment-681126260:799,Energy Efficiency,reduce,reduces,799,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:; Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:; In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m); * Accuracy (on case study chr20); - Indel F1: 0.983872 —> 0.982728; - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:; 1. Amplicon will likely use the height=100; 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-681126260
https://github.com/google/deepvariant/issues/338#issuecomment-681126260:1095,Performance,bottleneck,bottleneck,1095,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:; Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:; In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m); * Accuracy (on case study chr20); - Indel F1: 0.983872 —> 0.982728; - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:; 1. Amplicon will likely use the height=100; 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-681126260
https://github.com/google/deepvariant/issues/338#issuecomment-681126260:960,Usability,feedback,feedback,960,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:; Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:; In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m); * Accuracy (on case study chr20); - Indel F1: 0.983872 —> 0.982728; - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:; 1. Amplicon will likely use the height=100; 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338#issuecomment-681126260
https://github.com/google/deepvariant/issues/339#issuecomment-681163396:254,Performance,tune,tuned,254,"It looks like you are trying to analyze the graph outside of DeepVariant, which we don't currently have documentation on, since it isn't needed to run DeepVariant or even train it. But we do have some interpretability research we are working on, so stay tuned for that on the blog!. In the meantime, can you give a bit more context on what you are trying to do? Where did you find that code snippet?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-681163396
https://github.com/google/deepvariant/issues/339#issuecomment-681204545:83,Availability,avail,available,83,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545
https://github.com/google/deepvariant/issues/339#issuecomment-681204545:93,Availability,checkpoint,checkpoints,93,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545
https://github.com/google/deepvariant/issues/339#issuecomment-681204545:485,Availability,checkpoint,checkpoint,485,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545
https://github.com/google/deepvariant/issues/339#issuecomment-681204545:338,Deployability,configurat,configurations,338,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545
https://github.com/google/deepvariant/issues/339#issuecomment-681204545:338,Modifiability,config,configurations,338,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545
https://github.com/google/deepvariant/issues/339#issuecomment-681204545:634,Testability,test,testdata,634,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545
https://github.com/google/deepvariant/issues/339#issuecomment-681204545:227,Usability,guid,guidance,227,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545
https://github.com/google/deepvariant/issues/339#issuecomment-682069726:33,Availability,checkpoint,checkpoints,33,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim.; ```; ! pip install tf-slim. import tensorflow.compat.v1 as tf; import os; import tf_slim as slim; from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/; ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():; images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):; _, end_points = inception_v3.inception_v3(images, is_training=False, ; num_classes=3,; create_aux_logits=False); ; print(""end_points:""); print(end_points.keys()); # Restore the checkpoint; sess = tf.Session(graph=graph); saver = tf.train.Saver(); saver.restore(sess, ckpt_file); ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-682069726
https://github.com/google/deepvariant/issues/339#issuecomment-682069726:201,Availability,checkpoint,checkpoints,201,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim.; ```; ! pip install tf-slim. import tensorflow.compat.v1 as tf; import os; import tf_slim as slim; from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/; ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():; images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):; _, end_points = inception_v3.inception_v3(images, is_training=False, ; num_classes=3,; create_aux_logits=False); ; print(""end_points:""); print(end_points.keys()); # Restore the checkpoint; sess = tf.Session(graph=graph); saver = tf.train.Saver(); saver.restore(sess, ckpt_file); ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-682069726
https://github.com/google/deepvariant/issues/339#issuecomment-682069726:954,Availability,checkpoint,checkpoint,954,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim.; ```; ! pip install tf-slim. import tensorflow.compat.v1 as tf; import os; import tf_slim as slim; from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/; ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():; images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):; _, end_points = inception_v3.inception_v3(images, is_training=False, ; num_classes=3,; create_aux_logits=False); ; print(""end_points:""); print(end_points.keys()); # Restore the checkpoint; sess = tf.Session(graph=graph); saver = tf.train.Saver(); saver.restore(sess, ckpt_file); ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-682069726
https://github.com/google/deepvariant/issues/339#issuecomment-682069726:333,Deployability,install,install,333,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim.; ```; ! pip install tf-slim. import tensorflow.compat.v1 as tf; import os; import tf_slim as slim; from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/; ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():; images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):; _, end_points = inception_v3.inception_v3(images, is_training=False, ; num_classes=3,; create_aux_logits=False); ; print(""end_points:""); print(end_points.keys()); # Restore the checkpoint; sess = tf.Session(graph=graph); saver = tf.train.Saver(); saver.restore(sess, ckpt_file); ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-682069726
https://github.com/google/deepvariant/issues/339#issuecomment-682069726:1135,Testability,test,testdata,1135,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim.; ```; ! pip install tf-slim. import tensorflow.compat.v1 as tf; import os; import tf_slim as slim; from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/; ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():; images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):; _, end_points = inception_v3.inception_v3(images, is_training=False, ; num_classes=3,; create_aux_logits=False); ; print(""end_points:""); print(end_points.keys()); # Restore the checkpoint; sess = tf.Session(graph=graph); saver = tf.train.Saver(); saver.restore(sess, ckpt_file); ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339#issuecomment-682069726
https://github.com/google/deepvariant/issues/340#issuecomment-685893807:101,Testability,test,test,101,"Hi @jaydevshelat, this [doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-build-test.md) shows you how to build from source. Note that it mentions support for Ubuntu 14 and 16, so you may need to make some changes in order for it to work on your OS. Feel free to followup if you hit any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/340#issuecomment-685893807
https://github.com/google/deepvariant/issues/341#issuecomment-686052590:160,Deployability,release,released,160,"Hi @GuillaumeHolley, thanks for raising the issue. Could you try to run this with 1.0.0 image and let me know whether the issue persists? We haven't officially released 1.0.0 just yet, but we can use it to test this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341#issuecomment-686052590
https://github.com/google/deepvariant/issues/341#issuecomment-686052590:206,Testability,test,test,206,"Hi @GuillaumeHolley, thanks for raising the issue. Could you try to run this with 1.0.0 image and let me know whether the issue persists? We haven't officially released 1.0.0 just yet, but we can use it to test this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341#issuecomment-686052590
https://github.com/google/deepvariant/issues/341#issuecomment-686397941:114,Availability,error,error,114,"Hi @sgoe1,. Using the 1.0.0 image, the variant calling fails on nearly all of my input samples with the following error (sites vary from one sample to another):; ```; 2020-09-03 10:01:02.237494: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpqosj7aak/call_variants_output.tfrecord.gz; 2020-09-03 10:01:02.575525: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 63099; I0903 10:01:02.988874 140046207305472 postprocess_variants.py:1079] CVO sorting took 0.012529869874318441 minutes; I0903 10:01:02.989492 140046207305472 postprocess_variants.py:1081] Transforming call_variants_output to variants.; I0903 10:01:02.989910 140046207305472 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF.; I0903 10:01:02.996278 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.vcf.gz with NativeVcfWriter; I0903 10:01:02.998338 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter; W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT""; alternate_bases: ""CT""; alternate_bases: ""CTTT""; alternate_bases: ""CTTTTTTTCT""; calls {; info {; key: ""AD""; value {; values {; int_value: 7; }; values {; int_value: 7; }; values {; int_value: 0; }; values {; int_value: 0; }; }; }; info {; key: ""DP""; value {; values {; int_value: 19; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.3684210526315789; }; values {; number_value: 0.0; }; values {; number_value: 0.0; }; }; }; genotype: -1; genotype: -1; call_set_name: ""ID""; }; end: 21441513; reference_name: ""chr16""; start: 21441510; is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341#issuecomment-686397941
https://github.com/google/deepvariant/issues/341#issuecomment-686397941:3639,Safety,sanity check,sanity check,3639,"2], [1], [1, 2], [2]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main; vcf_writer, gvcf_writer); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 962, in merge_and_write_variants_and_nonvariants; variant = next_or_none(variant_iterable); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none; return next(iterable); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 110, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 807, in _transform_call_variants_output_to_variants; multiallelic_model=multiallelic_model); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 680, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341#issuecomment-686397941
https://github.com/google/deepvariant/issues/341#issuecomment-686397941:3705,Safety,sanity check,sanity check,3705,"2], [1], [1, 2], [2]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main; vcf_writer, gvcf_writer); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 962, in merge_and_write_variants_and_nonvariants; variant = next_or_none(variant_iterable); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none; return next(iterable); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 110, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 807, in _transform_call_variants_output_to_variants; multiallelic_model=multiallelic_model); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 680, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341#issuecomment-686397941
https://github.com/google/deepvariant/issues/342#issuecomment-696223840:85,Availability,failure,failure,85,"Apologies for no/late response. And Thank You for following-up. . My issue was : the failure to do ""docker build"" after git clone of the repo. . However, I am successful in getting the 0.8 docker image --> install the SW pre-reqs to recompile binaries inside docker --> export to new image for future use. . This issue should be closed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342#issuecomment-696223840
https://github.com/google/deepvariant/issues/342#issuecomment-696223840:206,Deployability,install,install,206,"Apologies for no/late response. And Thank You for following-up. . My issue was : the failure to do ""docker build"" after git clone of the repo. . However, I am successful in getting the 0.8 docker image --> install the SW pre-reqs to recompile binaries inside docker --> export to new image for future use. . This issue should be closed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342#issuecomment-696223840
https://github.com/google/deepvariant/issues/342#issuecomment-696230691:26,Deployability,update,update,26,@Mipsology Thanks for the update. I'll keep this issue closed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342#issuecomment-696230691
https://github.com/google/deepvariant/issues/343#issuecomment-688064227:823,Deployability,pipeline,pipeline,823,"Hi @karthick1087. DeepVariant will be able to run on a BAM file for one microbial species relative to its reference genome and should be able to produce a VCF file with variants. However, DeepVariant is designed with diploid variant calling in mind, so it will call HET variants, and it also not trained with subclonal variant fractions in mind. In addition, the training data for DeepVariant is diploid human data. So it hasn't really been designed to address the problem of calling on haploid microorganisms. External groups have benchmarked DeepVariant for bacterial variant calling alongside many other callers designed for this purpose (https://academic.oup.com/gigascience/article/9/2/giaa007/5728470). Given the limitations to DeepVariant mentioned above, it performs surprisingly well, but is not the most accurate pipeline. . It does, however, seem to have a high precision (DeepVariant with NextGenMap had the highest precision of any of the methods), but with lower recall. So I would not recommend DeepVariant as the only method used to analyze microbial data. However, given its high precision and given that it is a very different approach to other methods you might run, it could be of use to use DeepVariant as a secondary caller. In this use case, you would have a high degree of confidence in variants called by both DeepVariant and the alternative method. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/343#issuecomment-688064227
https://github.com/google/deepvariant/issues/343#issuecomment-688064227:766,Performance,perform,performs,766,"Hi @karthick1087. DeepVariant will be able to run on a BAM file for one microbial species relative to its reference genome and should be able to produce a VCF file with variants. However, DeepVariant is designed with diploid variant calling in mind, so it will call HET variants, and it also not trained with subclonal variant fractions in mind. In addition, the training data for DeepVariant is diploid human data. So it hasn't really been designed to address the problem of calling on haploid microorganisms. External groups have benchmarked DeepVariant for bacterial variant calling alongside many other callers designed for this purpose (https://academic.oup.com/gigascience/article/9/2/giaa007/5728470). Given the limitations to DeepVariant mentioned above, it performs surprisingly well, but is not the most accurate pipeline. . It does, however, seem to have a high precision (DeepVariant with NextGenMap had the highest precision of any of the methods), but with lower recall. So I would not recommend DeepVariant as the only method used to analyze microbial data. However, given its high precision and given that it is a very different approach to other methods you might run, it could be of use to use DeepVariant as a secondary caller. In this use case, you would have a high degree of confidence in variants called by both DeepVariant and the alternative method. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/343#issuecomment-688064227
https://github.com/google/deepvariant/issues/343#issuecomment-688064227:532,Testability,benchmark,benchmarked,532,"Hi @karthick1087. DeepVariant will be able to run on a BAM file for one microbial species relative to its reference genome and should be able to produce a VCF file with variants. However, DeepVariant is designed with diploid variant calling in mind, so it will call HET variants, and it also not trained with subclonal variant fractions in mind. In addition, the training data for DeepVariant is diploid human data. So it hasn't really been designed to address the problem of calling on haploid microorganisms. External groups have benchmarked DeepVariant for bacterial variant calling alongside many other callers designed for this purpose (https://academic.oup.com/gigascience/article/9/2/giaa007/5728470). Given the limitations to DeepVariant mentioned above, it performs surprisingly well, but is not the most accurate pipeline. . It does, however, seem to have a high precision (DeepVariant with NextGenMap had the highest precision of any of the methods), but with lower recall. So I would not recommend DeepVariant as the only method used to analyze microbial data. However, given its high precision and given that it is a very different approach to other methods you might run, it could be of use to use DeepVariant as a secondary caller. In this use case, you would have a high degree of confidence in variants called by both DeepVariant and the alternative method. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/343#issuecomment-688064227
https://github.com/google/deepvariant/issues/344#issuecomment-689698461:197,Testability,test,test,197,"Hi @JakeHagen ; Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344#issuecomment-689698461
https://github.com/google/deepvariant/issues/344#issuecomment-689698461:235,Usability,learn,learning,235,"Hi @JakeHagen ; Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344#issuecomment-689698461
https://github.com/google/deepvariant/issues/345#issuecomment-689891360:34,Availability,error,error,34,"Hi @sh940202123 ,; Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:; https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```; ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options; with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_1ip",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689891360
https://github.com/google/deepvariant/issues/345#issuecomment-689891360:4044,Availability,error,error,4044,".10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s; user 0m5.770s; sys 0m5.974s; I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1.; ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325; Can you see if this might be relevant to your issue?. If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689891360
https://github.com/google/deepvariant/issues/345#issuecomment-689891360:4548,Availability,error,error,4548,".10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s; user 0m5.770s; sys 0m5.974s; I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1.; ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325; Can you see if this might be relevant to your issue?. If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689891360
https://github.com/google/deepvariant/issues/345#issuecomment-689891360:40,Integrability,message,messages,40,"Hi @sh940202123 ,; Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:; https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```; ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options; with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_1ip",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689891360
https://github.com/google/deepvariant/issues/345#issuecomment-689891360:4050,Integrability,message,message,4050,".10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s; user 0m5.770s; sys 0m5.974s; I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1.; ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325; Can you see if this might be relevant to your issue?. If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689891360
https://github.com/google/deepvariant/issues/345#issuecomment-689891360:4148,Integrability,message,message,4148,".10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s; user 0m5.770s; sys 0m5.974s; I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1.; ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325; Can you see if this might be relevant to your issue?. If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689891360
https://github.com/google/deepvariant/issues/345#issuecomment-689891360:4554,Integrability,message,message,4554,".10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s; user 0m5.770s; sys 0m5.974s; I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1.; ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325; Can you see if this might be relevant to your issue?. If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689891360
https://github.com/google/deepvariant/issues/345#issuecomment-689891360:3966,Testability,log,log,3966,".10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s; user 0m5.770s; sys 0m5.974s; I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1.; ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325; Can you see if this might be relevant to your issue?. If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689891360
https://github.com/google/deepvariant/issues/345#issuecomment-689932270:168,Availability,error,error,168,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 ; But it still didn't show any additional error message like the issue ; I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh; $ BIN_VERSION=""1.0.0""; $ sudo docker pull google/deepvariant:""${BIN_VERSION}""; Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8; Status: Downloaded newer image for google/deepvariant:1.0.0; docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata""; $ ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output""; $ ls ${OUTPUT_DIR}; intermediate_results_dir. $ df -h; 檔案系統 容量 已用 可用 已用% 掛載點; udev 7.8G 0 7.8G 0% /dev; tmpfs 1.6G 11M 1.6G 1% /run; /dev/sda1 109G 95G 8.9G 92% /; tmpfs 7.9G 200K 7.9G 1% /dev/shm; tmpfs 5.0M 4.0K 5.0M 1% /run/lock; tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup; /dev/loop1 56M 56M 0 100% /snap/core18/1885; /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502; /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128; /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116; /dev/loop0 30M 30M 0 100% /snap/snapd/8790; /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506; /dev/loop6 55M 55M 0 100% /snap/core18/1880; /dev/loop7 30M 30M 0 100% /snap/snapd/8542; /dev/loop8 39M 39M 0 100% /snap/remmina/4309; /dev/loop9 40M 40M 0 100% /snap/remmina/4324; tmpfs 1.6G 40K 1.6G 1% /run/user/108; tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \; > --user root \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689932270
https://github.com/google/deepvariant/issues/345#issuecomment-689932270:477,Availability,Down,Downloaded,477,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 ; But it still didn't show any additional error message like the issue ; I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh; $ BIN_VERSION=""1.0.0""; $ sudo docker pull google/deepvariant:""${BIN_VERSION}""; Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8; Status: Downloaded newer image for google/deepvariant:1.0.0; docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata""; $ ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output""; $ ls ${OUTPUT_DIR}; intermediate_results_dir. $ df -h; 檔案系統 容量 已用 可用 已用% 掛載點; udev 7.8G 0 7.8G 0% /dev; tmpfs 1.6G 11M 1.6G 1% /run; /dev/sda1 109G 95G 8.9G 92% /; tmpfs 7.9G 200K 7.9G 1% /dev/shm; tmpfs 5.0M 4.0K 5.0M 1% /run/lock; tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup; /dev/loop1 56M 56M 0 100% /snap/core18/1885; /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502; /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128; /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116; /dev/loop0 30M 30M 0 100% /snap/snapd/8790; /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506; /dev/loop6 55M 55M 0 100% /snap/core18/1880; /dev/loop7 30M 30M 0 100% /snap/snapd/8542; /dev/loop8 39M 39M 0 100% /snap/remmina/4309; /dev/loop9 40M 40M 0 100% /snap/remmina/4324; tmpfs 1.6G 40K 1.6G 1% /run/user/108; tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \; > --user root \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689932270
https://github.com/google/deepvariant/issues/345#issuecomment-689932270:4617,Availability,error,error,4617,"cker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s; user 0m1.033s; sys 0m0.705s; I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message?. Best,; Jerry",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689932270
https://github.com/google/deepvariant/issues/345#issuecomment-689932270:174,Integrability,message,message,174,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 ; But it still didn't show any additional error message like the issue ; I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh; $ BIN_VERSION=""1.0.0""; $ sudo docker pull google/deepvariant:""${BIN_VERSION}""; Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8; Status: Downloaded newer image for google/deepvariant:1.0.0; docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata""; $ ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output""; $ ls ${OUTPUT_DIR}; intermediate_results_dir. $ df -h; 檔案系統 容量 已用 可用 已用% 掛載點; udev 7.8G 0 7.8G 0% /dev; tmpfs 1.6G 11M 1.6G 1% /run; /dev/sda1 109G 95G 8.9G 92% /; tmpfs 7.9G 200K 7.9G 1% /dev/shm; tmpfs 5.0M 4.0K 5.0M 1% /run/lock; tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup; /dev/loop1 56M 56M 0 100% /snap/core18/1885; /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502; /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128; /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116; /dev/loop0 30M 30M 0 100% /snap/snapd/8790; /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506; /dev/loop6 55M 55M 0 100% /snap/core18/1880; /dev/loop7 30M 30M 0 100% /snap/snapd/8542; /dev/loop8 39M 39M 0 100% /snap/remmina/4309; /dev/loop9 40M 40M 0 100% /snap/remmina/4324; tmpfs 1.6G 40K 1.6G 1% /run/user/108; tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \; > --user root \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689932270
https://github.com/google/deepvariant/issues/345#issuecomment-689932270:286,Integrability,message,message,286,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 ; But it still didn't show any additional error message like the issue ; I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh; $ BIN_VERSION=""1.0.0""; $ sudo docker pull google/deepvariant:""${BIN_VERSION}""; Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8; Status: Downloaded newer image for google/deepvariant:1.0.0; docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata""; $ ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output""; $ ls ${OUTPUT_DIR}; intermediate_results_dir. $ df -h; 檔案系統 容量 已用 可用 已用% 掛載點; udev 7.8G 0 7.8G 0% /dev; tmpfs 1.6G 11M 1.6G 1% /run; /dev/sda1 109G 95G 8.9G 92% /; tmpfs 7.9G 200K 7.9G 1% /dev/shm; tmpfs 5.0M 4.0K 5.0M 1% /run/lock; tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup; /dev/loop1 56M 56M 0 100% /snap/core18/1885; /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502; /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128; /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116; /dev/loop0 30M 30M 0 100% /snap/snapd/8790; /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506; /dev/loop6 55M 55M 0 100% /snap/core18/1880; /dev/loop7 30M 30M 0 100% /snap/snapd/8542; /dev/loop8 39M 39M 0 100% /snap/remmina/4309; /dev/loop9 40M 40M 0 100% /snap/remmina/4324; tmpfs 1.6G 40K 1.6G 1% /run/user/108; tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \; > --user root \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689932270
https://github.com/google/deepvariant/issues/345#issuecomment-689932270:4623,Integrability,message,message,4623,"cker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s; user 0m1.033s; sys 0m0.705s; I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message?. Best,; Jerry",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689932270
https://github.com/google/deepvariant/issues/345#issuecomment-689932270:597,Testability,test,testdata,597,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 ; But it still didn't show any additional error message like the issue ; I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh; $ BIN_VERSION=""1.0.0""; $ sudo docker pull google/deepvariant:""${BIN_VERSION}""; Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8; Status: Downloaded newer image for google/deepvariant:1.0.0; docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata""; $ ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output""; $ ls ${OUTPUT_DIR}; intermediate_results_dir. $ df -h; 檔案系統 容量 已用 可用 已用% 掛載點; udev 7.8G 0 7.8G 0% /dev; tmpfs 1.6G 11M 1.6G 1% /run; /dev/sda1 109G 95G 8.9G 92% /; tmpfs 7.9G 200K 7.9G 1% /dev/shm; tmpfs 5.0M 4.0K 5.0M 1% /run/lock; tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup; /dev/loop1 56M 56M 0 100% /snap/core18/1885; /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502; /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128; /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116; /dev/loop0 30M 30M 0 100% /snap/snapd/8790; /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506; /dev/loop6 55M 55M 0 100% /snap/core18/1880; /dev/loop7 30M 30M 0 100% /snap/snapd/8542; /dev/loop8 39M 39M 0 100% /snap/remmina/4309; /dev/loop9 40M 40M 0 100% /snap/remmina/4324; tmpfs 1.6G 40K 1.6G 1% /run/user/108; tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \; > --user root \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689932270
https://github.com/google/deepvariant/issues/345#issuecomment-689958395:66,Availability,error,error,66,"Hi @pichuan ~. I tried the code you provided but it print nothing error.; There is also nothing output file in the output folder. Here is the code:. ```sh; $ sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/make_examples \; > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \; > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \; > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \; > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \; > --regions chr20:10,000,000-10,010,000 \; > --task 0. $ ls; quickstart-output quickstart-testdata. $ ls quickstart-output/; intermediate_results_dir. ```. Best,; Jerry",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689958395
https://github.com/google/deepvariant/issues/345#issuecomment-689958395:670,Testability,test,testdata,670,"Hi @pichuan ~. I tried the code you provided but it print nothing error.; There is also nothing output file in the output folder. Here is the code:. ```sh; $ sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/make_examples \; > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \; > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \; > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \; > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \; > --regions chr20:10,000,000-10,010,000 \; > --task 0. $ ls; quickstart-output quickstart-testdata. $ ls quickstart-output/; intermediate_results_dir. ```. Best,; Jerry",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-689958395
https://github.com/google/deepvariant/issues/345#issuecomment-690806128:90,Availability,error,error,90,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:; ```; sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help; ```; And see if the information comes out correctly?. I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690806128
https://github.com/google/deepvariant/issues/345#issuecomment-690806128:96,Integrability,message,message,96,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:; ```; sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help; ```; And see if the information comes out correctly?. I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690806128
https://github.com/google/deepvariant/issues/345#issuecomment-690806128:149,Usability,simpl,simpler,149,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:; ```; sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help; ```; And see if the information comes out correctly?. I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690806128
https://github.com/google/deepvariant/issues/345#issuecomment-690820723:521,Availability,error,error,521,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690820723
https://github.com/google/deepvariant/issues/345#issuecomment-690820723:252,Deployability,release,release,252,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690820723
https://github.com/google/deepvariant/issues/345#issuecomment-690820723:679,Deployability,release,released,679,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690820723
https://github.com/google/deepvariant/issues/345#issuecomment-690820723:527,Integrability,message,message,527,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690820723
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:2832,Availability,Down,Downloaded,2832,"d 48a66213fe; ```. - Test run command; ```sh; # BIN_VERSION=""1.0.0""; # ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --intermediate_results_dir /output/intermediate_results_dir \; > --num_shards=1 \; >. Status: Downloaded newer image for google/deepvariant:1.0.0; I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs; I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:5415,Availability,checkpoint,checkpoint,5415,"at we will decode CRAM using the reference you passed in with --ref; 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]; I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants; I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s; user 0m5.898s; sys 0m3.792s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz; 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:; 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:1762,Deployability,release,release,1762,"evel : 15; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d; bogomips : 4595.05; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```; - OS ,kernel & docker version; ```sh; # uname -a; Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release; CentOS Linux release 7.7.1908 (Core). # docker -v; Docker version 19.03.12, build 48a66213fe; ```. - Test run command; ```sh; # BIN_VERSION=""1.0.0""; # ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --intermediate_results_dir /out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:1784,Deployability,release,release,1784,"evel : 15; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d; bogomips : 4595.05; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```; - OS ,kernel & docker version; ```sh; # uname -a; Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release; CentOS Linux release 7.7.1908 (Core). # docker -v; Docker version 19.03.12, build 48a66213fe; ```. - Test run command; ```sh; # BIN_VERSION=""1.0.0""; # ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --intermediate_results_dir /out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:1058,Energy Efficiency,monitor,monitor,1058,"command which @pichuan provided but it still print nothing on terminal. ; And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice.; Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info; ```text; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 63; model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz; stepping : 2; microcode : 0x43; cpu MHz : 1199.975; cache size : 25600 KB; physical id : 0; siblings : 20; core id : 0; cpu cores : 10; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 15; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d; bogomips : 4595.05; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```; - OS ,kernel & docker version; ```sh; # uname -a; Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release; CentOS Linux release 7.7.1908 (Core). # docker -v; Docker version 19.03.12, build 48a66213fe; ```. - Test run command; ```sh; # BIN_VERSION=""1.0.0""; # ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:1563,Energy Efficiency,power,power,1563,"; stepping : 2; microcode : 0x43; cpu MHz : 1199.975; cache size : 25600 KB; physical id : 0; siblings : 20; core id : 0; cpu cores : 10; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 15; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d; bogomips : 4595.05; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```; - OS ,kernel & docker version; ```sh; # uname -a; Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release; CentOS Linux release 7.7.1908 (Core). # docker -v; Docker version 19.03.12, build 48a66213fe; ```. - Test run command; ```sh; # BIN_VERSION=""1.0.0""; # ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:6657,Modifiability,config,config,6657,"re/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz; 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:; 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp; I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz; W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:7982,Modifiability,layers,layers,7982,"n_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz; W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions for updating:; If using Keras pass *_constraint arguments to layers.; W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:9228,Modifiability,layers,layers,9228,"ave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn.; W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn.; I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized.; I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt; I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op.; I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op.; I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA...; I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt; I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]; I0911 02:28:56.265293 13993768646",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:9235,Modifiability,layers,layers,9235,"ave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn.; W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn.; I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized.; I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt; I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op.; I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op.; I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA...; I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt; I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]; I0911 02:28:56.265293 13993768646",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:613,Performance,cache,cache,613,"command which @pichuan provided but it still print nothing on terminal. ; And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice.; Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info; ```text; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 63; model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz; stepping : 2; microcode : 0x43; cpu MHz : 1199.975; cache size : 25600 KB; physical id : 0; siblings : 20; core id : 0; cpu cores : 10; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 15; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d; bogomips : 4595.05; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```; - OS ,kernel & docker version; ```sh; # uname -a; Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release; CentOS Linux release 7.7.1908 (Core). # docker -v; Docker version 19.03.12, build 48a66213fe; ```. - Test run command; ```sh; # BIN_VERSION=""1.0.0""; # ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:5663,Performance,optimiz,optimized,5663,":39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]; I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants; I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s; user 0m5.898s; sys 0m3.792s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz; 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:; 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp; I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:5736,Performance,perform,performance,5736,":39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]; I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants; I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s; user 0m5.898s; sys 0m3.792s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz; 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:; 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp; I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:6414,Performance,Tune,Tune,6414,"odel.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz; 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:; 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp; I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:6463,Performance,perform,performance,6463,"odel.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz; 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:; 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp; I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:8977,Performance,optimiz,optimizations,8977,"788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn.; W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn.; I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized.; I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt; I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op.; I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op.; I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA...; I0911 02:28:54.011811 139937686464256 sav",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:269,Testability,test,test,269,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. ; And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice.; Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info; ```text; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 63; model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz; stepping : 2; microcode : 0x43; cpu MHz : 1199.975; cache size : 25600 KB; physical id : 0; siblings : 20; core id : 0; cpu cores : 10; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 15; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d; bogomips : 4595.05; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```; - OS ,kernel & docker version; ```sh; # uname -a; Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release; CentOS Linux release 7.7.1908 (Core). # docker -v; Docker version 19.03.12, build 48a66213fe; ```. - Test run command; ```sh; # BIN_VERSION=""1.0.0""; # ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/345#issuecomment-690842263:1872,Testability,Test,Test,1872,"xsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d; bogomips : 4595.05; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```; - OS ,kernel & docker version; ```sh; # uname -a; Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release; CentOS Linux release 7.7.1908 (Core). # docker -v; Docker version 19.03.12, build 48a66213fe; ```. - Test run command; ```sh; # BIN_VERSION=""1.0.0""; # ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --intermediate_results_dir /output/intermediate_results_dir \; > --num_shards=1 \; >. Status: Downloaded newer image for google/deepvariant:1.0.0; I0911 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263
https://github.com/google/deepvariant/issues/346#issuecomment-692271819:1135,Availability,down,downstream,1135,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-692271819
https://github.com/google/deepvariant/issues/346#issuecomment-692271819:798,Deployability,update,update,798,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-692271819
https://github.com/google/deepvariant/issues/346#issuecomment-692271819:831,Deployability,update,update,831,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-692271819
https://github.com/google/deepvariant/issues/346#issuecomment-692271819:1289,Deployability,update,update,1289,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-692271819
https://github.com/google/deepvariant/issues/346#issuecomment-694394538:25,Usability,feedback,feedback,25,"Hi Ted,; Thanks for your feedback. Just to be sure I've got this right. You confirm that variants with 0/0 genotype and zero DP in the merged VCF are actually positions with no reads in that sample and so can be set to missing?; I have also an additional comment about deepvariant v1.0.0. I've noticed that it is much slower than v.0.9.0. I've used them both on the sample samples (30-60X WGS) using singularity and v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample. Is this expected?. Many thanks!; Edoardo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-694394538
https://github.com/google/deepvariant/issues/346#issuecomment-695121760:860,Performance,perform,performance,860,"Hi Edoardo, in the current implementation, `DP=0` (which is used in variant records) will imply that there is no read covering that position, while `MIN_DP=0` (used in hom. ref. band in gVCF) does not necessarily imply that there is no read covering the reference band, due to consolidation of multiple hom.ref. records into one record for the reason stated in my previous response (as the name suggests, it means the *minimum* of DPs in all of the consolidated records were 0). Please also note that reads that don't satisfy the minimum mapping quality threshold (set to 5 by default [here](https://github.com/google/deepvariant/blob/r1.0/deepvariant/make_examples.py#L219)), don't count for DP/MIN_DP as far as I know. Again, we agree with your suggestion about separating out the `DP=0` regions into a separate record and setting the genotype to `./.`. The performance issue you mentioned is not expected as far as I know, and I asked my colleague to follow up on that aspect. I hope this help and please let me know if I can help with anything else. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-695121760
https://github.com/google/deepvariant/issues/346#issuecomment-695126772:1008,Deployability,release,release,1008,"p with the runtime increase question. There are a few reasons I can think of that might have increased overall runtime between v0.9.0 and v1.0.0, but the magnitude of increase you're observing is not quite what I'd expected. First, let's take a quick comparison on our GitHub documentation:; - https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md : 82m+189m+36m = 307m; - https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase.; 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most.; 2. On exactly",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-695126772
https://github.com/google/deepvariant/issues/346#issuecomment-695126772:1061,Deployability,release,releases,1061,"n. There are a few reasons I can think of that might have increased overall runtime between v0.9.0 and v1.0.0, but the magnitude of increase you're observing is not quite what I'd expected. First, let's take a quick comparison on our GitHub documentation:; - https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md : 82m+189m+36m = 307m; - https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase.; 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most.; 2. On exactly the same BAM, just with different v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-695126772
https://github.com/google/deepvariant/issues/346#issuecomment-695126772:1419,Deployability,release,release,1419,"b.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase.; 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most.; 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF?; For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh); ```; $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l; 7753721; $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l; 91714977; ```; And here is the breakdown of P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-695126772
https://github.com/google/deepvariant/issues/346#issuecomment-695126772:1472,Deployability,release,releases,1472,"docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase.; 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most.; 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF?; For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh); ```; $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l; 7753721; $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l; 91714977; ```; And here is the breakdown of PASS/RefCall:; ```; $ zcat HG002.outp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-695126772
https://github.com/google/deepvariant/issues/346#issuecomment-698786904:41,Testability,test,test,41,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:; - **v0.9.0**: 290+1494+281 = 2065min (~34h); - **v1.0.0**: 335+1487+300 = 2122min (~35h); - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:; - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall); - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall); - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):; ```; singularity exec \; --bind /data/ref/genomes/GRCh38:/genomes \; --bind /data/projects/HICF2_project/BAM:/bam_files \; /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \; --reads=""/bam_files/${bamfile}"" \; --output_vcf=""VCF/${sampleID}.vcf.gz"" \; --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \; --intermediate_results_dir=""tmp_data"" \; --num_shards=10; ```. I've uploaded the log files from the 3 runs if you want to take a look:; [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log); [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log); [deepvar_0.9.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-698786904
https://github.com/google/deepvariant/issues/346#issuecomment-698786904:296,Testability,test,test,296,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:; - **v0.9.0**: 290+1494+281 = 2065min (~34h); - **v1.0.0**: 335+1487+300 = 2122min (~35h); - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:; - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall); - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall); - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):; ```; singularity exec \; --bind /data/ref/genomes/GRCh38:/genomes \; --bind /data/projects/HICF2_project/BAM:/bam_files \; /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \; --reads=""/bam_files/${bamfile}"" \; --output_vcf=""VCF/${sampleID}.vcf.gz"" \; --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \; --intermediate_results_dir=""tmp_data"" \; --num_shards=10; ```. I've uploaded the log files from the 3 runs if you want to take a look:; [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log); [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log); [deepvar_0.9.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-698786904
https://github.com/google/deepvariant/issues/346#issuecomment-698786904:527,Testability,TEST,TEST,527,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:; - **v0.9.0**: 290+1494+281 = 2065min (~34h); - **v1.0.0**: 335+1487+300 = 2122min (~35h); - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:; - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall); - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall); - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):; ```; singularity exec \; --bind /data/ref/genomes/GRCh38:/genomes \; --bind /data/projects/HICF2_project/BAM:/bam_files \; /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \; --reads=""/bam_files/${bamfile}"" \; --output_vcf=""VCF/${sampleID}.vcf.gz"" \; --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \; --intermediate_results_dir=""tmp_data"" \; --num_shards=10; ```. I've uploaded the log files from the 3 runs if you want to take a look:; [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log); [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log); [deepvar_0.9.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-698786904
https://github.com/google/deepvariant/issues/346#issuecomment-698786904:799,Testability,TEST,TEST,799,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:; - **v0.9.0**: 290+1494+281 = 2065min (~34h); - **v1.0.0**: 335+1487+300 = 2122min (~35h); - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:; - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall); - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall); - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):; ```; singularity exec \; --bind /data/ref/genomes/GRCh38:/genomes \; --bind /data/projects/HICF2_project/BAM:/bam_files \; /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \; --reads=""/bam_files/${bamfile}"" \; --output_vcf=""VCF/${sampleID}.vcf.gz"" \; --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \; --intermediate_results_dir=""tmp_data"" \; --num_shards=10; ```. I've uploaded the log files from the 3 runs if you want to take a look:; [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log); [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log); [deepvar_0.9.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-698786904
https://github.com/google/deepvariant/issues/346#issuecomment-698786904:1735,Testability,log,log,1735,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:; - **v0.9.0**: 290+1494+281 = 2065min (~34h); - **v1.0.0**: 335+1487+300 = 2122min (~35h); - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:; - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall); - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall); - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):; ```; singularity exec \; --bind /data/ref/genomes/GRCh38:/genomes \; --bind /data/projects/HICF2_project/BAM:/bam_files \; /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \; --reads=""/bam_files/${bamfile}"" \; --output_vcf=""VCF/${sampleID}.vcf.gz"" \; --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \; --intermediate_results_dir=""tmp_data"" \; --num_shards=10; ```. I've uploaded the log files from the 3 runs if you want to take a look:; [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log); [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log); [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-698786904
https://github.com/google/deepvariant/issues/346#issuecomment-698786904:1812,Testability,log,log,1812,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:; - **v0.9.0**: 290+1494+281 = 2065min (~34h); - **v1.0.0**: 335+1487+300 = 2122min (~35h); - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:; - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall); - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall); - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):; ```; singularity exec \; --bind /data/ref/genomes/GRCh38:/genomes \; --bind /data/projects/HICF2_project/BAM:/bam_files \; /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \; --reads=""/bam_files/${bamfile}"" \; --output_vcf=""VCF/${sampleID}.vcf.gz"" \; --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \; --intermediate_results_dir=""tmp_data"" \; --num_shards=10; ```. I've uploaded the log files from the 3 runs if you want to take a look:; [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log); [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log); [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-698786904
https://github.com/google/deepvariant/issues/346#issuecomment-698786904:1890,Testability,log,log,1890,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:; - **v0.9.0**: 290+1494+281 = 2065min (~34h); - **v1.0.0**: 335+1487+300 = 2122min (~35h); - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:; - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall); - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall); - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):; ```; singularity exec \; --bind /data/ref/genomes/GRCh38:/genomes \; --bind /data/projects/HICF2_project/BAM:/bam_files \; /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \; --reads=""/bam_files/${bamfile}"" \; --output_vcf=""VCF/${sampleID}.vcf.gz"" \; --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \; --intermediate_results_dir=""tmp_data"" \; --num_shards=10; ```. I've uploaded the log files from the 3 runs if you want to take a look:; [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log); [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log); [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-698786904
https://github.com/google/deepvariant/issues/346#issuecomment-698786904:1911,Testability,log,log,1911,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:; - **v0.9.0**: 290+1494+281 = 2065min (~34h); - **v1.0.0**: 335+1487+300 = 2122min (~35h); - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:; - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall); - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall); - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):; ```; singularity exec \; --bind /data/ref/genomes/GRCh38:/genomes \; --bind /data/projects/HICF2_project/BAM:/bam_files \; /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \; --reads=""/bam_files/${bamfile}"" \; --output_vcf=""VCF/${sampleID}.vcf.gz"" \; --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \; --intermediate_results_dir=""tmp_data"" \; --num_shards=10; ```. I've uploaded the log files from the 3 runs if you want to take a look:; [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log); [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log); [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-698786904
https://github.com/google/deepvariant/issues/346#issuecomment-698786904:1982,Testability,log,log,1982,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:; - **v0.9.0**: 290+1494+281 = 2065min (~34h); - **v1.0.0**: 335+1487+300 = 2122min (~35h); - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:; - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall); - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall); - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):; ```; singularity exec \; --bind /data/ref/genomes/GRCh38:/genomes \; --bind /data/projects/HICF2_project/BAM:/bam_files \; /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \; --reads=""/bam_files/${bamfile}"" \; --output_vcf=""VCF/${sampleID}.vcf.gz"" \; --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \; --intermediate_results_dir=""tmp_data"" \; --num_shards=10; ```. I've uploaded the log files from the 3 runs if you want to take a look:; [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log); [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log); [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-698786904
https://github.com/google/deepvariant/issues/346#issuecomment-698786904:2003,Testability,log,log,2003,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:; - **v0.9.0**: 290+1494+281 = 2065min (~34h); - **v1.0.0**: 335+1487+300 = 2122min (~35h); - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:; - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall); - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall); - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):; ```; singularity exec \; --bind /data/ref/genomes/GRCh38:/genomes \; --bind /data/projects/HICF2_project/BAM:/bam_files \; /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \; --reads=""/bam_files/${bamfile}"" \; --output_vcf=""VCF/${sampleID}.vcf.gz"" \; --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \; --intermediate_results_dir=""tmp_data"" \; --num_shards=10; ```. I've uploaded the log files from the 3 runs if you want to take a look:; [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log); [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log); [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-698786904
https://github.com/google/deepvariant/issues/346#issuecomment-698786904:2074,Testability,log,log,2074,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:; - **v0.9.0**: 290+1494+281 = 2065min (~34h); - **v1.0.0**: 335+1487+300 = 2122min (~35h); - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:; - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall); - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall); - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):; ```; singularity exec \; --bind /data/ref/genomes/GRCh38:/genomes \; --bind /data/projects/HICF2_project/BAM:/bam_files \; /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \; --reads=""/bam_files/${bamfile}"" \; --output_vcf=""VCF/${sampleID}.vcf.gz"" \; --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \; --intermediate_results_dir=""tmp_data"" \; --num_shards=10; ```. I've uploaded the log files from the 3 runs if you want to take a look:; [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log); [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log); [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-698786904
https://github.com/google/deepvariant/issues/346#issuecomment-700190764:90,Testability,TEST,TEST,90,"Thanks @edg1983 ; Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**; 1. make_examples runtime: 290/198 = about 1.5 times; 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries; 3. #entries in gVCF: 531371190/213244705 = about 2.5 times; 4. call_variants runtime: 1494/456 = about 3.3 times; 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:; (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs.; (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me.; (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-700190764
https://github.com/google/deepvariant/issues/346#issuecomment-700190764:884,Testability,benchmark,benchmarking,884,"Thanks @edg1983 ; Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**; 1. make_examples runtime: 290/198 = about 1.5 times; 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries; 3. #entries in gVCF: 531371190/213244705 = about 2.5 times; 4. call_variants runtime: 1494/456 = about 3.3 times; 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:; (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs.; (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me.; (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-700190764
https://github.com/google/deepvariant/issues/346#issuecomment-700196639:346,Performance,optimiz,optimizations,346,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-700196639
https://github.com/google/deepvariant/issues/346#issuecomment-700196639:83,Testability,log,log,83,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-700196639
https://github.com/google/deepvariant/issues/346#issuecomment-700196941:161,Energy Efficiency,reduce,reduce,161,"And an follow up on gVCF file size, (thanks to @tedyun for reminding me of the flag):. There is a flag in `gvcf_gq_binsize` in make_examples that you can set to reduce the size of gVCF file, the tradeoff being the gVCF file size & runtime vs. more accurate representation of GQ in each hom. ref. site.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-700196941
https://github.com/google/deepvariant/issues/346#issuecomment-704106130:22,Deployability,update,update,22,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-704106130
https://github.com/google/deepvariant/issues/346#issuecomment-704106130:387,Energy Efficiency,allocate,allocate,387,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-704106130
https://github.com/google/deepvariant/issues/346#issuecomment-704106130:236,Testability,test,test,236,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346#issuecomment-704106130
https://github.com/google/deepvariant/issues/347#issuecomment-693053180:407,Availability,error,error,407,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347#issuecomment-693053180
https://github.com/google/deepvariant/issues/347#issuecomment-693053180:377,Testability,log,logic,377,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347#issuecomment-693053180
https://github.com/google/deepvariant/issues/347#issuecomment-693053180:618,Usability,Feedback,Feedback,618,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347#issuecomment-693053180
https://github.com/google/deepvariant/issues/347#issuecomment-693080237:1253,Availability,error,error,1253,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and; resequencing projects will move towards HiFi reads rather than CLR reads.; However, there is a lot of CLR sequencing data that has been generated in; the past couple of years and continues to be produced currently and could; still be useful for groups without the means to resequence using the novel; HiFi reads. So, I definitely see a niche in a large part of the; bioinformatics community that do a lot of data reusing (nowadays data; parasites). So, if there is anything we can do to help you n development,; please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<; notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>; >; > For the question about multi-allelic heterozygous calls - yes, DeepVariant; > is able to all 1/2 events, and will represent these in one line as a GT 1/2; > call in the VCF.; >; > For CLR calling in DeepVariant. It is theoretically possible for us to; > make a model for DeepVariant that can call CLR data. However, this requires; > us to write a special candidate generation logic to deal with the higher; > error rate. Based on what we perceive for the direction of future use in; > the genomics community, we think that data generated will be increasingly; > HiFi, so we have not been able to highly prioritize CLR models. Feedback; > from users like yourself will be useful to us in evaluating if that; > prioritization makes sense. For now, I can't commit to a timeframe under; > which we would support a PacBio CLR model.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347#issuecomment-693080237
https://github.com/google/deepvariant/issues/347#issuecomment-693080237:1220,Testability,log,logic,1220,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and; resequencing projects will move towards HiFi reads rather than CLR reads.; However, there is a lot of CLR sequencing data that has been generated in; the past couple of years and continues to be produced currently and could; still be useful for groups without the means to resequence using the novel; HiFi reads. So, I definitely see a niche in a large part of the; bioinformatics community that do a lot of data reusing (nowadays data; parasites). So, if there is anything we can do to help you n development,; please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<; notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>; >; > For the question about multi-allelic heterozygous calls - yes, DeepVariant; > is able to all 1/2 events, and will represent these in one line as a GT 1/2; > call in the VCF.; >; > For CLR calling in DeepVariant. It is theoretically possible for us to; > make a model for DeepVariant that can call CLR data. However, this requires; > us to write a special candidate generation logic to deal with the higher; > error rate. Based on what we perceive for the direction of future use in; > the genomics community, we think that data generated will be increasingly; > HiFi, so we have not been able to highly prioritize CLR models. Feedback; > from users like yourself will be useful to us in evaluating if that; > prioritization makes sense. For now, I can't commit to a timeframe under; > which we would support a PacBio CLR model.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347#issuecomment-693080237
https://github.com/google/deepvariant/issues/347#issuecomment-693080237:1470,Usability,Feedback,Feedback,1470,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and; resequencing projects will move towards HiFi reads rather than CLR reads.; However, there is a lot of CLR sequencing data that has been generated in; the past couple of years and continues to be produced currently and could; still be useful for groups without the means to resequence using the novel; HiFi reads. So, I definitely see a niche in a large part of the; bioinformatics community that do a lot of data reusing (nowadays data; parasites). So, if there is anything we can do to help you n development,; please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<; notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>; >; > For the question about multi-allelic heterozygous calls - yes, DeepVariant; > is able to all 1/2 events, and will represent these in one line as a GT 1/2; > call in the VCF.; >; > For CLR calling in DeepVariant. It is theoretically possible for us to; > make a model for DeepVariant that can call CLR data. However, this requires; > us to write a special candidate generation logic to deal with the higher; > error rate. Based on what we perceive for the direction of future use in; > the genomics community, we think that data generated will be increasingly; > HiFi, so we have not been able to highly prioritize CLR models. Feedback; > from users like yourself will be useful to us in evaluating if that; > prioritization makes sense. For now, I can't commit to a timeframe under; > which we would support a PacBio CLR model.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347#issuecomment-693080237
https://github.com/google/deepvariant/issues/347#issuecomment-696512260:92,Integrability,message,message,92,Hi @jdmontenegro . I am going to close this issue for now. I will make a note to send you a message if/when we can revisit the CLR model. Thank you for your perspective on what data the community has and what will be valuable to them.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347#issuecomment-696512260
https://github.com/google/deepvariant/issues/351#issuecomment-696510915:193,Availability,avail,available,193,"Hi @ahgillmo . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. . We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351#issuecomment-696510915
https://github.com/google/deepvariant/issues/351#issuecomment-888743997:200,Availability,avail,available,200,"> Hi @ahgillmo; > ; > We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality.; > ; > We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Where can I find the prototype and try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351#issuecomment-888743997
https://github.com/google/deepvariant/issues/351#issuecomment-1019480983:510,Performance,perform,performs,510,">We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Hi, I think I have a decent set of multi-tumor samples that would allow us to define ground truth sets for somatic clonal and somatic sub-clonal mutations, (and also pick up on potential noise from our inhouse variant caller). ; @AndrewCarroll, @pichuan - I would be interested in testing out a prototype version of your somatic caller, or perhaps collaborating to see how it performs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351#issuecomment-1019480983
https://github.com/google/deepvariant/issues/351#issuecomment-1019480983:415,Testability,test,testing,415,">We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Hi, I think I have a decent set of multi-tumor samples that would allow us to define ground truth sets for somatic clonal and somatic sub-clonal mutations, (and also pick up on potential noise from our inhouse variant caller). ; @AndrewCarroll, @pichuan - I would be interested in testing out a prototype version of your somatic caller, or perhaps collaborating to see how it performs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351#issuecomment-1019480983
https://github.com/google/deepvariant/issues/352#issuecomment-696511944:191,Availability,avail,available,191,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352#issuecomment-696511944
https://github.com/google/deepvariant/issues/352#issuecomment-696511944:639,Deployability,release,released,639,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352#issuecomment-696511944
https://github.com/google/deepvariant/issues/353#issuecomment-696330532:92,Safety,avoid,avoid,92,Thanks for reporting this. ; I removed the gs://deepvariant/singularity_images directory to avoid future confusion.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/353#issuecomment-696330532
https://github.com/google/deepvariant/issues/354#issuecomment-695943329:9,Availability,error,error,9,The same error happens with `google/deepvariant:1.0.0`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354#issuecomment-695943329
https://github.com/google/deepvariant/issues/354#issuecomment-696243149:92,Deployability,release,release,92,"Hi @Redmar-van-den-Berg, thanks for reporting this! We will look into the issue and plan to release a fix in the next version of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354#issuecomment-696243149
https://github.com/google/deepvariant/issues/354#issuecomment-698468546:97,Deployability,release,release,97,"@Redmar-van-den-Berg this issue has been fixed internally, and the fix will be out with the next release. When gVCF records are present, we will try to extract the sample name from those, instead of using 'default'. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354#issuecomment-698468546
https://github.com/google/deepvariant/issues/355#issuecomment-696846763:235,Availability,Error,Error,235,"Hi @anands-repo, can you share more details on the following?. - Operating system; - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version; - Command used to build; - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355#issuecomment-696846763
https://github.com/google/deepvariant/issues/355#issuecomment-697093712:251,Availability,ERROR,ERROR,251,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows; ```; (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform ; Traceback (most recent call last): ; File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>; from clif.python.proto import start ; File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>; from clif.python.utils import proto_util ; ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:; ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355#issuecomment-697093712
https://github.com/google/deepvariant/issues/355#issuecomment-697093712:419,Availability,error,error,419,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows; ```; (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform ; Traceback (most recent call last): ; File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>; from clif.python.proto import start ; File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>; from clif.python.utils import proto_util ; ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:; ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355#issuecomment-697093712
https://github.com/google/deepvariant/issues/355#issuecomment-697093712:1565,Modifiability,variab,variable,1565,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows; ```; (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform ; Traceback (most recent call last): ; File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>; from clif.python.proto import start ; File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>; from clif.python.utils import proto_util ; ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:; ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355#issuecomment-697093712
https://github.com/google/deepvariant/issues/355#issuecomment-697093712:455,Performance,cache,cache,455,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows; ```; (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform ; Traceback (most recent call last): ; File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>; from clif.python.proto import start ; File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>; from clif.python.utils import proto_util ; ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:; ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355#issuecomment-697093712
https://github.com/google/deepvariant/issues/355#issuecomment-697093712:1357,Testability,test,test,1357,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows; ```; (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform ; Traceback (most recent call last): ; File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>; from clif.python.proto import start ; File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>; from clif.python.utils import proto_util ; ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:; ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355#issuecomment-697093712
https://github.com/google/deepvariant/issues/355#issuecomment-697537665:0,Deployability,Install,Installing,0,"Installing everything to standard path or updating ldconf solves the issue. I still do not have the whole build working, because bazel is looking for standard python libraries (which is not what I am using) to be used. Overall it seems the installation expects everything to be in standard paths which I believe is a little too stringent, and it may be possible to fix that without too much trouble may be. But my original issues are no longer issues, so closing this ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355#issuecomment-697537665
https://github.com/google/deepvariant/issues/355#issuecomment-697537665:240,Deployability,install,installation,240,"Installing everything to standard path or updating ldconf solves the issue. I still do not have the whole build working, because bazel is looking for standard python libraries (which is not what I am using) to be used. Overall it seems the installation expects everything to be in standard paths which I believe is a little too stringent, and it may be possible to fix that without too much trouble may be. But my original issues are no longer issues, so closing this ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355#issuecomment-697537665
https://github.com/google/deepvariant/issues/356#issuecomment-698549305:213,Usability,simpl,simplest,213,"Hi @anands-repo, glad you were able to get it working! I don't have any other comments on the fix and will defer to the relevant bazel issue. In general, I would recommend running DeepVariant using Docker for the simplest setup. If you are building from source because you want to experiment with changes to the codebase, I'd still recommend Docker. You can clone the DeepVariant repo, modify the source code, and build a Docker image with your changes using [the provided Dockerfile](https://github.com/google/deepvariant/blob/r1.0/Dockerfile).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356#issuecomment-698549305
https://github.com/google/deepvariant/issues/356#issuecomment-699007209:1001,Availability,error,error,1001,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:; ```; # Bazel's --build_python_zip replaces our carefully engineered symbolic links; # with copies. This function puts the symbolic links back.; function fix_zip_file {; orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place.; TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX); # The .zip version of the binary doesn't have the header that makes it; # self-executable. We use that version because otherwise unzip would; # complain and raise an error code.; cp ""${orig_zip_file}.zip"" ""${TMPDIR}""; ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356#issuecomment-699007209
https://github.com/google/deepvariant/issues/356#issuecomment-699007209:93,Deployability,install,installation,93,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:; ```; # Bazel's --build_python_zip replaces our carefully engineered symbolic links; # with copies. This function puts the symbolic links back.; function fix_zip_file {; orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place.; TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX); # The .zip version of the binary doesn't have the header that makes it; # self-executable. We use that version because otherwise unzip would; # complain and raise an error code.; cp ""${orig_zip_file}.zip"" ""${TMPDIR}""; ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356#issuecomment-699007209
https://github.com/google/deepvariant/issues/356#issuecomment-699007209:71,Energy Efficiency,power,power,71,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:; ```; # Bazel's --build_python_zip replaces our carefully engineered symbolic links; # with copies. This function puts the symbolic links back.; function fix_zip_file {; orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place.; TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX); # The .zip version of the binary doesn't have the header that makes it; # self-executable. We use that version because otherwise unzip would; # complain and raise an error code.; cp ""${orig_zip_file}.zip"" ""${TMPDIR}""; ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356#issuecomment-699007209
https://github.com/google/deepvariant/issues/356#issuecomment-699007209:520,Performance,perform,performing,520,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:; ```; # Bazel's --build_python_zip replaces our carefully engineered symbolic links; # with copies. This function puts the symbolic links back.; function fix_zip_file {; orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place.; TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX); # The .zip version of the binary doesn't have the header that makes it; # self-executable. We use that version because otherwise unzip would; # complain and raise an error code.; cp ""${orig_zip_file}.zip"" ""${TMPDIR}""; ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356#issuecomment-699007209
https://github.com/google/deepvariant/issues/356#issuecomment-699007209:267,Testability,test,tests,267,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:; ```; # Bazel's --build_python_zip replaces our carefully engineered symbolic links; # with copies. This function puts the symbolic links back.; function fix_zip_file {; orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place.; TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX); # The .zip version of the binary doesn't have the header that makes it; # self-executable. We use that version because otherwise unzip would; # complain and raise an error code.; cp ""${orig_zip_file}.zip"" ""${TMPDIR}""; ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356#issuecomment-699007209
https://github.com/google/deepvariant/issues/357#issuecomment-698486292:546,Performance,perform,perform,546,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357#issuecomment-698486292
https://github.com/google/deepvariant/issues/357#issuecomment-698486292:611,Performance,perform,perform,611,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357#issuecomment-698486292
https://github.com/google/deepvariant/issues/357#issuecomment-698486292:519,Usability,clear,clear,519,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357#issuecomment-698486292
https://github.com/google/deepvariant/issues/357#issuecomment-698608583:114,Availability,robust,robust,114,"Hi!. I read both of blog posts, and I could see that even the primary goal is working with human data, DV is very robust. My specie is diploid, so Iguess it helps, and the variant density should no be too different from humam or rice. . I Actually have two datasets; one is the megagametophyte (1n) of the same individuals (2n). Unfortunatelly, at this point, I don't have a gold dataset to train the data. I'm trying to get at least a truth set to help me in this quest. . Thanks for enlightening",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357#issuecomment-698608583
https://github.com/google/deepvariant/issues/357#issuecomment-698738740:111,Deployability,release,released,111,"@leorippel since your data is diploid and of similar variant density to human data, you could try applying the released DeepVariant models. That said, we can't be sure how well the models will perform on this data. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357#issuecomment-698738740
https://github.com/google/deepvariant/issues/357#issuecomment-698738740:193,Performance,perform,perform,193,"@leorippel since your data is diploid and of similar variant density to human data, you could try applying the released DeepVariant models. That said, we can't be sure how well the models will perform on this data. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357#issuecomment-698738740
https://github.com/google/deepvariant/issues/358#issuecomment-703346071:9849,Availability,avail,available,9849,f-00030.gz; -rw-r--r-- 1 root root 52K Sep 24 15:47 make_examples.tfrecord-00000-of-00030.gz; -rw-r--r-- 1 root root 5.9K Sep 24 15:47 gvcf.tfrecord-00003-of-00030.gz; -rw-r--r-- 1 root root 5.7K Sep 24 15:47 gvcf.tfrecord-00004-of-00030.gz; -rw-r--r-- 1 root root 3.6K Sep 24 15:47 gvcf.tfrecord-00022-of-00030.gz; -rw-r--r-- 1 root root 4.5K Sep 24 15:47 gvcf.tfrecord-00025-of-00030.gz; -rw-r--r-- 1 root root 22K Sep 24 15:47 make_examples.tfrecord-00004-of-00030.gz; -rw-r--r-- 1 root root 31K Sep 24 15:47 make_examples.tfrecord-00003-of-00030.gz; -rw-r--r-- 1 root root 16K Sep 24 15:47 make_examples.tfrecord-00022-of-00030.gz; -rw-r--r-- 1 root root 6.2K Sep 24 15:47 gvcf.tfrecord-00008-of-00030.gz; -rw-r--r-- 1 root root 56K Sep 24 15:47 make_examples.tfrecord-00008-of-00030.gz; -rw-r--r-- 1 root root 7.5K Sep 24 15:47 make_examples.tfrecord-00025-of-00030.gz; -rw-r--r-- 1 root root 4.6K Sep 24 15:47 gvcf.tfrecord-00026-of-00030.gz; -rw-r--r-- 1 root root 38K Sep 24 15:47 make_examples.tfrecord-00026-of-00030.gz; -rw-r--r-- 1 root root 5.2K Sep 24 15:47 gvcf.tfrecord-00002-of-00030.gz; -rw-r--r-- 1 root root 32K Sep 24 15:47 make_examples.tfrecord-00002-of-00030.gz; -rw-r--r-- 1 root root 7.0K Sep 24 15:47 gvcf.tfrecord-00007-of-00030.gz; -rw-r--r-- 1 root root 3.0K Sep 24 15:47 gvcf.tfrecord-00024-of-00030.gz; -rw-r--r-- 1 root root 44K Sep 24 15:47 make_examples.tfrecord-00007-of-00030.gz; -rw-r--r-- 1 root root 11K Sep 24 15:47 make_examples.tfrecord-00024-of-00030.gz; -rw-r--r-- 1 root root 5.3K Sep 24 15:47 gvcf.tfrecord-00013-of-00030.gz; -rw-r--r-- 1 root root 27K Sep 24 15:47 make_examples.tfrecord-00013-of-00030.gz; -rw-r--r-- 1 root root 4.9K Sep 24 15:47 gvcf.tfrecord-00005-of-00030.gz; -rw-r--r-- 1 root root 20K Sep 24 15:47 make_examples.tfrecord-00005-of-00030.gz. ```; This is how my `free -h` is looking like right now:. ```; total used free shared buff/cache available; Mem: 125G 123G 776M 224M 933M 354M; Swap: 101G 100G 1.7G. ```. Thank you. Cheers.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358#issuecomment-703346071
https://github.com/google/deepvariant/issues/358#issuecomment-703346071:9843,Performance,cache,cache,9843,f-00030.gz; -rw-r--r-- 1 root root 52K Sep 24 15:47 make_examples.tfrecord-00000-of-00030.gz; -rw-r--r-- 1 root root 5.9K Sep 24 15:47 gvcf.tfrecord-00003-of-00030.gz; -rw-r--r-- 1 root root 5.7K Sep 24 15:47 gvcf.tfrecord-00004-of-00030.gz; -rw-r--r-- 1 root root 3.6K Sep 24 15:47 gvcf.tfrecord-00022-of-00030.gz; -rw-r--r-- 1 root root 4.5K Sep 24 15:47 gvcf.tfrecord-00025-of-00030.gz; -rw-r--r-- 1 root root 22K Sep 24 15:47 make_examples.tfrecord-00004-of-00030.gz; -rw-r--r-- 1 root root 31K Sep 24 15:47 make_examples.tfrecord-00003-of-00030.gz; -rw-r--r-- 1 root root 16K Sep 24 15:47 make_examples.tfrecord-00022-of-00030.gz; -rw-r--r-- 1 root root 6.2K Sep 24 15:47 gvcf.tfrecord-00008-of-00030.gz; -rw-r--r-- 1 root root 56K Sep 24 15:47 make_examples.tfrecord-00008-of-00030.gz; -rw-r--r-- 1 root root 7.5K Sep 24 15:47 make_examples.tfrecord-00025-of-00030.gz; -rw-r--r-- 1 root root 4.6K Sep 24 15:47 gvcf.tfrecord-00026-of-00030.gz; -rw-r--r-- 1 root root 38K Sep 24 15:47 make_examples.tfrecord-00026-of-00030.gz; -rw-r--r-- 1 root root 5.2K Sep 24 15:47 gvcf.tfrecord-00002-of-00030.gz; -rw-r--r-- 1 root root 32K Sep 24 15:47 make_examples.tfrecord-00002-of-00030.gz; -rw-r--r-- 1 root root 7.0K Sep 24 15:47 gvcf.tfrecord-00007-of-00030.gz; -rw-r--r-- 1 root root 3.0K Sep 24 15:47 gvcf.tfrecord-00024-of-00030.gz; -rw-r--r-- 1 root root 44K Sep 24 15:47 make_examples.tfrecord-00007-of-00030.gz; -rw-r--r-- 1 root root 11K Sep 24 15:47 make_examples.tfrecord-00024-of-00030.gz; -rw-r--r-- 1 root root 5.3K Sep 24 15:47 gvcf.tfrecord-00013-of-00030.gz; -rw-r--r-- 1 root root 27K Sep 24 15:47 make_examples.tfrecord-00013-of-00030.gz; -rw-r--r-- 1 root root 4.9K Sep 24 15:47 gvcf.tfrecord-00005-of-00030.gz; -rw-r--r-- 1 root root 20K Sep 24 15:47 make_examples.tfrecord-00005-of-00030.gz. ```; This is how my `free -h` is looking like right now:. ```; total used free shared buff/cache available; Mem: 125G 123G 776M 224M 933M 354M; Swap: 101G 100G 1.7G. ```. Thank you. Cheers.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358#issuecomment-703346071
https://github.com/google/deepvariant/issues/358#issuecomment-704411609:651,Safety,sanity check,sanity check,651,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358#issuecomment-704411609
https://github.com/google/deepvariant/issues/358#issuecomment-708002351:400,Energy Efficiency,consumption,consumption,400,"> @leorippel got it, thanks for the clarification! I'm not sure what the issue is in this case, but if you are able to use a machine with more memory, that would be the easiest option. Another option is to shard your data by chromosome and run DeepVariant separately for each chromosome (or groups of chromosomes). I took the intermediate results and ran into a bigger machine. It worked. The memory consumption rised to 435G !! by far the hungry step. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358#issuecomment-708002351
https://github.com/google/deepvariant/issues/359#issuecomment-701664438:344,Integrability,depend,dependencies,344,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first.; Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359#issuecomment-701664438
https://github.com/google/deepvariant/issues/359#issuecomment-701664438:570,Testability,test,test,570,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first.; Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359#issuecomment-701664438
https://github.com/google/deepvariant/issues/360#issuecomment-707423529:206,Availability,error,errors,206,"Hi @pichuan . I am not running on Google Cloud, but on a local machine. So I went with the default runner. When I use DataflowRunner the shuffle script requests arguments relevant to GCS. For example I get errors such as:. ```Invalid GCS path (<PATH>), given for the option: temp_location```. <PATH> here is actually a valid path in my machine. Kindly advise. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360#issuecomment-707423529
https://github.com/google/deepvariant/issues/360#issuecomment-707424697:333,Availability,error,error,333,"Command that works, but runs out of memory is this:. ```; python $SCRIPTPATH/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN \; --output_pattern_prefix=$OUTPUT_PREFIX \; --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \; --output_dataset_name=$OUTPUT_DATASET_NAME; ```. Command that doesn't run and gives error:; ```; python $SCRIPTPATH/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN \; --output_pattern_prefix=$OUTPUT_PREFIX \; --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \; --output_dataset_name=$OUTPUT_DATASET_NAME \; --job_name=$JOBNAME \; --project=$PROJECT_NAME \; --temp_location=$TEMPLOCATION \; --save_main_session \; --region us-east1; ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360#issuecomment-707424697
https://github.com/google/deepvariant/issues/360#issuecomment-707424697:722,Availability,error,error,722,"Command that works, but runs out of memory is this:. ```; python $SCRIPTPATH/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN \; --output_pattern_prefix=$OUTPUT_PREFIX \; --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \; --output_dataset_name=$OUTPUT_DATASET_NAME; ```. Command that doesn't run and gives error:; ```; python $SCRIPTPATH/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN \; --output_pattern_prefix=$OUTPUT_PREFIX \; --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \; --output_dataset_name=$OUTPUT_DATASET_NAME \; --job_name=$JOBNAME \; --project=$PROJECT_NAME \; --temp_location=$TEMPLOCATION \; --save_main_session \; --region us-east1; ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360#issuecomment-707424697
https://github.com/google/deepvariant/issues/360#issuecomment-707424697:1100,Availability,avail,available,1100,"Command that works, but runs out of memory is this:. ```; python $SCRIPTPATH/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN \; --output_pattern_prefix=$OUTPUT_PREFIX \; --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \; --output_dataset_name=$OUTPUT_DATASET_NAME; ```. Command that doesn't run and gives error:; ```; python $SCRIPTPATH/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN \; --output_pattern_prefix=$OUTPUT_PREFIX \; --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \; --output_dataset_name=$OUTPUT_DATASET_NAME \; --job_name=$JOBNAME \; --project=$PROJECT_NAME \; --temp_location=$TEMPLOCATION \; --save_main_session \; --region us-east1; ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360#issuecomment-707424697
https://github.com/google/deepvariant/issues/360#issuecomment-707444280:289,Integrability,depend,depend,289,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360#issuecomment-707444280
https://github.com/google/deepvariant/issues/360#issuecomment-713149241:611,Safety,sanity check,sanity check,611,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360#issuecomment-713149241
https://github.com/google/deepvariant/issues/360#issuecomment-713149241:589,Usability,simpl,simply,589,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360#issuecomment-713149241
https://github.com/google/deepvariant/issues/360#issuecomment-713149241:661,Usability,simpl,simply,661,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360#issuecomment-713149241
https://github.com/google/deepvariant/issues/360#issuecomment-713149241:725,Usability,simpl,simply,725,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360#issuecomment-713149241
https://github.com/google/deepvariant/issues/360#issuecomment-1019990366:263,Availability,Down,Downside,263,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360#issuecomment-1019990366
https://github.com/google/deepvariant/issues/360#issuecomment-2420163031:197,Modifiability,portab,portability,197,"@pichuan when shuffle the datasets using local runner, `direct_num_workers` is set to 0, it will use all the local CPUs.; I got this warning that make me thinking; ```; WARNING:apache_beam.runners.portability.fn_api_runner.fn_runner:If direct_num_workers is not equal to 1, direct_running_mode should be `multi_processing` or `multi_threading` instead of `in_memory` in order for it to have the desired worker parallelism effect. direct_num_workers: 8 ; running_mode: in_memory; ```. Is there a reason we use `in_memory` rather than other modes?. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360#issuecomment-2420163031
https://github.com/google/deepvariant/issues/361#issuecomment-709562825:787,Availability,checkpoint,checkpoint,787,"Hi @aderzelle ,; I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:; https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`; For example:. ```; sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""; ```. With this extra arg, I do see that:; ```; I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10; 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {; key: ""cpu"" ; value: 1; }; intra_op_parallelism_threads: 1; inter_op_parallelism_threads: 1; ```; But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:; `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361#issuecomment-709562825
https://github.com/google/deepvariant/issues/361#issuecomment-709562825:244,Deployability,configurat,configuration-,244,"Hi @aderzelle ,; I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:; https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`; For example:. ```; sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""; ```. With this extra arg, I do see that:; ```; I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10; 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {; key: ""cpu"" ; value: 1; }; intra_op_parallelism_threads: 1; inter_op_parallelism_threads: 1; ```; But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:; `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361#issuecomment-709562825
https://github.com/google/deepvariant/issues/361#issuecomment-709562825:52,Modifiability,config,config,52,"Hi @aderzelle ,; I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:; https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`; For example:. ```; sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""; ```. With this extra arg, I do see that:; ```; I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10; 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {; key: ""cpu"" ; value: 1; }; intra_op_parallelism_threads: 1; inter_op_parallelism_threads: 1; ```; But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:; `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361#issuecomment-709562825
https://github.com/google/deepvariant/issues/361#issuecomment-709562825:244,Modifiability,config,configuration-,244,"Hi @aderzelle ,; I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:; https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`; For example:. ```; sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""; ```. With this extra arg, I do see that:; ```; I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10; 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {; key: ""cpu"" ; value: 1; }; intra_op_parallelism_threads: 1; inter_op_parallelism_threads: 1; ```; But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:; `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361#issuecomment-709562825
https://github.com/google/deepvariant/issues/361#issuecomment-709562825:1055,Modifiability,config,config,1055,"Hi @aderzelle ,; I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:; https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`; For example:. ```; sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""; ```. With this extra arg, I do see that:; ```; I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10; 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {; key: ""cpu"" ; value: 1; }; intra_op_parallelism_threads: 1; inter_op_parallelism_threads: 1; ```; But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:; `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361#issuecomment-709562825
https://github.com/google/deepvariant/issues/362#issuecomment-709713660:524,Usability,clear,clear,524,"Hi @loipf , ; You can use `--help` with the different binaries, for example:. ```; docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --help; ```. To see the various binaries, you can use:; ```; docker run google/deepvariant:1.0.0 ls /opt/deepvariant/bin/; ```; to list all the binaries. Then, for example, you can run:. ```; docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/show_examples --help; ```. or any other binaries. I understand your point though. It might be easier if we have a more clear help page without knowing the structure. I'll think about this and see if we can improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362#issuecomment-709713660
https://github.com/google/deepvariant/issues/362#issuecomment-737653058:75,Deployability,release,release,75,"Hi @loipf ; I've made changes in internal code. It'll come out in the next release.; After the next release, feel free to let us know if have more feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362#issuecomment-737653058
https://github.com/google/deepvariant/issues/362#issuecomment-737653058:100,Deployability,release,release,100,"Hi @loipf ; I've made changes in internal code. It'll come out in the next release.; After the next release, feel free to let us know if have more feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362#issuecomment-737653058
https://github.com/google/deepvariant/issues/362#issuecomment-737653058:147,Usability,feedback,feedback,147,"Hi @loipf ; I've made changes in internal code. It'll come out in the next release.; After the next release, feel free to let us know if have more feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/362#issuecomment-737653058
https://github.com/google/deepvariant/pull/363#issuecomment-709411831:136,Deployability,pipeline,pipeline,136,"Hi! We would like to propose optional OpenVINO backend for `call_variants` step. Do you accept external PRs?. Also, this PR has Actions pipeline. Feel free to enable by https://github.com/google/deepvariant/actions to run it. /cc @pichuan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709411831
https://github.com/google/deepvariant/pull/363#issuecomment-709920659:674,Deployability,release,release,674,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709920659
https://github.com/google/deepvariant/pull/363#issuecomment-709920659:1378,Deployability,update,updates,1378,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709920659
https://github.com/google/deepvariant/pull/363#issuecomment-709920659:1033,Performance,perform,perform,1033,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709920659
https://github.com/google/deepvariant/pull/363#issuecomment-709920659:1588,Performance,perform,performance,1588,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709920659
https://github.com/google/deepvariant/pull/363#issuecomment-709920659:878,Testability,benchmark,benchmark,878,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709920659
https://github.com/google/deepvariant/pull/363#issuecomment-709941019:492,Deployability,release,release,492,"@AndrewCarroll, many thanks for such quick response!. > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
https://github.com/google/deepvariant/pull/363#issuecomment-709941019:1790,Deployability,update,updates,1790,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
https://github.com/google/deepvariant/pull/363#issuecomment-709941019:2041,Energy Efficiency,reduce,reduce,2041,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
https://github.com/google/deepvariant/pull/363#issuecomment-709941019:1148,Performance,optimiz,optimizations,1148,"s. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
https://github.com/google/deepvariant/pull/363#issuecomment-709941019:1260,Performance,perform,perform,1260,"s, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
https://github.com/google/deepvariant/pull/363#issuecomment-709941019:2169,Performance,perform,perform,2169,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
https://github.com/google/deepvariant/pull/363#issuecomment-709941019:880,Testability,benchmark,benchmark,880,"@AndrewCarroll, many thanks for such quick response!. > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
https://github.com/google/deepvariant/pull/363#issuecomment-709941019:1072,Testability,benchmark,benchmark,1072,"ternal PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
https://github.com/google/deepvariant/pull/363#issuecomment-709941019:1609,Testability,test,test,1609,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
https://github.com/google/deepvariant/pull/363#issuecomment-709941019:2006,Testability,log,logic,2006,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
https://github.com/google/deepvariant/pull/363#issuecomment-709941019:2185,Testability,test,tests,2185,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
https://github.com/google/deepvariant/pull/363#issuecomment-709941019:2423,Testability,log,logs,2423,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
https://github.com/google/deepvariant/pull/363#issuecomment-709941019:1002,Usability,learn,learning,1002,"@AndrewCarroll, many thanks for such quick response!. > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019
https://github.com/google/deepvariant/pull/363#issuecomment-710709303:557,Deployability,update,update,557,"Hi @dkurt ,; thank you for sending this PR. . From the discussion between you and Andrew above, here is my current summary:. 1. You are planning to do more benchmarking on this change, and will let us know when you have some numbers on runtime improvement.; 2. You want to know whether we're interested in enabling GitHub Actions. For 2., I am not familiar with GitHub Actions, but it seems interesting! I'll file an internal issue to look into this. This will likely fall under a lower priority, but I want to let you know that we'll track it and give you update if any. If there are more details that you wish to contact directly, please feel free to email me at pichuan@google.com. We can also continue to communicate here to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-710709303
https://github.com/google/deepvariant/pull/363#issuecomment-710709303:156,Testability,benchmark,benchmarking,156,"Hi @dkurt ,; thank you for sending this PR. . From the discussion between you and Andrew above, here is my current summary:. 1. You are planning to do more benchmarking on this change, and will let us know when you have some numbers on runtime improvement.; 2. You want to know whether we're interested in enabling GitHub Actions. For 2., I am not familiar with GitHub Actions, but it seems interesting! I'll file an internal issue to look into this. This will likely fall under a lower priority, but I want to let you know that we'll track it and give you update if any. If there are more details that you wish to contact directly, please feel free to email me at pichuan@google.com. We can also continue to communicate here to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-710709303
https://github.com/google/deepvariant/pull/363#issuecomment-712402658:15,Deployability,update,update,15,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```; ...; W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>; 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>; W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>; I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz; I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]; I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]; I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-712402658
https://github.com/google/deepvariant/pull/363#issuecomment-712402658:1039,Performance,Tune,Tune,1039," the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```; ...; W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>; 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>; W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>; I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz; I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]; I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]; I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec pe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-712402658
https://github.com/google/deepvariant/pull/363#issuecomment-712402658:1426,Performance,optimiz,optimizations,1426,"re is a strange thing I found in my log:. ```; ...; W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>; 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>; W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>; I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz; I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]; I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]; I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]; I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]; I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]; I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]; ...; ```. The st",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-712402658
https://github.com/google/deepvariant/pull/363#issuecomment-712402658:490,Testability,log,log,490,"Hi @dkurt , an update and a question for you:. I was curious about the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```; ...; W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>; 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>; W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>; I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz; I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]; I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]; I1019 09:14:07.867975 140673783289600 call_var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-712402658
https://github.com/google/deepvariant/pull/363#issuecomment-712402658:2604,Testability,log,log,2604,"_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>; 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>; W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>; I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz; I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]; I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]; I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]; I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]; I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]; I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]; ...; ```. The strange thing is: ; There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? ; But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-712402658
https://github.com/google/deepvariant/pull/363#issuecomment-712794761:145,Safety,predict,predicted,145,"@pichuan, It might be an effect of current implementation - all the processing is done at iterator initialization and then `__getitem__` returns predicted results without delay. We will take a look at overall efficiency and check what can be improved here, thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-712794761
https://github.com/google/deepvariant/pull/363#issuecomment-723242914:1886,Availability,Down,Download,1886,"erpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions); ```bash; wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb ; dpkg -x parallel_20161222-1_all.deb parallel; export PATH=$HOME/parallel/usr/bin:$PATH; ```. 5. Install TensorFlow MKL-DNN; ```bash; WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl; wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}""; pip3 install --upgrade ""/tmp/${WHEEL_NAME}""; ```. 6. Run; ```bash; export INPUT_DIR=""${PWD}/quickstart-testdata""; export OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914
https://github.com/google/deepvariant/pull/363#issuecomment-723242914:993,Deployability,pipeline,pipeline,993,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |; | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permiss",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914
https://github.com/google/deepvariant/pull/363#issuecomment-723242914:1599,Deployability,patch,patches,1599,"m6.662s<br>user 0m7.946s<br>sys 0m4.841s |; | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions); ```bash; wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb ; dpkg -x parallel_20161222-1_all.deb parallel; export PATH=$HOME/parallel/usr/bin:$PATH; ```. 5. Install TensorFlow MKL-DNN; ```bash; WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl; wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}""; pip3 install --upgrade ""/tmp/${WHEEL_NAME}""; ```. 6. Run; ```bash; export INPUT_DIR=""${PWD}/quickstart-testdata""; export OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH; python3 ./ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914
https://github.com/google/deepvariant/pull/363#issuecomment-723242914:2188,Deployability,Install,Install,2188,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions); ```bash; wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb ; dpkg -x parallel_20161222-1_all.deb parallel; export PATH=$HOME/parallel/usr/bin:$PATH; ```. 5. Install TensorFlow MKL-DNN; ```bash; WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl; wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}""; pip3 install --upgrade ""/tmp/${WHEEL_NAME}""; ```. 6. Run; ```bash; export INPUT_DIR=""${PWD}/quickstart-testdata""; export OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --call_variants_extra_args=""use_openvino=True"" \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914
https://github.com/google/deepvariant/pull/363#issuecomment-723242914:2395,Deployability,install,install,2395,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions); ```bash; wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb ; dpkg -x parallel_20161222-1_all.deb parallel; export PATH=$HOME/parallel/usr/bin:$PATH; ```. 5. Install TensorFlow MKL-DNN; ```bash; WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl; wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}""; pip3 install --upgrade ""/tmp/${WHEEL_NAME}""; ```. 6. Run; ```bash; export INPUT_DIR=""${PWD}/quickstart-testdata""; export OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --call_variants_extra_args=""use_openvino=True"" \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914
https://github.com/google/deepvariant/pull/363#issuecomment-723242914:2405,Deployability,upgrade,upgrade,2405,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions); ```bash; wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb ; dpkg -x parallel_20161222-1_all.deb parallel; export PATH=$HOME/parallel/usr/bin:$PATH; ```. 5. Install TensorFlow MKL-DNN; ```bash; WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl; wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}""; pip3 install --upgrade ""/tmp/${WHEEL_NAME}""; ```. 6. Run; ```bash; export INPUT_DIR=""${PWD}/quickstart-testdata""; export OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --call_variants_extra_args=""use_openvino=True"" \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914
https://github.com/google/deepvariant/pull/363#issuecomment-723242914:83,Modifiability,portab,portable,83,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |; | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permiss",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914
https://github.com/google/deepvariant/pull/363#issuecomment-723242914:95,Testability,benchmark,benchmark,95,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |; | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permiss",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914
https://github.com/google/deepvariant/pull/363#issuecomment-723242914:237,Testability,test,testdata,237,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |; | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permiss",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914
https://github.com/google/deepvariant/pull/363#issuecomment-723242914:1069,Testability,test,test,1069,"build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |; | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions); ```bash; wget http://launchpadlibrarian.net/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914
https://github.com/google/deepvariant/pull/363#issuecomment-723242914:1149,Testability,test,test,1149,"e initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |; | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions); ```bash; wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb ; dpkg -x parallel_20161222-1_all.deb parallel; e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914
https://github.com/google/deepvariant/pull/363#issuecomment-723242914:2493,Testability,test,testdata,2493,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions); ```bash; wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb ; dpkg -x parallel_20161222-1_all.deb parallel; export PATH=$HOME/parallel/usr/bin:$PATH; ```. 5. Install TensorFlow MKL-DNN; ```bash; WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl; wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}""; pip3 install --upgrade ""/tmp/${WHEEL_NAME}""; ```. 6. Run; ```bash; export INPUT_DIR=""${PWD}/quickstart-testdata""; export OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --call_variants_extra_args=""use_openvino=True"" \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914
https://github.com/google/deepvariant/pull/363#issuecomment-724316304:15,Performance,optimiz,optimizations,15,"Hi!. Made some optimizations and tested on chr20 from https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-case-study.md. Can you please tell me if it's a representative launch?. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | 3m7.950s | 5m59.221s | 1m5.724s |; | OpenVINO | 3m6.239s | 3m46.756s (x1.58) | 1m7.640s |. (""real"" times are in the table). ```bash; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=./reference/GRCh38_no_alt_analysis_set.fasta \; --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \; --num_shards=16 \; --regions ""chr20"" \; --call_variants_extra_args=""use_openvino=True""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-724316304
https://github.com/google/deepvariant/pull/363#issuecomment-724316304:33,Testability,test,tested,33,"Hi!. Made some optimizations and tested on chr20 from https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-case-study.md. Can you please tell me if it's a representative launch?. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | 3m7.950s | 5m59.221s | 1m5.724s |; | OpenVINO | 3m6.239s | 3m46.756s (x1.58) | 1m7.640s |. (""real"" times are in the table). ```bash; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=./reference/GRCh38_no_alt_analysis_set.fasta \; --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \; --num_shards=16 \; --regions ""chr20"" \; --call_variants_extra_args=""use_openvino=True""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-724316304
https://github.com/google/deepvariant/pull/363#issuecomment-725869282:52,Deployability,configurat,configuration,52,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |; | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \; --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \; --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \; --num_shards=16 \; --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \; --call_variants_extra_args=""use_openvino=True""; ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-725869282
https://github.com/google/deepvariant/pull/363#issuecomment-725869282:52,Modifiability,config,configuration,52,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |; | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \; --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \; --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \; --num_shards=16 \; --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \; --call_variants_extra_args=""use_openvino=True""; ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-725869282
https://github.com/google/deepvariant/pull/363#issuecomment-725869282:25,Security,access,access,25,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |; | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \; --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \; --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \; --num_shards=16 \; --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \; --call_variants_extra_args=""use_openvino=True""; ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-725869282
https://github.com/google/deepvariant/pull/363#issuecomment-725869282:972,Security,access,access,972,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |; | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \; --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \; --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \; --num_shards=16 \; --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \; --call_variants_extra_args=""use_openvino=True""; ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-725869282
https://github.com/google/deepvariant/pull/363#issuecomment-734258287:116,Security,validat,validate,116,I have published an image for the latest state of PR at https://hub.docker.com/r/dkurtaev/deepvariant. May I ask to validate it?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-734258287
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:1113,Availability,echo,echo,1113,"riant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details.; With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:; ----. # Commands. Tested on the same machine:. All below were done with command like:; ```; scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log; ```; with some code diffs below:. 1. Use your Docker image, use_openvino=true; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:1781,Availability,echo,echo,1781," aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:2532,Availability,echo,echo,2532,"input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m24.429s; real 6m32.705s; ```. 3. Use v1.0.0 image.; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..88fb0c1 100755;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:3201,Availability,echo,echo,3201,"aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m24.429s; real 6m32.705s; ```. 3. Use v1.0.0 image.; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..88fb0c1 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -72,7 +72,7 @@ sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime; ```; $ grep '^real",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:4158,Availability,echo,echo,4158,"r1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m24.429s; real 6m32.705s; ```. 3. Use v1.0.0 image.; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..88fb0c1 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -72,7 +72,7 @@ sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime; ```; $ grep '^real' /tmp/openvino.log; real 7m26.887s; real 20m40.889s; real 6m25.257s; ```. ---. # Machine details. I got the machine with this command:. ```; gcloud compute instances create ""${USER}-openvino-expt"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ## `lscpu`; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 64; On-line CPU(s) list: 0-63; Thread(s) per core: 2; Core(s) per socket: 32; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 85; Model name: Intel(R) Xeon(R) CPU @ 2.00GHz; Stepping: 3; CPU MHz: 2000.178; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:6984,Energy Efficiency,power,power,6984,"8; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(s): 0-63; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities; ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```; $ cat /proc/cpuinfo ; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 85; model name : Intel(R) Xeon(R) CPU @ 2.00GHz; stepping : 3; microcode : 0x1; cpu MHz : 2000.178; cache size : 39424 KB; physical id : 0; siblings : 64; core id : 0; cpu cores : 32; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 13; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa; bogomips : 4000.35; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:5083,Performance,cache,cache,5083,"ino-expt"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ## `lscpu`; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 64; On-line CPU(s) list: 0-63; Thread(s) per core: 2; Core(s) per socket: 32; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 85; Model name: Intel(R) Xeon(R) CPU @ 2.00GHz; Stepping: 3; CPU MHz: 2000.178; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(s): 0-63; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities; ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```; $ cat /proc/cpuinfo ; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 85; model name : Intel(R) Xeon(R) CPU @ 2.00GHz; stepping : 3; microcode : 0x1; cpu MHz : 2000.178; cache size : 39424 KB; physical id : 0; siblings : 64; core id : 0; cpu cores : 32; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 13; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:5099,Performance,cache,cache,5099,"ino-expt"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ## `lscpu`; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 64; On-line CPU(s) list: 0-63; Thread(s) per core: 2; Core(s) per socket: 32; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 85; Model name: Intel(R) Xeon(R) CPU @ 2.00GHz; Stepping: 3; CPU MHz: 2000.178; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(s): 0-63; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities; ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```; $ cat /proc/cpuinfo ; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 85; model name : Intel(R) Xeon(R) CPU @ 2.00GHz; stepping : 3; microcode : 0x1; cpu MHz : 2000.178; cache size : 39424 KB; physical id : 0; siblings : 64; core id : 0; cpu cores : 32; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 13; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:5114,Performance,cache,cache,5114,"ino-expt"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ## `lscpu`; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 64; On-line CPU(s) list: 0-63; Thread(s) per core: 2; Core(s) per socket: 32; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 85; Model name: Intel(R) Xeon(R) CPU @ 2.00GHz; Stepping: 3; CPU MHz: 2000.178; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(s): 0-63; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities; ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```; $ cat /proc/cpuinfo ; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 85; model name : Intel(R) Xeon(R) CPU @ 2.00GHz; stepping : 3; microcode : 0x1; cpu MHz : 2000.178; cache size : 39424 KB; physical id : 0; siblings : 64; core id : 0; cpu cores : 32; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 13; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:5131,Performance,cache,cache,5131,"ino-expt"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ## `lscpu`; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 64; On-line CPU(s) list: 0-63; Thread(s) per core: 2; Core(s) per socket: 32; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 85; Model name: Intel(R) Xeon(R) CPU @ 2.00GHz; Stepping: 3; CPU MHz: 2000.178; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(s): 0-63; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities; ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```; $ cat /proc/cpuinfo ; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 85; model name : Intel(R) Xeon(R) CPU @ 2.00GHz; stepping : 3; microcode : 0x1; cpu MHz : 2000.178; cache size : 39424 KB; physical id : 0; siblings : 64; core id : 0; cpu cores : 32; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 13; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:6028,Performance,cache,cache,6028,"8; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(s): 0-63; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities; ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```; $ cat /proc/cpuinfo ; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 85; model name : Intel(R) Xeon(R) CPU @ 2.00GHz; stepping : 3; microcode : 0x1; cpu MHz : 2000.178; cache size : 39424 KB; physical id : 0; siblings : 64; core id : 0; cpu cores : 32; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 13; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa; bogomips : 4000.35; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:2,Testability,test,tested,2,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details.; With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:; ----. # Commands. Tested on the same machine:. All below were done with command like:; ```; scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log; ```; with some code diffs below:. 1. Use your Docker image, use_openvino=true; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:302,Testability,Test,Tested,302,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details.; With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:; ----. # Commands. Tested on the same machine:. All below were done with command like:; ```; scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log; ```; with some code diffs below:. 1. Use your Docker image, use_openvino=true; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:438,Testability,log,log,438,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details.; With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:; ----. # Commands. Tested on the same machine:. All below were done with command like:; ```; scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log; ```; with some code diffs below:. 1. Use your Docker image, use_openvino=true; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:858,Testability,test,testda,858,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details.; With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:; ----. # Commands. Tested on the same machine:. All below were done with command like:; ```; scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log; ```; with some code diffs below:. 1. Use your Docker image, use_openvino=true; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:940,Testability,test,testdata,940,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details.; With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:; ----. # Commands. Tested on the same machine:. All below were done with command like:; ```; scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log; ```; with some code diffs below:. 1. Use your Docker image, use_openvino=true; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:1775,Testability,log,log,1775," aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:2277,Testability,test,testda,2277,"n_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:2359,Testability,test,testdata,2359,"/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m24.429s; real 6m32.705s; ```. 3. Use v1.0.0 image.; The code diff:; `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:3195,Testability,log,log,3195,"aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m24.429s; real 6m32.705s; ```. 3. Use v1.0.0 image.; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..88fb0c1 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -72,7 +72,7 @@ sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime; ```; $ grep '^real",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:4152,Testability,log,log,4152,"r1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m24.429s; real 6m32.705s; ```. 3. Use v1.0.0 image.; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..88fb0c1 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -72,7 +72,7 @@ sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime; ```; $ grep '^real' /tmp/openvino.log; real 7m26.887s; real 20m40.889s; real 6m25.257s; ```. ---. # Machine details. I got the machine with this command:. ```; gcloud compute instances create ""${USER}-openvino-expt"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ## `lscpu`; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 64; On-line CPU(s) list: 0-63; Thread(s) per core: 2; Core(s) per socket: 32; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 85; Model name: Intel(R) Xeon(R) CPU @ 2.00GHz; Stepping: 3; CPU MHz: 2000.178; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276044:4220,Testability,log,log,4220,":; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m24.429s; real 6m32.705s; ```. 3. Use v1.0.0 image.; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..88fb0c1 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -72,7 +72,7 @@ sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime; ```; $ grep '^real' /tmp/openvino.log; real 7m26.887s; real 20m40.889s; real 6m25.257s; ```. ---. # Machine details. I got the machine with this command:. ```; gcloud compute instances create ""${USER}-openvino-expt"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ## `lscpu`; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 64; On-line CPU(s) list: 0-63; Thread(s) per core: 2; Core(s) per socket: 32; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 85; Model name: Intel(R) Xeon(R) CPU @ 2.00GHz; Stepping: 3; CPU MHz: 2000.178; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(s): 0-63; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044
https://github.com/google/deepvariant/pull/363#issuecomment-735276922:488,Availability,checkpoint,checkpoint,488,"Following up on my previous comment,; I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]; 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz; 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:; 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922
https://github.com/google/deepvariant/pull/363#issuecomment-735276922:751,Performance,optimiz,optimized,751,"Following up on my previous comment,; I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]; 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz; 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:; 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922
https://github.com/google/deepvariant/pull/363#issuecomment-735276922:824,Performance,perform,performance,824,"Following up on my previous comment,; I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]; 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz; 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:; 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922
https://github.com/google/deepvariant/pull/363#issuecomment-735276922:1510,Performance,Tune,Tune,1510,"no. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]; 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz; 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:; 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batchin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922
https://github.com/google/deepvariant/pull/363#issuecomment-735276922:1559,Performance,perform,performance,1559,"no. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]; 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz; 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:; 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batchin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922
https://github.com/google/deepvariant/pull/363#issuecomment-735276922:2251,Performance,Tune,Tune,2251,"/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz; I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]; I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922
https://github.com/google/deepvariant/pull/363#issuecomment-735276922:2300,Performance,perform,performance,2300,"/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz; I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]; I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922
https://github.com/google/deepvariant/pull/363#issuecomment-735276922:2763,Performance,optimiz,optimizations,2763,"l.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz; I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]; I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]; I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]; I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]; I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]; I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922
https://github.com/google/deepvariant/pull/363#issuecomment-735276922:9214,Performance,optimiz,optimizations,9214,"ts.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]; I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]; I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]; I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]; I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]; I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]; I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]; I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]; I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s; user 763m44.970s; sys 58m35.140s; ```. You can see these lines:; ```; W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz; ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922
https://github.com/google/deepvariant/pull/363#issuecomment-735276922:280,Testability,log,logs,280,"Following up on my previous comment,; I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]; 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz; 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:; 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922
https://github.com/google/deepvariant/pull/363#issuecomment-735388012:362,Performance,perform,performance,362,"@pichuan, thank you for very detailed experiment! Looking forward to see whole genome results. > @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735388012
https://github.com/google/deepvariant/pull/363#issuecomment-735388012:263,Testability,log,logs,263,"@pichuan, thank you for very detailed experiment! Looking forward to see whole genome results. > @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735388012
https://github.com/google/deepvariant/pull/363#issuecomment-735450709:414,Deployability,Update,Updated,414,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:; > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735450709
https://github.com/google/deepvariant/pull/363#issuecomment-735450709:271,Performance,perform,performance,271,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:; > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735450709
https://github.com/google/deepvariant/pull/363#issuecomment-735450709:168,Testability,log,logs,168,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:; > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735450709
https://github.com/google/deepvariant/pull/363#issuecomment-735510277:361,Deployability,release,release,361,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s.; So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines?; Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735510277
https://github.com/google/deepvariant/pull/363#issuecomment-735510277:449,Deployability,release,release,449,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s.; So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines?; Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735510277
https://github.com/google/deepvariant/pull/363#issuecomment-735510277:497,Deployability,release,release,497,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s.; So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines?; Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735510277
https://github.com/google/deepvariant/pull/363#issuecomment-735510277:632,Deployability,release,releases,632,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s.; So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines?; Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735510277
https://github.com/google/deepvariant/pull/363#issuecomment-735510277:660,Safety,safe,safe,660,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s.; So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines?; Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735510277
https://github.com/google/deepvariant/pull/363#issuecomment-735539751:34,Safety,safe,safe,34,"@pichuan, thank you! It should be safe to build Docker image with OpenVINO backend and just keep it disabled by default, so users can turn on it only manually by `--call_variants_extra_args=""use_openvino=True""`. OpenVINO import is surrounded by try-catch and I guess that it won't crash on non-Intel CPU:; ```python; try:; from openvino.inference_engine import IECore, StatusCode; except:; pass; ```. Anyway, I'll try to run on some public CI to confirm.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735539751
https://github.com/google/deepvariant/pull/363#issuecomment-735541974:25,Availability,down,downside,25,@dkurt Thanks! One small downside is that the Docker image will have to include the old ckpt format as well as the new format. One question for you - do you know if the converted model format can be read and used with regular Estimator as well?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735541974
https://github.com/google/deepvariant/pull/363#issuecomment-735543558:63,Availability,checkpoint,checkpoint,63,"@pichuan, I'll take a look. Can you please refer what is a new checkpoint format? I've tried only checkpoints that were since r0.9.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735543558
https://github.com/google/deepvariant/pull/363#issuecomment-735543558:98,Availability,checkpoint,checkpoints,98,"@pichuan, I'll take a look. Can you please refer what is a new checkpoint format? I've tried only checkpoints that were since r0.9.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735543558
https://github.com/google/deepvariant/pull/363#issuecomment-735551532:1152,Performance,load,load,1152,"@dkurt Sorry for the confusion. I meant:; ```; $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/; total 449M; -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin; -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001; -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index; -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta; -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping; -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml; ```. These are the extra files after enabling OpenVINO:; ```; -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin; -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping; -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml; ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):; ```; -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001; -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index; -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta; ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work!; (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735551532
https://github.com/google/deepvariant/pull/363#issuecomment-735606528:258,Deployability,install,installation,258,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```; rm model.pb;; ```; However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735606528
https://github.com/google/deepvariant/pull/363#issuecomment-735606528:232,Energy Efficiency,reduce,reduce,232,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```; rm model.pb;; ```; However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735606528
https://github.com/google/deepvariant/pull/363#issuecomment-735612657:201,Availability,down,downside,201,"@dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :) . @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735612657
https://github.com/google/deepvariant/pull/363#issuecomment-735612657:422,Testability,test,test,422,"@dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :) . @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735612657
https://github.com/google/deepvariant/pull/363#issuecomment-735703602:183,Availability,Checkpoint,Checkpoint,183,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735703602
https://github.com/google/deepvariant/pull/363#issuecomment-735703602:638,Availability,down,downside,638,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735703602
https://github.com/google/deepvariant/pull/363#issuecomment-735703602:859,Testability,test,test,859,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735703602
https://github.com/google/deepvariant/pull/363#issuecomment-735703602:546,Usability,simpl,simpler,546,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735703602
https://github.com/google/deepvariant/pull/363#issuecomment-735915820:654,Deployability,release,release,654,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?. Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:; For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though.; Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either.; I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735915820
https://github.com/google/deepvariant/pull/363#issuecomment-735915820:332,Testability,test,testing,332,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?. Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:; For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though.; Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either.; I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735915820
https://github.com/google/deepvariant/pull/363#issuecomment-735940070:6,Deployability,update,update,6,"Quick update and FYI for you @dkurt ; I ran with the internal latest code (which we switched all metrics to be on HG003 BAMs). Here are the improvements with `--use_openvino=true`.; * wgs: 233m14.191s --> 204m35.065s; * wes: 1m41.381s --> 1m31.513s; * pacbio: 193m20.407s --> 169m45.878s; * hybrid_pacbio_illumina: 241m7.426s --> 189m40.148s. This was after your ""Process OpenVINO in thread (#8)"" change yesterday.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735940070
https://github.com/google/deepvariant/pull/363#issuecomment-735959351:35,Modifiability,flexible,flexible,35,"@pichuan, I'll think and propose a flexible solution for OpenVINO acceleration. Can you please add some details about the latest numbers? Is that regression or improvement? Because last time we saw 266m46.183s --> 198m46.734s for WGS (call_variants) now it's 233m14.191s --> 204m35.065s.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735959351
https://github.com/google/deepvariant/pull/363#issuecomment-735976955:265,Testability,log,logging,265,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: ; 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). ; 2. The second thing that has changed is your change to improve the logging.; 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-735976955
https://github.com/google/deepvariant/pull/363#issuecomment-736009433:317,Deployability,configurat,configuration,317,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome!. I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736009433
https://github.com/google/deepvariant/pull/363#issuecomment-736009433:317,Modifiability,config,configuration,317,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome!. I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736009433
https://github.com/google/deepvariant/pull/363#issuecomment-736009433:73,Testability,benchmark,benchmarked,73,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome!. I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736009433
https://github.com/google/deepvariant/pull/363#issuecomment-736009433:110,Testability,log,logging,110,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome!. I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736009433
https://github.com/google/deepvariant/pull/363#issuecomment-736148441:27,Deployability,update,update,27,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release.; 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion.; 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False.; 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736148441
https://github.com/google/deepvariant/pull/363#issuecomment-736148441:326,Deployability,release,release,326,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release.; 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion.; 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False.; 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736148441
https://github.com/google/deepvariant/pull/363#issuecomment-736148441:961,Deployability,release,release,961,"Hi @dkurt , to give you an update on our discussion in the team, here is my current decision:. 1. I'm getting your first 5 commits (up to https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release.; 2. @gunjanbaid has a question about EMA. She'll follow up in this discussion.; 3. Currently, even though the hap.py results are the same, we did notice the VCFs are not exactly the same. I understand that this is likely expected, but to be extra careful, I'm going to still keep `use_openvino` by default as False.; 4. We'll plan to build our Docker images with `--build-arg DV_OPENVINO_BUILD=1` on, and we will plan to add to our documentation so users will be aware that they can try out adding `--call_variants_extra_args=""use_openvino=true""` to speed up their CPU runs. I really want to get this to be the default :) But release is happening soon and I don't want to break the default behavior, so I'm being extra careful here. Let me know if you have more thoughts about the decisions above. (Also adding @akolesnikov @AndrewCarroll @gunjanbaid @danielecook FYI about the current status above)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736148441
https://github.com/google/deepvariant/pull/363#issuecomment-736149821:415,Performance,load,loaded,415,"@dkurt we noticed some slight differences in the output VCF with and without OpenVINO. The quality scores are different in the example below (46 vs. 46.1) for an internal dataset. These quality scores are derived from the output probabilities. Are slight differences in output probabilities expected with and without OpenVINO? In the past, I've noticed such slight differences for the same hardware when EMA is not loaded in correctly at inference time. I wanted to bring this to your attention in case EMA is the reason for these differences. ```; -chr1 16895912 . G A 46 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0; +chr1 16895912 . G A 46.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736149821
https://github.com/google/deepvariant/pull/363#issuecomment-736278644:142,Deployability,release,release,142,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```; sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; ```; It would be safer to replace with something like this:; ```patch; diff --git a/Dockerfile b/Dockerfile; index 0432fd8..a57364d 100644; --- a/Dockerfile; +++ b/Dockerfile; @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*; # Convert model to OpenVINO format; RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; python3 -m pip install networkx defusedxml test-generator==0.1.1; \; - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \; for model in wgs wes pacbio hybrid_pacbio_illumina; do \; cd /opt/models/${model}; \; @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; --scale 128; \; rm model.pb; \; done \; + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; fi; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736278644
https://github.com/google/deepvariant/pull/363#issuecomment-736278644:421,Deployability,patch,patch,421,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```; sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; ```; It would be safer to replace with something like this:; ```patch; diff --git a/Dockerfile b/Dockerfile; index 0432fd8..a57364d 100644; --- a/Dockerfile; +++ b/Dockerfile; @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*; # Convert model to OpenVINO format; RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; python3 -m pip install networkx defusedxml test-generator==0.1.1; \; - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \; for model in wgs wes pacbio hybrid_pacbio_illumina; do \; cd /opt/models/${model}; \; @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; --scale 128; \; rm model.pb; \; done \; + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; fi; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736278644
https://github.com/google/deepvariant/pull/363#issuecomment-736278644:712,Deployability,install,install,712,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```; sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; ```; It would be safer to replace with something like this:; ```patch; diff --git a/Dockerfile b/Dockerfile; index 0432fd8..a57364d 100644; --- a/Dockerfile; +++ b/Dockerfile; @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*; # Convert model to OpenVINO format; RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; python3 -m pip install networkx defusedxml test-generator==0.1.1; \; - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \; for model in wgs wes pacbio hybrid_pacbio_illumina; do \; cd /opt/models/${model}; \; @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; --scale 128; \; rm model.pb; \; done \; + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; fi; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736278644
https://github.com/google/deepvariant/pull/363#issuecomment-736278644:374,Safety,safe,safer,374,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```; sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; ```; It would be safer to replace with something like this:; ```patch; diff --git a/Dockerfile b/Dockerfile; index 0432fd8..a57364d 100644; --- a/Dockerfile; +++ b/Dockerfile; @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*; # Convert model to OpenVINO format; RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; python3 -m pip install networkx defusedxml test-generator==0.1.1; \; - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \; for model in wgs wes pacbio hybrid_pacbio_illumina; do \; cd /opt/models/${model}; \; @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; --scale 128; \; rm model.pb; \; done \; + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; fi; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736278644
https://github.com/google/deepvariant/pull/363#issuecomment-736278644:740,Testability,test,test-generator,740,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```; sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; ```; It would be safer to replace with something like this:; ```patch; diff --git a/Dockerfile b/Dockerfile; index 0432fd8..a57364d 100644; --- a/Dockerfile; +++ b/Dockerfile; @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*; # Convert model to OpenVINO format; RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; python3 -m pip install networkx defusedxml test-generator==0.1.1; \; - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \; for model in wgs wes pacbio hybrid_pacbio_illumina; do \; cd /opt/models/${model}; \; @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; --scale 128; \; rm model.pb; \; done \; + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; fi; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736278644
https://github.com/google/deepvariant/pull/363#issuecomment-736694275:23,Deployability,update,update,23,"@dkurt Thanks. Another update for you - I am now trying to incorporate your on-the-fly conversion code:; https://github.com/google/deepvariant/pull/363/commits/f0ed01891c3e612d4c7093e5e844f855beae707a. I think it'll be cleaner, and also removes the need for https://github.com/google/deepvariant/pull/363#issuecomment-736278644. I do have a question for the code. I'll comment inline.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-736694275
https://github.com/google/deepvariant/pull/363#issuecomment-737626363:15,Deployability,update,update,15,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on); I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-737626363
https://github.com/google/deepvariant/pull/363#issuecomment-737626363:305,Testability,log,logging,305,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on); I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-737626363
https://github.com/google/deepvariant/pull/363#issuecomment-737655909:17,Deployability,update,update,17,"> @dkurt A quick update:; > ; > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on); > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth.; > ; > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected).; > ; > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉; (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-737655909
https://github.com/google/deepvariant/pull/363#issuecomment-737655909:1013,Deployability,release,release,1013,"> @dkurt A quick update:; > ; > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on); > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth.; > ; > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected).; > ; > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉; (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-737655909
https://github.com/google/deepvariant/pull/363#issuecomment-737655909:1046,Integrability,message,message,1046,"> @dkurt A quick update:; > ; > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on); > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth.; > ; > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected).; > ; > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉; (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-737655909
https://github.com/google/deepvariant/pull/363#issuecomment-737655909:315,Testability,log,logging,315,"> @dkurt A quick update:; > ; > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on); > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth.; > ; > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected).; > ; > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉; (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-737655909
https://github.com/google/deepvariant/pull/363#issuecomment-737655909:1085,Testability,log,logging,1085,"> @dkurt A quick update:; > ; > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on); > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth.; > ; > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected).; > ; > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉; (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-737655909
https://github.com/google/deepvariant/pull/363#issuecomment-737655909:1167,Testability,log,logging,1167,"> @dkurt A quick update:; > ; > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on); > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth.; > ; > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected).; > ; > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉; (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-737655909
https://github.com/google/deepvariant/pull/363#issuecomment-738803637:41,Deployability,patch,patch,41,Fixed non-deterministic behavior by last patch.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-738803637
https://github.com/google/deepvariant/pull/363#issuecomment-740139289:79,Deployability,release,releases,79,"@dkurt FYI , the OpenVINO changes are in https://github.com/google/deepvariant/releases/tag/v1.1.0; Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/363#issuecomment-740139289
https://github.com/google/deepvariant/pull/365#issuecomment-713317674:898,Security,authoriz,authorized,898,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F365) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-713317674
https://github.com/google/deepvariant/pull/365#issuecomment-713317674:966,Security,authoriz,authorized,966,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F365) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-713317674
https://github.com/google/deepvariant/pull/365#issuecomment-713317674:1239,Security,authoriz,authorized,1239,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F365) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-713317674
https://github.com/google/deepvariant/pull/365#issuecomment-713317674:1534,Security,authoriz,authorized,1534,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F365) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-713317674
https://github.com/google/deepvariant/pull/365#issuecomment-716869195:323,Deployability,patch,patch,323,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-716869195
https://github.com/google/deepvariant/pull/365#issuecomment-716869195:411,Deployability,release,release,411,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-716869195
https://github.com/google/deepvariant/pull/365#issuecomment-716869195:474,Deployability,release,release,474,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-716869195
https://github.com/google/deepvariant/pull/365#issuecomment-716869195:237,Testability,test,test,237,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-716869195
https://github.com/google/deepvariant/pull/365#issuecomment-717368605:495,Performance,load,loading,495,"Hi @gunjanbaid , I am a novice when it comes to beam. So I will defer to you on https://github.com/google/deepvariant/pull/365#discussion_r512315819. My setup works with Spark/Flink so I can test it out. I am looking at another part of the code that is potentially fatal for execution at the moment:; ``` ; return (input_examples; | 'Randomize' >> beam.Map(lambda x: (sha1(x), x)); | 'Groupby' >> beam.GroupByKey(); | 'DropKey' >> beam.FlatMap(lambda x: x[1])); ```. I notice that GroupByKey is loading all of the data into the memory of the worker. Is this not a problem for DataflowRunner?. I am trying to run shuffle on tfrecords produced from 6 BAM files. The gzipped tfrecords are approximately 120GB in total, and GroupByKey quickly runs out of memory when the machine has over 600GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-717368605
https://github.com/google/deepvariant/pull/365#issuecomment-717368605:191,Testability,test,test,191,"Hi @gunjanbaid , I am a novice when it comes to beam. So I will defer to you on https://github.com/google/deepvariant/pull/365#discussion_r512315819. My setup works with Spark/Flink so I can test it out. I am looking at another part of the code that is potentially fatal for execution at the moment:; ``` ; return (input_examples; | 'Randomize' >> beam.Map(lambda x: (sha1(x), x)); | 'Groupby' >> beam.GroupByKey(); | 'DropKey' >> beam.FlatMap(lambda x: x[1])); ```. I notice that GroupByKey is loading all of the data into the memory of the worker. Is this not a problem for DataflowRunner?. I am trying to run shuffle on tfrecords produced from 6 BAM files. The gzipped tfrecords are approximately 120GB in total, and GroupByKey quickly runs out of memory when the machine has over 600GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-717368605
https://github.com/google/deepvariant/pull/365#issuecomment-720681308:92,Performance,load,loaded,92,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-720681308
https://github.com/google/deepvariant/pull/365#issuecomment-720681308:451,Performance,optimiz,optimization,451,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-720681308
https://github.com/google/deepvariant/pull/365#issuecomment-720871901:1337,Integrability,depend,depends,1337,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:; ```; input_data = readers | ""FlattenInputs"" >> beam.Flatten(); partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>); for i, p in enumerate(partitions):; writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...); ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-720871901
https://github.com/google/deepvariant/pull/365#issuecomment-720871901:534,Usability,simpl,simply,534,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:; ```; input_data = readers | ""FlattenInputs"" >> beam.Flatten(); partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>); for i, p in enumerate(partitions):; writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...); ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-720871901
https://github.com/google/deepvariant/pull/365#issuecomment-723468092:1068,Testability,test,tested,1068,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-723468092
https://github.com/google/deepvariant/pull/365#issuecomment-723468092:1173,Testability,test,testcase,1173,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-723468092
https://github.com/google/deepvariant/pull/365#issuecomment-723468092:1254,Usability,clear,clear,1254,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/365#issuecomment-723468092
https://github.com/google/deepvariant/issues/366#issuecomment-716197107:99,Performance,perform,perform,99,"Hi,. Couple of points:; * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. ; * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. ; * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366#issuecomment-716197107
https://github.com/google/deepvariant/issues/366#issuecomment-716197107:341,Performance,perform,performs,341,"Hi,. Couple of points:; * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. ; * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. ; * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366#issuecomment-716197107
https://github.com/google/deepvariant/issues/366#issuecomment-716272982:417,Testability,log,logfile,417,"thanks for your responses. command line：; docker run \; -v /sfs-grand-med-research/:/sfs-grand-med-research/ \; swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=human_g1k_v37.main_chrom.fasta \; --reads=202022.hg19.pbmm2.sort.MT.bam \; --output_vcf=202022.MT.vcf.gz \; --num_shards=16 \; --intermediate_results_dir=/tmp/. logfile：; [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366#issuecomment-716272982
https://github.com/google/deepvariant/issues/366#issuecomment-716272982:448,Testability,log,log,448,"thanks for your responses. command line：; docker run \; -v /sfs-grand-med-research/:/sfs-grand-med-research/ \; swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=human_g1k_v37.main_chrom.fasta \; --reads=202022.hg19.pbmm2.sort.MT.bam \; --output_vcf=202022.MT.vcf.gz \; --num_shards=16 \; --intermediate_results_dir=/tmp/. logfile：; [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366#issuecomment-716272982
https://github.com/google/deepvariant/issues/366#issuecomment-716272982:525,Testability,log,log,525,"thanks for your responses. command line：; docker run \; -v /sfs-grand-med-research/:/sfs-grand-med-research/ \; swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=human_g1k_v37.main_chrom.fasta \; --reads=202022.hg19.pbmm2.sort.MT.bam \; --output_vcf=202022.MT.vcf.gz \; --num_shards=16 \; --intermediate_results_dir=/tmp/. logfile：; [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/366#issuecomment-716272982
