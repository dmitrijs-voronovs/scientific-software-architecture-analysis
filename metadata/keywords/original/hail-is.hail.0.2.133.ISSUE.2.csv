id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/issues/3235:1638,Testability,assert,assertion,1638,"e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/typecheck/check.py"", line 480, in _typecheck; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 1419, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 2241, in _process_joins; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/table.py"", line 1233, in <lambda>; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 250 in stage 16.0 failed 20 times, most recent failure: Lost task 250.19 in stage 16.0 (TID 5993, exomes2-sw-znhp.c.broad-mpg-gnomad.internal, executor 1): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:751); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); 	at scala.collection.Iterator$$anon$1.head(Iterator.scala:997); 	at is.hail.utils.richUtils.RichIterator$$anon$5.value(RichIterator.scala:18); 	at is.hail.utils.StagingIterator.value(FlipbookIterator.scala:47); 	at is.hail.utils.FlipbookIterator$$anon$5.value(FlipbookIterator.scala:167); 	at is.hail.utils.FlipbookIterator$$anon$5.isValid(FlipbookIterator.scala:168); 	at is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:46); 	at is.hail.utils.FlipbookIterator.exhaust(FlipbookIterator.scala:110); 	at is.hail.utils.FlipbookIterator$$anon$6.advance(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.staircased(FlipbookIterator.scala:186); 	at is.hail.annotations.OrderedRVIterator.staircase",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:1674,Testability,assert,assert,1674,"ip/hail/typecheck/check.py"", line 480, in _typecheck; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 1419, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 2241, in _process_joins; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/table.py"", line 1233, in <lambda>; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 250 in stage 16.0 failed 20 times, most recent failure: Lost task 250.19 in stage 16.0 (TID 5993, exomes2-sw-znhp.c.broad-mpg-gnomad.internal, executor 1): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:751); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); 	at scala.collection.Iterator$$anon$1.head(Iterator.scala:997); 	at is.hail.utils.richUtils.RichIterator$$anon$5.value(RichIterator.scala:18); 	at is.hail.utils.StagingIterator.value(FlipbookIterator.scala:47); 	at is.hail.utils.FlipbookIterator$$anon$5.value(FlipbookIterator.scala:167); 	at is.hail.utils.FlipbookIterator$$anon$5.isValid(FlipbookIterator.scala:168); 	at is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:46); 	at is.hail.utils.FlipbookIterator.exhaust(FlipbookIterator.scala:110); 	at is.hail.utils.FlipbookIterator$$anon$6.advance(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.staircased(FlipbookIterator.scala:186); 	at is.hail.annotations.OrderedRVIterator.staircase(OrderedRVIterator.scala:18); 	at is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:8988,Testability,Assert,AssertionError,8988,.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:745); 	at is.hail.variant.MatrixTable.annotateColsTable(MatrixTable.scala:874); 	at is.hail.variant.MatrixTable.annotateColsTable(MatrixTable.scala:827); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:751); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); 	at scala.collection.Iterator$$anon$1.head(Iterator.scala:997); 	at is.hail.utils.richUtils.RichIterator$$anon$5.value(RichIterator.scala:18); 	at is.hail.utils.StagingIterator.value(FlipbookIterator.scala:47); 	at is.hail.utils.FlipbookIterator$$anon$5.value(FlipbookIterator.scala:167); 	at is.hail.utils.FlipbookIterator$$anon$5.isValid(FlipbookIterator.scala:168); 	at is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:46); 	at is.hail.utils.FlipbookIterator.exhaust(FlipbookIterator.scala:110); 	at is.hail.utils.FlipbookIterator$$anon$6.advance(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.staircased(FlipbookIterator.scala:186); 	at is.hail.annotations.OrderedRVIterator.staircase,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:9004,Testability,assert,assertion,9004,.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:745); 	at is.hail.variant.MatrixTable.annotateColsTable(MatrixTable.scala:874); 	at is.hail.variant.MatrixTable.annotateColsTable(MatrixTable.scala:827); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:751); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); 	at scala.collection.Iterator$$anon$1.head(Iterator.scala:997); 	at is.hail.utils.richUtils.RichIterator$$anon$5.value(RichIterator.scala:18); 	at is.hail.utils.StagingIterator.value(FlipbookIterator.scala:47); 	at is.hail.utils.FlipbookIterator$$anon$5.value(FlipbookIterator.scala:167); 	at is.hail.utils.FlipbookIterator$$anon$5.isValid(FlipbookIterator.scala:168); 	at is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:46); 	at is.hail.utils.FlipbookIterator.exhaust(FlipbookIterator.scala:110); 	at is.hail.utils.FlipbookIterator$$anon$6.advance(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.staircased(FlipbookIterator.scala:186); 	at is.hail.annotations.OrderedRVIterator.staircase,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:9040,Testability,assert,assert,9040,ache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:745); 	at is.hail.variant.MatrixTable.annotateColsTable(MatrixTable.scala:874); 	at is.hail.variant.MatrixTable.annotateColsTable(MatrixTable.scala:827); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:751); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); 	at scala.collection.Iterator$$anon$1.head(Iterator.scala:997); 	at is.hail.utils.richUtils.RichIterator$$anon$5.value(RichIterator.scala:18); 	at is.hail.utils.StagingIterator.value(FlipbookIterator.scala:47); 	at is.hail.utils.FlipbookIterator$$anon$5.value(FlipbookIterator.scala:167); 	at is.hail.utils.FlipbookIterator$$anon$5.isValid(FlipbookIterator.scala:168); 	at is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:46); 	at is.hail.utils.FlipbookIterator.exhaust(FlipbookIterator.scala:110); 	at is.hail.utils.FlipbookIterator$$anon$6.advance(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.staircased(FlipbookIterator.scala:186); 	at is.hail.annotations.OrderedRVIterator.staircase(OrderedRVIterator.scala:18); 	at is.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:13036,Testability,Assert,AssertionError,13036,.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-279ddd2; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:13052,Testability,assert,assertion,13052,.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-279ddd2; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3236:5,Availability,Error,Error,5,```; Error summary: ClassCastException: is.hail.annotations.BroadcastValue cannot be cast to org.apache.spark.sql.Row; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3236
https://github.com/hail-is/hail/issues/3249:69,Testability,Assert,AssertionError,69,"```; In [2]: mt.filter_rows(5).count(); Java stack trace:; java.lang.AssertionError: assertion failed: bool, int32, IsNA(Ref(filterRows_pred,int32)); 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:37); 	at is.hail.expr.ir.Infer$.is$hail$expr$ir$Infer$$infer$1(Infer.scala:10); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:42); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:7); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3249
https://github.com/hail-is/hail/issues/3249:85,Testability,assert,assertion,85,"```; In [2]: mt.filter_rows(5).count(); Java stack trace:; java.lang.AssertionError: assertion failed: bool, int32, IsNA(Ref(filterRows_pred,int32)); 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:37); 	at is.hail.expr.ir.Infer$.is$hail$expr$ir$Infer$$infer$1(Infer.scala:10); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:42); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:7); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3249
https://github.com/hail-is/hail/issues/3249:168,Testability,assert,assert,168,"```; In [2]: mt.filter_rows(5).count(); Java stack trace:; java.lang.AssertionError: assertion failed: bool, int32, IsNA(Ref(filterRows_pred,int32)); 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:37); 	at is.hail.expr.ir.Infer$.is$hail$expr$ir$Infer$$infer$1(Infer.scala:10); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:42); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:7); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3249
https://github.com/hail-is/hail/pull/3250:306,Testability,test,test,306,"- Added examples of working with intervals to `import_locus_intervals` and `import_bed`; - Fix end bound of BedAnnotator; - Fix parser to account for ends of contigs properly; - Add function to normalize interval bounds to be valid loci in reference.; - Changed bed file and interval list example files to test contig ends. Example:; (0, END + 1) => [1, END]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3250
https://github.com/hail-is/hail/pull/3256:184,Safety,unsafe,unsafeOrdering,184,"@maccum ran into a problem where a join was failing because the two tables had different requiredness in their key fields. This should resolve the problem, by adding an overload `Type.unsafeOrdering` which takes a `rightType: Type`, requiring that `this isOfType rightType`, but allowing them to have different requiredness.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3256
https://github.com/hail-is/hail/pull/3264:304,Testability,test,test,304,"Resolves https://github.com/hail-is/hail/pull/2471 by reimplementing in Python. Required changes to bind to permit binding multiple args; this required flipping the order of the function and the binding. You shouldn't need to do much correctness checking in the implementation, because the files used to test basically catch every edge case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3264
https://github.com/hail-is/hail/pull/3265:0,Integrability,Depend,Depends,0,"Depends on #3259 . - Added TableImportIR; - Rewrote BedAnnotator and IntervalList in Python; - Removed BedAnnotator from Scala; - Could not easily remove IntervalList from the Scala code, so I kept it in there. @cseed Can you please take a look at this to make sure it is consistent with what you had in mind for TableImportIR?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3265
https://github.com/hail-is/hail/pull/3266:35,Deployability,release,release,35,Add the support for latest Nirvana release,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3266
https://github.com/hail-is/hail/pull/3269:77,Testability,test,tests,77,"Builds on: https://github.com/hail-is/hail/pull/3268. Well, moved the to the tests section. The test section is becoming a real cesspool.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3269
https://github.com/hail-is/hail/pull/3269:96,Testability,test,test,96,"Builds on: https://github.com/hail-is/hail/pull/3268. Well, moved the to the tests section. The test section is becoming a real cesspool.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3269
https://github.com/hail-is/hail/issues/3276:243,Availability,failure,failure,243,"to replicate:; ```python; In [2]: mt = hl.import_vcf('src/test/resources/sample.vcf'); In [3]: mt.aggregate_entries(hl.agg.counter(hl.agg.explode(mt.PL))); ```; ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.NullPointerException; 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:79",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3276
https://github.com/hail-is/hail/issues/3276:300,Availability,failure,failure,300,"to replicate:; ```python; In [2]: mt = hl.import_vcf('src/test/resources/sample.vcf'); In [3]: mt.aggregate_entries(hl.agg.counter(hl.agg.explode(mt.PL))); ```; ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.NullPointerException; 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:79",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3276
https://github.com/hail-is/hail/issues/3276:2321,Energy Efficiency,schedul,scheduler,2321,la:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3276
https://github.com/hail-is/hail/issues/3276:2393,Energy Efficiency,schedul,scheduler,2393,la:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3276
https://github.com/hail-is/hail/issues/3276:2518,Performance,concurren,concurrent,2518,la:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3276
https://github.com/hail-is/hail/issues/3276:2603,Performance,concurren,concurrent,2603,la:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3276
https://github.com/hail-is/hail/issues/3276:222,Safety,abort,aborted,222,"to replicate:; ```python; In [2]: mt = hl.import_vcf('src/test/resources/sample.vcf'); In [3]: mt.aggregate_entries(hl.agg.counter(hl.agg.explode(mt.PL))); ```; ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.NullPointerException; 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:79",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3276
https://github.com/hail-is/hail/issues/3276:1396,Safety,Unsafe,UnsafeIndexedSeq,1396, 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Ta,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3276
https://github.com/hail-is/hail/issues/3276:1421,Safety,Unsafe,UnsafeRow,1421,en.generated.C1.apply(Unknown Source); 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3276
https://github.com/hail-is/hail/issues/3276:58,Testability,test,test,58,"to replicate:; ```python; In [2]: mt = hl.import_vcf('src/test/resources/sample.vcf'); In [3]: mt.aggregate_entries(hl.agg.counter(hl.agg.explode(mt.PL))); ```; ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.NullPointerException; 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.codegen.generated.C1.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:270); 	at is.hail.expr.AST$$anonfun$runAggregator$1.apply(AST.scala:268); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:304); 	at is.hail.methods.Aggregators$$anonfun$11.apply(Aggregators.scala:300); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1743); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64$$anonfun$apply$65.apply(MatrixTable.scala:1741); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at is.hail.annotations.UnsafeIndexedSeq.foreach(UnsafeRow.scala:51); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1741); 	at is.hail.variant.MatrixTable$$anonfun$82$$anonfun$apply$64.apply(MatrixTable.scala:1734); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1734); 	at is.hail.variant.MatrixTable$$anonfun$82.apply(MatrixTable.scala:1728); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:79",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3276
https://github.com/hail-is/hail/issues/3278:37,Testability,test,test,37,"```; In [5]: mt = hl.import_vcf('src/test/resources/sample.vcf'); 2018-03-31 22:19:52 Hail: INFO: Coerced sorted dataset. In [6]: mt.aggregate_entries(hl.agg.take(mt.s, 5)); Out[6]: [None, None, None, None, None]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3278
https://github.com/hail-is/hail/pull/3280:234,Deployability,pipeline,pipeline,234,"This PR removes the `as_array` parameter on `pca` and `hwe_normalized_pca`. The scores and loadings tables are constructed to always have a field of array type, mirroring the eigenvalues array. This simplifies the interface and makes pipeline (and PC indexing) behavior more predictable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3280
https://github.com/hail-is/hail/pull/3280:214,Integrability,interface,interface,214,"This PR removes the `as_array` parameter on `pca` and `hwe_normalized_pca`. The scores and loadings tables are constructed to always have a field of array type, mirroring the eigenvalues array. This simplifies the interface and makes pipeline (and PC indexing) behavior more predictable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3280
https://github.com/hail-is/hail/pull/3280:91,Performance,load,loadings,91,"This PR removes the `as_array` parameter on `pca` and `hwe_normalized_pca`. The scores and loadings tables are constructed to always have a field of array type, mirroring the eigenvalues array. This simplifies the interface and makes pipeline (and PC indexing) behavior more predictable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3280
https://github.com/hail-is/hail/pull/3280:275,Safety,predict,predictable,275,"This PR removes the `as_array` parameter on `pca` and `hwe_normalized_pca`. The scores and loadings tables are constructed to always have a field of array type, mirroring the eigenvalues array. This simplifies the interface and makes pipeline (and PC indexing) behavior more predictable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3280
https://github.com/hail-is/hail/pull/3280:199,Usability,simpl,simplifies,199,"This PR removes the `as_array` parameter on `pca` and `hwe_normalized_pca`. The scores and loadings tables are constructed to always have a field of array type, mirroring the eigenvalues array. This simplifies the interface and makes pipeline (and PC indexing) behavior more predictable.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3280
https://github.com/hail-is/hail/pull/3285:45,Testability,test,tests,45,Looking for some feedback and advice on what tests to build.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3285
https://github.com/hail-is/hail/pull/3285:17,Usability,feedback,feedback,17,Looking for some feedback and advice on what tests to build.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3285
https://github.com/hail-is/hail/pull/3287:156,Deployability,update,updated,156,"These functions are now only used in tests. I re-implemented computeRRM via expression language, interim to deleting entirely on the Scala side once LMM is updated and KinshipMatrix goes away. Note `realized_relationship_matrix` is tested in `test_rrm` on the Python side, and computed independently of the ComputeRRM code. I also moved RichMatrixTable to testUtils alongside RichTable, rather than with the Suites in utils.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3287
https://github.com/hail-is/hail/pull/3287:37,Testability,test,tests,37,"These functions are now only used in tests. I re-implemented computeRRM via expression language, interim to deleting entirely on the Scala side once LMM is updated and KinshipMatrix goes away. Note `realized_relationship_matrix` is tested in `test_rrm` on the Python side, and computed independently of the ComputeRRM code. I also moved RichMatrixTable to testUtils alongside RichTable, rather than with the Suites in utils.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3287
https://github.com/hail-is/hail/pull/3287:232,Testability,test,tested,232,"These functions are now only used in tests. I re-implemented computeRRM via expression language, interim to deleting entirely on the Scala side once LMM is updated and KinshipMatrix goes away. Note `realized_relationship_matrix` is tested in `test_rrm` on the Python side, and computed independently of the ComputeRRM code. I also moved RichMatrixTable to testUtils alongside RichTable, rather than with the Suites in utils.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3287
https://github.com/hail-is/hail/pull/3287:356,Testability,test,testUtils,356,"These functions are now only used in tests. I re-implemented computeRRM via expression language, interim to deleting entirely on the Scala side once LMM is updated and KinshipMatrix goes away. Note `realized_relationship_matrix` is tested in `test_rrm` on the Python side, and computed independently of the ComputeRRM code. I also moved RichMatrixTable to testUtils alongside RichTable, rather than with the Suites in utils.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3287
https://github.com/hail-is/hail/pull/3288:260,Availability,error,errors,260,"It turns out that Kryo serialization is extra sneaky and will often just try to serialize the parts of a class if the class itself doesn't implement the KryoSerializable interface. I made a trait, `UnKryoSerializable`, that extends KryoSerializable but throws errors on read and write to try to weed out the rest of the places where UnsafeRows are being serialized. The biggest place this popped up was with colValues. For now, they're just being broadcast as safe Annotations everywhere. This depends on a change in #3258 and I'll rebase when that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3288
https://github.com/hail-is/hail/pull/3288:170,Integrability,interface,interface,170,"It turns out that Kryo serialization is extra sneaky and will often just try to serialize the parts of a class if the class itself doesn't implement the KryoSerializable interface. I made a trait, `UnKryoSerializable`, that extends KryoSerializable but throws errors on read and write to try to weed out the rest of the places where UnsafeRows are being serialized. The biggest place this popped up was with colValues. For now, they're just being broadcast as safe Annotations everywhere. This depends on a change in #3258 and I'll rebase when that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3288
https://github.com/hail-is/hail/pull/3288:494,Integrability,depend,depends,494,"It turns out that Kryo serialization is extra sneaky and will often just try to serialize the parts of a class if the class itself doesn't implement the KryoSerializable interface. I made a trait, `UnKryoSerializable`, that extends KryoSerializable but throws errors on read and write to try to weed out the rest of the places where UnsafeRows are being serialized. The biggest place this popped up was with colValues. For now, they're just being broadcast as safe Annotations everywhere. This depends on a change in #3258 and I'll rebase when that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3288
https://github.com/hail-is/hail/pull/3288:224,Modifiability,extend,extends,224,"It turns out that Kryo serialization is extra sneaky and will often just try to serialize the parts of a class if the class itself doesn't implement the KryoSerializable interface. I made a trait, `UnKryoSerializable`, that extends KryoSerializable but throws errors on read and write to try to weed out the rest of the places where UnsafeRows are being serialized. The biggest place this popped up was with colValues. For now, they're just being broadcast as safe Annotations everywhere. This depends on a change in #3258 and I'll rebase when that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3288
https://github.com/hail-is/hail/pull/3288:333,Safety,Unsafe,UnsafeRows,333,"It turns out that Kryo serialization is extra sneaky and will often just try to serialize the parts of a class if the class itself doesn't implement the KryoSerializable interface. I made a trait, `UnKryoSerializable`, that extends KryoSerializable but throws errors on read and write to try to weed out the rest of the places where UnsafeRows are being serialized. The biggest place this popped up was with colValues. For now, they're just being broadcast as safe Annotations everywhere. This depends on a change in #3258 and I'll rebase when that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3288
https://github.com/hail-is/hail/pull/3288:460,Safety,safe,safe,460,"It turns out that Kryo serialization is extra sneaky and will often just try to serialize the parts of a class if the class itself doesn't implement the KryoSerializable interface. I made a trait, `UnKryoSerializable`, that extends KryoSerializable but throws errors on read and write to try to weed out the rest of the places where UnsafeRows are being serialized. The biggest place this popped up was with colValues. For now, they're just being broadcast as safe Annotations everywhere. This depends on a change in #3258 and I'll rebase when that goes in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3288
https://github.com/hail-is/hail/pull/3289:281,Deployability,Update,Updated,281,"For linreg, logrem, lmmreg, and skat:; - changed Python implementation to annotate or select on `x` if not a field and always pass `x_field`, which must be float64 but may have missing values.; - changed Scala linreg, logrem, lmmreg, and skat to take `xField` rather than `xExpr`. Updated Scala tests with selectEntry accordingly.; - replaced RegressionUtils `inputVector` with `setMeanImputedDoubles`; - removed `dataset` parameter from Python. Now all methods that take a dataset and one or more required expressions on that dataset now only take the expressions. Updated docs, tests, tutorial accordingly.; - added `req_tstring` to linear_mixed_regression and `We plan to change the interface to this method in Hail 0.2 while maintaining its functionality.` The constraint is due to string assumption made when comparing and filtering column keys against keys on KinshipMatrix. Since the latter is going away (and marked as such), I don't think it's worth more changes to remove the constraint.; - made docs more consistent and variable names more generic (sample=>col, variant=>row, etc)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3289
https://github.com/hail-is/hail/pull/3289:566,Deployability,Update,Updated,566,"For linreg, logrem, lmmreg, and skat:; - changed Python implementation to annotate or select on `x` if not a field and always pass `x_field`, which must be float64 but may have missing values.; - changed Scala linreg, logrem, lmmreg, and skat to take `xField` rather than `xExpr`. Updated Scala tests with selectEntry accordingly.; - replaced RegressionUtils `inputVector` with `setMeanImputedDoubles`; - removed `dataset` parameter from Python. Now all methods that take a dataset and one or more required expressions on that dataset now only take the expressions. Updated docs, tests, tutorial accordingly.; - added `req_tstring` to linear_mixed_regression and `We plan to change the interface to this method in Hail 0.2 while maintaining its functionality.` The constraint is due to string assumption made when comparing and filtering column keys against keys on KinshipMatrix. Since the latter is going away (and marked as such), I don't think it's worth more changes to remove the constraint.; - made docs more consistent and variable names more generic (sample=>col, variant=>row, etc)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3289
https://github.com/hail-is/hail/pull/3289:686,Integrability,interface,interface,686,"For linreg, logrem, lmmreg, and skat:; - changed Python implementation to annotate or select on `x` if not a field and always pass `x_field`, which must be float64 but may have missing values.; - changed Scala linreg, logrem, lmmreg, and skat to take `xField` rather than `xExpr`. Updated Scala tests with selectEntry accordingly.; - replaced RegressionUtils `inputVector` with `setMeanImputedDoubles`; - removed `dataset` parameter from Python. Now all methods that take a dataset and one or more required expressions on that dataset now only take the expressions. Updated docs, tests, tutorial accordingly.; - added `req_tstring` to linear_mixed_regression and `We plan to change the interface to this method in Hail 0.2 while maintaining its functionality.` The constraint is due to string assumption made when comparing and filtering column keys against keys on KinshipMatrix. Since the latter is going away (and marked as such), I don't think it's worth more changes to remove the constraint.; - made docs more consistent and variable names more generic (sample=>col, variant=>row, etc)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3289
https://github.com/hail-is/hail/pull/3289:1031,Modifiability,variab,variable,1031,"For linreg, logrem, lmmreg, and skat:; - changed Python implementation to annotate or select on `x` if not a field and always pass `x_field`, which must be float64 but may have missing values.; - changed Scala linreg, logrem, lmmreg, and skat to take `xField` rather than `xExpr`. Updated Scala tests with selectEntry accordingly.; - replaced RegressionUtils `inputVector` with `setMeanImputedDoubles`; - removed `dataset` parameter from Python. Now all methods that take a dataset and one or more required expressions on that dataset now only take the expressions. Updated docs, tests, tutorial accordingly.; - added `req_tstring` to linear_mixed_regression and `We plan to change the interface to this method in Hail 0.2 while maintaining its functionality.` The constraint is due to string assumption made when comparing and filtering column keys against keys on KinshipMatrix. Since the latter is going away (and marked as such), I don't think it's worth more changes to remove the constraint.; - made docs more consistent and variable names more generic (sample=>col, variant=>row, etc)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3289
https://github.com/hail-is/hail/pull/3289:12,Testability,log,logrem,12,"For linreg, logrem, lmmreg, and skat:; - changed Python implementation to annotate or select on `x` if not a field and always pass `x_field`, which must be float64 but may have missing values.; - changed Scala linreg, logrem, lmmreg, and skat to take `xField` rather than `xExpr`. Updated Scala tests with selectEntry accordingly.; - replaced RegressionUtils `inputVector` with `setMeanImputedDoubles`; - removed `dataset` parameter from Python. Now all methods that take a dataset and one or more required expressions on that dataset now only take the expressions. Updated docs, tests, tutorial accordingly.; - added `req_tstring` to linear_mixed_regression and `We plan to change the interface to this method in Hail 0.2 while maintaining its functionality.` The constraint is due to string assumption made when comparing and filtering column keys against keys on KinshipMatrix. Since the latter is going away (and marked as such), I don't think it's worth more changes to remove the constraint.; - made docs more consistent and variable names more generic (sample=>col, variant=>row, etc)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3289
https://github.com/hail-is/hail/pull/3289:218,Testability,log,logrem,218,"For linreg, logrem, lmmreg, and skat:; - changed Python implementation to annotate or select on `x` if not a field and always pass `x_field`, which must be float64 but may have missing values.; - changed Scala linreg, logrem, lmmreg, and skat to take `xField` rather than `xExpr`. Updated Scala tests with selectEntry accordingly.; - replaced RegressionUtils `inputVector` with `setMeanImputedDoubles`; - removed `dataset` parameter from Python. Now all methods that take a dataset and one or more required expressions on that dataset now only take the expressions. Updated docs, tests, tutorial accordingly.; - added `req_tstring` to linear_mixed_regression and `We plan to change the interface to this method in Hail 0.2 while maintaining its functionality.` The constraint is due to string assumption made when comparing and filtering column keys against keys on KinshipMatrix. Since the latter is going away (and marked as such), I don't think it's worth more changes to remove the constraint.; - made docs more consistent and variable names more generic (sample=>col, variant=>row, etc)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3289
https://github.com/hail-is/hail/pull/3289:295,Testability,test,tests,295,"For linreg, logrem, lmmreg, and skat:; - changed Python implementation to annotate or select on `x` if not a field and always pass `x_field`, which must be float64 but may have missing values.; - changed Scala linreg, logrem, lmmreg, and skat to take `xField` rather than `xExpr`. Updated Scala tests with selectEntry accordingly.; - replaced RegressionUtils `inputVector` with `setMeanImputedDoubles`; - removed `dataset` parameter from Python. Now all methods that take a dataset and one or more required expressions on that dataset now only take the expressions. Updated docs, tests, tutorial accordingly.; - added `req_tstring` to linear_mixed_regression and `We plan to change the interface to this method in Hail 0.2 while maintaining its functionality.` The constraint is due to string assumption made when comparing and filtering column keys against keys on KinshipMatrix. Since the latter is going away (and marked as such), I don't think it's worth more changes to remove the constraint.; - made docs more consistent and variable names more generic (sample=>col, variant=>row, etc)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3289
https://github.com/hail-is/hail/pull/3289:580,Testability,test,tests,580,"For linreg, logrem, lmmreg, and skat:; - changed Python implementation to annotate or select on `x` if not a field and always pass `x_field`, which must be float64 but may have missing values.; - changed Scala linreg, logrem, lmmreg, and skat to take `xField` rather than `xExpr`. Updated Scala tests with selectEntry accordingly.; - replaced RegressionUtils `inputVector` with `setMeanImputedDoubles`; - removed `dataset` parameter from Python. Now all methods that take a dataset and one or more required expressions on that dataset now only take the expressions. Updated docs, tests, tutorial accordingly.; - added `req_tstring` to linear_mixed_regression and `We plan to change the interface to this method in Hail 0.2 while maintaining its functionality.` The constraint is due to string assumption made when comparing and filtering column keys against keys on KinshipMatrix. Since the latter is going away (and marked as such), I don't think it's worth more changes to remove the constraint.; - made docs more consistent and variable names more generic (sample=>col, variant=>row, etc)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3289
https://github.com/hail-is/hail/pull/3295:213,Integrability,depend,dependent,213,"- changed behavior of `linear_regression` to parallel that of the other stats method when an expression is passed for `y`. When `y` is a list of expressions (even a list of one expression), the the five phenotype-dependent annotations are still array of float64.; - expanded `test_linreg` to test that all the interfaces work as expected; - changed `n_complete_samples` to just `n` since we may introduce an option to subset rather than impute per variant (which would cause n to change per variant) and I don't want to overload the notion of complete_sample to depend on the mode. I also think `n` is clear in linear regression context as the number of data points used, and in any case it's documented as the number of columns used.; - fixed a bug whereby `sum_x` was still `AC` on the Scala side.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3295
https://github.com/hail-is/hail/pull/3295:310,Integrability,interface,interfaces,310,"- changed behavior of `linear_regression` to parallel that of the other stats method when an expression is passed for `y`. When `y` is a list of expressions (even a list of one expression), the the five phenotype-dependent annotations are still array of float64.; - expanded `test_linreg` to test that all the interfaces work as expected; - changed `n_complete_samples` to just `n` since we may introduce an option to subset rather than impute per variant (which would cause n to change per variant) and I don't want to overload the notion of complete_sample to depend on the mode. I also think `n` is clear in linear regression context as the number of data points used, and in any case it's documented as the number of columns used.; - fixed a bug whereby `sum_x` was still `AC` on the Scala side.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3295
https://github.com/hail-is/hail/pull/3295:562,Integrability,depend,depend,562,"- changed behavior of `linear_regression` to parallel that of the other stats method when an expression is passed for `y`. When `y` is a list of expressions (even a list of one expression), the the five phenotype-dependent annotations are still array of float64.; - expanded `test_linreg` to test that all the interfaces work as expected; - changed `n_complete_samples` to just `n` since we may introduce an option to subset rather than impute per variant (which would cause n to change per variant) and I don't want to overload the notion of complete_sample to depend on the mode. I also think `n` is clear in linear regression context as the number of data points used, and in any case it's documented as the number of columns used.; - fixed a bug whereby `sum_x` was still `AC` on the Scala side.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3295
https://github.com/hail-is/hail/pull/3295:292,Testability,test,test,292,"- changed behavior of `linear_regression` to parallel that of the other stats method when an expression is passed for `y`. When `y` is a list of expressions (even a list of one expression), the the five phenotype-dependent annotations are still array of float64.; - expanded `test_linreg` to test that all the interfaces work as expected; - changed `n_complete_samples` to just `n` since we may introduce an option to subset rather than impute per variant (which would cause n to change per variant) and I don't want to overload the notion of complete_sample to depend on the mode. I also think `n` is clear in linear regression context as the number of data points used, and in any case it's documented as the number of columns used.; - fixed a bug whereby `sum_x` was still `AC` on the Scala side.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3295
https://github.com/hail-is/hail/pull/3295:602,Usability,clear,clear,602,"- changed behavior of `linear_regression` to parallel that of the other stats method when an expression is passed for `y`. When `y` is a list of expressions (even a list of one expression), the the five phenotype-dependent annotations are still array of float64.; - expanded `test_linreg` to test that all the interfaces work as expected; - changed `n_complete_samples` to just `n` since we may introduce an option to subset rather than impute per variant (which would cause n to change per variant) and I don't want to overload the notion of complete_sample to depend on the mode. I also think `n` is clear in linear regression context as the number of data points used, and in any case it's documented as the number of columns used.; - fixed a bug whereby `sum_x` was still `AC` on the Scala side.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3295
https://github.com/hail-is/hail/pull/3297:150,Integrability,interface,interface,150,"currently just for the rows of MatrixTable, but I'd appreciate some feedback on what I've got so far. I think I'm probably just going to make all the interface changes on this branch/in this PR so they're all in one place before they go in?. from:; http://dev.hail.is/t/proposed-changes-to-the-python-select-annotate-drop-interface-for-keys/88",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3297
https://github.com/hail-is/hail/pull/3297:322,Integrability,interface,interface-for-keys,322,"currently just for the rows of MatrixTable, but I'd appreciate some feedback on what I've got so far. I think I'm probably just going to make all the interface changes on this branch/in this PR so they're all in one place before they go in?. from:; http://dev.hail.is/t/proposed-changes-to-the-python-select-annotate-drop-interface-for-keys/88",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3297
https://github.com/hail-is/hail/pull/3297:68,Usability,feedback,feedback,68,"currently just for the rows of MatrixTable, but I'd appreciate some feedback on what I've got so far. I think I'm probably just going to make all the interface changes on this branch/in this PR so they're all in one place before they go in?. from:; http://dev.hail.is/t/proposed-changes-to-the-python-select-annotate-drop-interface-for-keys/88",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3297
https://github.com/hail-is/hail/pull/3301:193,Testability,test,tests,193,"So, uh, just tell me if this is too big for one PR. This is all the changes necessary to actually use the regions produces in the ContextRDD. Calls to `Region`'s constructors should only be in tests or by the RVDContext. Sorry @jbloom22 the die came up Jon today. NB: Region is still on-heap, there's no free'ing yet. . cc: @cseed @catoverdrive",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3301
https://github.com/hail-is/hail/issues/3307:46,Testability,test,test,46,"replicate with:; ```; mt = hl.import_vcf('src/test/resources/sample2.vcf'); mt = hl.split_multi_hts(mt); mt.write('/tmp/foo.mt', overwrite=True); mt2 = hl.read_matrix_table('/tmp/foo.mt'); mt._same(mt2) # False; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3307
https://github.com/hail-is/hail/pull/3308:205,Modifiability,variab,variable,205,"fyi @cseed . All of the auxilliary methods have the same arguments as the original emitted function, to preserve the behavior of In and aggregator stuff without change. Any IR that binds a new environment variable (`Let`, `Map`, etc.) now stores those values on a class field so that other methods don't have to worry about whether or not there exist such values in scope. That's basically the only change I made to the structure of Emit; `MakeArray`, `MakeStruct`, and `MakeTuple` now check how big their fields/elements are. I'm a little worried we'll start hitting the class size limit; turning up one of the tests really far already hits it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3308
https://github.com/hail-is/hail/pull/3308:612,Testability,test,tests,612,"fyi @cseed . All of the auxilliary methods have the same arguments as the original emitted function, to preserve the behavior of In and aggregator stuff without change. Any IR that binds a new environment variable (`Let`, `Map`, etc.) now stores those values on a class field so that other methods don't have to worry about whether or not there exist such values in scope. That's basically the only change I made to the structure of Emit; `MakeArray`, `MakeStruct`, and `MakeTuple` now check how big their fields/elements are. I'm a little worried we'll start hitting the class size limit; turning up one of the tests really far already hits it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3308
https://github.com/hail-is/hail/pull/3311:116,Modifiability,refactor,refactoring,116,An unfortunately large set of changes to enable `writeRows` on a `ContextRDD`. I took the chance to do a wee bit of refactoring between the write methods of `UnpartitionedRVD` and `OrderedRVD`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3311
https://github.com/hail-is/hail/pull/3312:0,Testability,log,log,0,log.info IR when executing or interpreting. Example output:. ```; 2018-04-07 16:57:01 root: INFO: in MatrixTable.value: execute:; (MapEntries; (MatrixLiteral); (InsertFields; (Ref g); (x; (ApplyBinaryPrimOp Add; (GetField row_idx; (Ref va)); (GetField col_idx; (Ref sa))))); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3312
https://github.com/hail-is/hail/pull/3318:150,Availability,error,error,150,removed AnnotationImpex; removed some dead code from SparkAnnotationImpex; Table.toDF now requires expandTypes to have been run (will fail with match error in SparkAnnotationImpex.exportType); Handle tuple in Spark conversion (convert to struct),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3318
https://github.com/hail-is/hail/pull/3321:0,Performance,load,loadIRIntermediate,0,"loadIRIntermediate load binary and array, so it must be passed the element, but the element offset. It probably shouldn't do that but this was a quicker fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3321
https://github.com/hail-is/hail/pull/3321:19,Performance,load,load,19,"loadIRIntermediate load binary and array, so it must be passed the element, but the element offset. It probably shouldn't do that but this was a quicker fix.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3321
https://github.com/hail-is/hail/pull/3324:29,Testability,test,test,29,move the old entry points to test area,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3324
https://github.com/hail-is/hail/pull/3325:123,Integrability,Wrap,WrappedArrays,123,"Added FastSeq, FastIndexedSeq constructors.; .toFastSeq, toFastIndexedSeq now work on Iterable[T], not just array. Mapping WrappedArrays returns ArrayBuffers. I think this is forced by the Iterable/Seq interface, since map cannot take a ClassTag for new element type. ```; scala> Seq(1, 2, 3); res0: Seq[Int] = List(1, 2, 3). scala> val a = Seq(1, 2, 3); a: Seq[Int] = List(1, 2, 3). scala> val b: Seq[Int] = Array(1, 2, 3); b: Seq[Int] = WrappedArray(1, 2, 3). scala> a.map(_ + 1); res1: Seq[Int] = List(2, 3, 4). scala> b.map(_ + 1); res2: Seq[Int] = ArrayBuffer(2, 3, 4); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3325
https://github.com/hail-is/hail/pull/3325:202,Integrability,interface,interface,202,"Added FastSeq, FastIndexedSeq constructors.; .toFastSeq, toFastIndexedSeq now work on Iterable[T], not just array. Mapping WrappedArrays returns ArrayBuffers. I think this is forced by the Iterable/Seq interface, since map cannot take a ClassTag for new element type. ```; scala> Seq(1, 2, 3); res0: Seq[Int] = List(1, 2, 3). scala> val a = Seq(1, 2, 3); a: Seq[Int] = List(1, 2, 3). scala> val b: Seq[Int] = Array(1, 2, 3); b: Seq[Int] = WrappedArray(1, 2, 3). scala> a.map(_ + 1); res1: Seq[Int] = List(2, 3, 4). scala> b.map(_ + 1); res2: Seq[Int] = ArrayBuffer(2, 3, 4); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3325
https://github.com/hail-is/hail/pull/3325:439,Integrability,Wrap,WrappedArray,439,"Added FastSeq, FastIndexedSeq constructors.; .toFastSeq, toFastIndexedSeq now work on Iterable[T], not just array. Mapping WrappedArrays returns ArrayBuffers. I think this is forced by the Iterable/Seq interface, since map cannot take a ClassTag for new element type. ```; scala> Seq(1, 2, 3); res0: Seq[Int] = List(1, 2, 3). scala> val a = Seq(1, 2, 3); a: Seq[Int] = List(1, 2, 3). scala> val b: Seq[Int] = Array(1, 2, 3); b: Seq[Int] = WrappedArray(1, 2, 3). scala> a.map(_ + 1); res1: Seq[Int] = List(2, 3, 4). scala> b.map(_ + 1); res2: Seq[Int] = ArrayBuffer(2, 3, 4); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3325
https://github.com/hail-is/hail/pull/3330:32,Modifiability,rewrite,rewriters,32,"Mostly code reorg. Also:. moved rewriters to ir objects; call Optimize before intepreting; removed Filter{Rows, Cols} rules (non-IR), those should get folded back into the MT methods like other AST-based rules; re-enabled Fitler{Rows, Cols}IR fusion rules since logical and/or is back",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3330
https://github.com/hail-is/hail/pull/3330:62,Performance,Optimiz,Optimize,62,"Mostly code reorg. Also:. moved rewriters to ir objects; call Optimize before intepreting; removed Filter{Rows, Cols} rules (non-IR), those should get folded back into the MT methods like other AST-based rules; re-enabled Fitler{Rows, Cols}IR fusion rules since logical and/or is back",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3330
https://github.com/hail-is/hail/pull/3330:262,Testability,log,logical,262,"Mostly code reorg. Also:. moved rewriters to ir objects; call Optimize before intepreting; removed Filter{Rows, Cols} rules (non-IR), those should get folded back into the MT methods like other AST-based rules; re-enabled Fitler{Rows, Cols}IR fusion rules since logical and/or is back",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3330
https://github.com/hail-is/hail/pull/3332:625,Integrability,depend,dependent,625,"This probably needs a little cleanup. What's this for? Well, in another branch I have a bunch of IR rewrite optimizations. Those rewrite rules (1) want to test types (e.g. eliminate a cast of a type to itself), and they (2) also want to create new IRs which therefore need well-formed types. Calculating all the intermediate types explicitly (or calling Infer) everywhere both seem like non-starters. Therefore, I changed the IR nodes to compute their own types. I repurposed Infer, but it is no longer recursive. This meant that In, InAgg and Ref needed to carry their types, becuase, in the old, Infer-based way, they were dependent on the environment to type themselves, but that's no longer possible. I also repurposed Infer as a recursive type checker. This created two subtle problems: (1) the IR code uses rvRowType everywhere in stead of rowType (so it can reuse pointers to the full row) and (2) toIR needs to set the Ref type from the symbol table, but the symbol table strips out all missing bits, so the types on Ref terms disagreed with the actual values flowing around. I resolved this in two ways: (1) va now refers to the full rvRowType in all eval contexts, everywhere. (This is closer to the existing IR behavior.) (2) the symbol table no longer strips missingness, but it is stripped by Ref when the symbol is referenced. Ref also records the unstripped type which is used by toIR. The sooner we can kill AST, the better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3332
https://github.com/hail-is/hail/pull/3332:100,Modifiability,rewrite,rewrite,100,"This probably needs a little cleanup. What's this for? Well, in another branch I have a bunch of IR rewrite optimizations. Those rewrite rules (1) want to test types (e.g. eliminate a cast of a type to itself), and they (2) also want to create new IRs which therefore need well-formed types. Calculating all the intermediate types explicitly (or calling Infer) everywhere both seem like non-starters. Therefore, I changed the IR nodes to compute their own types. I repurposed Infer, but it is no longer recursive. This meant that In, InAgg and Ref needed to carry their types, becuase, in the old, Infer-based way, they were dependent on the environment to type themselves, but that's no longer possible. I also repurposed Infer as a recursive type checker. This created two subtle problems: (1) the IR code uses rvRowType everywhere in stead of rowType (so it can reuse pointers to the full row) and (2) toIR needs to set the Ref type from the symbol table, but the symbol table strips out all missing bits, so the types on Ref terms disagreed with the actual values flowing around. I resolved this in two ways: (1) va now refers to the full rvRowType in all eval contexts, everywhere. (This is closer to the existing IR behavior.) (2) the symbol table no longer strips missingness, but it is stripped by Ref when the symbol is referenced. Ref also records the unstripped type which is used by toIR. The sooner we can kill AST, the better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3332
https://github.com/hail-is/hail/pull/3332:129,Modifiability,rewrite,rewrite,129,"This probably needs a little cleanup. What's this for? Well, in another branch I have a bunch of IR rewrite optimizations. Those rewrite rules (1) want to test types (e.g. eliminate a cast of a type to itself), and they (2) also want to create new IRs which therefore need well-formed types. Calculating all the intermediate types explicitly (or calling Infer) everywhere both seem like non-starters. Therefore, I changed the IR nodes to compute their own types. I repurposed Infer, but it is no longer recursive. This meant that In, InAgg and Ref needed to carry their types, becuase, in the old, Infer-based way, they were dependent on the environment to type themselves, but that's no longer possible. I also repurposed Infer as a recursive type checker. This created two subtle problems: (1) the IR code uses rvRowType everywhere in stead of rowType (so it can reuse pointers to the full row) and (2) toIR needs to set the Ref type from the symbol table, but the symbol table strips out all missing bits, so the types on Ref terms disagreed with the actual values flowing around. I resolved this in two ways: (1) va now refers to the full rvRowType in all eval contexts, everywhere. (This is closer to the existing IR behavior.) (2) the symbol table no longer strips missingness, but it is stripped by Ref when the symbol is referenced. Ref also records the unstripped type which is used by toIR. The sooner we can kill AST, the better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3332
https://github.com/hail-is/hail/pull/3332:108,Performance,optimiz,optimizations,108,"This probably needs a little cleanup. What's this for? Well, in another branch I have a bunch of IR rewrite optimizations. Those rewrite rules (1) want to test types (e.g. eliminate a cast of a type to itself), and they (2) also want to create new IRs which therefore need well-formed types. Calculating all the intermediate types explicitly (or calling Infer) everywhere both seem like non-starters. Therefore, I changed the IR nodes to compute their own types. I repurposed Infer, but it is no longer recursive. This meant that In, InAgg and Ref needed to carry their types, becuase, in the old, Infer-based way, they were dependent on the environment to type themselves, but that's no longer possible. I also repurposed Infer as a recursive type checker. This created two subtle problems: (1) the IR code uses rvRowType everywhere in stead of rowType (so it can reuse pointers to the full row) and (2) toIR needs to set the Ref type from the symbol table, but the symbol table strips out all missing bits, so the types on Ref terms disagreed with the actual values flowing around. I resolved this in two ways: (1) va now refers to the full rvRowType in all eval contexts, everywhere. (This is closer to the existing IR behavior.) (2) the symbol table no longer strips missingness, but it is stripped by Ref when the symbol is referenced. Ref also records the unstripped type which is used by toIR. The sooner we can kill AST, the better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3332
https://github.com/hail-is/hail/pull/3332:155,Testability,test,test,155,"This probably needs a little cleanup. What's this for? Well, in another branch I have a bunch of IR rewrite optimizations. Those rewrite rules (1) want to test types (e.g. eliminate a cast of a type to itself), and they (2) also want to create new IRs which therefore need well-formed types. Calculating all the intermediate types explicitly (or calling Infer) everywhere both seem like non-starters. Therefore, I changed the IR nodes to compute their own types. I repurposed Infer, but it is no longer recursive. This meant that In, InAgg and Ref needed to carry their types, becuase, in the old, Infer-based way, they were dependent on the environment to type themselves, but that's no longer possible. I also repurposed Infer as a recursive type checker. This created two subtle problems: (1) the IR code uses rvRowType everywhere in stead of rowType (so it can reuse pointers to the full row) and (2) toIR needs to set the Ref type from the symbol table, but the symbol table strips out all missing bits, so the types on Ref terms disagreed with the actual values flowing around. I resolved this in two ways: (1) va now refers to the full rvRowType in all eval contexts, everywhere. (This is closer to the existing IR behavior.) (2) the symbol table no longer strips missingness, but it is stripped by Ref when the symbol is referenced. Ref also records the unstripped type which is used by toIR. The sooner we can kill AST, the better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3332
https://github.com/hail-is/hail/issues/3333:8,Performance,optimiz,optimize,8,less to optimize.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3333
https://github.com/hail-is/hail/pull/3336:0,Modifiability,rewrite,rewrite,0,rewrite select/annotate/drop to go through `_select_cols` in Python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3336
https://github.com/hail-is/hail/issues/3341:13,Performance,Load,LoadVCF,13,"It exists on LoadVCF in Scala, but isn't exposed in Python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3341
https://github.com/hail-is/hail/issues/3341:41,Security,expose,exposed,41,"It exists on LoadVCF in Scala, but isn't exposed in Python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3341
https://github.com/hail-is/hail/issues/3342:745,Availability,error,error,745,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:devel-abac611. ### What you did: spark-submit --jars build/libs/hail-all-spark.jar --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator --py-files build/distributions/hail-python.zip --num-executors 6 test.py; where test.y is:; import hail as hl; hl.init(master='yarn',default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test.vdf'). ### What went wrong (all error messages here, including the full java stack trace):; Error summary: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-abac611; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(243 + 1) / 244]2018-04-10 09:30:24 Hail: INFO: Coerced sorted dataset; Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:805,Availability,Error,Error,805,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:devel-abac611. ### What you did: spark-submit --jars build/libs/hail-all-spark.jar --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator --py-files build/distributions/hail-python.zip --num-executors 6 test.py; where test.y is:; import hail as hl; hl.init(master='yarn',default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test.vdf'). ### What went wrong (all error messages here, including the full java stack trace):; Error summary: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-abac611; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(243 + 1) / 244]2018-04-10 09:30:24 Hail: INFO: Coerced sorted dataset; Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:938,Availability,avail,available,938,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:devel-abac611. ### What you did: spark-submit --jars build/libs/hail-all-spark.jar --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator --py-files build/distributions/hail-python.zip --num-executors 6 test.py; where test.y is:; import hail as hl; hl.init(master='yarn',default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test.vdf'). ### What went wrong (all error messages here, including the full java stack trace):; Error summary: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-abac611; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(243 + 1) / 244]2018-04-10 09:30:24 Hail: INFO: Coerced sorted dataset; Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:2338,Availability,failure,failure,2338,"ed dataset; Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 4 times, most recent failure: Lost task 6.3 in stage 2.0 (TID 2 53, scc-q15.scc.bu.edu, executor 1): java.io.IOException: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.R",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:2395,Availability,failure,failure,2395,"ricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 4 times, most recent failure: Lost task 6.3 in stage 2.0 (TID 2 53, scc-q15.scc.bu.edu, executor 1): java.io.IOException: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:2537,Availability,Error,Error,2537,"llelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 4 times, most recent failure: Lost task 6.3 in stage 2.0 (TID 2 53, scc-q15.scc.bu.edu, executor 1): java.io.IOException: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:4069,Availability,Error,Error,4069,chRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:3,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:8695,Availability,Error,Error,8695,.collect(RDD.scala:935); at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:806); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:390); at is.hail.variant.MatrixTable.write(MatrixTable.scala:2428); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)java.io.IOException: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:10214,Availability,Error,Error,10214,is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:1953,Deployability,install,install,1953,"25.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-abac611; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(243 + 1) / 244]2018-04-10 09:30:24 Hail: INFO: Coerced sorted dataset; Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 4 times, most recent failure: Lost task 6.3 in stage 2.0 (TID 2 53, scc-q15.scc.bu.edu, executor 1): java.io.IOException: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:3626,Energy Efficiency,schedul,scheduler,3626,la:1310); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:3697,Energy Efficiency,schedul,scheduler,3697,lock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(Torrent,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:5807,Energy Efficiency,schedul,scheduler,5807,by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:5847,Energy Efficiency,schedul,scheduler,5847,zableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:5945,Energy Efficiency,schedul,scheduler,5945,ava:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:6042,Energy Efficiency,schedul,scheduler,6042,Loader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:6293,Energy Efficiency,schedul,scheduler,6293,); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:6373,Energy Efficiency,schedul,scheduler,6373,t java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:6478,Energy Efficiency,schedul,scheduler,6478,OrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:6626,Energy Efficiency,schedul,scheduler,6626,dObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:6714,Energy Efficiency,schedul,scheduler,6714,izer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:6811,Energy Efficiency,schedul,scheduler,6811,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:806); at is.h,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:6906,Energy Efficiency,schedul,scheduler,6906,.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:806); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:390); at is.hail.variant.MatrixTable.write(M,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:7069,Energy Efficiency,schedul,scheduler,7069,abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:806); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:390); at is.hail.variant.MatrixTable.write(MatrixTable.scala:2428); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:9784,Energy Efficiency,schedul,scheduler,9784,la:1310); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.To,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:9855,Energy Efficiency,schedul,scheduler,9855,lock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:11922,Energy Efficiency,schedul,scheduler,11922,la:1303); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:11993,Energy Efficiency,schedul,scheduler,11993,lock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(Objec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:14835,Energy Efficiency,schedul,scheduler,14835,ware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:14906,Energy Efficiency,schedul,scheduler,14906,ware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:751,Integrability,message,messages,751,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:devel-abac611. ### What you did: spark-submit --jars build/libs/hail-all-spark.jar --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator --py-files build/distributions/hail-python.zip --num-executors 6 test.py; where test.y is:; import hail as hl; hl.init(master='yarn',default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test.vdf'). ### What went wrong (all error messages here, including the full java stack trace):; Error summary: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-abac611; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(243 + 1) / 244]2018-04-10 09:30:24 Hail: INFO: Coerced sorted dataset; Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:1105,Integrability,Interface,Interfaces,1105,"cuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:devel-abac611. ### What you did: spark-submit --jars build/libs/hail-all-spark.jar --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator --py-files build/distributions/hail-python.zip --num-executors 6 test.py; where test.y is:; import hail as hl; hl.init(master='yarn',default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test.vdf'). ### What went wrong (all error messages here, including the full java stack trace):; Error summary: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-abac611; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(243 + 1) / 244]2018-04-10 09:30:24 Hail: INFO: Coerced sorted dataset; Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/u",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:3820,Performance,concurren,concurrent,3820,6); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:3904,Performance,concurren,concurrent,3904,; at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.U,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:4985,Performance,load,loadClass,4985,un(Thread.java:745); Caused by: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:5054,Performance,load,loadClass,5054,tion: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:5109,Performance,load,loadClass,5109,software.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); ... 18 more; Caused by: java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:9978,Performance,concurren,concurrent,9978,6); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:10062,Performance,concurren,concurrent,10062,; at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:65); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.Torren,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:12116,Performance,concurren,concurrent,12116,6); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:12200,Performance,concurren,concurrent,12200,; at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.Ja,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:12480,Performance,load,loadClass,12480,la:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:12549,Performance,load,loadClass,12549,x$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Ut,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:12604,Performance,load,loadClass,12604,.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBr,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:15029,Performance,concurren,concurrent,15029,ware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:15113,Performance,concurren,concurrent,15113,ware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790); at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:246); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:2317,Safety,abort,aborted,2317,"ed dataset; Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 4 times, most recent failure: Lost task 6.3 in stage 2.0 (TID 2 53, scc-q15.scc.bu.edu, executor 1): java.io.IOException: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:757); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.R",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:5977,Safety,abort,abortStage,5977,.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:6074,Safety,abort,abortStage,6074,her.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:6316,Safety,abort,abortStage,6316,m.readNonProxyDesc(ObjectInputStream.java:1819); at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1986); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); ... 25 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:463,Testability,test,test,463,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:devel-abac611. ### What you did: spark-submit --jars build/libs/hail-all-spark.jar --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator --py-files build/distributions/hail-python.zip --num-executors 6 test.py; where test.y is:; import hail as hl; hl.init(master='yarn',default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test.vdf'). ### What went wrong (all error messages here, including the full java stack trace):; Error summary: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-abac611; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(243 + 1) / 244]2018-04-10 09:30:24 Hail: INFO: Coerced sorted dataset; Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:478,Testability,test,test,478,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:devel-abac611. ### What you did: spark-submit --jars build/libs/hail-all-spark.jar --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator --py-files build/distributions/hail-python.zip --num-executors 6 test.py; where test.y is:; import hail as hl; hl.init(master='yarn',default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test.vdf'). ### What went wrong (all error messages here, including the full java stack trace):; Error summary: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-abac611; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(243 + 1) / 244]2018-04-10 09:30:24 Hail: INFO: Coerced sorted dataset; Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:708,Testability,test,test,708,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:devel-abac611. ### What you did: spark-submit --jars build/libs/hail-all-spark.jar --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator --py-files build/distributions/hail-python.zip --num-executors 6 test.py; where test.y is:; import hail as hl; hl.init(master='yarn',default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test.vdf'). ### What went wrong (all error messages here, including the full java stack trace):; Error summary: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-abac611; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(243 + 1) / 244]2018-04-10 09:30:24 Hail: INFO: Coerced sorted dataset; Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:1426,Testability,test,test,1426,"il-all-spark.jar --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator --py-files build/distributions/hail-python.zip --num-executors 6 test.py; where test.y is:; import hail as hl; hl.init(master='yarn',default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test.vdf'). ### What went wrong (all error messages here, including the full java stack trace):; Error summary: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-abac611; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(243 + 1) / 244]2018-04-10 09:30:24 Hail: INFO: Coerced sorted dataset; Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration. Java stack trace:; org.apache.spark.SparkException: Job a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/issues/3342:1605,Testability,test,test,1605,"sp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test.vdf'). ### What went wrong (all error messages here, including the full java stack trace):; Error summary: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-abac611; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(243 + 1) / 244]2018-04-10 09:30:24 Hail: INFO: Coerced sorted dataset; Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/test.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/gatk.hc/adsp-5k.hg38.tileDB.recalibrate_SNP.chr22.biallelic.4795samples.g.vcf.bgz').write('/project/casa/vdf.5k/test. vdf'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassNotFoundException: is.hail.utils.SerializableHadoopConfiguration. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 4 times, most recent failure: Lost task 6.3 in stage 2.0 (TID 2 53, scc-q15.scc.bu.edu, executor 1): java.io.IOException: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; at org.apache.spark.util.Utils$.t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3342
https://github.com/hail-is/hail/pull/3344:83,Integrability,depend,depends,83,"These are some easy to remove references to `RVD.rdd`. I added `treeReduce`, which depends on `treeAggregate` (#3310) which will hopefully be merged soon. In the meantime, I've added the `treeAggregate` methods here so this PR will pass its tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3344
https://github.com/hail-is/hail/pull/3344:241,Testability,test,tests,241,"These are some easy to remove references to `RVD.rdd`. I added `treeReduce`, which depends on `treeAggregate` (#3310) which will hopefully be merged soon. In the meantime, I've added the `treeAggregate` methods here so this PR will pass its tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3344
https://github.com/hail-is/hail/issues/3361:578,Availability,error,error,578,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-abac611 using Apache Spark version 2.2.0. ### What you did:; Tried converting joint genotyped Delly vcf file to vdf file. ```; import hail as hl; hl.init(default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Error reading a GL field. ; ##FORMAT=<ID=GL,Number=G,Type=Float,Description=""Log10-scaled genotype likelihoods for RR,RA,AA genotypes"">. where the entry was:; 0/1:-66.2667,0,-25.4754:10000:PASS:5639:13071:8160:2:0:0:13:27. Here are the log and error messages... ```; [Stage 1:======================================================>(415 + 1) / 416]2018-04-12 07:57:52 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 36) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:638,Availability,Error,Error,638,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-abac611 using Apache Spark version 2.2.0. ### What you did:; Tried converting joint genotyped Delly vcf file to vdf file. ```; import hail as hl; hl.init(default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Error reading a GL field. ; ##FORMAT=<ID=GL,Number=G,Type=Float,Description=""Log10-scaled genotype likelihoods for RR,RA,AA genotypes"">. where the entry was:; 0/1:-66.2667,0,-25.4754:10000:PASS:5639:13071:8160:2:0:0:13:27. Here are the log and error messages... ```; [Stage 1:======================================================>(415 + 1) / 416]2018-04-12 07:57:52 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 36) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:882,Availability,error,error,882,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-abac611 using Apache Spark version 2.2.0. ### What you did:; Tried converting joint genotyped Delly vcf file to vdf file. ```; import hail as hl; hl.init(default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Error reading a GL field. ; ##FORMAT=<ID=GL,Number=G,Type=Float,Description=""Log10-scaled genotype likelihoods for RR,RA,AA genotypes"">. where the entry was:; 0/1:-66.2667,0,-25.4754:10000:PASS:5639:13071:8160:2:0:0:13:27. Here are the log and error messages... ```; [Stage 1:======================================================>(415 + 1) / 416]2018-04-12 07:57:52 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 36) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:2016,Availability,failure,failure,2016,": INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 36) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:2074,Availability,failure,failure,2074,"416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:2308,Availability,Error,ErrorHandling,2308,"r-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:2334,Availability,Error,ErrorHandling,2334," in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:9404,Availability,Error,ErrorHandling,9404,"s.hail.variant.MatrixTable.write(MatrixTable.scala:2428); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz: caught java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:9430,Availability,Error,ErrorHandling,9430,"xTable.write(MatrixTable.scala:2428); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz: caught java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:16237,Availability,Error,Error,16237,"n$apply$2$$anonfun$apply$3.apply(RowStore.scala:767); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:766); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:766); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:763); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:763); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-abac611; Error summary: NumberFormatException: For input string: ""-66.2667,0,-25.4754""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:1638,Deployability,install,install,1638,"g a GL field. ; ##FORMAT=<ID=GL,Number=G,Type=Float,Description=""Log10-scaled genotype likelihoods for RR,RA,AA genotypes"">. where the entry was:; 0/1:-66.2667,0,-25.4754:10000:PASS:5639:13071:8160:2:0:0:13:27. Here are the log and error messages... ```; [Stage 1:======================================================>(415 + 1) / 416]2018-04-12 07:57:52 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 36) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:4995,Energy Efficiency,schedul,scheduler,4995,"anonfun$apply$2.apply(RowStore.scala:763); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:763); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddFiel",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:5066,Energy Efficiency,schedul,scheduler,5066,"ing(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:763); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6344,Energy Efficiency,schedul,scheduler,6344,"va.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6384,Energy Efficiency,schedul,scheduler,6384,"umberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6482,Energy Efficiency,schedul,scheduler,6482,tingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6579,Energy Efficiency,schedul,scheduler,6579,ble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6830,Energy Efficiency,schedul,scheduler,6830,Line.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6910,Energy Efficiency,schedul,scheduler,6910,eAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:7015,Energy Efficiency,schedul,scheduler,7015,; at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:7163,Energy Efficiency,schedul,scheduler,7163,$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:7251,Energy Efficiency,schedul,scheduler,7251,anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:7348,Energy Efficiency,schedul,scheduler,7348,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:806); at is.h,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:7443,Energy Efficiency,schedul,scheduler,7443,.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:806); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:390); at is.hail.variant.MatrixTable.write(M,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:7606,Energy Efficiency,schedul,scheduler,7606,abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:806); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:390); at is.hail.variant.MatrixTable.write(MatrixTable.scala:2428); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:12091,Energy Efficiency,schedul,scheduler,12091,"anonfun$apply$2.apply(RowStore.scala:763); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:763); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:12162,Energy Efficiency,schedul,scheduler,12162,"ing(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:763); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:15817,Energy Efficiency,schedul,scheduler,15817,"n$apply$2$$anonfun$apply$3.apply(RowStore.scala:767); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:766); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:766); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:763); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:763); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-abac611; Error summary: NumberFormatException: For input string: ""-66.2667,0,-25.4754""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:15888,Energy Efficiency,schedul,scheduler,15888,"n$apply$2$$anonfun$apply$3.apply(RowStore.scala:767); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:766); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:766); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:763); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:763); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-abac611; Error summary: NumberFormatException: For input string: ""-66.2667,0,-25.4754""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:584,Integrability,message,messages,584,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-abac611 using Apache Spark version 2.2.0. ### What you did:; Tried converting joint genotyped Delly vcf file to vdf file. ```; import hail as hl; hl.init(default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Error reading a GL field. ; ##FORMAT=<ID=GL,Number=G,Type=Float,Description=""Log10-scaled genotype likelihoods for RR,RA,AA genotypes"">. where the entry was:; 0/1:-66.2667,0,-25.4754:10000:PASS:5639:13071:8160:2:0:0:13:27. Here are the log and error messages... ```; [Stage 1:======================================================>(415 + 1) / 416]2018-04-12 07:57:52 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 36) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:888,Integrability,message,messages,888,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-abac611 using Apache Spark version 2.2.0. ### What you did:; Tried converting joint genotyped Delly vcf file to vdf file. ```; import hail as hl; hl.init(default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Error reading a GL field. ; ##FORMAT=<ID=GL,Number=G,Type=Float,Description=""Log10-scaled genotype likelihoods for RR,RA,AA genotypes"">. where the entry was:; 0/1:-66.2667,0,-25.4754:10000:PASS:5639:13071:8160:2:0:0:13:27. Here are the log and error messages... ```; [Stage 1:======================================================>(415 + 1) / 416]2018-04-12 07:57:52 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 36) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:2435,Integrability,wrap,wrapException,2435,"/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.Ric",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:9531,Integrability,wrap,wrapException,9531,".reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz: caught java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.Ric",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:2486,Performance,Load,LoadVCF,2486,"ectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:2532,Performance,Load,LoadVCF,2532,"ild/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(Ro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:5189,Performance,concurren,concurrent,5189,"io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:5273,Performance,concurren,concurrent,5273,"ail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:5865,Performance,Load,LoadVCF,5865," org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:5937,Performance,Load,LoadVCF,5937,".apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGSched",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6002,Performance,Load,LoadVCF,6002,"duler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6059,Performance,Load,LoadVCF,6059,"spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSet",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6097,Performance,Load,LoadVCF,6097,"g.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6123,Performance,Load,LoadVCF,6123,"ecutor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6161,Performance,Load,LoadVCF,6161,"35); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.schedule",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6187,Performance,Load,LoadVCF,6187,".concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6225,Performance,Load,LoadVCF,6225,"tor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6271,Performance,Load,LoadVCF,6271,"l.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:9582,Performance,Load,LoadVCF,9582,"orImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz: caught java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:9628,Performance,Load,LoadVCF,9628,"lect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz: caught java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(Ro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:12285,Performance,concurren,concurrent,12285,"io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:12369,Performance,concurren,concurrent,12369,"ail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:12948,Performance,Load,LoadVCF,12948,"scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:13020,Performance,Load,LoadVCF,13020,":323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:13085,Performance,Load,LoadVCF,13085,"he.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:13142,Performance,Load,LoadVCF,13142,"t org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:13180,Performance,Load,LoadVCF,13180,"a:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:13206,Performance,Load,LoadVCF,13206,"ache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:13244,Performance,Load,LoadVCF,13244,"cutor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:13270,Performance,Load,LoadVCF,13270," at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:13308,Performance,Load,LoadVCF,13308,"readPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:13354,Performance,Load,LoadVCF,13354,"; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745)java.lang.NumberFormatException: For input string: ""-66.2667,0,-25.4754""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:774); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:767); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(Ro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:16011,Performance,concurren,concurrent,16011,"n$apply$2$$anonfun$apply$3.apply(RowStore.scala:767); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:766); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:766); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:763); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:763); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-abac611; Error summary: NumberFormatException: For input string: ""-66.2667,0,-25.4754""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:16095,Performance,concurren,concurrent,16095,"n$apply$2$$anonfun$apply$3.apply(RowStore.scala:767); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:766); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:766); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:763); at is.hail.utils.package$.using(package.scala:576); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:763); at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:762); at is.hail.utils.package$.using(package.scala:576); at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:762); at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:756); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-abac611; Error summary: NumberFormatException: For input string: ""-66.2667,0,-25.4754""; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:1995,Safety,abort,aborted,1995,": INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 36) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 48 in stage 2.0 failed 4 times, most recent failure: Lost task 48.3 in stage 2.0 (TID 536, scc-q14.scc.bu.edu, executor 1): is.hail.utils.HailExcput string: ""-66.2667,0,-25.4754""; offending line: chr2 130824417 DEL00068296 AGAACAGGACATCCCAGGCAGCTACAGCCCATC...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:741); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:412); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:750); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6514,Safety,abort,abortStage,6514,rmatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6611,Safety,abort,abortStage,6611,va:110); at java.lang.Double.parseDouble(Double.java:538); at scala.collection.immutable.StringLike$class.toDouble(StringLike.scala:284); at scala.collection.immutable.StringOps.toDouble(StringOps.scala:29); at is.hail.io.vcf.VCFLine.parseDoubleInFormatArray(LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:6853,Safety,abort,abortStage,6853,LoadVCF.scala:371); at is.hail.io.vcf.VCFLine.parseAddFormatArrayDouble(LoadVCF.scala:431); at is.hail.io.vcf.FormatParser.parseAddField(LoadVCF.scala:483); at is.hail.io.vcf.FormatParser.parse(LoadVCF.scala:514); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:867); at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:848); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:717); ... 35 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/issues/3361:874,Testability,log,log,874,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-abac611 using Apache Spark version 2.2.0. ### What you did:; Tried converting joint genotyped Delly vcf file to vdf file. ```; import hail as hl; hl.init(default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Error reading a GL field. ; ##FORMAT=<ID=GL,Number=G,Type=Float,Description=""Log10-scaled genotype likelihoods for RR,RA,AA genotypes"">. where the entry was:; 0/1:-66.2667,0,-25.4754:10000:PASS:5639:13071:8160:2:0:0:13:27. Here are the log and error messages... ```; [Stage 1:======================================================>(415 + 1) / 416]2018-04-12 07:57:52 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 36) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-546>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 1956, in write; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NumberFormatException: For input string: ""-66.2667,0,-25.4754"". Java stack trace:; org.apache.spark.SparkException: Job aborte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3361
https://github.com/hail-is/hail/pull/3364:62,Availability,toler,tolerance,62,Current behavior is it's testing both D_== and abs(d1 -d2) <= tolerance. Now `absolute=True` specifies use `abs(d1-d2)`. Behavior used to be D_== for everything until I changed it for the BGEN test. So `absolute=True` should only be in the BGEN tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3364
https://github.com/hail-is/hail/pull/3364:25,Testability,test,testing,25,Current behavior is it's testing both D_== and abs(d1 -d2) <= tolerance. Now `absolute=True` specifies use `abs(d1-d2)`. Behavior used to be D_== for everything until I changed it for the BGEN test. So `absolute=True` should only be in the BGEN tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3364
https://github.com/hail-is/hail/pull/3364:193,Testability,test,test,193,Current behavior is it's testing both D_== and abs(d1 -d2) <= tolerance. Now `absolute=True` specifies use `abs(d1-d2)`. Behavior used to be D_== for everything until I changed it for the BGEN test. So `absolute=True` should only be in the BGEN tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3364
https://github.com/hail-is/hail/pull/3364:245,Testability,test,tests,245,Current behavior is it's testing both D_== and abs(d1 -d2) <= tolerance. Now `absolute=True` specifies use `abs(d1-d2)`. Behavior used to be D_== for everything until I changed it for the BGEN test. So `absolute=True` should only be in the BGEN tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3364
https://github.com/hail-is/hail/pull/3366:124,Testability,test,tests,124,- renamed query to aggregate to be consistent with Python; - moved aggregateGlobal to RichMatrixTable bc only used in Scala tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3366
https://github.com/hail-is/hail/pull/3371:483,Testability,test,tests,483,"This PR is incomplete. The goals are:; * Make the key on `Table` and `TableType` `Option`al.; * Make `Table.keyBy` produce an `OrderedRVD` backed `Table`. When setting `key = None`, the table has an `UnpartitionedRVD`.; * Make `RepartionedOrderedRVD` handle the case where the new partitioner has shorter partition keys than the old partitioner. Some things I know still need to be done:; * Fix `Table.same` to handle the case where the tables are unkeyed. This is currently causing tests `testKeyTableToDF` and `testSame` in `TableSuite` to fail.; * Make `Table.keyBy` smarter. In some cases, no sorting needs to be done, or maybe only local sorting.; * Add tests checking that writing then reading back an `OrderedRVD` backed table remains so, and that writing and reading can handle the `key = None` case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3371
https://github.com/hail-is/hail/pull/3371:490,Testability,test,testKeyTableToDF,490,"This PR is incomplete. The goals are:; * Make the key on `Table` and `TableType` `Option`al.; * Make `Table.keyBy` produce an `OrderedRVD` backed `Table`. When setting `key = None`, the table has an `UnpartitionedRVD`.; * Make `RepartionedOrderedRVD` handle the case where the new partitioner has shorter partition keys than the old partitioner. Some things I know still need to be done:; * Fix `Table.same` to handle the case where the tables are unkeyed. This is currently causing tests `testKeyTableToDF` and `testSame` in `TableSuite` to fail.; * Make `Table.keyBy` smarter. In some cases, no sorting needs to be done, or maybe only local sorting.; * Add tests checking that writing then reading back an `OrderedRVD` backed table remains so, and that writing and reading can handle the `key = None` case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3371
https://github.com/hail-is/hail/pull/3371:513,Testability,test,testSame,513,"This PR is incomplete. The goals are:; * Make the key on `Table` and `TableType` `Option`al.; * Make `Table.keyBy` produce an `OrderedRVD` backed `Table`. When setting `key = None`, the table has an `UnpartitionedRVD`.; * Make `RepartionedOrderedRVD` handle the case where the new partitioner has shorter partition keys than the old partitioner. Some things I know still need to be done:; * Fix `Table.same` to handle the case where the tables are unkeyed. This is currently causing tests `testKeyTableToDF` and `testSame` in `TableSuite` to fail.; * Make `Table.keyBy` smarter. In some cases, no sorting needs to be done, or maybe only local sorting.; * Add tests checking that writing then reading back an `OrderedRVD` backed table remains so, and that writing and reading can handle the `key = None` case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3371
https://github.com/hail-is/hail/pull/3371:659,Testability,test,tests,659,"This PR is incomplete. The goals are:; * Make the key on `Table` and `TableType` `Option`al.; * Make `Table.keyBy` produce an `OrderedRVD` backed `Table`. When setting `key = None`, the table has an `UnpartitionedRVD`.; * Make `RepartionedOrderedRVD` handle the case where the new partitioner has shorter partition keys than the old partitioner. Some things I know still need to be done:; * Fix `Table.same` to handle the case where the tables are unkeyed. This is currently causing tests `testKeyTableToDF` and `testSame` in `TableSuite` to fail.; * Make `Table.keyBy` smarter. In some cases, no sorting needs to be done, or maybe only local sorting.; * Add tests checking that writing then reading back an `OrderedRVD` backed table remains so, and that writing and reading can handle the `key = None` case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3371
https://github.com/hail-is/hail/issues/3372:255,Testability,log,logic,255,"This check:; ```; if e._ast.search(lambda ast: not isinstance(ast, TopLevelReference) and not isinstance(ast, Select)): ; ```; will let a join through, since a join isn't an AST node. We should probably just make a Join AST node, and push all of the join logic into the AST",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3372
https://github.com/hail-is/hail/pull/3373:0,Integrability,Depend,Depends,0,Depends on #3364.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3373
https://github.com/hail-is/hail/pull/3376:43,Testability,test,test,43,"Currently not used anywhere, but I wrote a test for it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3376
https://github.com/hail-is/hail/issues/3379:506,Availability,error,error,506,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-544bf8f. ### What you did: import vcf from delly; ```; import hail as hl; hl.init(default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. The CN field is an integer. When the integer is -1 for CN, an error is generated. ./.:0,0,0:0:LowQual:0:0:0:**-1**:0:0:0:0. The header defines CN with this:. ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Read-depth based copy-number estimate for autosomal sites"">. ```; Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; ```; Here is the full log and exception..... ```; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-544bf8f; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(414 + 2) / 416]2018-04-15 14:38:32 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 34) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-552>"", line 2, in write; File ""/restricted/projectnb/genpro/github/ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:628,Availability,error,error,628,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-544bf8f. ### What you did: import vcf from delly; ```; import hail as hl; hl.init(default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. The CN field is an integer. When the integer is -1 for CN, an error is generated. ./.:0,0,0:0:LowQual:0:0:0:**-1**:0:0:0:0. The header defines CN with this:. ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Read-depth based copy-number estimate for autosomal sites"">. ```; Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; ```; Here is the full log and exception..... ```; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-544bf8f; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(414 + 2) / 416]2018-04-15 14:38:32 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 34) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-552>"", line 2, in write; File ""/restricted/projectnb/genpro/github/ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:870,Availability,Error,Error,870,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-544bf8f. ### What you did: import vcf from delly; ```; import hail as hl; hl.init(default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. The CN field is an integer. When the integer is -1 for CN, an error is generated. ./.:0,0,0:0:LowQual:0:0:0:**-1**:0:0:0:0. The header defines CN with this:. ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Read-depth based copy-number estimate for autosomal sites"">. ```; Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; ```; Here is the full log and exception..... ```; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-544bf8f; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(414 + 2) / 416]2018-04-15 14:38:32 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 34) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-552>"", line 2, in write; File ""/restricted/projectnb/genpro/github/ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:1244,Availability,avail,available,1244,"n: devel-544bf8f. ### What you did: import vcf from delly; ```; import hail as hl; hl.init(default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. The CN field is an integer. When the integer is -1 for CN, an error is generated. ./.:0,0,0:0:LowQual:0:0:0:**-1**:0:0:0:0. The header defines CN with this:. ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Read-depth based copy-number estimate for autosomal sites"">. ```; Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; ```; Here is the full log and exception..... ```; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-544bf8f; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(414 + 2) / 416]2018-04-15 14:38:32 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 34) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-552>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/matrixtable.py"", line 1935, in write; self._jvds.write(output, overwrite, _codec_spec); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:2460,Availability,Error,Error,2460,"commend pulling; the latest changes weekly.; [Stage 1:======================================================>(414 + 2) / 416]2018-04-15 14:38:32 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 34) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-552>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/matrixtable.py"", line 1935, in write; self._jvds.write(output, overwrite, _codec_spec); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:2949,Availability,failure,failure,2949,"e 2, in write; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/matrixtable.py"", line 1935, in write; self._jvds.write(output, overwrite, _codec_spec); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(Order",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:3007,Availability,failure,failure,3007,"/python/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/matrixtable.py"", line 1935, in write; self._jvds.write(output, overwrite, _codec_spec); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasN",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:3426,Availability,Error,ErrorHandling,3426,"y"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowSto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:3452,Availability,Error,ErrorHandling,3452,"o; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:11550,Availability,Error,ErrorHandling,11550,"(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowSto",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:11576,Availability,Error,ErrorHandling,11576,"sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:16167,Availability,Error,Error,16167,"n.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:2279,Deployability,install,install,2279,"to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-544bf8f; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(414 + 2) / 416]2018-04-15 14:38:32 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 34) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-552>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/matrixtable.py"", line 1935, in write; self._jvds.write(output, overwrite, _codec_spec); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:7623,Energy Efficiency,schedul,scheduler,7623,lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.schedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:7694,Energy Efficiency,schedul,scheduler,7694,la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:8054,Energy Efficiency,schedul,scheduler,8054,); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:8094,Energy Efficiency,schedul,scheduler,8094,cala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:8192,Energy Efficiency,schedul,scheduler,8192,cala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:8289,Energy Efficiency,schedul,scheduler,8289,.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:8540,Energy Efficiency,schedul,scheduler,8540,Context$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:8620,Energy Efficiency,schedul,scheduler,8620,heduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:8725,Energy Efficiency,schedul,scheduler,8725,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:8873,Energy Efficiency,schedul,scheduler,8873,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:8961,Energy Efficiency,schedul,scheduler,8961,617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:9058,Energy Efficiency,schedul,scheduler,9058,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:95); at is.hail.io.RichConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:9153,Energy Efficiency,schedul,scheduler,9153,.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:95); at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:806); at is.hail.rvd.OrderedRVD.write,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:9316,Energy Efficiency,schedul,scheduler,9316,abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:95); at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:806); at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:391); at is.hail.expr.MatrixValue.write(Relational.scala:112); at is.hail.expr.ir.Interpret$.is$hail$expr$ir$Interpret$$interpret(Interp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:15747,Energy Efficiency,schedul,scheduler,15747,"n.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:15818,Energy Efficiency,schedul,scheduler,15818,"n.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:512,Integrability,message,messages,512,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-544bf8f. ### What you did: import vcf from delly; ```; import hail as hl; hl.init(default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. The CN field is an integer. When the integer is -1 for CN, an error is generated. ./.:0,0,0:0:LowQual:0:0:0:**-1**:0:0:0:0. The header defines CN with this:. ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Read-depth based copy-number estimate for autosomal sites"">. ```; Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; ```; Here is the full log and exception..... ```; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-544bf8f; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(414 + 2) / 416]2018-04-15 14:38:32 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 34) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-552>"", line 2, in write; File ""/restricted/projectnb/genpro/github/ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:1411,Integrability,Interface,Interfaces,1411,"bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. The CN field is an integer. When the integer is -1 for CN, an error is generated. ./.:0,0,0:0:LowQual:0:0:0:**-1**:0:0:0:0. The header defines CN with this:. ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Read-depth based copy-number estimate for autosomal sites"">. ```; Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; ```; Here is the full log and exception..... ```; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-544bf8f; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(414 + 2) / 416]2018-04-15 14:38:32 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 34) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-552>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/matrixtable.py"", line 1935, in write; self._jvds.write(output, overwrite, _codec_spec); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:3546,Performance,Load,LoadVCF,3546,"ailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is.hail.utils.package$.using(package.scala:575); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:3592,Performance,Load,LoadVCF,3592,".5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is.hail.utils.package$.using(package.scala:575); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:7817,Performance,concurren,concurrent,7817,eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:7901,Performance,concurren,concurrent,7901,ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:11670,Performance,Load,LoadVCF,11670,"dAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is.hail.utils.package$.using(package.scala:575); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:11716,Performance,Load,LoadVCF,11716,"tingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:768); at is.hail.utils.package$.using(package.scala:575); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:15941,Performance,concurren,concurrent,15941,"n.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:16025,Performance,concurren,concurrent,16025,"n.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:2928,Safety,abort,aborted,2928,"e 2, in write; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/matrixtable.py"", line 1935, in write; self._jvds.write(output, overwrite, _codec_spec); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(Order",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:8224,Safety,abort,abortStage,8224,llection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:8321,Safety,abort,abortStage,8321,ect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:8563,Safety,abort,abortStage,8563,y(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:1169,Testability,log,log,1169,"-------------------------------------------------------------------. ### Hail version: devel-544bf8f. ### What you did: import vcf from delly; ```; import hail as hl; hl.init(default_reference='GRCh38'); hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); ```. ### What went wrong (all error messages here, including the full java stack trace):. The CN field is an integer. When the integer is -1 for CN, an error is generated. ./.:0,0,0:0:LowQual:0:0:0:**-1**:0:0:0:0. The header defines CN with this:. ##FORMAT=<ID=CN,Number=1,Type=Integer,Description=""Read-depth based copy-number estimate for autosomal sites"">. ```; Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; ```; Here is the full log and exception..... ```; Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-544bf8f; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; [Stage 1:======================================================>(414 + 2) / 416]2018-04-15 14:38:32 Hail: INFO: Coerced almost-sorted dataset; [Stage 2:> (0 + 34) / 416]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail/delly_vcf2vdf.py"", line 3, in <module>; hl.import_vcf('/project/casa/vcf.5k/delly/gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-552>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/matr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:2839,Testability,log,log,2839,"gcad.sv.delly.5k.vcf.bgz').write('/project/casa/vdf.5k/delly'); File ""<decorator-gen-552>"", line 2, in write; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/matrixtable.py"", line 1935, in write; self._jvds.write(output, overwrite, _codec_spec); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:3376,Testability,log,log,3376,"ctnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 4 times, most recent failure: Lost task 20.3 in stage 2.0 (TID 485, scc-q08.scc.bu.edu, executor 2): is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:11500,Testability,log,log,11500," at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:12); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:744); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:775); at is.hail.io.RichContextRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/issues/3379:16457,Testability,log,log,16457,"n.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-544bf8f; Error summary: HailException: gcad.sv.delly.5k.vcf.bgz:column 80816: invalid character '-' in integer literal; ... 2:0:0:0:6 ./.:0,0,0:0:LowQual:0:0:0:-1:0:0:0:0 ./.:0,0,0:0:LowQual:0:0:0 ...; ^; offending line: chr1 152267996 DEL00028254 AATATATATACTTTACGTAAAGT A . PASS ...; see the Hail log for the full offending line; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3379
https://github.com/hail-is/hail/pull/3389:595,Usability,clear,clear,595,"Added:. - `textFilesLines` to go directly form array of file names to ContextRDD of strings. - `parallelize` with an `Option[Int]` (in addition to an overloaded version with an int parameter having a default value). - moved `run` to the top since it's super important. - eliminated two useless methods. - colocated `zipPartitions` with other zip-like methods. - added some `*AndContext*` methods that let the API user decide what context to give to the producer of values. The AndContext methods are useful for these two situations:. - One input value becomes multiple output values. We want to clear the input region separately from clearing the output region. - A zipPartitions / join where there are two inputs, we might want to use the same region for both inputs, but use a new region for the output. We might also want to use three distinct regions. All of these options are enabled by `czipPartitionsAndContext`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3389
https://github.com/hail-is/hail/pull/3389:634,Usability,clear,clearing,634,"Added:. - `textFilesLines` to go directly form array of file names to ContextRDD of strings. - `parallelize` with an `Option[Int]` (in addition to an overloaded version with an int parameter having a default value). - moved `run` to the top since it's super important. - eliminated two useless methods. - colocated `zipPartitions` with other zip-like methods. - added some `*AndContext*` methods that let the API user decide what context to give to the producer of values. The AndContext methods are useful for these two situations:. - One input value becomes multiple output values. We want to clear the input region separately from clearing the output region. - A zipPartitions / join where there are two inputs, we might want to use the same region for both inputs, but use a new region for the output. We might also want to use three distinct regions. All of these options are enabled by `czipPartitionsAndContext`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3389
https://github.com/hail-is/hail/pull/3391:325,Safety,avoid,avoids,325,"I also included a little change to use `Region.scoped` for writing a local `IndexedSeq[Annotation]`. In general, we cannot move Region off-heap until all the allocations of Regions are matched with `Region.close()`. We don't use the context's region yet in the persist. I'll introduce that in a later pull request. This just avoids serializing regions and region values, which will soon become off-heap, non-serializable values.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3391
https://github.com/hail-is/hail/pull/3392:375,Performance,Load,LoadVCF,375,"We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous small clean ups like unifying `RVD.rdd` and adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3392
https://github.com/hail-is/hail/pull/3392:682,Testability,test,tested,682,"We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous small clean ups like unifying `RVD.rdd` and adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3392
https://github.com/hail-is/hail/pull/3392:775,Testability,test,testReadWrite,775,"We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous small clean ups like unifying `RVD.rdd` and adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3392
https://github.com/hail-is/hail/pull/3392:1018,Testability,test,tests,1018,"We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous small clean ups like unifying `RVD.rdd` and adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3392
https://github.com/hail-is/hail/pull/3392:492,Usability,clear,clearing,492,"We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous small clean ups like unifying `RVD.rdd` and adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3392
https://github.com/hail-is/hail/pull/3392:603,Usability,clear,clear,603,"We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous small clean ups like unifying `RVD.rdd` and adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3392
https://github.com/hail-is/hail/pull/3392:646,Usability,clear,clear,646,"We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous small clean ups like unifying `RVD.rdd` and adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3392
https://github.com/hail-is/hail/pull/3392:832,Usability,clear,clear,832,"We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous small clean ups like unifying `RVD.rdd` and adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3392
https://github.com/hail-is/hail/pull/3392:921,Usability,clear,clear,921,"We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous small clean ups like unifying `RVD.rdd` and adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3392
https://github.com/hail-is/hail/pull/3394:495,Performance,Load,LoadVCF,495,"I could not re-open [the old PR](https://github.com/hail-is/hail/pull/3392) because I force-pushed after a rebase. ---. We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous clean ups like unifying `OrderedRVD.rdd` and `UnpartitionedRVD.rdd` as well as adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3394
https://github.com/hail-is/hail/pull/3394:802,Testability,test,tested,802,"I could not re-open [the old PR](https://github.com/hail-is/hail/pull/3392) because I force-pushed after a rebase. ---. We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous clean ups like unifying `OrderedRVD.rdd` and `UnpartitionedRVD.rdd` as well as adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3394
https://github.com/hail-is/hail/pull/3394:895,Testability,test,testReadWrite,895,"I could not re-open [the old PR](https://github.com/hail-is/hail/pull/3392) because I force-pushed after a rebase. ---. We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous clean ups like unifying `OrderedRVD.rdd` and `UnpartitionedRVD.rdd` as well as adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3394
https://github.com/hail-is/hail/pull/3394:1138,Testability,test,tests,1138,"I could not re-open [the old PR](https://github.com/hail-is/hail/pull/3392) because I force-pushed after a rebase. ---. We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous clean ups like unifying `OrderedRVD.rdd` and `UnpartitionedRVD.rdd` as well as adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3394
https://github.com/hail-is/hail/pull/3394:612,Usability,clear,clearing,612,"I could not re-open [the old PR](https://github.com/hail-is/hail/pull/3392) because I force-pushed after a rebase. ---. We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous clean ups like unifying `OrderedRVD.rdd` and `UnpartitionedRVD.rdd` as well as adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3394
https://github.com/hail-is/hail/pull/3394:723,Usability,clear,clear,723,"I could not re-open [the old PR](https://github.com/hail-is/hail/pull/3392) because I force-pushed after a rebase. ---. We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous clean ups like unifying `OrderedRVD.rdd` and `UnpartitionedRVD.rdd` as well as adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3394
https://github.com/hail-is/hail/pull/3394:766,Usability,clear,clear,766,"I could not re-open [the old PR](https://github.com/hail-is/hail/pull/3392) because I force-pushed after a rebase. ---. We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous clean ups like unifying `OrderedRVD.rdd` and `UnpartitionedRVD.rdd` as well as adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3394
https://github.com/hail-is/hail/pull/3394:952,Usability,clear,clear,952,"I could not re-open [the old PR](https://github.com/hail-is/hail/pull/3392) because I force-pushed after a rebase. ---. We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous clean ups like unifying `OrderedRVD.rdd` and `UnpartitionedRVD.rdd` as well as adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3394
https://github.com/hail-is/hail/pull/3394:1041,Usability,clear,clear,1041,"I could not re-open [the old PR](https://github.com/hail-is/hail/pull/3392) because I force-pushed after a rebase. ---. We want all allocations of `Region` to be controlled with a `using` or within a `RVDContext` (which will be appropriately closed). When we have achieved this, we can move the `Region` off-heap which provides a number of benefits including the use of raw-pointers in our Hail Object Representation as well as allocation free communication with other languages. This PR makes `LoadVCF` and `HailContext.readRows` use the regions in the `RVDContext`. Note that the _consumer_ is responsible for clearing the region when they're done with the current values. This is why `writePartitions` now includes `ctx.clear()`. Moreover, _producers_ must _not_ clear the region. These changes are tested by our whole infrastructure, but in particular, `is.hail.annotations.AnnotationsSuite.testReadWrite` exercises a lot of this. NB: We no longer clear the region between each read of a row. This means we could blow memory if we don't clear in the consumer. The other consumers are: aggregations, collects, shuffles, and joins. The tests pass though, so I guess I'm not too concerned for now. Once this is merged, I'll follow swiftly with uses of the RVDContext's region else where in our infrastructure. cc: @cseed . ---. I also included a couple miscellaneous clean ups like unifying `OrderedRVD.rdd` and `UnpartitionedRVD.rdd` as well as adding a use of `Region.scoped` in `HailContext`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3394
https://github.com/hail-is/hail/pull/3397:91,Integrability,interface,interfaces,91,"Core infrastructure enclosed. Opening now to enable feedback as I continue to higher level interfaces, tests, and examples.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3397
https://github.com/hail-is/hail/pull/3397:103,Testability,test,tests,103,"Core infrastructure enclosed. Opening now to enable feedback as I continue to higher level interfaces, tests, and examples.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3397
https://github.com/hail-is/hail/pull/3397:52,Usability,feedback,feedback,52,"Core infrastructure enclosed. Opening now to enable feedback as I continue to higher level interfaces, tests, and examples.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3397
https://github.com/hail-is/hail/issues/3398:156,Availability,error,error,156,"### Hail version:; latest master. ### What you did:; remove `region.clear()` from the `it.map` function inside `RVD.persistRVRDD`. ### What went wrong (all error messages here, including the full java stack trace):. `LDPruneSuite.testNoPrune` fails giving 313 variants instead of 338.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3398
https://github.com/hail-is/hail/issues/3398:162,Integrability,message,messages,162,"### Hail version:; latest master. ### What you did:; remove `region.clear()` from the `it.map` function inside `RVD.persistRVRDD`. ### What went wrong (all error messages here, including the full java stack trace):. `LDPruneSuite.testNoPrune` fails giving 313 variants instead of 338.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3398
https://github.com/hail-is/hail/issues/3398:230,Testability,test,testNoPrune,230,"### Hail version:; latest master. ### What you did:; remove `region.clear()` from the `it.map` function inside `RVD.persistRVRDD`. ### What went wrong (all error messages here, including the full java stack trace):. `LDPruneSuite.testNoPrune` fails giving 313 variants instead of 338.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3398
https://github.com/hail-is/hail/issues/3398:68,Usability,clear,clear,68,"### Hail version:; latest master. ### What you did:; remove `region.clear()` from the `it.map` function inside `RVD.persistRVRDD`. ### What went wrong (all error messages here, including the full java stack trace):. `LDPruneSuite.testNoPrune` fails giving 313 variants instead of 338.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3398
https://github.com/hail-is/hail/pull/3399:107,Usability,clear,cleared,107,"When regions are off-heap, we can allow the globals to live in a separate, longer-lived Region that is not cleared until the whole partition is finished. For now, we pay the memory cost. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3399
https://github.com/hail-is/hail/pull/3400:85,Safety,safe,safer,85,This Region will get cleared by consumers. I introduced the zip primitive which is a safer way to; zip two RVDs because it does not rely on the user correctly; clearing the regions used by the left and right hand sides; of the zip. cc: @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3400
https://github.com/hail-is/hail/pull/3400:21,Usability,clear,cleared,21,This Region will get cleared by consumers. I introduced the zip primitive which is a safer way to; zip two RVDs because it does not rely on the user correctly; clearing the regions used by the left and right hand sides; of the zip. cc: @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3400
https://github.com/hail-is/hail/pull/3400:160,Usability,clear,clearing,160,This Region will get cleared by consumers. I introduced the zip primitive which is a safer way to; zip two RVDs because it does not rely on the user correctly; clearing the regions used by the left and right hand sides; of the zip. cc: @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3400
https://github.com/hail-is/hail/issues/3404:957,Availability,Error,Error,957,"```; Traceback (most recent call last):; File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/pyscripts_rlCXpu.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; func(*args); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/assign_subpops.py"", line 16, in main; pop_table = exome_pop_table.union(genome_pop_table); File ""<decorator-gen-484>"", line 2, in union; File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/table.py"", line 1496, in union; return Table(self._jt.union([table._jt for table in tables])); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.table.Table.union(Table.scala:931); at is.hail.table.Table.union(Table.scala:928); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail v",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3404
https://github.com/hail-is/hail/issues/3404:2024,Availability,Error,Error,2024,"05c4bf85/pyscripts_rlCXpu.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; func(*args); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/assign_subpops.py"", line 16, in main; pop_table = exome_pop_table.union(genome_pop_table); File ""<decorator-gen-484>"", line 2, in union; File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/table.py"", line 1496, in union; return Table(self._jt.union([table._jt for table in tables])); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.table.Table.union(Table.scala:931); at is.hail.table.Table.union(Table.scala:928); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-5f23872; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3404
https://github.com/hail-is/hail/issues/3404:1061,Testability,Assert,AssertionError,1061,"2555c05c4bf85/pyscripts_rlCXpu.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; func(*args); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/assign_subpops.py"", line 16, in main; pop_table = exome_pop_table.union(genome_pop_table); File ""<decorator-gen-484>"", line 2, in union; File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/table.py"", line 1496, in union; return Table(self._jt.union([table._jt for table in tables])); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.table.Table.union(Table.scala:931); at is.hail.table.Table.union(Table.scala:928); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-5f23872; Error summary: AssertionError: assertion failed;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3404
https://github.com/hail-is/hail/issues/3404:1077,Testability,assert,assertion,1077,"2555c05c4bf85/pyscripts_rlCXpu.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; func(*args); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/assign_subpops.py"", line 16, in main; pop_table = exome_pop_table.union(genome_pop_table); File ""<decorator-gen-484>"", line 2, in union; File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/table.py"", line 1496, in union; return Table(self._jt.union([table._jt for table in tables])); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.table.Table.union(Table.scala:931); at is.hail.table.Table.union(Table.scala:928); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-5f23872; Error summary: AssertionError: assertion failed;",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3404
https://github.com/hail-is/hail/issues/3404:1124,Testability,Assert,AssertionError,1124,"05c4bf85/pyscripts_rlCXpu.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; func(*args); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/assign_subpops.py"", line 16, in main; pop_table = exome_pop_table.union(genome_pop_table); File ""<decorator-gen-484>"", line 2, in union; File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/table.py"", line 1496, in union; return Table(self._jt.union([table._jt for table in tables])); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.table.Table.union(Table.scala:931); at is.hail.table.Table.union(Table.scala:928); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-5f23872; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3404
https://github.com/hail-is/hail/issues/3404:1140,Testability,assert,assertion,1140,"05c4bf85/pyscripts_rlCXpu.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; func(*args); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/assign_subpops.py"", line 16, in main; pop_table = exome_pop_table.union(genome_pop_table); File ""<decorator-gen-484>"", line 2, in union; File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/table.py"", line 1496, in union; return Table(self._jt.union([table._jt for table in tables])); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.table.Table.union(Table.scala:931); at is.hail.table.Table.union(Table.scala:928); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-5f23872; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3404
https://github.com/hail-is/hail/issues/3404:1175,Testability,assert,assert,1175,"05c4bf85/pyscripts_rlCXpu.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; func(*args); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/assign_subpops.py"", line 16, in main; pop_table = exome_pop_table.union(genome_pop_table); File ""<decorator-gen-484>"", line 2, in union; File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/table.py"", line 1496, in union; return Table(self._jt.union([table._jt for table in tables])); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.table.Table.union(Table.scala:931); at is.hail.table.Table.union(Table.scala:928); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-5f23872; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3404
https://github.com/hail-is/hail/issues/3404:2039,Testability,Assert,AssertionError,2039,"05c4bf85/pyscripts_rlCXpu.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; func(*args); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/assign_subpops.py"", line 16, in main; pop_table = exome_pop_table.union(genome_pop_table); File ""<decorator-gen-484>"", line 2, in union; File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/table.py"", line 1496, in union; return Table(self._jt.union([table._jt for table in tables])); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.table.Table.union(Table.scala:931); at is.hail.table.Table.union(Table.scala:928); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-5f23872; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3404
https://github.com/hail-is/hail/issues/3404:2055,Testability,assert,assertion,2055,"05c4bf85/pyscripts_rlCXpu.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; func(*args); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/assign_subpops.py"", line 16, in main; pop_table = exome_pop_table.union(genome_pop_table); File ""<decorator-gen-484>"", line 2, in union; File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/typecheck/check.py"", line 481, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/table.py"", line 1496, in union; return Table(self._jt.union([table._jt for table in tables])); File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/aa0cd79aaa1a4f1ba652555c05c4bf85/hail-devel-3da0e7424af0.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.table.Table.union(Table.scala:931); at is.hail.table.Table.union(Table.scala:928); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-5f23872; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3404
https://github.com/hail-is/hail/pull/3409:105,Testability,test,test,105,"With 1 core on my local computer, it went from 49.832s to 10.254s with this change. I removed the random test (count=10) of pruning `sample.vcf` with random partition numbers, window size, and r2 threshold. This is replaced with one test with default parameters (R^2 = 0.2, window size = 1MB). I also moved the vcf import as a class value so it wasn't being reimported for each test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3409
https://github.com/hail-is/hail/pull/3409:233,Testability,test,test,233,"With 1 core on my local computer, it went from 49.832s to 10.254s with this change. I removed the random test (count=10) of pruning `sample.vcf` with random partition numbers, window size, and r2 threshold. This is replaced with one test with default parameters (R^2 = 0.2, window size = 1MB). I also moved the vcf import as a class value so it wasn't being reimported for each test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3409
https://github.com/hail-is/hail/pull/3409:378,Testability,test,test,378,"With 1 core on my local computer, it went from 49.832s to 10.254s with this change. I removed the random test (count=10) of pruning `sample.vcf` with random partition numbers, window size, and r2 threshold. This is replaced with one test with default parameters (R^2 = 0.2, window size = 1MB). I also moved the vcf import as a class value so it wasn't being reimported for each test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3409
https://github.com/hail-is/hail/issues/3413:392,Availability,error,error,392,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-63d60cc. ### What you did:; Tried to run sample_qc on mt table imported from Delly vcf. ; hl.sample_qc(ds). ### What went wrong (all error messages here, including the full java stack trace):. The sample_qc had a problem when alt ref is <DEL>. ```; [Stage 3:> (0 + 140) / 415]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail.qc/delly-qc.py"", line 35, in <module>; ds = hl.sample_qc(ds); File ""<decorator-gen-902>"", line 2, in sample_qc; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 490, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:1292,Availability,Error,Error,1292,"sample_qc on mt table imported from Delly vcf. ; hl.sample_qc(ds). ### What went wrong (all error messages here, including the full java stack trace):. The sample_qc had a problem when alt ref is <DEL>. ```; [Stage 3:> (0 + 140) / 415]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail.qc/delly-qc.py"", line 35, in <module>; ds = hl.sample_qc(ds); File ""<decorator-gen-902>"", line 2, in sample_qc; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 490, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.col",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:1512,Availability,failure,failure,1512,"`; [Stage 3:> (0 + 140) / 415]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail.qc/delly-qc.py"", line 35, in <module>; ds = hl.sample_qc(ds); File ""<decorator-gen-902>"", line 2, in sample_qc; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 490, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:1569,Availability,failure,failure,1569,"ast):; File ""/restricted/projectnb/genpro/github/hail.qc/delly-qc.py"", line 35, in <module>; ds = hl.sample_qc(ds); File ""<decorator-gen-902>"", line 2, in sample_qc; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 490, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:1718,Availability,Error,ErrorHandling,1718,"; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 490, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:1744,Availability,Error,ErrorHandling,1744,"projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 490, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:7447,Availability,Error,ErrorHandling,7447," at org.apache.spark.rdd.RDD.treeReduce(RDD.scala:1037); at is.hail.methods.SampleQC$.results(SampleQC.scala:206); at is.hail.methods.SampleQC$.apply(SampleQC.scala:221); at is.hail.methods.SampleQC.apply(SampleQC.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsR",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:7473,Availability,Error,ErrorHandling,7473,".rdd.RDD.treeReduce(RDD.scala:1037); at is.hail.methods.SampleQC$.results(SampleQC.scala:206); at is.hail.methods.SampleQC$.apply(SampleQC.scala:221); at is.hail.methods.SampleQC.apply(SampleQC.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:9738,Availability,Error,Error,9738,"leQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-63d60cc; Error summary: HailException: invalid allele ""<DEL>"". ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:1111,Deployability,install,install,1111,"/discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-63d60cc. ### What you did:; Tried to run sample_qc on mt table imported from Delly vcf. ; hl.sample_qc(ds). ### What went wrong (all error messages here, including the full java stack trace):. The sample_qc had a problem when alt ref is <DEL>. ```; [Stage 3:> (0 + 140) / 415]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail.qc/delly-qc.py"", line 35, in <module>; ds = hl.sample_qc(ds); File ""<decorator-gen-902>"", line 2, in sample_qc; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 490, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:3502,Energy Efficiency,schedul,scheduler,3502,org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:3581,Energy Efficiency,schedul,scheduler,3581,cala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:3660,Energy Efficiency,schedul,scheduler,3660,23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:4020,Energy Efficiency,schedul,scheduler,4020,a:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:4060,Energy Efficiency,schedul,scheduler,4060,; at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:4158,Energy Efficiency,schedul,scheduler,4158,at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:4255,Energy Efficiency,schedul,scheduler,4255,terator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:4506,Energy Efficiency,schedul,scheduler,4506,.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$reduc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:4586,Energy Efficiency,schedul,scheduler,4586,ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); at org.apache.spark.rdd.RDDOperationScope$.withScope(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:4691,Energy Efficiency,schedul,scheduler,4691,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:4839,Energy Efficiency,schedul,scheduler,4839,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); at org.apache.spark.rdd.RDD$$anonf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:4927,Energy Efficiency,schedul,scheduler,4927,617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); at org.apache.spark.rdd.RDDOperationScope$.wit,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:5024,Energy Efficiency,schedul,scheduler,5024,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOper,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:5119,Energy Efficiency,schedul,scheduler,5119,.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:5282,Energy Efficiency,schedul,scheduler,5282,abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128); at org.apache.spark.rdd.RDD$$anonfun$treeReduce$1.apply(RDD.scala:1059); at org.apache.spark.rdd.RDDOperationScope$.withSc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:5506,Energy Efficiency,reduce,reduce,5506,k.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128); at org.apache.spark.rdd.RDD$$anonfun$treeReduce$1.apply(RDD.scala:1059); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.treeReduce(RDD.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:5786,Energy Efficiency,reduce,reduce,5786,.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128); at org.apache.spark.rdd.RDD$$anonfun$treeReduce$1.apply(RDD.scala:1059); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.treeReduce(RDD.scala:1037); at is.hail.methods.SampleQC$.results(SampleQC.scala:206); at is.hail.methods.SampleQC$.apply(SampleQC.scala:221); at is.hail.methods.SampleQC.apply(SampleQC.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(Na,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:9231,Energy Efficiency,schedul,scheduler,9231,"leQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-63d60cc; Error summary: HailException: invalid allele ""<DEL>"". ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:9310,Energy Efficiency,schedul,scheduler,9310,"leQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-63d60cc; Error summary: HailException: invalid allele ""<DEL>"". ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:9389,Energy Efficiency,schedul,scheduler,9389,"leQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-63d60cc; Error summary: HailException: invalid allele ""<DEL>"". ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:398,Integrability,message,messages,398,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-63d60cc. ### What you did:; Tried to run sample_qc on mt table imported from Delly vcf. ; hl.sample_qc(ds). ### What went wrong (all error messages here, including the full java stack trace):. The sample_qc had a problem when alt ref is <DEL>. ```; [Stage 3:> (0 + 140) / 415]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail.qc/delly-qc.py"", line 35, in <module>; ds = hl.sample_qc(ds); File ""<decorator-gen-902>"", line 2, in sample_qc; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 490, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:3783,Performance,concurren,concurrent,3783,dd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:3867,Performance,concurren,concurrent,3867,RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:9512,Performance,concurren,concurrent,9512,"leQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-63d60cc; Error summary: HailException: invalid allele ""<DEL>"". ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:9596,Performance,concurren,concurrent,9596,"leQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: devel-63d60cc; Error summary: HailException: invalid allele ""<DEL>"". ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:1491,Safety,abort,aborted,1491,"`; [Stage 3:> (0 + 140) / 415]Traceback (most recent call last):; File ""/restricted/projectnb/genpro/github/hail.qc/delly-qc.py"", line 35, in <module>; ds = hl.sample_qc(ds); File ""<decorator-gen-902>"", line 2, in sample_qc; File ""/restricted/projectnb/genpro/github/hail/python/hail/typecheck/check.py"", line 490, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:4190,Safety,abort,abortStage,4190,d.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:4287,Safety,abort,abortStage,4287,; at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:4529,Safety,abort,abortStage,4529,eMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:1856,Security,validat,validate,1856,"rgs_); File ""/restricted/projectnb/genpro/github/hail/python/hail/methods/qc.py"", line 91, in sample_qc; return MatrixTable(Env.hail().methods.SampleQC.apply(require_biallelic(dataset, 'sample_qc')._jvds, name)); File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: HailException: invalid allele ""<DEL>"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 3.0 failed 4 times, most recent failure: Lost task 2.3 in stage 3.0 (TID 160, scc-q01.scc.bu.edu, executor 4): is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/issues/3413:7585,Security,validat,validate,7585,"C$.apply(SampleQC.scala:221); at is.hail.methods.SampleQC.apply(SampleQC.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: invalid allele ""<DEL>""; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:29); at is.hail.methods.SampleQCCombiner$.alleleIndices(SampleQC.scala:44); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:178); at is.hail.methods.SampleQC$$anonfun$results$1$$anonfun$apply$1.apply(SampleQC.scala:175); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:175); at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterat",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3413
https://github.com/hail-is/hail/pull/3417:31,Testability,test,tests,31,"- new IR node `ExportPlink`; - tests in Python; - Python `export_plink` now takes exprs for bim, fam, and call arguments; - BED and BIM file written in one pass now directly from region values; - BitPacker class",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3417
https://github.com/hail-is/hail/pull/3418:128,Safety,avoid,avoid,128,"`RegionValueBuilder` isn't serializable. Therefore, I had to initialize it for each seqop in `treeAggregate`. Is there a way to avoid this? @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3418
https://github.com/hail-is/hail/pull/3419:91,Testability,Test,Tested,91,13.7% improvement (47.3 => 40.8) over and above https://github.com/hail-is/hail/pull/3415. Tested on a shard of gnomAD with:. ```; mt = hl.read_matrix_table('gnomad.mt'); mt._force_count_rows(); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3419
https://github.com/hail-is/hail/pull/3420:39,Deployability,pipeline,pipeline,39,This improves a benchmark variant of a pipeline of @konradjk with six aggregators by 24% (140.6s => 106.8s) on a shard of gnomAD. The improvement will be larger with more aggregators. This change compiles the seqOp for all aggregators together into a single function and eliminates a loop over aggregators where the aggregations are invoked (e.g. MatrixMapRows). I added a SeqOp node that represents the call of a seqOp on a single region value aggregator. This replaces ApplyAggOp when extracting aggregators. I added a Begin node which just sequences collection of void-typed IRs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3420
https://github.com/hail-is/hail/pull/3420:16,Testability,benchmark,benchmark,16,This improves a benchmark variant of a pipeline of @konradjk with six aggregators by 24% (140.6s => 106.8s) on a shard of gnomAD. The improvement will be larger with more aggregators. This change compiles the seqOp for all aggregators together into a single function and eliminates a loop over aggregators where the aggregations are invoked (e.g. MatrixMapRows). I added a SeqOp node that represents the call of a seqOp on a single region value aggregator. This replaces ApplyAggOp when extracting aggregators. I added a Begin node which just sequences collection of void-typed IRs.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3420
https://github.com/hail-is/hail/pull/3422:959,Integrability,interface,interface,959,"Previously, MatrixMapRows was calling into an IR-generated function for each element. Now we aggregate the entire row from within Python. On a 6-aggregator benchmark on a shard of gnomAD, this improved things about 50% (1m59 => 1m02). Some notable changes:; - I added a Begin for sequencing void-type IR,; - I added ArrayFor for looping over arrays (@danking); - I added a SeqOp that represents calling the RegionValueAggregator in seqOp in the IR after extracting aggregators, it holds the index of the aggregator to call seqsOp on, since there might be multiple,; - added Void literal (which I didn't end up using, but I left it in for now),; - TAggreable symbol table is no longer used in compiling the extracted aggregators. The arguments need to specified explicitly and otherwise it is just another function (but takes an extra special argument, the array of RVAggregators),. This suggests some additional improvements/simplifications to the aggregator interface that I will write up on the dev forum.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3422
https://github.com/hail-is/hail/pull/3422:156,Testability,benchmark,benchmark,156,"Previously, MatrixMapRows was calling into an IR-generated function for each element. Now we aggregate the entire row from within Python. On a 6-aggregator benchmark on a shard of gnomAD, this improved things about 50% (1m59 => 1m02). Some notable changes:; - I added a Begin for sequencing void-type IR,; - I added ArrayFor for looping over arrays (@danking); - I added a SeqOp that represents calling the RegionValueAggregator in seqOp in the IR after extracting aggregators, it holds the index of the aggregator to call seqsOp on, since there might be multiple,; - added Void literal (which I didn't end up using, but I left it in for now),; - TAggreable symbol table is no longer used in compiling the extracted aggregators. The arguments need to specified explicitly and otherwise it is just another function (but takes an extra special argument, the array of RVAggregators),. This suggests some additional improvements/simplifications to the aggregator interface that I will write up on the dev forum.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3422
https://github.com/hail-is/hail/pull/3422:925,Usability,simpl,simplifications,925,"Previously, MatrixMapRows was calling into an IR-generated function for each element. Now we aggregate the entire row from within Python. On a 6-aggregator benchmark on a shard of gnomAD, this improved things about 50% (1m59 => 1m02). Some notable changes:; - I added a Begin for sequencing void-type IR,; - I added ArrayFor for looping over arrays (@danking); - I added a SeqOp that represents calling the RegionValueAggregator in seqOp in the IR after extracting aggregators, it holds the index of the aggregator to call seqsOp on, since there might be multiple,; - added Void literal (which I didn't end up using, but I left it in for now),; - TAggreable symbol table is no longer used in compiling the extracted aggregators. The arguments need to specified explicitly and otherwise it is just another function (but takes an extra special argument, the array of RVAggregators),. This suggests some additional improvements/simplifications to the aggregator interface that I will write up on the dev forum.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3422
https://github.com/hail-is/hail/pull/3424:239,Energy Efficiency,allocate,allocated,239,"Unfortunately, this killed the type inference ( Scala). That made the functions kind of unwieldy in-line, so I made them inner-method-definitions instead and added a type alias for `MultiArray2[RegionValueAggregator]`. The RVB will be allocated once per aggregated partition.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3424
https://github.com/hail-is/hail/pull/3426:89,Availability,down,down,89,"@cseed @danking it's a lot of lines of code, but I couldn't really figure out how to cut down on the boilerplate. Suggestions welcome.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3426
https://github.com/hail-is/hail/pull/3429:99,Modifiability,variab,variables,99,"remove unused let; push aggfilter into aggmap (if possible); fuse aggmaps; Added ir.Binds (list of variables bound by an IR); Added ir.Mentions (whether a variable is free in an IR); propagate NA where strict; fuse ArrayMaps. Also improved ir.Pretty (more headers, was missing some closing parens). This cleans up most of the small IR opportunities I've seen in @konradjk's scripts, although probably won't make a huge difference (except maybe pushing AggFilter into AggMap).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3429
https://github.com/hail-is/hail/pull/3429:155,Modifiability,variab,variable,155,"remove unused let; push aggfilter into aggmap (if possible); fuse aggmaps; Added ir.Binds (list of variables bound by an IR); Added ir.Mentions (whether a variable is free in an IR); propagate NA where strict; fuse ArrayMaps. Also improved ir.Pretty (more headers, was missing some closing parens). This cleans up most of the small IR opportunities I've seen in @konradjk's scripts, although probably won't make a huge difference (except maybe pushing AggFilter into AggMap).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3429
https://github.com/hail-is/hail/pull/3432:20,Usability,simpl,simplify,20,added flatten union simplify rule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3432
https://github.com/hail-is/hail/pull/3434:6,Performance,load,loadElement,6,"also `loadElement` instead of `elementOffset`; although they're equivalent as of now because we store structs inline, it was sematincally wrong since we need the pointer to the actual element itself and not the pointer inside the array. (this would cause problems if, e.g., we had physical types that stored the struct as a pointer.). This (TAGG thing) got caught by some tests in #3431.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3434
https://github.com/hail-is/hail/pull/3434:372,Testability,test,tests,372,"also `loadElement` instead of `elementOffset`; although they're equivalent as of now because we store structs inline, it was sematincally wrong since we need the pointer to the actual element itself and not the pointer inside the array. (this would cause problems if, e.g., we had physical types that stored the struct as a pointer.). This (TAGG thing) got caught by some tests in #3431.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3434
https://github.com/hail-is/hail/issues/3436:391,Availability,error,error,391,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-a870693. ### What you did:. ```; In [3]: x = hl.literal([1.0,2.2])\; In [4]: hl.eval_expr(hl.sum(x)); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-11-c37a34e6bd74> in <module>(); ----> 1 hl.eval_expr(hl.sum(x)). <decorator-gen-355> in eval_expr(expression). ~/tools/hail/python/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 488 def _typecheck(__orig_func__, *args, **kwargs):; 489 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 490 return __orig_func__(*args_, **kwargs_); 491; 492 return decorator(_typecheck). ~/tools/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 136 Result of evaluating `expression`.; 137 """"""; --> 138 return eval_expr_typed(expression)[0]; 139; 140. <decorator-gen-357> in eval_expr_typed(expression). ~/tools/hail/python/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 488 def _typecheck(__orig_func__, *args, **kwargs):; 489 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 490 return __orig_func__(*args_, **kwargs_); 491; 492 return decorator(_typecheck). ~/tools/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173; --> 174 return expression.collect()[0], expression.dtype; 175; 176. ~/tools/hail/python/hail/expr/expressions/base_expression.py in collect(self); 770 """"""; 771 uid = Env.get_uid(); --> 772 t = self._to_table(uid); 773 return [r[ui",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3436
https://github.com/hail-is/hail/issues/3436:4006,Availability,Error,Error,4006,"/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 479 def _typecheck(__orig_func__, *args, **kwargs):; 480 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=True); --> 481 return __orig_func__(*args_, **kwargs_); 482; 483 return decorator(_typecheck). ~/tools/hail/python/hail/table.py in _select(self, caller, s); 377 base, cleanup = self._process_joins(s); 378 analyze(caller, s, self._row_indices); --> 379 return cleanup(Table(base._jt.select(s._ast.to_hql()))); 380; 381 @typecheck_method(caller=str, s=expr_struct()). ~/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. ~/tools/hail/python/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: ClassCastException: java.lang.Integer cannot be cast to java.lang.Double. Java stack trace:; java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Double; 	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114); 	at is.hail.expr.ir.Literal$.apply(IR.scala:31); 	at is.hail.expr.ir.functions.UtilFunctions$$anonfun$registerAll$2.apply(UtilFunctions.scala:22); 	at is.hail.expr.ir.functions.UtilFunctions$$anonfun$registerAll$2.apply(UtilFunctions.scala:18); 	at is.hail.expr.ir.functions.RegistryFunctions$$anonfun$registerIR$2.apply(Functions.scala:165); 	at is.hail.expr.ir.functions.RegistryFunctions$$anonfun$registerIR$2.apply(Functions.scala:165); 	at is.hail.expr.ApplyMethod$$anonfun$toIR$12$$anonfun$apply$58.apply(AST.scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3436
https://github.com/hail-is/hail/issues/3436:6797,Availability,Error,Error,6797,	at is.hail.expr.ApplyMethod$$anonfun$toIR$12$$anonfun$apply$58.apply(AST.scala:844); 	at is.hail.expr.ApplyMethod$$anonfun$toIR$12$$anonfun$apply$58.apply(AST.scala:844); 	at scala.Option.map(Option.scala:146); 	at is.hail.expr.ApplyMethod$$anonfun$toIR$12.apply(AST.scala:844); 	at is.hail.expr.ApplyMethod$$anonfun$toIR$12.apply(AST.scala:842); 	at scala.Option.flatMap(Option.scala:171); 	at is.hail.expr.ApplyMethod.toIR(AST.scala:842); 	at is.hail.expr.StructConstructor$$anonfun$toIR$5.apply(AST.scala:410); 	at is.hail.expr.StructConstructor$$anonfun$toIR$5.apply(AST.scala:410); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.expr.StructConstructor.toIR(AST.scala:410); 	at is.hail.table.Table.select(Table.scala:512); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-a870693; Error summary: ClassCastException: java.lang.Integer cannot be cast to java.lang.Double; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3436
https://github.com/hail-is/hail/issues/3436:397,Integrability,message,messages,397,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-a870693. ### What you did:. ```; In [3]: x = hl.literal([1.0,2.2])\; In [4]: hl.eval_expr(hl.sum(x)); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-11-c37a34e6bd74> in <module>(); ----> 1 hl.eval_expr(hl.sum(x)). <decorator-gen-355> in eval_expr(expression). ~/tools/hail/python/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 488 def _typecheck(__orig_func__, *args, **kwargs):; 489 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 490 return __orig_func__(*args_, **kwargs_); 491; 492 return decorator(_typecheck). ~/tools/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 136 Result of evaluating `expression`.; 137 """"""; --> 138 return eval_expr_typed(expression)[0]; 139; 140. <decorator-gen-357> in eval_expr_typed(expression). ~/tools/hail/python/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 488 def _typecheck(__orig_func__, *args, **kwargs):; 489 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 490 return __orig_func__(*args_, **kwargs_); 491; 492 return decorator(_typecheck). ~/tools/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173; --> 174 return expression.collect()[0], expression.dtype; 175; 176. ~/tools/hail/python/hail/expr/expressions/base_expression.py in collect(self); 770 """"""; 771 uid = Env.get_uid(); --> 772 t = self._to_table(uid); 773 return [r[ui",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3436
https://github.com/hail-is/hail/pull/3439:10,Modifiability,refactor,refactored,10,binomTest refactored to use integer enum instead of string.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3439
https://github.com/hail-is/hail/pull/3442:127,Energy Efficiency,efficient,efficient,127,"Below, when I say ""a table `t` is ordered"" I mean `t.rvd.isInstanceOf[OrderedRVD]`. The goals of this PR are to; * Choose more efficient implementation options when a table is ordered, such as in joins.; * Preserve ordering when reading and writing table to disk (this already worked, but was not enforced in tests).; * Make the `key` field on `Table` and `TableType` optional. `key` should almost never be an empty list, because if ordered this would force all rows to compare equal under the key ordering, hence there can be only one partition. Instead, make `key = None`, in which case the table is required to be backed by an `UnpartitionedRVD`.; * Tables should by default be ordered. In particular, make `Table.keyBy` sort the table by default.; * Fix joins to allow the case where the left has a shorter partition key than the right. This is accomplished by fixing `RepartitionedOrderedRDD` to allow the new partition key to be shorter than the old.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3442
https://github.com/hail-is/hail/pull/3442:309,Testability,test,tests,309,"Below, when I say ""a table `t` is ordered"" I mean `t.rvd.isInstanceOf[OrderedRVD]`. The goals of this PR are to; * Choose more efficient implementation options when a table is ordered, such as in joins.; * Preserve ordering when reading and writing table to disk (this already worked, but was not enforced in tests).; * Make the `key` field on `Table` and `TableType` optional. `key` should almost never be an empty list, because if ordered this would force all rows to compare equal under the key ordering, hence there can be only one partition. Instead, make `key = None`, in which case the table is required to be backed by an `UnpartitionedRVD`.; * Tables should by default be ordered. In particular, make `Table.keyBy` sort the table by default.; * Fix joins to allow the case where the left has a shorter partition key than the right. This is accomplished by fixing `RepartitionedOrderedRDD` to allow the new partition key to be shorter than the old.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3442
https://github.com/hail-is/hail/issues/3446:236,Availability,failure,failure,236,"While running mendel_errors:; ```; hail.utils.java.FatalError: ClassCastException: is.hail.codegen.generated.C71 cannot be cast to is.hail.asm4s.AsmFunction7. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 143 in stage 13.0 failed 20 times, most recent failure: Lost task 143.19 in stage 13.0 (TID 4198, exomes2-sw-k2p2.c.broad-mpg-gnomad.internal, executor 212): java.lang.ClassCastException: is.hail.codegen.generated.C71 cannot be cast to is.hail.asm4s.AsmFunction7; 	at is.hail.expr.MatrixFilterEntries$$anonfun$54.apply(Relational.scala:1752); 	at is.hail.expr.MatrixFilterEntries$$anonfun$54.apply(Relational.scala:1750); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [...]; 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:478); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:488); 	at is.hail.rvd.OrderedRVD$.coerce(OrderedRVD.scala:556); 	at is.hail.rvd.OrderedRVD$.coerce(OrderedRVD.scala:514); 	at is.hail.table.Table.toOrderedRVD(Table.scala:1152); 	at is.hail.table.Table.distinctByKey(Table.scala:540); 	at sun.reflect.Nati",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3446
https://github.com/hail-is/hail/issues/3446:297,Availability,failure,failure,297,"While running mendel_errors:; ```; hail.utils.java.FatalError: ClassCastException: is.hail.codegen.generated.C71 cannot be cast to is.hail.asm4s.AsmFunction7. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 143 in stage 13.0 failed 20 times, most recent failure: Lost task 143.19 in stage 13.0 (TID 4198, exomes2-sw-k2p2.c.broad-mpg-gnomad.internal, executor 212): java.lang.ClassCastException: is.hail.codegen.generated.C71 cannot be cast to is.hail.asm4s.AsmFunction7; 	at is.hail.expr.MatrixFilterEntries$$anonfun$54.apply(Relational.scala:1752); 	at is.hail.expr.MatrixFilterEntries$$anonfun$54.apply(Relational.scala:1750); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [...]; 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:478); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:488); 	at is.hail.rvd.OrderedRVD$.coerce(OrderedRVD.scala:556); 	at is.hail.rvd.OrderedRVD$.coerce(OrderedRVD.scala:514); 	at is.hail.table.Table.toOrderedRVD(Table.scala:1152); 	at is.hail.table.Table.distinctByKey(Table.scala:540); 	at sun.reflect.Nati",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3446
https://github.com/hail-is/hail/issues/3446:2155,Availability,error,error,2155,"onsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [...]; 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:478); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:488); 	at is.hail.rvd.OrderedRVD$.coerce(OrderedRVD.scala:556); 	at is.hail.rvd.OrderedRVD$.coerce(OrderedRVD.scala:514); 	at is.hail.table.Table.toOrderedRVD(Table.scala:1152); 	at is.hail.table.Table.distinctByKey(Table.scala:540); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); ```; @gtiao got a similar error running ld_prune:; ```; Traceback (most recent call last):; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/assign_subpops.py"", line 157, in <module>; main(args); File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/assign_subpops.py"", line 98, in main; pca_mt = hl.ld_prune(pca_mt, r2=0.1, n_cores=args.num_cores); File ""<decorator-gen-788>"", line 2, in ld_prune; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/typecheck/check.py"", line 490, in _typecheck; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/methods/statgen.py"", line 2918, in ld_prune; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3446
https://github.com/hail-is/hail/issues/3446:215,Safety,abort,aborted,215,"While running mendel_errors:; ```; hail.utils.java.FatalError: ClassCastException: is.hail.codegen.generated.C71 cannot be cast to is.hail.asm4s.AsmFunction7. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 143 in stage 13.0 failed 20 times, most recent failure: Lost task 143.19 in stage 13.0 (TID 4198, exomes2-sw-k2p2.c.broad-mpg-gnomad.internal, executor 212): java.lang.ClassCastException: is.hail.codegen.generated.C71 cannot be cast to is.hail.asm4s.AsmFunction7; 	at is.hail.expr.MatrixFilterEntries$$anonfun$54.apply(Relational.scala:1752); 	at is.hail.expr.MatrixFilterEntries$$anonfun$54.apply(Relational.scala:1750); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [...]; 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:478); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:488); 	at is.hail.rvd.OrderedRVD$.coerce(OrderedRVD.scala:556); 	at is.hail.rvd.OrderedRVD$.coerce(OrderedRVD.scala:514); 	at is.hail.table.Table.toOrderedRVD(Table.scala:1152); 	at is.hail.table.Table.distinctByKey(Table.scala:540); 	at sun.reflect.Nati",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3446
https://github.com/hail-is/hail/issues/3447:363,Availability,failure,failure,363,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:422,Availability,failure,failure,422,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:3341,Availability,Error,Error,3341, org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at is.hail.utils.richUtils.RichRDD$.countPerPartition$extension(RichRDD.scala:121); at is.hail.rvd.RVD$class.countPerPartition(RVD.scala:185); at is.hail.rvd.OrderedRVD.countPerPartition(OrderedRVD.scala:19); at is.hail.variant.MatrixTable.partitionCounts(MatrixTable.scala:535); at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:1128); at is.hail.variant.MatrixTable.count(MatrixTable.scala:1126); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassCastException: null; at . Hail version: devel-e6de08e; Error summary: ClassCastException: null; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:584,Energy Efficiency,schedul,scheduler,584,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:624,Energy Efficiency,schedul,scheduler,624,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:722,Energy Efficiency,schedul,scheduler,722,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:819,Energy Efficiency,schedul,scheduler,819,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:1070,Energy Efficiency,schedul,scheduler,1070,"gen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(Sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:1150,Energy Efficiency,schedul,scheduler,1150,"s the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:1255,Energy Efficiency,schedul,scheduler,1255,"ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at is.hail.utils.richUtils.RichRDD$.countPerPartition$extension(RichRDD.scala:121); at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:1403,Energy Efficiency,schedul,scheduler,1403,", most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at is.hail.utils.richUtils.RichRDD$.countPerPartition$extension(RichRDD.scala:121); at is.hail.rvd.RVD$class.countPerPartition(RVD.scala:185); at is.hail.rvd.OrderedRVD.countPerPartition(OrderedRVD.scala:19); at is.hail.variant.MatrixTable.par",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:1491,Energy Efficiency,schedul,scheduler,1491,"al, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at is.hail.utils.richUtils.RichRDD$.countPerPartition$extension(RichRDD.scala:121); at is.hail.rvd.RVD$class.countPerPartition(RVD.scala:185); at is.hail.rvd.OrderedRVD.countPerPartition(OrderedRVD.scala:19); at is.hail.variant.MatrixTable.partitionCounts(MatrixTable.scala:535); at is.hail.variant.MatrixTable.countRows(MatrixTabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:1588,Energy Efficiency,schedul,scheduler,1588,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at is.hail.utils.richUtils.RichRDD$.countPerPartition$extension(RichRDD.scala:121); at is.hail.rvd.RVD$class.countPerPartition(RVD.scala:185); at is.hail.rvd.OrderedRVD.countPerPartition(OrderedRVD.scala:19); at is.hail.variant.MatrixTable.partitionCounts(MatrixTable.scala:535); at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:1128); at is.hail.variant.MatrixTable.count(MatrixTable.scala:1126); at sun.reflect.Nativ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:1683,Energy Efficiency,schedul,scheduler,1683,.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at is.hail.utils.richUtils.RichRDD$.countPerPartition$extension(RichRDD.scala:121); at is.hail.rvd.RVD$class.countPerPartition(RVD.scala:185); at is.hail.rvd.OrderedRVD.countPerPartition(OrderedRVD.scala:19); at is.hail.variant.MatrixTable.partitionCounts(MatrixTable.scala:535); at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:1128); at is.hail.variant.MatrixTable.count(MatrixTable.scala:1126); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(Nati,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:1846,Energy Efficiency,schedul,scheduler,1846,abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at is.hail.utils.richUtils.RichRDD$.countPerPartition$extension(RichRDD.scala:121); at is.hail.rvd.RVD$class.countPerPartition(RVD.scala:185); at is.hail.rvd.OrderedRVD.countPerPartition(OrderedRVD.scala:19); at is.hail.variant.MatrixTable.partitionCounts(MatrixTable.scala:535); at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:1128); at is.hail.variant.MatrixTable.count(MatrixTable.scala:1126); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:62,Performance,load,loaded,62,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:342,Safety,abort,aborted,342,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:754,Safety,abort,abortStage,754,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:851,Safety,abort,abortStage,851,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:1093,Safety,abort,abortStage,1093,"ut `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/issues/3447:27,Usability,simpl,simple,27,"From @armartin on a pretty simple line of code (ukbb was just loaded from bgen, tgp was just ld_pruned, but `count`ed before that, so I don't think that was the problem):. `ukbb_in_tgp = ukbb.filter_rows(hl.is_defined(tgp[ukbb.row_key, :]))`. ```; FatalError: ClassCastException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 20 times, most recent failure: Lost task 0.19 in stage 40.0 (TID 2222, pca-w-8.c.daly-ibd.internal, executor 25): java.lang.ClassCastException. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3447
https://github.com/hail-is/hail/pull/3449:79,Availability,error,error,79,Expands the interface to permit importing with no entry fields (previously was error),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3449
https://github.com/hail-is/hail/pull/3449:12,Integrability,interface,interface,12,Expands the interface to permit importing with no entry fields (previously was error),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3449
https://github.com/hail-is/hail/pull/3455:146,Deployability,upgrade,upgraded,146,"Fixes https://github.com/hail-is/hail/issues/3454. Removed cassandra and solr collectors which, as far as I know, are now unused. Note, vs 0.1, I upgraded the elasticsearch-Spark connector to 6.2.4 and modified our code slightly (I call the Spark SQL DataFrame writer instead of the legacy, not the basic RDD writer). I'm not sure if this will effect seqr compatibility. @bw2 Any plans to migrate to Hail 0.2? I guess you need the annotationdb to be ported first?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3455
https://github.com/hail-is/hail/pull/3461:1013,Deployability,install,installed,1013,"This is the current state of the C++ support. If you look at the tests in src/test/is/hail/nativecode/NativeCodeSuite.scala that should give a; quick overview of how it works, viz. 1. Generate C++ source code as a Scala String, then create a NativeModule which handles; the grunt work of getting it compiled, linked, and loaded, and allows you to look up functions; by name, and get a callable Scala object corresponding to the C++ function. 2. The NativeModule also allows the binary of the DLL to be passed around and instantiated; on other cluster nodes (but note that those nodes will need to have the correct versions of; the C++ runtime shared libraries in the right directories to allow symbols in the DLL to be; correctly resolved). This is not tested yet. 3. I have been using llvm-6.0.0 on Mac, and llvm-5.0 on linux. It makes a half-hearted attempt; to use whatever other compiler you have, but that may not work. We probably need to figure; out a standardized and automated way to get the right tools installed in the right place (and; get the right shared libraries on worker nodes). 4. Data which needs to be accessible to both Scala and C++ is held in C++ objects inheriting; from NativeObj, with lifetimes managed by std::shared_ptr, i.e. reference-counted. There's; some dirty under-the-hood plumbing to allow a shared_ptr to be smuggled into a Scala; object derived from NativeBase. These Scala-side object references must be managed; carefully using copyAssign/moveAssign/close in order to maintain the off-heap ref-counts. 5. There are some gnarly differences between Linux and MacOSX in the linker and dynamic; loading. I think I'm converging on the right compile/link options for each, but in getting; Linux to work it's possible that Mac is temporarily broken ... Not really expecting that we'll merge this right away, but I wanted to put it out there to get the; review process started before it grows any bigger.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3461
https://github.com/hail-is/hail/pull/3461:1179,Modifiability,inherit,inheriting,1179,"This is the current state of the C++ support. If you look at the tests in src/test/is/hail/nativecode/NativeCodeSuite.scala that should give a; quick overview of how it works, viz. 1. Generate C++ source code as a Scala String, then create a NativeModule which handles; the grunt work of getting it compiled, linked, and loaded, and allows you to look up functions; by name, and get a callable Scala object corresponding to the C++ function. 2. The NativeModule also allows the binary of the DLL to be passed around and instantiated; on other cluster nodes (but note that those nodes will need to have the correct versions of; the C++ runtime shared libraries in the right directories to allow symbols in the DLL to be; correctly resolved). This is not tested yet. 3. I have been using llvm-6.0.0 on Mac, and llvm-5.0 on linux. It makes a half-hearted attempt; to use whatever other compiler you have, but that may not work. We probably need to figure; out a standardized and automated way to get the right tools installed in the right place (and; get the right shared libraries on worker nodes). 4. Data which needs to be accessible to both Scala and C++ is held in C++ objects inheriting; from NativeObj, with lifetimes managed by std::shared_ptr, i.e. reference-counted. There's; some dirty under-the-hood plumbing to allow a shared_ptr to be smuggled into a Scala; object derived from NativeBase. These Scala-side object references must be managed; carefully using copyAssign/moveAssign/close in order to maintain the off-heap ref-counts. 5. There are some gnarly differences between Linux and MacOSX in the linker and dynamic; loading. I think I'm converging on the right compile/link options for each, but in getting; Linux to work it's possible that Mac is temporarily broken ... Not really expecting that we'll merge this right away, but I wanted to put it out there to get the; review process started before it grows any bigger.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3461
https://github.com/hail-is/hail/pull/3461:321,Performance,load,loaded,321,"This is the current state of the C++ support. If you look at the tests in src/test/is/hail/nativecode/NativeCodeSuite.scala that should give a; quick overview of how it works, viz. 1. Generate C++ source code as a Scala String, then create a NativeModule which handles; the grunt work of getting it compiled, linked, and loaded, and allows you to look up functions; by name, and get a callable Scala object corresponding to the C++ function. 2. The NativeModule also allows the binary of the DLL to be passed around and instantiated; on other cluster nodes (but note that those nodes will need to have the correct versions of; the C++ runtime shared libraries in the right directories to allow symbols in the DLL to be; correctly resolved). This is not tested yet. 3. I have been using llvm-6.0.0 on Mac, and llvm-5.0 on linux. It makes a half-hearted attempt; to use whatever other compiler you have, but that may not work. We probably need to figure; out a standardized and automated way to get the right tools installed in the right place (and; get the right shared libraries on worker nodes). 4. Data which needs to be accessible to both Scala and C++ is held in C++ objects inheriting; from NativeObj, with lifetimes managed by std::shared_ptr, i.e. reference-counted. There's; some dirty under-the-hood plumbing to allow a shared_ptr to be smuggled into a Scala; object derived from NativeBase. These Scala-side object references must be managed; carefully using copyAssign/moveAssign/close in order to maintain the off-heap ref-counts. 5. There are some gnarly differences between Linux and MacOSX in the linker and dynamic; loading. I think I'm converging on the right compile/link options for each, but in getting; Linux to work it's possible that Mac is temporarily broken ... Not really expecting that we'll merge this right away, but I wanted to put it out there to get the; review process started before it grows any bigger.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3461
https://github.com/hail-is/hail/pull/3461:1632,Performance,load,loading,1632,"This is the current state of the C++ support. If you look at the tests in src/test/is/hail/nativecode/NativeCodeSuite.scala that should give a; quick overview of how it works, viz. 1. Generate C++ source code as a Scala String, then create a NativeModule which handles; the grunt work of getting it compiled, linked, and loaded, and allows you to look up functions; by name, and get a callable Scala object corresponding to the C++ function. 2. The NativeModule also allows the binary of the DLL to be passed around and instantiated; on other cluster nodes (but note that those nodes will need to have the correct versions of; the C++ runtime shared libraries in the right directories to allow symbols in the DLL to be; correctly resolved). This is not tested yet. 3. I have been using llvm-6.0.0 on Mac, and llvm-5.0 on linux. It makes a half-hearted attempt; to use whatever other compiler you have, but that may not work. We probably need to figure; out a standardized and automated way to get the right tools installed in the right place (and; get the right shared libraries on worker nodes). 4. Data which needs to be accessible to both Scala and C++ is held in C++ objects inheriting; from NativeObj, with lifetimes managed by std::shared_ptr, i.e. reference-counted. There's; some dirty under-the-hood plumbing to allow a shared_ptr to be smuggled into a Scala; object derived from NativeBase. These Scala-side object references must be managed; carefully using copyAssign/moveAssign/close in order to maintain the off-heap ref-counts. 5. There are some gnarly differences between Linux and MacOSX in the linker and dynamic; loading. I think I'm converging on the right compile/link options for each, but in getting; Linux to work it's possible that Mac is temporarily broken ... Not really expecting that we'll merge this right away, but I wanted to put it out there to get the; review process started before it grows any bigger.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3461
https://github.com/hail-is/hail/pull/3461:1123,Security,access,accessible,1123,"This is the current state of the C++ support. If you look at the tests in src/test/is/hail/nativecode/NativeCodeSuite.scala that should give a; quick overview of how it works, viz. 1. Generate C++ source code as a Scala String, then create a NativeModule which handles; the grunt work of getting it compiled, linked, and loaded, and allows you to look up functions; by name, and get a callable Scala object corresponding to the C++ function. 2. The NativeModule also allows the binary of the DLL to be passed around and instantiated; on other cluster nodes (but note that those nodes will need to have the correct versions of; the C++ runtime shared libraries in the right directories to allow symbols in the DLL to be; correctly resolved). This is not tested yet. 3. I have been using llvm-6.0.0 on Mac, and llvm-5.0 on linux. It makes a half-hearted attempt; to use whatever other compiler you have, but that may not work. We probably need to figure; out a standardized and automated way to get the right tools installed in the right place (and; get the right shared libraries on worker nodes). 4. Data which needs to be accessible to both Scala and C++ is held in C++ objects inheriting; from NativeObj, with lifetimes managed by std::shared_ptr, i.e. reference-counted. There's; some dirty under-the-hood plumbing to allow a shared_ptr to be smuggled into a Scala; object derived from NativeBase. These Scala-side object references must be managed; carefully using copyAssign/moveAssign/close in order to maintain the off-heap ref-counts. 5. There are some gnarly differences between Linux and MacOSX in the linker and dynamic; loading. I think I'm converging on the right compile/link options for each, but in getting; Linux to work it's possible that Mac is temporarily broken ... Not really expecting that we'll merge this right away, but I wanted to put it out there to get the; review process started before it grows any bigger.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3461
https://github.com/hail-is/hail/pull/3461:65,Testability,test,tests,65,"This is the current state of the C++ support. If you look at the tests in src/test/is/hail/nativecode/NativeCodeSuite.scala that should give a; quick overview of how it works, viz. 1. Generate C++ source code as a Scala String, then create a NativeModule which handles; the grunt work of getting it compiled, linked, and loaded, and allows you to look up functions; by name, and get a callable Scala object corresponding to the C++ function. 2. The NativeModule also allows the binary of the DLL to be passed around and instantiated; on other cluster nodes (but note that those nodes will need to have the correct versions of; the C++ runtime shared libraries in the right directories to allow symbols in the DLL to be; correctly resolved). This is not tested yet. 3. I have been using llvm-6.0.0 on Mac, and llvm-5.0 on linux. It makes a half-hearted attempt; to use whatever other compiler you have, but that may not work. We probably need to figure; out a standardized and automated way to get the right tools installed in the right place (and; get the right shared libraries on worker nodes). 4. Data which needs to be accessible to both Scala and C++ is held in C++ objects inheriting; from NativeObj, with lifetimes managed by std::shared_ptr, i.e. reference-counted. There's; some dirty under-the-hood plumbing to allow a shared_ptr to be smuggled into a Scala; object derived from NativeBase. These Scala-side object references must be managed; carefully using copyAssign/moveAssign/close in order to maintain the off-heap ref-counts. 5. There are some gnarly differences between Linux and MacOSX in the linker and dynamic; loading. I think I'm converging on the right compile/link options for each, but in getting; Linux to work it's possible that Mac is temporarily broken ... Not really expecting that we'll merge this right away, but I wanted to put it out there to get the; review process started before it grows any bigger.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3461
https://github.com/hail-is/hail/pull/3461:78,Testability,test,test,78,"This is the current state of the C++ support. If you look at the tests in src/test/is/hail/nativecode/NativeCodeSuite.scala that should give a; quick overview of how it works, viz. 1. Generate C++ source code as a Scala String, then create a NativeModule which handles; the grunt work of getting it compiled, linked, and loaded, and allows you to look up functions; by name, and get a callable Scala object corresponding to the C++ function. 2. The NativeModule also allows the binary of the DLL to be passed around and instantiated; on other cluster nodes (but note that those nodes will need to have the correct versions of; the C++ runtime shared libraries in the right directories to allow symbols in the DLL to be; correctly resolved). This is not tested yet. 3. I have been using llvm-6.0.0 on Mac, and llvm-5.0 on linux. It makes a half-hearted attempt; to use whatever other compiler you have, but that may not work. We probably need to figure; out a standardized and automated way to get the right tools installed in the right place (and; get the right shared libraries on worker nodes). 4. Data which needs to be accessible to both Scala and C++ is held in C++ objects inheriting; from NativeObj, with lifetimes managed by std::shared_ptr, i.e. reference-counted. There's; some dirty under-the-hood plumbing to allow a shared_ptr to be smuggled into a Scala; object derived from NativeBase. These Scala-side object references must be managed; carefully using copyAssign/moveAssign/close in order to maintain the off-heap ref-counts. 5. There are some gnarly differences between Linux and MacOSX in the linker and dynamic; loading. I think I'm converging on the right compile/link options for each, but in getting; Linux to work it's possible that Mac is temporarily broken ... Not really expecting that we'll merge this right away, but I wanted to put it out there to get the; review process started before it grows any bigger.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3461
https://github.com/hail-is/hail/pull/3461:753,Testability,test,tested,753,"This is the current state of the C++ support. If you look at the tests in src/test/is/hail/nativecode/NativeCodeSuite.scala that should give a; quick overview of how it works, viz. 1. Generate C++ source code as a Scala String, then create a NativeModule which handles; the grunt work of getting it compiled, linked, and loaded, and allows you to look up functions; by name, and get a callable Scala object corresponding to the C++ function. 2. The NativeModule also allows the binary of the DLL to be passed around and instantiated; on other cluster nodes (but note that those nodes will need to have the correct versions of; the C++ runtime shared libraries in the right directories to allow symbols in the DLL to be; correctly resolved). This is not tested yet. 3. I have been using llvm-6.0.0 on Mac, and llvm-5.0 on linux. It makes a half-hearted attempt; to use whatever other compiler you have, but that may not work. We probably need to figure; out a standardized and automated way to get the right tools installed in the right place (and; get the right shared libraries on worker nodes). 4. Data which needs to be accessible to both Scala and C++ is held in C++ objects inheriting; from NativeObj, with lifetimes managed by std::shared_ptr, i.e. reference-counted. There's; some dirty under-the-hood plumbing to allow a shared_ptr to be smuggled into a Scala; object derived from NativeBase. These Scala-side object references must be managed; carefully using copyAssign/moveAssign/close in order to maintain the off-heap ref-counts. 5. There are some gnarly differences between Linux and MacOSX in the linker and dynamic; loading. I think I'm converging on the right compile/link options for each, but in getting; Linux to work it's possible that Mac is temporarily broken ... Not really expecting that we'll merge this right away, but I wanted to put it out there to get the; review process started before it grows any bigger.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3461
https://github.com/hail-is/hail/issues/3463:781,Availability,heartbeat,heartbeatInterval,781,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-63d60cc. ### What you did:. Tried to run ld_prune and pc_relate on 5000 WGS samples. ```; spark-submit --verbose --master yarn --deploy-mode client \; --num-executors 14 \; --executor-cores 6 \; --jars $JAR \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf ""spark.driver.extraClassPath=$JAR"" \; --conf ""spark.executor.extraClassPath=$JAR"" \; --executor-memory 90G\; --driver-memory 80g\; --conf spark.yarn.executor.memoryOverhead=8000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120\; hc_prune.py; ```. Where hc_prune.py is:. ```; import matplotlib.pyplot as plt; import seaborn. import numpy as np; import pandas as pd; from collections import Counter; from math import log, isnan; from pprint import pprint; # hail; import hail as hl; import hail.expr.aggregators as agg; import hail.expr.functions. hl.init(default_reference='GRCh38'); print(""Read in PASS SNVs""); passed=hl.read_matrix_table('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass'); print(""Filtering Common Variants""); common=passed.filter_rows(passed.variant_qc.AF > 0.01).persist(); common.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass.common'); print(""Pruning LD Variants""); pruned =hl.ld_prune(common,30,r2=0.1, memory_per_core=2048); pruned.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pruned'); print(""Sample 20% of variants for running PC-Relate""); pruned_subsample = pruned.sample_rows(0.2).persist(); print(""Running PC_Relate""); rel = hl.pc_relate(pruned_subsample.GT, 0.01, k=10); rel_df = rel.to_pandas(); rel_df.describe(); pprint(rel_df); rel_df.to_csv('gcad_5k.snv.rel.csv'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Got a memor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:1930,Availability,error,error,1930,"collections import Counter; from math import log, isnan; from pprint import pprint; # hail; import hail as hl; import hail.expr.aggregators as agg; import hail.expr.functions. hl.init(default_reference='GRCh38'); print(""Read in PASS SNVs""); passed=hl.read_matrix_table('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass'); print(""Filtering Common Variants""); common=passed.filter_rows(passed.variant_qc.AF > 0.01).persist(); common.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass.common'); print(""Pruning LD Variants""); pruned =hl.ld_prune(common,30,r2=0.1, memory_per_core=2048); pruned.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pruned'); print(""Sample 20% of variants for running PC-Relate""); pruned_subsample = pruned.sample_rows(0.2).persist(); print(""Running PC_Relate""); rel = hl.pc_relate(pruned_subsample.GT, 0.01, k=10); rel_df = rel.to_pandas(); rel_df.describe(); pprint(rel_df); rel_df.to_csv('gcad_5k.snv.rel.csv'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Got a memory error. but not sure what memory needs to be increased. The job seems to restart but does not progress and need to kill. . java.lang.OutOfMemoryError: Java heap spaceop. Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-63d60cc; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; Read in PASS SNVs; Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:===============",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:2003,Availability,error,error,2003,"an; from pprint import pprint; # hail; import hail as hl; import hail.expr.aggregators as agg; import hail.expr.functions. hl.init(default_reference='GRCh38'); print(""Read in PASS SNVs""); passed=hl.read_matrix_table('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass'); print(""Filtering Common Variants""); common=passed.filter_rows(passed.variant_qc.AF > 0.01).persist(); common.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass.common'); print(""Pruning LD Variants""); pruned =hl.ld_prune(common,30,r2=0.1, memory_per_core=2048); pruned.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pruned'); print(""Sample 20% of variants for running PC-Relate""); pruned_subsample = pruned.sample_rows(0.2).persist(); print(""Running PC_Relate""); rel = hl.pc_relate(pruned_subsample.GT, 0.01, k=10); rel_df = rel.to_pandas(); rel_df.describe(); pprint(rel_df); rel_df.to_csv('gcad_5k.snv.rel.csv'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Got a memory error. but not sure what memory needs to be increased. The job seems to restart but does not progress and need to kill. . java.lang.OutOfMemoryError: Java heap spaceop. Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-63d60cc; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; Read in PASS SNVs; Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:2219,Availability,avail,available,2219,"casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass'); print(""Filtering Common Variants""); common=passed.filter_rows(passed.variant_qc.AF > 0.01).persist(); common.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass.common'); print(""Pruning LD Variants""); pruned =hl.ld_prune(common,30,r2=0.1, memory_per_core=2048); pruned.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pruned'); print(""Sample 20% of variants for running PC-Relate""); pruned_subsample = pruned.sample_rows(0.2).persist(); print(""Running PC_Relate""); rel = hl.pc_relate(pruned_subsample.GT, 0.01, k=10); rel_df = rel.to_pandas(); rel_df.describe(); pprint(rel_df); rel_df.to_csv('gcad_5k.snv.rel.csv'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Got a memory error. but not sure what memory needs to be increased. The job seems to restart but does not progress and need to kill. . java.lang.OutOfMemoryError: Java heap spaceop. Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-63d60cc; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; Read in PASS SNVs; Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:388,Deployability,deploy,deploy-mode,388,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-63d60cc. ### What you did:. Tried to run ld_prune and pc_relate on 5000 WGS samples. ```; spark-submit --verbose --master yarn --deploy-mode client \; --num-executors 14 \; --executor-cores 6 \; --jars $JAR \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf ""spark.driver.extraClassPath=$JAR"" \; --conf ""spark.executor.extraClassPath=$JAR"" \; --executor-memory 90G\; --driver-memory 80g\; --conf spark.yarn.executor.memoryOverhead=8000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120\; hc_prune.py; ```. Where hc_prune.py is:. ```; import matplotlib.pyplot as plt; import seaborn. import numpy as np; import pandas as pd; from collections import Counter; from math import log, isnan; from pprint import pprint; # hail; import hail as hl; import hail.expr.aggregators as agg; import hail.expr.functions. hl.init(default_reference='GRCh38'); print(""Read in PASS SNVs""); passed=hl.read_matrix_table('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass'); print(""Filtering Common Variants""); common=passed.filter_rows(passed.variant_qc.AF > 0.01).persist(); common.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass.common'); print(""Pruning LD Variants""); pruned =hl.ld_prune(common,30,r2=0.1, memory_per_core=2048); pruned.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pruned'); print(""Sample 20% of variants for running PC-Relate""); pruned_subsample = pruned.sample_rows(0.2).persist(); print(""Running PC_Relate""); rel = hl.pc_relate(pruned_subsample.GT, 0.01, k=10); rel_df = rel.to_pandas(); rel_df.describe(); pprint(rel_df); rel_df.to_csv('gcad_5k.snv.rel.csv'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Got a memor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:3483,Deployability,update,updateAccumulators,3483,"kly.; Read in PASS SNVs; Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at java.util.ArrayList.<init>(ArrayList.java:177); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:470); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:444); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1103); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1092); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1092); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1168); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); [Stage 2:=========================================> (79823 + 18) / 96601]. Used yarn application -kill to kill but driver still runs. Then use kill -KILL t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:3588,Deployability,update,updateAccumulators,3588," Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at java.util.ArrayList.<init>(ArrayList.java:177); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:470); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:444); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1103); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1092); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1092); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1168); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); [Stage 2:=========================================> (79823 + 18) / 96601]. Used yarn application -kill to kill but driver still runs. Then use kill -KILL to terminate the driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:3838,Deployability,update,updateAccumulators,3838," Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at java.util.ArrayList.<init>(ArrayList.java:177); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:470); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:444); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1103); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1092); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1092); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1168); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); [Stage 2:=========================================> (79823 + 18) / 96601]. Used yarn application -kill to kill but driver still runs. Then use kill -KILL to terminate the driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:3451,Energy Efficiency,schedul,scheduler,3451,"ng; the latest changes weekly.; Read in PASS SNVs; Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at java.util.ArrayList.<init>(ArrayList.java:177); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:470); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:444); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1103); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1092); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1092); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1168); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); [Stage 2:=========================================> (79823 + 18) / 96601]. Used yarn application -kill to kill but driver still ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:3556,Energy Efficiency,schedul,scheduler,3556," Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at java.util.ArrayList.<init>(ArrayList.java:177); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:470); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:444); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1103); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1092); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1092); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1168); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); [Stage 2:=========================================> (79823 + 18) / 96601]. Used yarn application -kill to kill but driver still runs. Then use kill -KILL to terminate the driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:3815,Energy Efficiency,schedul,scheduler,3815," Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at java.util.ArrayList.<init>(ArrayList.java:177); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:470); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:444); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1103); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1092); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1092); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1168); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); [Stage 2:=========================================> (79823 + 18) / 96601]. Used yarn application -kill to kill but driver still runs. Then use kill -KILL to terminate the driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:3903,Energy Efficiency,schedul,scheduler,3903," Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at java.util.ArrayList.<init>(ArrayList.java:177); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:470); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:444); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1103); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1092); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1092); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1168); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); [Stage 2:=========================================> (79823 + 18) / 96601]. Used yarn application -kill to kill but driver still runs. Then use kill -KILL to terminate the driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:3993,Energy Efficiency,schedul,scheduler,3993," Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at java.util.ArrayList.<init>(ArrayList.java:177); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:470); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:444); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1103); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1092); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1092); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1168); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); [Stage 2:=========================================> (79823 + 18) / 96601]. Used yarn application -kill to kill but driver still runs. Then use kill -KILL to terminate the driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:4090,Energy Efficiency,schedul,scheduler,4090," Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at java.util.ArrayList.<init>(ArrayList.java:177); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:470); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:444); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1103); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1092); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1092); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1168); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); [Stage 2:=========================================> (79823 + 18) / 96601]. Used yarn application -kill to kill but driver still runs. Then use kill -KILL to terminate the driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:4185,Energy Efficiency,schedul,scheduler,4185," Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at java.util.ArrayList.<init>(ArrayList.java:177); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:470); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:444); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1103); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1092); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1092); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1168); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); [Stage 2:=========================================> (79823 + 18) / 96601]. Used yarn application -kill to kill but driver still runs. Then use kill -KILL to terminate the driver.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:1936,Integrability,message,messages,1936,"collections import Counter; from math import log, isnan; from pprint import pprint; # hail; import hail as hl; import hail.expr.aggregators as agg; import hail.expr.functions. hl.init(default_reference='GRCh38'); print(""Read in PASS SNVs""); passed=hl.read_matrix_table('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass'); print(""Filtering Common Variants""); common=passed.filter_rows(passed.variant_qc.AF > 0.01).persist(); common.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass.common'); print(""Pruning LD Variants""); pruned =hl.ld_prune(common,30,r2=0.1, memory_per_core=2048); pruned.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pruned'); print(""Sample 20% of variants for running PC-Relate""); pruned_subsample = pruned.sample_rows(0.2).persist(); print(""Running PC_Relate""); rel = hl.pc_relate(pruned_subsample.GT, 0.01, k=10); rel_df = rel.to_pandas(); rel_df.describe(); pprint(rel_df); rel_df.to_csv('gcad_5k.snv.rel.csv'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Got a memory error. but not sure what memory needs to be increased. The job seems to restart but does not progress and need to kill. . java.lang.OutOfMemoryError: Java heap spaceop. Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-63d60cc; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; Read in PASS SNVs; Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:===============",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:2386,Integrability,Interface,Interfaces,2386,"/gatk.hc/gcad.5k.snv.vqsr.pass.common'); print(""Pruning LD Variants""); pruned =hl.ld_prune(common,30,r2=0.1, memory_per_core=2048); pruned.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pruned'); print(""Sample 20% of variants for running PC-Relate""); pruned_subsample = pruned.sample_rows(0.2).persist(); print(""Running PC_Relate""); rel = hl.pc_relate(pruned_subsample.GT, 0.01, k=10); rel_df = rel.to_pandas(); rel_df.describe(); pprint(rel_df); rel_df.to_csv('gcad_5k.snv.rel.csv'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Got a memory error. but not sure what memory needs to be increased. The job seems to restart but does not progress and need to kill. . java.lang.OutOfMemoryError: Java heap spaceop. Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-63d60cc; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; Read in PASS SNVs; Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at java.util.ArrayList.<init>(ArrayList.java:177); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:470); at org.apache.spark.util.CollectionAccumulator.value(Accu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:3167,Integrability,Synchroniz,SynchronizedCollection,3167,"Running on Apache Spark version 2.2.0; SparkUI available at http://10.48.225.55:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-63d60cc; NOTE: This is a beta version. Interfaces may change; during the beta period. We also recommend pulling; the latest changes weekly.; Read in PASS SNVs; Filtering Common Variants; [Stage 0:==================================================>(96600 + 1) / 96601]2018-04-27 20:54:43 Hail: INFO: wrote 11341822 items in 96601 partitions; Pruning LD Variants; [Stage 1:==================================================>(96598 + 3) / 96601]2018-04-27 21:19:04 Hail: INFO: Running LD prune with nSamples=4795, nVariants=11341822, nPartitions=96601, and maxQueueSize=429841.; [Stage 2:=========================================> (79823 + 18) / 96601]java.lang.OutOfMemoryError: Java heap spaceop""; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.toArray(ArrayList.java:376); at java.util.Collections$SynchronizedCollection.toArray(Collections.java:2024); at java.util.ArrayList.<init>(ArrayList.java:177); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:470); at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:444); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1103); at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1092); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1092); at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1168); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:744,Safety,timeout,timeout,744,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-63d60cc. ### What you did:. Tried to run ld_prune and pc_relate on 5000 WGS samples. ```; spark-submit --verbose --master yarn --deploy-mode client \; --num-executors 14 \; --executor-cores 6 \; --jars $JAR \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf ""spark.driver.extraClassPath=$JAR"" \; --conf ""spark.executor.extraClassPath=$JAR"" \; --executor-memory 90G\; --driver-memory 80g\; --conf spark.yarn.executor.memoryOverhead=8000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120\; hc_prune.py; ```. Where hc_prune.py is:. ```; import matplotlib.pyplot as plt; import seaborn. import numpy as np; import pandas as pd; from collections import Counter; from math import log, isnan; from pprint import pprint; # hail; import hail as hl; import hail.expr.aggregators as agg; import hail.expr.functions. hl.init(default_reference='GRCh38'); print(""Read in PASS SNVs""); passed=hl.read_matrix_table('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass'); print(""Filtering Common Variants""); common=passed.filter_rows(passed.variant_qc.AF > 0.01).persist(); common.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass.common'); print(""Pruning LD Variants""); pruned =hl.ld_prune(common,30,r2=0.1, memory_per_core=2048); pruned.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pruned'); print(""Sample 20% of variants for running PC-Relate""); pruned_subsample = pruned.sample_rows(0.2).persist(); print(""Running PC_Relate""); rel = hl.pc_relate(pruned_subsample.GT, 0.01, k=10); rel_df = rel.to_pandas(); rel_df.describe(); pprint(rel_df); rel_df.to_csv('gcad_5k.snv.rel.csv'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Got a memor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3463:991,Testability,log,log,991,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-63d60cc. ### What you did:. Tried to run ld_prune and pc_relate on 5000 WGS samples. ```; spark-submit --verbose --master yarn --deploy-mode client \; --num-executors 14 \; --executor-cores 6 \; --jars $JAR \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf ""spark.driver.extraClassPath=$JAR"" \; --conf ""spark.executor.extraClassPath=$JAR"" \; --executor-memory 90G\; --driver-memory 80g\; --conf spark.yarn.executor.memoryOverhead=8000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120\; hc_prune.py; ```. Where hc_prune.py is:. ```; import matplotlib.pyplot as plt; import seaborn. import numpy as np; import pandas as pd; from collections import Counter; from math import log, isnan; from pprint import pprint; # hail; import hail as hl; import hail.expr.aggregators as agg; import hail.expr.functions. hl.init(default_reference='GRCh38'); print(""Read in PASS SNVs""); passed=hl.read_matrix_table('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass'); print(""Filtering Common Variants""); common=passed.filter_rows(passed.variant_qc.AF > 0.01).persist(); common.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pass.common'); print(""Pruning LD Variants""); pruned =hl.ld_prune(common,30,r2=0.1, memory_per_core=2048); pruned.write('/project/casa/hail.ds/gatk.hc/gcad.5k.snv.vqsr.pruned'); print(""Sample 20% of variants for running PC-Relate""); pruned_subsample = pruned.sample_rows(0.2).persist(); print(""Running PC_Relate""); rel = hl.pc_relate(pruned_subsample.GT, 0.01, k=10); rel_df = rel.to_pandas(); rel_df.describe(); pprint(rel_df); rel_df.to_csv('gcad_5k.snv.rel.csv'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Got a memor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3463
https://github.com/hail-is/hail/issues/3465:1627,Availability,failure,failure,1627,"().write(); ```. ```; Traceback (most recent call last):; File ""/tmp/a913d6ce5b814a63ad7af31060416237/pyscripts_Xr0D99.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/a913d6ce5b814a63ad7af31060416237/generate_qc_annotations.py"", line 247, in main; generate_call_stats(mt).write(annotations_mt_path(data_type, 'call_stats'), args.overwrite); File ""<decorator-gen-556>"", line 2, in write; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/matrixtable.py"", line 2027, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: IllegalArgumentException: requirement failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 1.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 1.0 (TID 10060, exomes3-sw-dfpw.c.broad-mpg-gnomad.internal, executor 134): java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:212); 	at is.hail.variant.Call$.alleleByIndex(Call.scala:128); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:1688,Availability,failure,failure,1688,"""/tmp/a913d6ce5b814a63ad7af31060416237/pyscripts_Xr0D99.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/a913d6ce5b814a63ad7af31060416237/generate_qc_annotations.py"", line 247, in main; generate_call_stats(mt).write(annotations_mt_path(data_type, 'call_stats'), args.overwrite); File ""<decorator-gen-556>"", line 2, in write; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/matrixtable.py"", line 2027, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: IllegalArgumentException: requirement failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 1.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 1.0 (TID 10060, exomes3-sw-dfpw.c.broad-mpg-gnomad.internal, executor 134): java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:212); 	at is.hail.variant.Call$.alleleByIndex(Call.scala:128); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:18510,Availability,Error,Error,18510,s.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c37301a; Error summary: IllegalArgumentException: requirement failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:14,Deployability,pipeline,pipeline,14,"Pretty simple pipeline, only failed about ~3/4 through the final (write) stage...; ```; mt = hl.read_matrix_table(); sample_group_filters = {; ""qc_samples_raw"": mt.meta.high_quality,; ""release_samples_raw"": mt.meta.release,; ""all_samples_raw"": True; }; mt = mt.select_cols(**sample_group_filters); mt = unphase_mt(mt.select_rows(*mt.row_key)); call_stats_expression = []; for group in sample_group_filters.keys():; call_stats_expression.append(; hl.struct(call_stats=hl.agg.call_stats(hl.agg.filter(mt[group], mt.GT), mt.alleles),; meta={'group': group}); ); mt.annotate_rows(qc_callstats=call_stats_expression).drop_cols().write(); ```. ```; Traceback (most recent call last):; File ""/tmp/a913d6ce5b814a63ad7af31060416237/pyscripts_Xr0D99.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/a913d6ce5b814a63ad7af31060416237/generate_qc_annotations.py"", line 247, in main; generate_call_stats(mt).write(annotations_mt_path(data_type, 'call_stats'), args.overwrite); File ""<decorator-gen-556>"", line 2, in write; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/matrixtable.py"", line 2027, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: IllegalArgumentException: requirement failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 1.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 1.0 (TID 10060, exomes3-sw-dfpw.c.broad-mpg-gnomad.internal, executor 134): java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:212); 	at is.hail.variant.Call$.alleleByIndex(Call.scala:128); 	at is.hail.expr.FunctionRegistry$$anonfun$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:215,Deployability,release,release,215,"Pretty simple pipeline, only failed about ~3/4 through the final (write) stage...; ```; mt = hl.read_matrix_table(); sample_group_filters = {; ""qc_samples_raw"": mt.meta.high_quality,; ""release_samples_raw"": mt.meta.release,; ""all_samples_raw"": True; }; mt = mt.select_cols(**sample_group_filters); mt = unphase_mt(mt.select_rows(*mt.row_key)); call_stats_expression = []; for group in sample_group_filters.keys():; call_stats_expression.append(; hl.struct(call_stats=hl.agg.call_stats(hl.agg.filter(mt[group], mt.GT), mt.alleles),; meta={'group': group}); ); mt.annotate_rows(qc_callstats=call_stats_expression).drop_cols().write(); ```. ```; Traceback (most recent call last):; File ""/tmp/a913d6ce5b814a63ad7af31060416237/pyscripts_Xr0D99.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/a913d6ce5b814a63ad7af31060416237/generate_qc_annotations.py"", line 247, in main; generate_call_stats(mt).write(annotations_mt_path(data_type, 'call_stats'), args.overwrite); File ""<decorator-gen-556>"", line 2, in write; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/matrixtable.py"", line 2027, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: IllegalArgumentException: requirement failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 1.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 1.0 (TID 10060, exomes3-sw-dfpw.c.broad-mpg-gnomad.internal, executor 134): java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:212); 	at is.hail.variant.Call$.alleleByIndex(Call.scala:128); 	at is.hail.expr.FunctionRegistry$$anonfun$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:8124,Energy Efficiency,schedul,scheduler,8124,ic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:8196,Energy Efficiency,schedul,scheduler,8196,.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:8561,Energy Efficiency,schedul,scheduler,8561,ollection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:8601,Energy Efficiency,schedul,scheduler,8601,at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:8700,Energy Efficiency,schedul,scheduler,8700,t scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:8798,Energy Efficiency,schedul,scheduler,8798,onfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:9052,Energy Efficiency,schedul,scheduler,9052,nfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:9133,Energy Efficiency,schedul,scheduler,9133,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:9239,Energy Efficiency,schedul,scheduler,9239,he.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:9389,Energy Efficiency,schedul,scheduler,9389,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:9478,Energy Efficiency,schedul,scheduler,9478,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:9576,Energy Efficiency,schedul,scheduler,9576,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:9672,Energy Efficiency,schedul,scheduler,9672,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:946); 	at is.hail.rvd.O,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:9837,Energy Efficiency,schedul,scheduler,9837,.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.hail.io.RichContextRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:946); 	at is.hail.rvd.OrderedRVD.writeRowsSplit(OrderedRVD.scala:391); 	at is.hail.expr.MatrixValue.write(Relational.scala:117); 	at is.hail.expr.ir.Interpret$.is$hail$expr$ir$Interpret$$i,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:18085,Energy Efficiency,schedul,scheduler,18085,s.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c37301a; Error summary: IllegalArgumentException: requirement failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:18157,Energy Efficiency,schedul,scheduler,18157,s.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c37301a; Error summary: IllegalArgumentException: requirement failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:8321,Performance,concurren,concurrent,8321,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:8406,Performance,concurren,concurrent,8406, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:18282,Performance,concurren,concurrent,18282,s.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c37301a; Error summary: IllegalArgumentException: requirement failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:18367,Performance,concurren,concurrent,18367,s.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c37301a; Error summary: IllegalArgumentException: requirement failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:1606,Safety,abort,aborted,1606,"().write(); ```. ```; Traceback (most recent call last):; File ""/tmp/a913d6ce5b814a63ad7af31060416237/pyscripts_Xr0D99.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/a913d6ce5b814a63ad7af31060416237/generate_qc_annotations.py"", line 247, in main; generate_call_stats(mt).write(annotations_mt_path(data_type, 'call_stats'), args.overwrite); File ""<decorator-gen-556>"", line 2, in write; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/matrixtable.py"", line 2027, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: IllegalArgumentException: requirement failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 1.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 1.0 (TID 10060, exomes3-sw-dfpw.c.broad-mpg-gnomad.internal, executor 134): java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:212); 	at is.hail.variant.Call$.alleleByIndex(Call.scala:128); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:2944,Safety,unsafe,unsafeInsert,2944,ll.scala:128); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1112); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1102); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52$$anonfun$apply$5.apply$mcV$sp(MatrixTable.scala:1364); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:211); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:200); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1363); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1359); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:812); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:3025,Safety,unsafe,unsafeInsert,3025,ionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1112); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1102); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52$$anonfun$apply$5.apply$mcV$sp(MatrixTable.scala:1364); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:211); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:200); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1363); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1359); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:812); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:3106,Safety,unsafe,unsafeInsert,3106,ionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1112); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1102); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52$$anonfun$apply$5.apply$mcV$sp(MatrixTable.scala:1364); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:211); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:200); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1363); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1359); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:812); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:812); 	at scala.collection.Iterator$$anon$12.nex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:3187,Safety,unsafe,unsafeInsert,3187,ionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1112); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1102); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52$$anonfun$apply$5.apply$mcV$sp(MatrixTable.scala:1364); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:211); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:200); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1363); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1359); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:812); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:812); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:8732,Safety,abort,abortStage,8732,tractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:8830,Safety,abort,abortStage,8830,un$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:9075,Safety,abort,abortStage,9075,xt.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:12905,Safety,unsafe,unsafeInsert,12905,ll.scala:128); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1112); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1102); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52$$anonfun$apply$5.apply$mcV$sp(MatrixTable.scala:1364); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:211); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:200); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1363); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1359); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:812); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:12986,Safety,unsafe,unsafeInsert,12986,ionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1112); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1102); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52$$anonfun$apply$5.apply$mcV$sp(MatrixTable.scala:1364); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:211); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:200); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1363); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1359); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:812); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:13067,Safety,unsafe,unsafeInsert,13067,ionRegistry.scala:685); 	at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1112); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1102); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52$$anonfun$apply$5.apply$mcV$sp(MatrixTable.scala:1364); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:211); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:200); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1363); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1359); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:812); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:812); 	at scala.collection.Iterator$$anon$12.nex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:13148,Safety,unsafe,unsafeInsert,13148,ionRegistry.scala:685); 	at is.hail.expr.BinaryFun.apply(Fun.scala:122); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.codegen.generated.C9.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1112); 	at is.hail.variant.MatrixTable$$anonfun$selectEntries$2.apply(MatrixTable.scala:1102); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52$$anonfun$apply$5.apply$mcV$sp(MatrixTable.scala:1364); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:191); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:211); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:200); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1363); 	at is.hail.variant.MatrixTable$$anonfun$61$$anonfun$apply$52.apply(MatrixTable.scala:1359); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:812); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:818); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:812); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/issues/3465:7,Usability,simpl,simple,7,"Pretty simple pipeline, only failed about ~3/4 through the final (write) stage...; ```; mt = hl.read_matrix_table(); sample_group_filters = {; ""qc_samples_raw"": mt.meta.high_quality,; ""release_samples_raw"": mt.meta.release,; ""all_samples_raw"": True; }; mt = mt.select_cols(**sample_group_filters); mt = unphase_mt(mt.select_rows(*mt.row_key)); call_stats_expression = []; for group in sample_group_filters.keys():; call_stats_expression.append(; hl.struct(call_stats=hl.agg.call_stats(hl.agg.filter(mt[group], mt.GT), mt.alleles),; meta={'group': group}); ); mt.annotate_rows(qc_callstats=call_stats_expression).drop_cols().write(); ```. ```; Traceback (most recent call last):; File ""/tmp/a913d6ce5b814a63ad7af31060416237/pyscripts_Xr0D99.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/a913d6ce5b814a63ad7af31060416237/generate_qc_annotations.py"", line 247, in main; generate_call_stats(mt).write(annotations_mt_path(data_type, 'call_stats'), args.overwrite); File ""<decorator-gen-556>"", line 2, in write; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/typecheck/check.py"", line 481, in _typecheck; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/matrixtable.py"", line 2027, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/a913d6ce5b814a63ad7af31060416237/hail-devel-a1d6ecc71ce3.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: IllegalArgumentException: requirement failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 1.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 1.0 (TID 10060, exomes3-sw-dfpw.c.broad-mpg-gnomad.internal, executor 134): java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:212); 	at is.hail.variant.Call$.alleleByIndex(Call.scala:128); 	at is.hail.expr.FunctionRegistry$$anonfun$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3465
https://github.com/hail-is/hail/pull/3466:18,Security,expose,expose,18,"This PR begins to expose RowMatrix functions in Python.; - lots of docs; - fixed up parallel options in export; - added add_index option, useful for parallel export; - test of add_index. I broke this out from changes in sparse_block_matrix (v1 on its way, at which point I'll assign this and that) and lmm2. Three key uses for RowMatrix:; - BlockMatrix export; - class of inputs to per-variant LMM (see lmm2 branch); - BlockMatrix to MatrixTable conversion (once I add RowMatrix.toMatrixTable)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3466
https://github.com/hail-is/hail/pull/3466:168,Testability,test,test,168,"This PR begins to expose RowMatrix functions in Python.; - lots of docs; - fixed up parallel options in export; - added add_index option, useful for parallel export; - test of add_index. I broke this out from changes in sparse_block_matrix (v1 on its way, at which point I'll assign this and that) and lmm2. Three key uses for RowMatrix:; - BlockMatrix export; - class of inputs to per-variant LMM (see lmm2 branch); - BlockMatrix to MatrixTable conversion (once I add RowMatrix.toMatrixTable)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3466
https://github.com/hail-is/hail/issues/3467:424,Availability,failure,failure,424,"Tried to load 1kg public VCF using newest version of hail:; ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:482,Availability,failure,failure,482,"Tried to load 1kg public VCF using newest version of hail:; ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:832,Availability,Error,ErrorHandling,832,"Tried to load 1kg public VCF using newest version of hail:; ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:858,Availability,Error,ErrorHandling,858,"Tried to load 1kg public VCF using newest version of hail:; ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:7658,Availability,Error,ErrorHandling,7658,(Table.scala:1003); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:7684,Availability,Error,ErrorHandling,7684,	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:13960,Availability,Error,Error,13960,"tion.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-a1d6ecc; Error summary: NoSuchElementException: key not found: GT; ```; The file has `GT` in the format field, but it's missing the corresponding header line. Passing a custom `header_file=` fixes the problem, but it's unfortunate that that's required (especially on such a widely used publicly available dataset).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:14246,Availability,avail,available,14246,"tion.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-a1d6ecc; Error summary: NoSuchElementException: key not found: GT; ```; The file has `GT` in the format field, but it's missing the corresponding header line. Passing a custom `header_file=` fixes the problem, but it's unfortunate that that's required (especially on such a widely used publicly available dataset).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:3143,Energy Efficiency,schedul,scheduler,3143,eneric.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(A,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:3215,Energy Efficiency,schedul,scheduler,3215,tion.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(Trave,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4713,Energy Efficiency,schedul,scheduler,4713,9); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4753,Energy Efficiency,schedul,scheduler,4753,141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4852,Energy Efficiency,schedul,scheduler,4852,.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4950,Energy Efficiency,schedul,scheduler,4950,Like$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:5204,Energy Efficiency,schedul,scheduler,5204,s.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:5285,Energy Efficiency,schedul,scheduler,5285,(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.rdd.RDD$$anonfun$take$1.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:5391,Energy Efficiency,schedul,scheduler,5391,ineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:5541,Energy Efficiency,schedul,scheduler,5541,.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:5630,Energy Efficiency,schedul,scheduler,5630,xt(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.take(RDD.scala:1327); 	at is.hail.table.Table.take(Table.scala:914); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:5728,Energy Efficiency,schedul,scheduler,5728,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.take(RDD.scala:1327); 	at is.hail.table.Table.take(Table.scala:914); 	at is.hail.table.Table.showString(Table.scala:1003); 	at sun.reflect.NativeMethodAccessorImpl.invo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:5824,Energy Efficiency,schedul,scheduler,5824,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.take(RDD.scala:1327); 	at is.hail.table.Table.take(Table.scala:914); 	at is.hail.table.Table.showString(Table.scala:1003); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:5989,Energy Efficiency,schedul,scheduler,5989,.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.take(RDD.scala:1327); 	at is.hail.table.Table.take(Table.scala:914); 	at is.hail.table.Table.showString(Table.scala:1003); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.refl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:9969,Energy Efficiency,schedul,scheduler,9969,eneric.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:10041,Energy Efficiency,schedul,scheduler,10041,tion.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:13535,Energy Efficiency,schedul,scheduler,13535,"tion.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-a1d6ecc; Error summary: NoSuchElementException: key not found: GT; ```; The file has `GT` in the format field, but it's missing the corresponding header line. Passing a custom `header_file=` fixes the problem, but it's unfortunate that that's required (especially on such a widely used publicly available dataset).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:13607,Energy Efficiency,schedul,scheduler,13607,"tion.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-a1d6ecc; Error summary: NoSuchElementException: key not found: GT; ```; The file has `GT` in the format field, but it's missing the corresponding header line. Passing a custom `header_file=` fixes the problem, but it's unfortunate that that's required (especially on such a widely used publicly available dataset).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:961,Integrability,wrap,wrapException,961,"Tried to load 1kg public VCF using newest version of hail:; ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:7787,Integrability,wrap,wrapException,7787,ke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:9,Performance,load,load,9,"Tried to load 1kg public VCF using newest version of hail:; ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:1013,Performance,Load,LoadVCF,1013,"ng newest version of hail:; ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.for",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:1059,Performance,Load,LoadVCF,1059," ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:3340,Performance,concurren,concurrent,3340,ffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:3425,Performance,concurren,concurrent,3425,10); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4351,Performance,Load,LoadVCF,4351,hreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4423,Performance,Load,LoadVCF,4423,.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anon,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4462,Performance,Load,LoadVCF,4462,PoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:81,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4488,Performance,Load,LoadVCF,4488,:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Opt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4527,Performance,Load,LoadVCF,4527,; Caused by: java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4553,Performance,Load,LoadVCF,4553,.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4592,Performance,Load,LoadVCF,4592,a.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4638,Performance,Load,LoadVCF,4638,default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.schedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:7839,Performance,Load,LoadVCF,7839,tingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.for,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:7885,Performance,Load,LoadVCF,7885,ke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:10166,Performance,concurren,concurrent,10166,ffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:10251,Performance,concurren,concurrent,10251,10); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:11164,Performance,Load,LoadVCF,11164,.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Itera,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:11236,Performance,Load,LoadVCF,11236,49); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iter,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:11275,Performance,Load,LoadVCF,11275,er.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:11301,Performance,Load,LoadVCF,11301,Executor.java:624); 	at java.lang.Thread.run(Thread.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:11340,Performance,Load,LoadVCF,11340,ead.java:748)java.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$an,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:11366,Performance,Load,LoadVCF,11366,.util.NoSuchElementException: key not found: GT; 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Ite,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:11405,Performance,Load,LoadVCF,11405,a.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.for,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:11451,Performance,Load,LoadVCF,11451,default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.MapLike$class.apply(MapLike.scala:141); 	at scala.collection.AbstractMap.apply(Map.scala:59); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:13732,Performance,concurren,concurrent,13732,"tion.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-a1d6ecc; Error summary: NoSuchElementException: key not found: GT; ```; The file has `GT` in the format field, but it's missing the corresponding header line. Passing a custom `header_file=` fixes the problem, but it's unfortunate that that's required (especially on such a widely used publicly available dataset).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:13817,Performance,concurren,concurrent,13817,"tion.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-a1d6ecc; Error summary: NoSuchElementException: key not found: GT; ```; The file has `GT` in the format field, but it's missing the corresponding header line. Passing a custom `header_file=` fixes the problem, but it's unfortunate that that's required (especially on such a widely used publicly available dataset).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:403,Safety,abort,aborted,403,"Tried to load 1kg public VCF using newest version of hail:; ```; tgp = hl.import_vcf('gs://genomics-public-data/1000-genomes-phase-3/vcf-20150220/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf'); tgp.describe(); tgp.rows().show(); ```; Getting:; ```; hail.utils.java.FatalError: NoSuchElementException: key not found: GT. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 20 times, most recent failure: Lost task 0.19 in stage 2.0 (TID 104, pca-w-1.c.daly-ibd.internal, executor 2): is.hail.utils.HailException: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf: caught java.util.NoSuchElementException: key not found: GT; offending line: 22	16050075	rs587697622	A	G	100	PASS	AC=1;AF=0.000199681;AN=...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:761); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:1004); 	at is.hail.rvd.OrderedRVD$$anon$2.hasNext(OrderedRVD.scala:413); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.hasNext(OrderedRVD.scala:815); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389); 	at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4884,Safety,abort,abortStage,4884,fun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:4982,Safety,abort,abortStage,4982,ply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3467:5227,Safety,abort,abortStage,5227,4); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.FormatParser$.apply(LoadVCF.scala:470); 	at is.hail.io.vcf.ParseLineContext.getFormatParser(LoadVCF.scala:551); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:886); 	at is.hail.io.vcf.LoadVCF$$anonfun$14.apply(LoadVCF.scala:869); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:737); 	... 34 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3467
https://github.com/hail-is/hail/issues/3469:886,Testability,test,test,886,"Hi,. Im trying to annotate a vcf with another vcf (gnomAD annotations), and the steps I follow are:. annotation_vds = hc.import_vcf(annotations_vcf.vcf).split_multi(); sample_vds = hc.import_vcf(sample_vcf.vcf).split_multi(). annotated_vds = sample_vds.annotate_variants_vds(annotation_vds,"" va.gnomAD_Ex_AF = vds.info.gnomAD_Ex_AF[vds.aIndex-1]""). Where vds.info.gnomAD_Ex_AF is an array with as many positions as alleles. But when the annotation file (with gnomAD annotations) doesnt have multiallelic variants, the aIndex field doesnt exist in the va annotations. However, if I run annotation_vds.was_split() it returns true. So I cant find a way to annotate the sample vcf if the annotation vcf doesnt have multiallelic variants and the info field does. Ive tried the following (without success):. annotated_vds = sample_vds.annotate_variants_vds(annotation_vds,va.test = if(! isMissing(va.aIndex)) va.info.AC[va.aIndex-1] else va.info.AC[0]); annotated_vds = sample_vds.annotate_variants_vds(annotation_vds,va.test = orElse(va.info.AC[va.aIndex-1],va.info.AC[0])). Is this normal behaviour and theres a way to annotate without knowing whether the vcf has multiallelic variants? Or is it not the expected behaviour?. The Hail version I'm using is 0.1. Thanks in advance,. Cristina.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3469
https://github.com/hail-is/hail/issues/3469:1034,Testability,test,test,1034,"Hi,. Im trying to annotate a vcf with another vcf (gnomAD annotations), and the steps I follow are:. annotation_vds = hc.import_vcf(annotations_vcf.vcf).split_multi(); sample_vds = hc.import_vcf(sample_vcf.vcf).split_multi(). annotated_vds = sample_vds.annotate_variants_vds(annotation_vds,"" va.gnomAD_Ex_AF = vds.info.gnomAD_Ex_AF[vds.aIndex-1]""). Where vds.info.gnomAD_Ex_AF is an array with as many positions as alleles. But when the annotation file (with gnomAD annotations) doesnt have multiallelic variants, the aIndex field doesnt exist in the va annotations. However, if I run annotation_vds.was_split() it returns true. So I cant find a way to annotate the sample vcf if the annotation vcf doesnt have multiallelic variants and the info field does. Ive tried the following (without success):. annotated_vds = sample_vds.annotate_variants_vds(annotation_vds,va.test = if(! isMissing(va.aIndex)) va.info.AC[va.aIndex-1] else va.info.AC[0]); annotated_vds = sample_vds.annotate_variants_vds(annotation_vds,va.test = orElse(va.info.AC[va.aIndex-1],va.info.AC[0])). Is this normal behaviour and theres a way to annotate without knowing whether the vcf has multiallelic variants? Or is it not the expected behaviour?. The Hail version I'm using is 0.1. Thanks in advance,. Cristina.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3469
https://github.com/hail-is/hail/pull/3470:46,Usability,feedback,feedback,46,"This PR is not ready to be merged, but I want feedback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3470
https://github.com/hail-is/hail/issues/3472:302,Availability,error,error,302,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.1. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):. The docs under ; https://hail.is/docs/stable/hail.HailContext.html?highlight=import_vcf#hail.HailContext.import_vcf; are; ```; If generic equals False (default), Hail makes certain assumptions about the genotype fields, see Representation. On import, Hail filters (sets to no-call) any genotype that violates these assumptions. Hail interprets the format fields: GT, AD, OD, DP, GQ, PL; all others are silently dropped.; ```; but clicking the `Representation` link does lead to any additional details.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3472
https://github.com/hail-is/hail/issues/3472:308,Integrability,message,messages,308,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.1. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):. The docs under ; https://hail.is/docs/stable/hail.HailContext.html?highlight=import_vcf#hail.HailContext.import_vcf; are; ```; If generic equals False (default), Hail makes certain assumptions about the genotype fields, see Representation. On import, Hail filters (sets to no-call) any genotype that violates these assumptions. Hail interprets the format fields: GT, AD, OD, DP, GQ, PL; all others are silently dropped.; ```; but clicking the `Representation` link does lead to any additional details.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3472
https://github.com/hail-is/hail/pull/3474:27,Performance,load,loads,27,verified this successfully loads natives on linux and osx. @jbloom22 FYI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3474
https://github.com/hail-is/hail/issues/3480:181,Availability,failure,failure,181,"Minrep (in split multi) throwing:; ```; hail.utils.java.FatalError: HailException: invalid allele ""GN"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 10.0 failed 20 times, most recent failure: Lost task 9.19 in stage 10.0 (TID 1997, exomes-w-1.c.broad-mpg-gnomad.internal, executor 15): is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.Tra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:240,Availability,failure,failure,240,"Minrep (in split multi) throwing:; ```; hail.utils.java.FatalError: HailException: invalid allele ""GN"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 10.0 failed 20 times, most recent failure: Lost task 9.19 in stage 10.0 (TID 1997, exomes-w-1.c.broad-mpg-gnomad.internal, executor 15): is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.Tra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:411,Availability,Error,ErrorHandling,411,"Minrep (in split multi) throwing:; ```; hail.utils.java.FatalError: HailException: invalid allele ""GN"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 10.0 failed 20 times, most recent failure: Lost task 9.19 in stage 10.0 (TID 1997, exomes-w-1.c.broad-mpg-gnomad.internal, executor 15): is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.Tra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:437,Availability,Error,ErrorHandling,437,"Minrep (in split multi) throwing:; ```; hail.utils.java.FatalError: HailException: invalid allele ""GN"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 10.0 failed 20 times, most recent failure: Lost task 9.19 in stage 10.0 (TID 1997, exomes-w-1.c.broad-mpg-gnomad.internal, executor 15): is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.Tra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:8746,Availability,Error,ErrorHandling,8746,"edRVD.scala:649); 	at is.hail.methods.SplitMulti$.unionMovedVariants(SplitMulti.scala:237); 	at is.hail.methods.SplitMulti.split(SplitMulti.scala:332); 	at is.hail.methods.SplitMulti$.apply(SplitMulti.scala:232); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.coll",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:8772,Availability,Error,ErrorHandling,8772,"t is.hail.methods.SplitMulti$.unionMovedVariants(SplitMulti.scala:237); 	at is.hail.methods.SplitMulti.split(SplitMulti.scala:332); 	at is.hail.methods.SplitMulti$.apply(SplitMulti.scala:232); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:5110,Energy Efficiency,schedul,scheduler,5110,ic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:5182,Energy Efficiency,schedul,scheduler,5182,.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:5547,Energy Efficiency,schedul,scheduler,5547,ollection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:5587,Energy Efficiency,schedul,scheduler,5587,at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:5686,Energy Efficiency,schedul,scheduler,5686,t scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:5784,Energy Efficiency,schedul,scheduler,5784,onfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:6038,Energy Efficiency,schedul,scheduler,6038,nfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:6119,Energy Efficiency,schedul,scheduler,6119,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:6225,Energy Efficiency,schedul,scheduler,6225,he.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:6375,Energy Efficiency,schedul,scheduler,6375,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:6464,Energy Efficiency,schedul,scheduler,6464,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:6562,Energy Efficiency,schedul,scheduler,6562,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:6658,Energy Efficiency,schedul,scheduler,6658,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:478); 	at is.hail.rvd.OrderedRVD$.getPar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:6823,Energy Efficiency,schedul,scheduler,6823,.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:478); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:488); 	at is.hail.rvd.OrderedRVD$.adjustBoundsAndShuffle(OrderedRVD.scala:649); 	at is.hail.methods.SplitMulti$.unionMovedVariants(Spl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:13445,Energy Efficiency,schedul,scheduler,13445,DD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:297); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:13517,Energy Efficiency,schedul,scheduler,13517,DD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:297); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:1120,Integrability,Wrap,WrappedArray,1120,".apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 10.0 failed 20 times, most recent failure: Lost task 9.19 in stage 10.0 (TID 1997, exomes-w-1.c.broad-mpg-gnomad.internal, executor 15): is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:1140,Integrability,Wrap,WrappedArray,1140,"rkException: Job aborted due to stage failure: Task 9 in stage 10.0 failed 20 times, most recent failure: Lost task 9.19 in stage 10.0 (TID 1997, exomes-w-1.c.broad-mpg-gnomad.internal, executor 15): is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collect",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:9455,Integrability,Wrap,WrappedArray,9455,"AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:9475,Integrability,Wrap,WrappedArray,9475,"nvokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collect",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:5307,Performance,concurren,concurrent,5307,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:5392,Performance,concurren,concurrent,5392, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:13642,Performance,concurren,concurrent,13642,DD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:297); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:13727,Performance,concurren,concurrent,13727,DD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:297); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:160,Safety,abort,aborted,160,"Minrep (in split multi) throwing:; ```; hail.utils.java.FatalError: HailException: invalid allele ""GN"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 10.0 failed 20 times, most recent failure: Lost task 9.19 in stage 10.0 (TID 1997, exomes-w-1.c.broad-mpg-gnomad.internal, executor 15): is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.Tra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:5718,Safety,abort,abortStage,5718,tractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:5816,Safety,abort,abortStage,5816,un$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:6061,Safety,abort,abortStage,6061,xt.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:551,Security,validat,validate,551,"Minrep (in split multi) throwing:; ```; hail.utils.java.FatalError: HailException: invalid allele ""GN"". Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 10.0 failed 20 times, most recent failure: Lost task 9.19 in stage 10.0 (TID 1997, exomes-w-1.c.broad-mpg-gnomad.internal, executor 15): is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.Tra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/issues/3480:8886,Security,validat,validate,8886,"la:332); 	at is.hail.methods.SplitMulti$.apply(SplitMulti.scala:232); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: invalid allele ""GN""; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.variant.AltAlleleMethods$.validate(AltAlleleMethods.scala:24); 	at is.hail.variant.AltAlleleMethods$.altAlleleType(AltAlleleMethods.scala:28); 	at is.hail.variant.AltAlleleMethods$.isStar(AltAlleleMethods.scala:73); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at is.hail.variant.VariantMethods$$anonfun$minRep$1.apply(VariantMethods.scala:43); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.WrappedArray.forall(WrappedArray.scala:35); 	at is.hail.variant.VariantMethods$.minRep(VariantMethods.scala:43); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:196); 	at is.hail.methods.SplitMultiPartitionContext$$anonfun$2.apply(SplitMulti.scala:192); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.Abstra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3480
https://github.com/hail-is/hail/pull/3482:2,Testability,test,tested,2,I tested this on a cluster on a bit of code that @konradjk provided. not super sure if we can actually write a test for this for our test suite. Fixes #3446,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3482
https://github.com/hail-is/hail/pull/3482:111,Testability,test,test,111,I tested this on a cluster on a bit of code that @konradjk provided. not super sure if we can actually write a test for this for our test suite. Fixes #3446,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3482
https://github.com/hail-is/hail/pull/3482:133,Testability,test,test,133,I tested this on a cluster on a bit of code that @konradjk provided. not super sure if we can actually write a test for this for our test suite. Fixes #3446,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3482
https://github.com/hail-is/hail/issues/3490:476,Availability,error,error,476,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-63d60cc. ### What you did:; Ran pc_relate on pruned matrix table (500K SNVs) containing 5k samples. About 800 family members are in the dataset. The data includes a number of replicates also. ### What went wrong (all error messages here, including the full java stack trace):. Replicates should have a kinship coefficient near 0.50 with themselves. Instead HAIL pc_relate calculated 0.18. King correctly calculated 0.498 on the exported plink file. ; ```; 279,A-CUHS-CU001167-BL-COL-32167BL1,A-CUHS-CU001167-BL-COL-32167BL1dea,0.18452354646323424; 280,A-CUHS-CU001167-BL-COL-32167BL1,A-CUHS-CU001167-BL-COL-32167BL1der,0.18468831264044877; 281,A-CUHS-CU001167-BL-COL-32167BL1dea,A-CUHS-CU001167-BL-COL-32167BL1der,0.18515942448861653; 282,A-CUHS-CU001167-BL-COL-32167BL1,A-CUHS-CU001167-BL-COL-32167BL1dew,0.1833498632291505; 283,A-CUHS-CU001167-BL-COL-32167BL1dea,A-CUHS-CU001167-BL-COL-32167BL1dew,0.18376901216269165; 284,A-CUHS-CU001167-BL-COL-32167BL1der,A-CUHS-CU001167-BL-COL-32167BL1dew,0.18367338097096797; 285,A-CUHS-CU001167-BL-COL-32167BL1,A-CUHS-CU001167-BL-COL-32167BL1fa,0.18470621273350465; 286,A-CUHS-CU001167-BL-COL-32167BL1dea,A-CUHS-CU001167-BL-COL-32167BL1fa,0.1852915664909736; 287,A-CUHS-CU001167-BL-COL-32167BL1der,A-CUHS-CU001167-BL-COL-32167BL1fa,0.18516143096293416; 288,A-CUHS-CU001167-BL-COL-32167BL1dew,A-CUHS-CU001167-BL-COL-32167BL1fa,0.1838098671960904; 289,A-CUHS-CU001167-BL-COL-32167BL1,A-CUHS-CU001167-BL-COL-32167BL1fr,0.1847714820370011; 290,A-CUHS-CU001167-BL-COL-32167BL1dea,A-CUHS-CU001167-BL-COL-32167BL1fr,0.18525793071221844; 291,A-CUHS-CU001167-BL-COL-32167BL1der,A-CUHS-CU001167-BL-COL-32167BL1fr,0.1852207373990892; 292,A-CUHS-CU001167-BL-COL-32167BL1dew,A-CUHS-CU001167-BL-COL-32167BL1fr,0.183",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490
https://github.com/hail-is/hail/issues/3490:482,Integrability,message,messages,482,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-63d60cc. ### What you did:; Ran pc_relate on pruned matrix table (500K SNVs) containing 5k samples. About 800 family members are in the dataset. The data includes a number of replicates also. ### What went wrong (all error messages here, including the full java stack trace):. Replicates should have a kinship coefficient near 0.50 with themselves. Instead HAIL pc_relate calculated 0.18. King correctly calculated 0.498 on the exported plink file. ; ```; 279,A-CUHS-CU001167-BL-COL-32167BL1,A-CUHS-CU001167-BL-COL-32167BL1dea,0.18452354646323424; 280,A-CUHS-CU001167-BL-COL-32167BL1,A-CUHS-CU001167-BL-COL-32167BL1der,0.18468831264044877; 281,A-CUHS-CU001167-BL-COL-32167BL1dea,A-CUHS-CU001167-BL-COL-32167BL1der,0.18515942448861653; 282,A-CUHS-CU001167-BL-COL-32167BL1,A-CUHS-CU001167-BL-COL-32167BL1dew,0.1833498632291505; 283,A-CUHS-CU001167-BL-COL-32167BL1dea,A-CUHS-CU001167-BL-COL-32167BL1dew,0.18376901216269165; 284,A-CUHS-CU001167-BL-COL-32167BL1der,A-CUHS-CU001167-BL-COL-32167BL1dew,0.18367338097096797; 285,A-CUHS-CU001167-BL-COL-32167BL1,A-CUHS-CU001167-BL-COL-32167BL1fa,0.18470621273350465; 286,A-CUHS-CU001167-BL-COL-32167BL1dea,A-CUHS-CU001167-BL-COL-32167BL1fa,0.1852915664909736; 287,A-CUHS-CU001167-BL-COL-32167BL1der,A-CUHS-CU001167-BL-COL-32167BL1fa,0.18516143096293416; 288,A-CUHS-CU001167-BL-COL-32167BL1dew,A-CUHS-CU001167-BL-COL-32167BL1fa,0.1838098671960904; 289,A-CUHS-CU001167-BL-COL-32167BL1,A-CUHS-CU001167-BL-COL-32167BL1fr,0.1847714820370011; 290,A-CUHS-CU001167-BL-COL-32167BL1dea,A-CUHS-CU001167-BL-COL-32167BL1fr,0.18525793071221844; 291,A-CUHS-CU001167-BL-COL-32167BL1der,A-CUHS-CU001167-BL-COL-32167BL1fr,0.1852207373990892; 292,A-CUHS-CU001167-BL-COL-32167BL1dew,A-CUHS-CU001167-BL-COL-32167BL1fr,0.183",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3490
https://github.com/hail-is/hail/pull/3496:51,Integrability,interface,interface,51,"Fixes #3465. `GC` had been removed from the Python interface (mostly), but it should have also been removed on the Scala side as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3496
https://github.com/hail-is/hail/pull/3497:43,Availability,Error,Error,43,"The stack trace looks like this now:. ```; Error; Traceback (most recent call last):; File ""/Users/jigold/hail/python/hail/expr/expressions/expression_typecheck.py"", line 73, in check; return self.coerce(to_expr(x)); File ""/Users/jigold/hail/python/hail/expr/expressions/base_expression.py"", line 101, in to_expr; dtype = impute_type(e); File ""/Users/jigold/hail/python/hail/expr/expressions/base_expression.py"", line 59, in impute_type; raise ExpressionException(""Cannot impute type of empty list. Use 'hl.empty_array' to create an empty array.""); hail.expr.expressions.base_expression.ExpressionException: Cannot impute type of empty list. Use 'hl.empty_array' to create an empty array. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/jigold/hail/python/hail/typecheck/check.py"", line 426, in check_all; arg_ = tc.check(arg, name, argname); File ""/Users/jigold/hail/python/hail/expr/expressions/expression_typecheck.py"", line 75, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/jigold/hail/python/hail/tests/test_expr.py"", line 1085, in test_empty_collection_error_msg; self.assertRaisesRegex(hl.expr.ExpressionException, ""Cannot impute type of empty list."", hl.array([])); File ""<decorator-gen-420>"", line 2, in array; File ""/Users/jigold/hail/python/hail/typecheck/check.py"", line 494, in _typecheck; args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); File ""/Users/jigold/hail/python/hail/typecheck/check.py"", line 436, in check_all; )) from e; TypeError: array: parameter 'collection': expected expression of type set<any> or array<any> or dict<('any', 'any')>, found list: []; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3497
https://github.com/hail-is/hail/pull/3497:1226,Testability,test,tests,1226,"The stack trace looks like this now:. ```; Error; Traceback (most recent call last):; File ""/Users/jigold/hail/python/hail/expr/expressions/expression_typecheck.py"", line 73, in check; return self.coerce(to_expr(x)); File ""/Users/jigold/hail/python/hail/expr/expressions/base_expression.py"", line 101, in to_expr; dtype = impute_type(e); File ""/Users/jigold/hail/python/hail/expr/expressions/base_expression.py"", line 59, in impute_type; raise ExpressionException(""Cannot impute type of empty list. Use 'hl.empty_array' to create an empty array.""); hail.expr.expressions.base_expression.ExpressionException: Cannot impute type of empty list. Use 'hl.empty_array' to create an empty array. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/jigold/hail/python/hail/typecheck/check.py"", line 426, in check_all; arg_ = tc.check(arg, name, argname); File ""/Users/jigold/hail/python/hail/expr/expressions/expression_typecheck.py"", line 75, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/jigold/hail/python/hail/tests/test_expr.py"", line 1085, in test_empty_collection_error_msg; self.assertRaisesRegex(hl.expr.ExpressionException, ""Cannot impute type of empty list."", hl.array([])); File ""<decorator-gen-420>"", line 2, in array; File ""/Users/jigold/hail/python/hail/typecheck/check.py"", line 494, in _typecheck; args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); File ""/Users/jigold/hail/python/hail/typecheck/check.py"", line 436, in check_all; )) from e; TypeError: array: parameter 'collection': expected expression of type set<any> or array<any> or dict<('any', 'any')>, found list: []; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3497
https://github.com/hail-is/hail/pull/3497:1299,Testability,assert,assertRaisesRegex,1299,"The stack trace looks like this now:. ```; Error; Traceback (most recent call last):; File ""/Users/jigold/hail/python/hail/expr/expressions/expression_typecheck.py"", line 73, in check; return self.coerce(to_expr(x)); File ""/Users/jigold/hail/python/hail/expr/expressions/base_expression.py"", line 101, in to_expr; dtype = impute_type(e); File ""/Users/jigold/hail/python/hail/expr/expressions/base_expression.py"", line 59, in impute_type; raise ExpressionException(""Cannot impute type of empty list. Use 'hl.empty_array' to create an empty array.""); hail.expr.expressions.base_expression.ExpressionException: Cannot impute type of empty list. Use 'hl.empty_array' to create an empty array. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/jigold/hail/python/hail/typecheck/check.py"", line 426, in check_all; arg_ = tc.check(arg, name, argname); File ""/Users/jigold/hail/python/hail/expr/expressions/expression_typecheck.py"", line 75, in check; raise TypecheckFailure from e; hail.typecheck.check.TypecheckFailure. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/jigold/hail/python/hail/tests/test_expr.py"", line 1085, in test_empty_collection_error_msg; self.assertRaisesRegex(hl.expr.ExpressionException, ""Cannot impute type of empty list."", hl.array([])); File ""<decorator-gen-420>"", line 2, in array; File ""/Users/jigold/hail/python/hail/typecheck/check.py"", line 494, in _typecheck; args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); File ""/Users/jigold/hail/python/hail/typecheck/check.py"", line 436, in check_all; )) from e; TypeError: array: parameter 'collection': expected expression of type set<any> or array<any> or dict<('any', 'any')>, found list: []; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3497
https://github.com/hail-is/hail/pull/3501:1014,Availability,Down,Down,1014,"Builds on #3500 . This PR introduces support for sparse block matrices. The only new command exposed in Python is `sparsify_row_intervals`. Matrix product currently always results in a dense block matrix. Sparse block matrices also support transpose, diagonal, and all non-mathematical operations except filtering. Element-wise mathematical operations are currently supported if and only if they cannot transform zeroed blocks to non-zero blocks. For example, all forms of element-wise multiplication are supported,; and element-wise multiplication results in a sparse block matrix with block support equal to the intersection of that of the operands. On the other hand, scalar addition is not supported, and matrix addition is supported only between block matrices with the same block sparsity. Once this is in, I'll expose a couple more sparsifiers (rectangles, band) and also bring in parallel export of many rectangles to TSV from another branch, so that users can proceed with LD / fine mapping applications. Down the line I plan to support for all operations by expanding to union of block support, or to all blocks, for some operations. And matrix multiplication ought to return the minimal number of blocks as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3501
https://github.com/hail-is/hail/pull/3501:93,Security,expose,exposed,93,"Builds on #3500 . This PR introduces support for sparse block matrices. The only new command exposed in Python is `sparsify_row_intervals`. Matrix product currently always results in a dense block matrix. Sparse block matrices also support transpose, diagonal, and all non-mathematical operations except filtering. Element-wise mathematical operations are currently supported if and only if they cannot transform zeroed blocks to non-zero blocks. For example, all forms of element-wise multiplication are supported,; and element-wise multiplication results in a sparse block matrix with block support equal to the intersection of that of the operands. On the other hand, scalar addition is not supported, and matrix addition is supported only between block matrices with the same block sparsity. Once this is in, I'll expose a couple more sparsifiers (rectangles, band) and also bring in parallel export of many rectangles to TSV from another branch, so that users can proceed with LD / fine mapping applications. Down the line I plan to support for all operations by expanding to union of block support, or to all blocks, for some operations. And matrix multiplication ought to return the minimal number of blocks as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3501
https://github.com/hail-is/hail/pull/3501:818,Security,expose,expose,818,"Builds on #3500 . This PR introduces support for sparse block matrices. The only new command exposed in Python is `sparsify_row_intervals`. Matrix product currently always results in a dense block matrix. Sparse block matrices also support transpose, diagonal, and all non-mathematical operations except filtering. Element-wise mathematical operations are currently supported if and only if they cannot transform zeroed blocks to non-zero blocks. For example, all forms of element-wise multiplication are supported,; and element-wise multiplication results in a sparse block matrix with block support equal to the intersection of that of the operands. On the other hand, scalar addition is not supported, and matrix addition is supported only between block matrices with the same block sparsity. Once this is in, I'll expose a couple more sparsifiers (rectangles, band) and also bring in parallel export of many rectangles to TSV from another branch, so that users can proceed with LD / fine mapping applications. Down the line I plan to support for all operations by expanding to union of block support, or to all blocks, for some operations. And matrix multiplication ought to return the minimal number of blocks as well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3501
https://github.com/hail-is/hail/pull/3505:81,Deployability,update,update,81,"- new filter_alleles method: takes MT and lambda,; produces the fields needed to update row/entry fields; (old_to_new, new_to_old, old_locus, old_alleles); - subset_entries_hts and downcode_entries_hts are now; filter_alleles_hts, which calls filter_alleles.; - Exposed min_rep expr function in Python; - Added min_rep to AST FunctionRegistry (Scala); - Removed hl.min_rep method to minrep a MT (easy with the; function now); - Deleted FilterAlleles; - Deleted FilterAlleles (Scala); - Deleted minRep (Scala); - Moved MinRepSuite to Python",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3505
https://github.com/hail-is/hail/pull/3505:262,Security,Expose,Exposed,262,"- new filter_alleles method: takes MT and lambda,; produces the fields needed to update row/entry fields; (old_to_new, new_to_old, old_locus, old_alleles); - subset_entries_hts and downcode_entries_hts are now; filter_alleles_hts, which calls filter_alleles.; - Exposed min_rep expr function in Python; - Added min_rep to AST FunctionRegistry (Scala); - Removed hl.min_rep method to minrep a MT (easy with the; function now); - Deleted FilterAlleles; - Deleted FilterAlleles (Scala); - Deleted minRep (Scala); - Moved MinRepSuite to Python",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3505
https://github.com/hail-is/hail/issues/3507:519,Availability,error,error,519,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-7cde2bf. ### What you did:; Tried to read in, repartition, and write 3 matrixtables. This then corrupted the previous version (as I used overwrite), so tried to recreate the matrix tables from the original vcf, and could not import. . ### What went wrong (all error messages here, including the full java stack trace):; repartition error:; ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-8-f1eea7557457> in <module>(); ----> 1 hm3_variants_vds = hm3_variants_vds.repartition(100); <decorator-gen-688> in repartition(self, n_partitions, shuffle); /home/hail/hail.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 484 def _typecheck(__orig_func__, *args, **kwargs):; 485 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=True); --> 486 return __orig_func__(*args_, **kwargs_); 487 ; 488 return decorator(_typecheck); /home/hail/hail.zip/hail/matrixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.Capture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:591,Availability,error,error,591,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-7cde2bf. ### What you did:; Tried to read in, repartition, and write 3 matrixtables. This then corrupted the previous version (as I used overwrite), so tried to recreate the matrix tables from the original vcf, and could not import. . ### What went wrong (all error messages here, including the full java stack trace):; repartition error:; ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-8-f1eea7557457> in <module>(); ----> 1 hm3_variants_vds = hm3_variants_vds.repartition(100); <decorator-gen-688> in repartition(self, n_partitions, shuffle); /home/hail/hail.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 484 def _typecheck(__orig_func__, *args, **kwargs):; 485 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=True); --> 486 return __orig_func__(*args_, **kwargs_); 487 ; 488 return decorator(_typecheck); /home/hail/hail.zip/hail/matrixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.Capture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:1889,Availability,Error,Error,1889,"88> in repartition(self, n_partitions, shuffle); /home/hail/hail.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 484 def _typecheck(__orig_func__, *args, **kwargs):; 485 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=True); --> 486 return __orig_func__(*args_, **kwargs_); 487 ; 488 return decorator(_typecheck); /home/hail/hail.zip/hail/matrixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:2196,Availability,failure,failure,2196," return decorator(_typecheck); /home/hail/hail.zip/hail/matrixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:2254,Availability,failure,failure,2254,"ixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:11513,Availability,Error,Error,11513,"scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:11578,Availability,error,error,11578,"scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:11951,Availability,Error,ErrorHandling,11951,"ly(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:922); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:915); at is.hail.utils.package$.using(package.scala:577); at is.h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:11977,Availability,Error,ErrorHandling,11977,"a:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:922); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:915); at is.hail.utils.package$.using(package.scala:577); at is.hail.io.RichContextRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:5070,Energy Efficiency,schedul,scheduler,5070,lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.schedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:5141,Energy Efficiency,schedul,scheduler,5141,la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:5501,Energy Efficiency,schedul,scheduler,5501,); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:5541,Energy Efficiency,schedul,scheduler,5541,cala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:5639,Energy Efficiency,schedul,scheduler,5639,cala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:5736,Energy Efficiency,schedul,scheduler,5736,.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:5987,Energy Efficiency,schedul,scheduler,5987,Context$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:6067,Energy Efficiency,schedul,scheduler,6067,heduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:6172,Energy Efficiency,schedul,scheduler,6172,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:6320,Energy Efficiency,schedul,scheduler,6320,va:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:6408,Energy Efficiency,schedul,scheduler,6408,624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:6505,Energy Efficiency,schedul,scheduler,6505,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); at is.hail.rvd.Ordered,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:6600,Energy Efficiency,schedul,scheduler,6600,.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:541); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:6763,Energy Efficiency,schedul,scheduler,6763,abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:541); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:200); at is.hail.variant.MatrixTable.coalesce(MatrixTable.scala:2073); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:11093,Energy Efficiency,schedul,scheduler,11093,"lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Contex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:11164,Energy Efficiency,schedul,scheduler,11164,"la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:15882,Energy Efficiency,schedul,scheduler,15882,lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NegativeArraySizeException; at java.util.Arrays.copyOf(Arrays.java:3236); at is.hail.annotations.Region.ensure(Region.scala:139); at is.hail.annotations.Region.allocate(Region.scala:152); at is.hail.annotations.Region.allocate(Region.scala:159); at is.hail.annotations.RegionValueBuilder.allocateRoot(RegionValueBuilder.scala:73); at is.hail.annotations.RegionValueBuilder.startBaseStruct(RegionValueBuilder.scala:92); at is.hail.annotations.RegionValueBuilder.startStruct(RegionValueBuilder.scala:115); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:740); ... 4,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:15953,Energy Efficiency,schedul,scheduler,15953,eric.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NegativeArraySizeException; at java.util.Arrays.copyOf(Arrays.java:3236); at is.hail.annotations.Region.ensure(Region.scala:139); at is.hail.annotations.Region.allocate(Region.scala:152); at is.hail.annotations.Region.allocate(Region.scala:159); at is.hail.annotations.RegionValueBuilder.allocateRoot(RegionValueBuilder.scala:73); at is.hail.annotations.RegionValueBuilder.startBaseStruct(RegionValueBuilder.scala:92); at is.hail.annotations.RegionValueBuilder.startStruct(RegionValueBuilder.scala:115); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:740); ... 49 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:16454,Energy Efficiency,allocate,allocate,16454,eric.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NegativeArraySizeException; at java.util.Arrays.copyOf(Arrays.java:3236); at is.hail.annotations.Region.ensure(Region.scala:139); at is.hail.annotations.Region.allocate(Region.scala:152); at is.hail.annotations.Region.allocate(Region.scala:159); at is.hail.annotations.RegionValueBuilder.allocateRoot(RegionValueBuilder.scala:73); at is.hail.annotations.RegionValueBuilder.startBaseStruct(RegionValueBuilder.scala:92); at is.hail.annotations.RegionValueBuilder.startStruct(RegionValueBuilder.scala:115); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:740); ... 49 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:16512,Energy Efficiency,allocate,allocate,16512,eric.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NegativeArraySizeException; at java.util.Arrays.copyOf(Arrays.java:3236); at is.hail.annotations.Region.ensure(Region.scala:139); at is.hail.annotations.Region.allocate(Region.scala:152); at is.hail.annotations.Region.allocate(Region.scala:159); at is.hail.annotations.RegionValueBuilder.allocateRoot(RegionValueBuilder.scala:73); at is.hail.annotations.RegionValueBuilder.startBaseStruct(RegionValueBuilder.scala:92); at is.hail.annotations.RegionValueBuilder.startStruct(RegionValueBuilder.scala:115); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:740); ... 49 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:16582,Energy Efficiency,allocate,allocateRoot,16582,eric.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NegativeArraySizeException; at java.util.Arrays.copyOf(Arrays.java:3236); at is.hail.annotations.Region.ensure(Region.scala:139); at is.hail.annotations.Region.allocate(Region.scala:152); at is.hail.annotations.Region.allocate(Region.scala:159); at is.hail.annotations.RegionValueBuilder.allocateRoot(RegionValueBuilder.scala:73); at is.hail.annotations.RegionValueBuilder.startBaseStruct(RegionValueBuilder.scala:92); at is.hail.annotations.RegionValueBuilder.startStruct(RegionValueBuilder.scala:115); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:740); ... 49 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:525,Integrability,message,messages,525,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-7cde2bf. ### What you did:; Tried to read in, repartition, and write 3 matrixtables. This then corrupted the previous version (as I used overwrite), so tried to recreate the matrix tables from the original vcf, and could not import. . ### What went wrong (all error messages here, including the full java stack trace):; repartition error:; ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-8-f1eea7557457> in <module>(); ----> 1 hm3_variants_vds = hm3_variants_vds.repartition(100); <decorator-gen-688> in repartition(self, n_partitions, shuffle); /home/hail/hail.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 484 def _typecheck(__orig_func__, *args, **kwargs):; 485 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=True); --> 486 return __orig_func__(*args_, **kwargs_); 487 ; 488 return decorator(_typecheck); /home/hail/hail.zip/hail/matrixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.Capture",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:11700,Integrability,message,message,11700,"scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:12078,Integrability,wrap,wrapException,12078,"ark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:922); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:915); at is.hail.utils.package$.using(package.scala:577); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:915); at is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:2473,Performance,load,loadAddress,2473,".zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:299); at is.hail.sparkextras.Co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:2537,Performance,load,loadField,2537,"self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:299); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:5264,Performance,concurren,concurrent,5264,eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:5348,Performance,concurren,concurrent,5348,ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:8496,Performance,load,loadAddress,8496,deredRVD$.getPartitionKeyInfo(OrderedRVD.scala:541); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:200); at is.hail.variant.MatrixTable.coalesce(MatrixTable.scala:2073); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:299); at is.hail.sparkextras.Co,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:8560,Performance,load,loadField,8560,d.OrderedRVD.coalesce(OrderedRVD.scala:200); at is.hail.variant.MatrixTable.coalesce(MatrixTable.scala:2073); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.apply(ContextRDD.scala:299); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$23.app,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:11287,Performance,concurren,concurrent,11287,"eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:11371,Performance,concurren,concurrent,11371,"ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$ano",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:12129,Performance,Load,LoadVCF,12129,"rg.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:922); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:915); at is.hail.utils.package$.using(package.scala:577); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:915); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:12175,Performance,Load,LoadVCF,12175,"ask.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:922); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4.apply(RowStore.scala:915); at is.hail.utils.package$.using(package.scala:577); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:915); at is.hail.io.RichContextRDDRegionValue$$anonfun$6$$anonfun$apply$1$$anonfun$apply$2$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:16076,Performance,concurren,concurrent,16076,eric.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NegativeArraySizeException; at java.util.Arrays.copyOf(Arrays.java:3236); at is.hail.annotations.Region.ensure(Region.scala:139); at is.hail.annotations.Region.allocate(Region.scala:152); at is.hail.annotations.Region.allocate(Region.scala:159); at is.hail.annotations.RegionValueBuilder.allocateRoot(RegionValueBuilder.scala:73); at is.hail.annotations.RegionValueBuilder.startBaseStruct(RegionValueBuilder.scala:92); at is.hail.annotations.RegionValueBuilder.startStruct(RegionValueBuilder.scala:115); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:740); ... 49 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:16160,Performance,concurren,concurrent,16160,eric.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NegativeArraySizeException; at java.util.Arrays.copyOf(Arrays.java:3236); at is.hail.annotations.Region.ensure(Region.scala:139); at is.hail.annotations.Region.allocate(Region.scala:152); at is.hail.annotations.Region.allocate(Region.scala:159); at is.hail.annotations.RegionValueBuilder.allocateRoot(RegionValueBuilder.scala:73); at is.hail.annotations.RegionValueBuilder.startBaseStruct(RegionValueBuilder.scala:92); at is.hail.annotations.RegionValueBuilder.startStruct(RegionValueBuilder.scala:115); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:740); ... 49 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:16816,Performance,Load,LoadVCF,16816,eric.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NegativeArraySizeException; at java.util.Arrays.copyOf(Arrays.java:3236); at is.hail.annotations.Region.ensure(Region.scala:139); at is.hail.annotations.Region.allocate(Region.scala:152); at is.hail.annotations.Region.allocate(Region.scala:159); at is.hail.annotations.RegionValueBuilder.allocateRoot(RegionValueBuilder.scala:73); at is.hail.annotations.RegionValueBuilder.startBaseStruct(RegionValueBuilder.scala:92); at is.hail.annotations.RegionValueBuilder.startStruct(RegionValueBuilder.scala:115); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:740); ... 49 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:16862,Performance,Load,LoadVCF,16862,eric.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NegativeArraySizeException; at java.util.Arrays.copyOf(Arrays.java:3236); at is.hail.annotations.Region.ensure(Region.scala:139); at is.hail.annotations.Region.allocate(Region.scala:152); at is.hail.annotations.Region.allocate(Region.scala:159); at is.hail.annotations.RegionValueBuilder.allocateRoot(RegionValueBuilder.scala:73); at is.hail.annotations.RegionValueBuilder.startBaseStruct(RegionValueBuilder.scala:92); at is.hail.annotations.RegionValueBuilder.startStruct(RegionValueBuilder.scala:115); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:740); ... 49 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:2175,Safety,abort,aborted,2175," return decorator(_typecheck); /home/hail/hail.zip/hail/matrixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:5671,Safety,abort,abortStage,5671,llection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:5768,Safety,abort,abortStage,5768,ect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:6010,Safety,abort,abortStage,6010,y(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:2085,Testability,Assert,AssertionError,2085,"ck_all(__orig_func__, args, kwargs, checkers, is_method=True); --> 486 return __orig_func__(*args_, **kwargs_); 487 ; 488 return decorator(_typecheck); /home/hail/hail.zip/hail/matrixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:2101,Testability,assert,assertion,2101,"ck_all(__orig_func__, args, kwargs, checkers, is_method=True); --> 486 return __orig_func__(*args_, **kwargs_); 487 ; 488 return decorator(_typecheck); /home/hail/hail.zip/hail/matrixtable.py in repartition(self, n_partitions, shuffle); 2505 Repartitioned dataset.; 2506 """"""; -> 2507 jvds = self._jvds.coalesce(n_partitions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:2366,Testability,Assert,AssertionError,2366,"ions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cma",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:2382,Testability,assert,assertion,2382,"ions, shuffle); 2508 return MatrixTable(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cma",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:2417,Testability,assert,assert,2417,"le(jvds); 2509 ; /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:; /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'; FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 7.0 failed 20 times, most recent failure: Lost task 4.19 in stage 7.0 (TID 601, mycluster-w-0.c.ukbb-all-phenos.internal, executor 2): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$appl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:8389,Testability,Assert,AssertionError,8389,.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:541); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:200); at is.hail.variant.MatrixTable.coalesce(MatrixTable.scala:2073); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cma,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:8405,Testability,assert,assertion,8405,.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:541); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:200); at is.hail.variant.MatrixTable.coalesce(MatrixTable.scala:2073); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cma,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:8440,Testability,assert,assert,8440,ontextRDD.collect(ContextRDD.scala:132); at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:541); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:200); at is.hail.variant.MatrixTable.coalesce(MatrixTable.scala:2073); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadAddress(Region.scala:63); at is.hail.expr.types.TBaseStruct.loadField(TBaseStruct.scala:215); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:341); at is.hail.annotations.WritableRegionValue.setSelect(WritableRegionValue.scala:38); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:511); at is.hail.rvd.OrderedRVD$$anonfun$getKeys$1$$anonfun$apply$9.apply(OrderedRVD.scala:510); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.rvd.OrderedRVPartitionInfo$.apply(OrderedRVPartitionInfo.scala:30); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:536); at is.hail.rvd.OrderedRVD$$anonfun$10.apply(OrderedRVD.scala:534); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:11528,Testability,Assert,AssertionError,11528,"scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3507:11544,Testability,assert,assertion,11544,"scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Hail version: devel-824968e; Error summary: AssertionError: assertion failed; ```; import_vcf error:; Just stayed at 0 out of 1 complete on the cloud, looked into the processes, it had failed 9 times, and here's the message I could dig out:; ```; is.hail.utils.HailException: hapmap_3.3_hg19_pop_stratified_af.vcf.gz: caught java.lang.NegativeArraySizeException: null; offending line: chr7 71494997 rs844684 A C . PASS AC=1191;AF=0.42627;ALL={A*...; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); at is.hail.utils.package$.fatal(package.scala:26); at is.hail.utils.Context.wrapException(Context.scala:23); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:767); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3507
https://github.com/hail-is/hail/issues/3508:2552,Availability,error,error,2552,"ple_file='gs://phenotype_31063/ukb31063.imputed_v3.autosomes.sample',; contig_recoding=contigs). # merge sumstats on bgen matrixtable; variants = variants.annotate_rows(ss = sumstats[variants.locus]). # handle strand/allele flips; variants = variants.annotate_rows(beta = hl.case(); .when(((variants.alleles[0] == variants.ss.nt1) &; (variants.alleles[1] == variants.ss.nt2)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt1) & ; (flip_text(variants.alleles[1]) == variants.ss.nt2)),; (-1*variants.ss.ldpred_inf_beta)); .when(((variants.alleles[0] == variants.ss.nt2) &; (variants.alleles[1] == variants.ss.nt1)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt2) & ; (flip_text(variants.alleles[1]) == variants.ss.nt1)),; variants.ss.ldpred_inf_beta); .or_missing()). variants = variants.filter_rows(hl.is_defined(variants.beta)); variants.beta.show(); ```. ### What went wrong (all error messages here, including the full java stack trace): When I went to try to show the beta column, Scala ""crashed"" such that I had to type in ""localhost:4040"" to reconnect and go into my application history to see what happened. I didn't get any errors in the Notebook I was using--it just stopped doing any work. . In the Scala tasks console, all of my workers had the following error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:2802,Availability,error,errors,2802," = sumstats[variants.locus]). # handle strand/allele flips; variants = variants.annotate_rows(beta = hl.case(); .when(((variants.alleles[0] == variants.ss.nt1) &; (variants.alleles[1] == variants.ss.nt2)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt1) & ; (flip_text(variants.alleles[1]) == variants.ss.nt2)),; (-1*variants.ss.ldpred_inf_beta)); .when(((variants.alleles[0] == variants.ss.nt2) &; (variants.alleles[1] == variants.ss.nt1)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt2) & ; (flip_text(variants.alleles[1]) == variants.ss.nt1)),; variants.ss.ldpred_inf_beta); .or_missing()). variants = variants.filter_rows(hl.is_defined(variants.beta)); variants.beta.show(); ```. ### What went wrong (all error messages here, including the full java stack trace): When I went to try to show the beta column, Scala ""crashed"" such that I had to type in ""localhost:4040"" to reconnect and go into my application history to see what happened. I didn't get any errors in the Notebook I was using--it just stopped doing any work. . In the Scala tasks console, all of my workers had the following error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:964); 	at scala.collection.Iterator$$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:2936,Availability,error,error,2936,"te_rows(beta = hl.case(); .when(((variants.alleles[0] == variants.ss.nt1) &; (variants.alleles[1] == variants.ss.nt2)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt1) & ; (flip_text(variants.alleles[1]) == variants.ss.nt2)),; (-1*variants.ss.ldpred_inf_beta)); .when(((variants.alleles[0] == variants.ss.nt2) &; (variants.alleles[1] == variants.ss.nt1)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt2) & ; (flip_text(variants.alleles[1]) == variants.ss.nt1)),; variants.ss.ldpred_inf_beta); .or_missing()). variants = variants.filter_rows(hl.is_defined(variants.beta)); variants.beta.show(); ```. ### What went wrong (all error messages here, including the full java stack trace): When I went to try to show the beta column, Scala ""crashed"" such that I had to type in ""localhost:4040"" to reconnect and go into my application history to see what happened. I didn't get any errors in the Notebook I was using--it just stopped doing any work. . In the Scala tasks console, all of my workers had the following error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:964); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:7507,Availability,Failure,Failure,7507,"rderedRVD.scala:285); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); is.hail.table.Table.showString(Table.scala:1031); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.lang.reflect.Method.invoke(Method.java:498); py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); py4j.Gateway.invoke(Gateway.java:280); py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); py4j.commands.CallCommand.execute(CallCommand.java:79); ```. Also under Failed Stages, the Failure Reason was given as:; ```; Job aborted due to stage failure: Task 0 in stage 10.0 failed 20 times, most recent failure: Lost task 0.19 in stage 10.0 (TID 526, ccarey-sw-svrp.c.ukbb-robinson.internal, executor 43): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:7567,Availability,failure,failure,7567,"rderedRVD.scala:285); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); is.hail.table.Table.showString(Table.scala:1031); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.lang.reflect.Method.invoke(Method.java:498); py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); py4j.Gateway.invoke(Gateway.java:280); py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); py4j.commands.CallCommand.execute(CallCommand.java:79); ```. Also under Failed Stages, the Failure Reason was given as:; ```; Job aborted due to stage failure: Task 0 in stage 10.0 failed 20 times, most recent failure: Lost task 0.19 in stage 10.0 (TID 526, ccarey-sw-svrp.c.ukbb-robinson.internal, executor 43): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:7626,Availability,failure,failure,7626,"D$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); is.hail.table.Table.showString(Table.scala:1031); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.lang.reflect.Method.invoke(Method.java:498); py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); py4j.Gateway.invoke(Gateway.java:280); py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); py4j.commands.CallCommand.execute(CallCommand.java:79); ```. Also under Failed Stages, the Failure Reason was given as:; ```; Job aborted due to stage failure: Task 0 in stage 10.0 failed 20 times, most recent failure: Lost task 0.19 in stage 10.0 (TID 526, ccarey-sw-svrp.c.ukbb-robinson.internal, executor 43): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:964); 	at scala.collection.Iterator$$anon$11.next(I",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:3122,Energy Efficiency,allocate,allocate,3122,"iants.ss.nt2)),; (-1*variants.ss.ldpred_inf_beta)); .when(((variants.alleles[0] == variants.ss.nt2) &; (variants.alleles[1] == variants.ss.nt1)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt2) & ; (flip_text(variants.alleles[1]) == variants.ss.nt1)),; variants.ss.ldpred_inf_beta); .or_missing()). variants = variants.filter_rows(hl.is_defined(variants.beta)); variants.beta.show(); ```. ### What went wrong (all error messages here, including the full java stack trace): When I went to try to show the beta column, Scala ""crashed"" such that I had to type in ""localhost:4040"" to reconnect and go into my application history to see what happened. I didn't get any errors in the Notebook I was using--it just stopped doing any work. . In the Scala tasks console, all of my workers had the following error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:964); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.nex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:3181,Energy Efficiency,allocate,allocate,3181,"(variants.alleles[0] == variants.ss.nt2) &; (variants.alleles[1] == variants.ss.nt1)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt2) & ; (flip_text(variants.alleles[1]) == variants.ss.nt1)),; variants.ss.ldpred_inf_beta); .or_missing()). variants = variants.filter_rows(hl.is_defined(variants.beta)); variants.beta.show(); ```. ### What went wrong (all error messages here, including the full java stack trace): When I went to try to show the beta column, Scala ""crashed"" such that I had to type in ""localhost:4040"" to reconnect and go into my application history to see what happened. I didn't get any errors in the Notebook I was using--it just stopped doing any work. . In the Scala tasks console, all of my workers had the following error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:964); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:3241,Energy Efficiency,allocate,allocate,3241,"s[1] == variants.ss.nt1)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt2) & ; (flip_text(variants.alleles[1]) == variants.ss.nt1)),; variants.ss.ldpred_inf_beta); .or_missing()). variants = variants.filter_rows(hl.is_defined(variants.beta)); variants.beta.show(); ```. ### What went wrong (all error messages here, including the full java stack trace): When I went to try to show the beta column, Scala ""crashed"" such that I had to type in ""localhost:4040"" to reconnect and go into my application history to see what happened. I didn't get any errors in the Notebook I was using--it just stopped doing any work. . In the Scala tasks console, all of my workers had the following error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:964); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:5846,Energy Efficiency,schedul,scheduler,5846,"r.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Under ""Failed Stages"", these were the details for what I was running:; ```; org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:285); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:5918,Energy Efficiency,schedul,scheduler,5918,"xt(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Under ""Failed Stages"", these were the details for what I was running:; ```; org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:285); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); is.hail.table.Table.showString(Table.scala:1031); sun.reflect.NativeMetho",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:7902,Energy Efficiency,allocate,allocate,7902,"t.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.lang.reflect.Method.invoke(Method.java:498); py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); py4j.Gateway.invoke(Gateway.java:280); py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); py4j.commands.CallCommand.execute(CallCommand.java:79); ```. Also under Failed Stages, the Failure Reason was given as:; ```; Job aborted due to stage failure: Task 0 in stage 10.0 failed 20 times, most recent failure: Lost task 0.19 in stage 10.0 (TID 526, ccarey-sw-svrp.c.ukbb-robinson.internal, executor 43): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:964); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.nex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:7961,Energy Efficiency,allocate,allocate,7961,"ect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.lang.reflect.Method.invoke(Method.java:498); py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); py4j.Gateway.invoke(Gateway.java:280); py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); py4j.commands.CallCommand.execute(CallCommand.java:79); ```. Also under Failed Stages, the Failure Reason was given as:; ```; Job aborted due to stage failure: Task 0 in stage 10.0 failed 20 times, most recent failure: Lost task 0.19 in stage 10.0 (TID 526, ccarey-sw-svrp.c.ukbb-robinson.internal, executor 43): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:964); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:8021,Energy Efficiency,allocate,allocate,8021,".java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.lang.reflect.Method.invoke(Method.java:498); py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); py4j.Gateway.invoke(Gateway.java:280); py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); py4j.commands.CallCommand.execute(CallCommand.java:79); ```. Also under Failed Stages, the Failure Reason was given as:; ```; Job aborted due to stage failure: Task 0 in stage 10.0 failed 20 times, most recent failure: Lost task 0.19 in stage 10.0 (TID 526, ccarey-sw-svrp.c.ukbb-robinson.internal, executor 43): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:964); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:10626,Energy Efficiency,schedul,scheduler,10626,xt(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:10698,Energy Efficiency,schedul,scheduler,10698,xt(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:2558,Integrability,message,messages,2558,"ple_file='gs://phenotype_31063/ukb31063.imputed_v3.autosomes.sample',; contig_recoding=contigs). # merge sumstats on bgen matrixtable; variants = variants.annotate_rows(ss = sumstats[variants.locus]). # handle strand/allele flips; variants = variants.annotate_rows(beta = hl.case(); .when(((variants.alleles[0] == variants.ss.nt1) &; (variants.alleles[1] == variants.ss.nt2)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt1) & ; (flip_text(variants.alleles[1]) == variants.ss.nt2)),; (-1*variants.ss.ldpred_inf_beta)); .when(((variants.alleles[0] == variants.ss.nt2) &; (variants.alleles[1] == variants.ss.nt1)) | ; ((flip_text(variants.alleles[0]) == variants.ss.nt2) & ; (flip_text(variants.alleles[1]) == variants.ss.nt1)),; variants.ss.ldpred_inf_beta); .or_missing()). variants = variants.filter_rows(hl.is_defined(variants.beta)); variants.beta.show(); ```. ### What went wrong (all error messages here, including the full java stack trace): When I went to try to show the beta column, Scala ""crashed"" such that I had to type in ""localhost:4040"" to reconnect and go into my application history to see what happened. I didn't get any errors in the Notebook I was using--it just stopped doing any work. . In the Scala tasks console, all of my workers had the following error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:780,Performance,load,load,780,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. Sorry for the awful formatting!! I'm not familiar with how to properly format for GitHub, and clearly some of my code got picked up as formatting.... . EDIT: Fixed the formatting :). ### Hail version: 0.2 / devel. ### What you did: ; ```; import. hail as hl. def flip_text(base):; """"""; :param StringExpression base: Expression of a single base; :return: StringExpression of flipped base; :rtype: StringExpression; """"""; return hl.cond(base == 'A', 'T',; hl.cond(base == 'T', 'A',; hl.cond(base == 'C', 'G',; hl.cond(base == 'G', 'C', base)))). # load ldpred sumstats file; sumstats = hl.import_table('gs://ukbb_prs/sumstats/UKB_SCZ_LDPred.txt', delimiter='\s+', impute=True). # create locus and alleles columns and key by locus; sumstats = (sumstats.annotate(locus=hl.parse_locus(sumstats.chrom[6:] + "":"" + hl.str(sumstats.pos)),; alleles=[sumstats.nt1,sumstats.nt2]); .key_by('locus'); .order_by('locus')). # repartition sumstats to save time; sumstats = sumstats.repartition(100). # write the sumstats table; sumstats.write('gs://ukbb_prs/sumstats/UKB_SCZ_LDPred.kt', overwrite=True). # read the sumstats table; sumstats = hl.read_table('gs://ukbb_prs/sumstats/UKB_SCZ_LDPred.kt'). # remove leading zeros from contigs; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}. # import variants; variants = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{22}_v3.bgen',; [],; sample_file='gs://phenotype_31063/ukb31063.imputed_v3.autosomes.sample',; contig_recoding=contigs). # merge sumstats on bgen matrixtable; variants = variants.annotate_rows(ss = sumstats[variants.locus]). # handle strand/allele flips; variants = variants.annotate_rows(beta = hl.case(); .when(((variants.alleles[0] == variants.ss.nt1) &; (variants.al",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:6043,Performance,concurren,concurrent,6043,"teratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Under ""Failed Stages"", these were the details for what I was running:; ```; org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:285); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); is.hail.table.Table.showString(Table.scala:1031); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.refle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:6128,Performance,concurren,concurrent,6128,"2.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Under ""Failed Stages"", these were the details for what I was running:; ```; org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:285); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); is.hail.table.Table.showString(Table.scala:1031); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.la",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:10823,Performance,concurren,concurrent,10823,xt(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:10908,Performance,concurren,concurrent,10908,xt(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:911); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:7546,Safety,abort,aborted,7546,"rderedRVD.scala:285); is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); is.hail.rvd.RVD$class.take(RVD.scala:247); is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); is.hail.table.Table.take(Table.scala:990); is.hail.table.Table.showString(Table.scala:1031); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.lang.reflect.Method.invoke(Method.java:498); py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); py4j.Gateway.invoke(Gateway.java:280); py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); py4j.commands.CallCommand.execute(CallCommand.java:79); ```. Also under Failed Stages, the Failure Reason was given as:; ```; Job aborted due to stage failure: Task 0 in stage 10.0 failed 20 times, most recent failure: Lost task 0.19 in stage 10.0 (TID 526, ccarey-sw-svrp.c.ukbb-robinson.internal, executor 43): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TBinary$.allocate(TBinary.scala:101); 	at is.hail.annotations.RegionValueBuilder.fixupBinary(RegionValueBuilder.scala:263); 	at is.hail.annotations.RegionValueBuilder.fixupStruct(RegionValueBuilder.scala:319); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:288); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfun$apply$21.apply(Relational.scala:975); 	at is.hail.expr.MatrixMapRows$$anonfun$31$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/issues/3508:329,Usability,clear,clearly,329,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. Sorry for the awful formatting!! I'm not familiar with how to properly format for GitHub, and clearly some of my code got picked up as formatting.... . EDIT: Fixed the formatting :). ### Hail version: 0.2 / devel. ### What you did: ; ```; import. hail as hl. def flip_text(base):; """"""; :param StringExpression base: Expression of a single base; :return: StringExpression of flipped base; :rtype: StringExpression; """"""; return hl.cond(base == 'A', 'T',; hl.cond(base == 'T', 'A',; hl.cond(base == 'C', 'G',; hl.cond(base == 'G', 'C', base)))). # load ldpred sumstats file; sumstats = hl.import_table('gs://ukbb_prs/sumstats/UKB_SCZ_LDPred.txt', delimiter='\s+', impute=True). # create locus and alleles columns and key by locus; sumstats = (sumstats.annotate(locus=hl.parse_locus(sumstats.chrom[6:] + "":"" + hl.str(sumstats.pos)),; alleles=[sumstats.nt1,sumstats.nt2]); .key_by('locus'); .order_by('locus')). # repartition sumstats to save time; sumstats = sumstats.repartition(100). # write the sumstats table; sumstats.write('gs://ukbb_prs/sumstats/UKB_SCZ_LDPred.kt', overwrite=True). # read the sumstats table; sumstats = hl.read_table('gs://ukbb_prs/sumstats/UKB_SCZ_LDPred.kt'). # remove leading zeros from contigs; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}. # import variants; variants = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{22}_v3.bgen',; [],; sample_file='gs://phenotype_31063/ukb31063.imputed_v3.autosomes.sample',; contig_recoding=contigs). # merge sumstats on bgen matrixtable; variants = variants.annotate_rows(ss = sumstats[variants.locus]). # handle strand/allele flips; variants = variants.annotate_rows(beta = hl.case(); .when(((variants.alleles[0] == variants.ss.nt1) &; (variants.al",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3508
https://github.com/hail-is/hail/pull/3512:47,Availability,toler,tolerant,47,I assume the local file reads are somehow more tolerant to being closed? I don't know why this doesn't fail like every single matrix read test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3512
https://github.com/hail-is/hail/pull/3512:138,Testability,test,test,138,I assume the local file reads are somehow more tolerant to being closed? I don't know why this doesn't fail like every single matrix read test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3512
https://github.com/hail-is/hail/pull/3514:37,Usability,clear,clears,37,I realized I was missing some region clears from aggregate methods on ContextRDD.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3514
https://github.com/hail-is/hail/issues/3515:1279,Availability,Error,Error,1279,From @berylc - possibly due to empty partitions? This was after 2 import_table's and a join:; ```; Java stack trace:; java.lang.UnsupportedOperationException: empty.min; at scala.collection.TraversableOnce$class.min(TraversableOnce.scala:222); at scala.collection.mutable.ArrayOps$ofRef.min(ArrayOps.scala:186); at is.hail.rvd.OrderedRVD$.calculateKeyRanges(OrderedRVD.scala:615); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:184); at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:19); at is.hail.table.Table.repartition(Table.scala:1003); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). Hail version: devel-3959178; Error summary: UnsupportedOperationException: empty.min; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3515
https://github.com/hail-is/hail/issues/3516:1546,Availability,Error,Error,1546,"""/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2963, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-9-304965820738>"", line 1, in <module>; x.key_by('y').show(); File ""<decorator-gen-598>"", line 2, in show; File ""/Users/konradk/Dropbox/src/python/hail/typecheck/check.py"", line 486, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1101, in show; print(self._show(n,width, truncate, types)); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1104, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/konradk/programs/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/Dropbox/src/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 49, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.C",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:1761,Availability,failure,failure,1761,"ile ""<ipython-input-9-304965820738>"", line 1, in <module>; x.key_by('y').show(); File ""<decorator-gen-598>"", line 2, in show; File ""/Users/konradk/Dropbox/src/python/hail/typecheck/check.py"", line 486, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1101, in show; print(self._show(n,width, truncate, types)); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1104, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/konradk/programs/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/Dropbox/src/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 49, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:1818,Availability,failure,failure,1818,"x.key_by('y').show(); File ""<decorator-gen-598>"", line 2, in show; File ""/Users/konradk/Dropbox/src/python/hail/typecheck/check.py"", line 486, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1101, in show; print(self._show(n,width, truncate, types)); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1104, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/konradk/programs/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/Dropbox/src/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 49, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:7837,Availability,Error,Error,7837,oke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Hail version: devel-9a5678f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:3036,Energy Efficiency,schedul,scheduler,3036,eredRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:3108,Energy Efficiency,schedul,scheduler,3108,.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:3473,Energy Efficiency,schedul,scheduler,3473,ob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:3513,Energy Efficiency,schedul,scheduler,3513,hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:3612,Energy Efficiency,schedul,scheduler,3612,.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:3710,Energy Efficiency,schedul,scheduler,3710,DD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:3964,Energy Efficiency,schedul,scheduler,3964,nfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:4045,Energy Efficiency,schedul,scheduler,4045,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at is.hail.sparkextras.ContextRDD.runJob(Con,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:4151,Energy Efficiency,schedul,scheduler,4151,he.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); 	at is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); 	at is.hail.rvd.Ordered,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:4301,Energy Efficiency,schedul,scheduler,4301,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); 	at is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:285); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); 	at is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); 	at is.hai,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:4390,Energy Efficiency,schedul,scheduler,4390,va.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); 	at is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:285); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); 	at is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); 	at is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); 	at is.hail.rvd.RVD$class.take(RVD.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:4488,Energy Efficiency,schedul,scheduler,4488,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); 	at is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:285); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); 	at is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); 	at is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); 	at is.hail.rvd.RVD$class.take(RVD.scala:247); 	at is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); 	at is.hail.table.Table.take(Table.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:4584,Energy Efficiency,schedul,scheduler,4584,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); 	at is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:285); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); 	at is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); 	at is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); 	at is.hail.rvd.RVD$class.take(RVD.scala:247); 	at is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); 	at is.hail.table.Table.take(Table.scala:990); 	at is.hail.table.Table.showString(Table.scala:1031); 	at sun.reflect.NativeMethodAc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:4749,Energy Efficiency,schedul,scheduler,4749,.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:456); 	at is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:433); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:285); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:21); 	at is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:243); 	at is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); 	at is.hail.rvd.RVD$class.take(RVD.scala:247); 	at is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); 	at is.hail.table.Table.take(Table.scala:990); 	at is.hail.table.Table.showString(Table.scala:1031); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.in,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:7412,Energy Efficiency,schedul,scheduler,7412,oke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Hail version: devel-9a5678f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:7484,Energy Efficiency,schedul,scheduler,7484,oke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Hail version: devel-9a5678f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:3233,Performance,concurren,concurrent,3233,teratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:3318,Performance,concurren,concurrent,3318,2.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:7609,Performance,concurren,concurrent,7609,oke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Hail version: devel-9a5678f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:7694,Performance,concurren,concurrent,7694,oke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Hail version: devel-9a5678f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:1740,Safety,abort,aborted,1740,"ile ""<ipython-input-9-304965820738>"", line 1, in <module>; x.key_by('y').show(); File ""<decorator-gen-598>"", line 2, in show; File ""/Users/konradk/Dropbox/src/python/hail/typecheck/check.py"", line 486, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1101, in show; print(self._show(n,width, truncate, types)); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1104, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/konradk/programs/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/Dropbox/src/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 49, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:3644,Safety,abort,abortStage,3644,il.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:3742,Safety,abort,abortStage,3742,pply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:3987,Safety,abort,abortStage,3987,xt.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:1650,Testability,Assert,AssertionError,1650,".py"", line 2963, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-9-304965820738>"", line 1, in <module>; x.key_by('y').show(); File ""<decorator-gen-598>"", line 2, in show; File ""/Users/konradk/Dropbox/src/python/hail/typecheck/check.py"", line 486, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1101, in show; print(self._show(n,width, truncate, types)); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1104, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/konradk/programs/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/Dropbox/src/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 49, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:1666,Testability,assert,assertion,1666,".py"", line 2963, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-9-304965820738>"", line 1, in <module>; x.key_by('y').show(); File ""<decorator-gen-598>"", line 2, in show; File ""/Users/konradk/Dropbox/src/python/hail/typecheck/check.py"", line 486, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1101, in show; print(self._show(n,width, truncate, types)); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1104, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/konradk/programs/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/Dropbox/src/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 49, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:1902,Testability,Assert,AssertionError,1902,"/typecheck/check.py"", line 486, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1101, in show; print(self._show(n,width, truncate, types)); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1104, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/konradk/programs/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/Dropbox/src/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 49, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:1918,Testability,assert,assertion,1918,"/typecheck/check.py"", line 486, in _typecheck; return __orig_func__(*args_, **kwargs_); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1101, in show; print(self._show(n,width, truncate, types)); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1104, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/konradk/programs/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/Dropbox/src/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 49, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:1954,Testability,assert,assert,1954,"ypecheck; return __orig_func__(*args_, **kwargs_); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1101, in show; print(self._show(n,width, truncate, types)); File ""/Users/konradk/Dropbox/src/python/hail/table.py"", line 1104, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/konradk/programs/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/konradk/Dropbox/src/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 49, localhost, executor driver): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkCont",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:6278,Testability,Assert,AssertionError,6278,is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); 	at is.hail.rvd.RVD$class.take(RVD.scala:247); 	at is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); 	at is.hail.table.Table.take(Table.scala:990); 	at is.hail.table.Table.showString(Table.scala:1031); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:6294,Testability,assert,assertion,6294,is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:21); 	at is.hail.rvd.RVD$class.take(RVD.scala:247); 	at is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); 	at is.hail.table.Table.take(Table.scala:990); 	at is.hail.table.Table.showString(Table.scala:1031); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:6330,Testability,assert,assert,6330,deredRVD.scala:21); 	at is.hail.rvd.RVD$class.take(RVD.scala:247); 	at is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:21); 	at is.hail.table.Table.take(Table.scala:990); 	at is.hail.table.Table.showString(Table.scala:1031); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkCont,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:7852,Testability,Assert,AssertionError,7852,oke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Hail version: devel-9a5678f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/issues/3516:7868,Testability,assert,assertion,7868,oke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:349); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:433); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1$$anonfun$apply$34.apply(ContextRDD.scala:458); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:458); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Hail version: devel-9a5678f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3516
https://github.com/hail-is/hail/pull/3518:149,Testability,log,log-of-breaking-changes-in-,149,"edit ld_prune to take a CallExpression instead of a matrix table, as well as some doc edits. will post to this [dev thread](http://discuss.hail.is/t/log-of-breaking-changes-in-0-2-beta/454/3) when it goes through",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3518
https://github.com/hail-is/hail/pull/3519:137,Energy Efficiency,efficient,efficient,137,"This is to make it easier to port things to the FunctionRegistry. This involves serializing the type, so it's not going to be especially efficient on most arbitrary objects, so i pulled strings out separately. It also doesn't deal with returning things that are arrays or structs or anything yet. (mostly just strings). The type serializing is also included in this PR. I moved the StringFunctions to use this so it should be moderately tested by the string function tests in python. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3519
https://github.com/hail-is/hail/pull/3519:437,Testability,test,tested,437,"This is to make it easier to port things to the FunctionRegistry. This involves serializing the type, so it's not going to be especially efficient on most arbitrary objects, so i pulled strings out separately. It also doesn't deal with returning things that are arrays or structs or anything yet. (mostly just strings). The type serializing is also included in this PR. I moved the StringFunctions to use this so it should be moderately tested by the string function tests in python. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3519
https://github.com/hail-is/hail/pull/3519:467,Testability,test,tests,467,"This is to make it easier to port things to the FunctionRegistry. This involves serializing the type, so it's not going to be especially efficient on most arbitrary objects, so i pulled strings out separately. It also doesn't deal with returning things that are arrays or structs or anything yet. (mostly just strings). The type serializing is also included in this PR. I moved the StringFunctions to use this so it should be moderately tested by the string function tests in python. cc @tpoterba",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3519
https://github.com/hail-is/hail/pull/3523:24,Safety,avoid,avoid,24,Use in MatrixMapRows to avoid region value allocations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3523
https://github.com/hail-is/hail/pull/3528:98,Security,expose,expose,98,"I did this and then realized unfilterEntries is only used in a test. @tpoterba, did you intend to expose this in Python?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3528
https://github.com/hail-is/hail/pull/3528:63,Testability,test,test,63,"I did this and then realized unfilterEntries is only used in a test. @tpoterba, did you intend to expose this in Python?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3528
https://github.com/hail-is/hail/pull/3531:38,Performance,optimiz,optimization,38,Correct import_bgen docs based on the optimization I made last week (empty `entry_fields` takes a fast path that doesn't parse the genotypes),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3531
https://github.com/hail-is/hail/pull/3535:162,Integrability,interface,interface,162,"Currently, the boolean versions aren't being used because there's no equivalent in the function registry. I removed the int and float versions because the Python interface didn't use them. Happy for feedback on what we want to expose. If we want to support booleans now, I'll add new Aggregators that work on annotations (not RV).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3535
https://github.com/hail-is/hail/pull/3535:227,Security,expose,expose,227,"Currently, the boolean versions aren't being used because there's no equivalent in the function registry. I removed the int and float versions because the Python interface didn't use them. Happy for feedback on what we want to expose. If we want to support booleans now, I'll add new Aggregators that work on annotations (not RV).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3535
https://github.com/hail-is/hail/pull/3535:199,Usability,feedback,feedback,199,"Currently, the boolean versions aren't being used because there's no equivalent in the function registry. I removed the int and float versions because the Python interface didn't use them. Happy for feedback on what we want to expose. If we want to support booleans now, I'll add new Aggregators that work on annotations (not RV).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3535
https://github.com/hail-is/hail/pull/3537:61,Testability,test,test,61,"I created and exported two genomicsdb shards using GATK. The test compares genomicsdb import to importing the exported VCFs from GATK (modulo some minor differences). This PR includes an 18MB compressed FASTA file for h38 chr20 (import_genomicsdb needs a FASTA file). Github has a 100MB file size limit. This seems fine, but if there is an objection we can switch to using git-lfs: https://git-lfs.github.com/. @lfrancioli I will test against gnomADv3 test data next and post some timings.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3537
https://github.com/hail-is/hail/pull/3537:430,Testability,test,test,430,"I created and exported two genomicsdb shards using GATK. The test compares genomicsdb import to importing the exported VCFs from GATK (modulo some minor differences). This PR includes an 18MB compressed FASTA file for h38 chr20 (import_genomicsdb needs a FASTA file). Github has a 100MB file size limit. This seems fine, but if there is an objection we can switch to using git-lfs: https://git-lfs.github.com/. @lfrancioli I will test against gnomADv3 test data next and post some timings.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3537
https://github.com/hail-is/hail/pull/3537:452,Testability,test,test,452,"I created and exported two genomicsdb shards using GATK. The test compares genomicsdb import to importing the exported VCFs from GATK (modulo some minor differences). This PR includes an 18MB compressed FASTA file for h38 chr20 (import_genomicsdb needs a FASTA file). Github has a 100MB file size limit. This seems fine, but if there is an objection we can switch to using git-lfs: https://git-lfs.github.com/. @lfrancioli I will test against gnomADv3 test data next and post some timings.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3537
https://github.com/hail-is/hail/pull/3538:43,Integrability,wrap,wrapping,43,"@cseed @tpoterba . this was my solution to wrapping the `registerIR` calls to the function registry as one unit. The `fn` tag at the beginning is more or less just a tag on what the function name was. If this looks ok, maybe we can just expose the ability to create and register these from python (maybe with uids as function names for distinctness) to wrap things like `alt_allele` and other things where the IR gets big and explody.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3538
https://github.com/hail-is/hail/pull/3538:353,Integrability,wrap,wrap,353,"@cseed @tpoterba . this was my solution to wrapping the `registerIR` calls to the function registry as one unit. The `fn` tag at the beginning is more or less just a tag on what the function name was. If this looks ok, maybe we can just expose the ability to create and register these from python (maybe with uids as function names for distinctness) to wrap things like `alt_allele` and other things where the IR gets big and explody.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3538
https://github.com/hail-is/hail/pull/3538:237,Security,expose,expose,237,"@cseed @tpoterba . this was my solution to wrapping the `registerIR` calls to the function registry as one unit. The `fn` tag at the beginning is more or less just a tag on what the function name was. If this looks ok, maybe we can just expose the ability to create and register these from python (maybe with uids as function names for distinctness) to wrap things like `alt_allele` and other things where the IR gets big and explody.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3538
https://github.com/hail-is/hail/pull/3539:1001,Performance,optimiz,optimize,1001,"Adds BlockMatrix sparsify functions for:; - band matrix; - upper/lower triangle (special case of band); - a collection of rectangles. For diagonal band, I switched GridPartitioner.filterBand to go from lower to upper diagonal index, rather than taking a lower and upper bandwidth. This is more general, e.g. the diagonal itself need not be in the band. Band and triangle zero out elements in partially overlapping blocks by default. Rectangles currently only supports dropping whole blocks. Also adds `export_rectangles` for exporting rectangular regions to TSV in parallel.; I use parameters `path_in` and `path_out`, and switched `BlockMatrix.export` to this convention as well from `input` and `output` to avoid using the reserved word `input`. I have not exposed export methods directly on BlockMatrix for now as it'd be very easy for users to needlessly write and read an already written BlockMatrix. I could add these in a later PR with a warning, or we can wait until we've moved to IR and can optimize read followed by export to export on the file. It'd also be good to add compression options (and float formatting options to `export` and `export_rectanges`). Along the way I fixed NaN checking (due to Double.NaN != Double.NaN) on scalar and vector `/` sparse block ops and added NaN and Infinity checking to scalar and vector `*` sparse ops. Together with `sparsify_row_intervals` in the first sparse matrix PR, this PR exposes all the BlockMatrix functionality needed for big LD applications of Kate/Ran and Jacob/Masa.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3539
https://github.com/hail-is/hail/pull/3539:709,Safety,avoid,avoid,709,"Adds BlockMatrix sparsify functions for:; - band matrix; - upper/lower triangle (special case of band); - a collection of rectangles. For diagonal band, I switched GridPartitioner.filterBand to go from lower to upper diagonal index, rather than taking a lower and upper bandwidth. This is more general, e.g. the diagonal itself need not be in the band. Band and triangle zero out elements in partially overlapping blocks by default. Rectangles currently only supports dropping whole blocks. Also adds `export_rectangles` for exporting rectangular regions to TSV in parallel.; I use parameters `path_in` and `path_out`, and switched `BlockMatrix.export` to this convention as well from `input` and `output` to avoid using the reserved word `input`. I have not exposed export methods directly on BlockMatrix for now as it'd be very easy for users to needlessly write and read an already written BlockMatrix. I could add these in a later PR with a warning, or we can wait until we've moved to IR and can optimize read followed by export to export on the file. It'd also be good to add compression options (and float formatting options to `export` and `export_rectanges`). Along the way I fixed NaN checking (due to Double.NaN != Double.NaN) on scalar and vector `/` sparse block ops and added NaN and Infinity checking to scalar and vector `*` sparse ops. Together with `sparsify_row_intervals` in the first sparse matrix PR, this PR exposes all the BlockMatrix functionality needed for big LD applications of Kate/Ran and Jacob/Masa.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3539
https://github.com/hail-is/hail/pull/3539:759,Security,expose,exposed,759,"Adds BlockMatrix sparsify functions for:; - band matrix; - upper/lower triangle (special case of band); - a collection of rectangles. For diagonal band, I switched GridPartitioner.filterBand to go from lower to upper diagonal index, rather than taking a lower and upper bandwidth. This is more general, e.g. the diagonal itself need not be in the band. Band and triangle zero out elements in partially overlapping blocks by default. Rectangles currently only supports dropping whole blocks. Also adds `export_rectangles` for exporting rectangular regions to TSV in parallel.; I use parameters `path_in` and `path_out`, and switched `BlockMatrix.export` to this convention as well from `input` and `output` to avoid using the reserved word `input`. I have not exposed export methods directly on BlockMatrix for now as it'd be very easy for users to needlessly write and read an already written BlockMatrix. I could add these in a later PR with a warning, or we can wait until we've moved to IR and can optimize read followed by export to export on the file. It'd also be good to add compression options (and float formatting options to `export` and `export_rectanges`). Along the way I fixed NaN checking (due to Double.NaN != Double.NaN) on scalar and vector `/` sparse block ops and added NaN and Infinity checking to scalar and vector `*` sparse ops. Together with `sparsify_row_intervals` in the first sparse matrix PR, this PR exposes all the BlockMatrix functionality needed for big LD applications of Kate/Ran and Jacob/Masa.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3539
https://github.com/hail-is/hail/pull/3539:1431,Security,expose,exposes,1431,"Adds BlockMatrix sparsify functions for:; - band matrix; - upper/lower triangle (special case of band); - a collection of rectangles. For diagonal band, I switched GridPartitioner.filterBand to go from lower to upper diagonal index, rather than taking a lower and upper bandwidth. This is more general, e.g. the diagonal itself need not be in the band. Band and triangle zero out elements in partially overlapping blocks by default. Rectangles currently only supports dropping whole blocks. Also adds `export_rectangles` for exporting rectangular regions to TSV in parallel.; I use parameters `path_in` and `path_out`, and switched `BlockMatrix.export` to this convention as well from `input` and `output` to avoid using the reserved word `input`. I have not exposed export methods directly on BlockMatrix for now as it'd be very easy for users to needlessly write and read an already written BlockMatrix. I could add these in a later PR with a warning, or we can wait until we've moved to IR and can optimize read followed by export to export on the file. It'd also be good to add compression options (and float formatting options to `export` and `export_rectanges`). Along the way I fixed NaN checking (due to Double.NaN != Double.NaN) on scalar and vector `/` sparse block ops and added NaN and Infinity checking to scalar and vector `*` sparse ops. Together with `sparsify_row_intervals` in the first sparse matrix PR, this PR exposes all the BlockMatrix functionality needed for big LD applications of Kate/Ran and Jacob/Masa.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3539
https://github.com/hail-is/hail/pull/3542:86,Testability,test,tested,86,cc @jigold @patrick-schultz . this is what `Locus.contig()` looks like now. I haven't tested it yet; will go though and make sure it does the right thing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3542
https://github.com/hail-is/hail/pull/3545:300,Availability,error,errors,300,"Example:. ```; 2018-05-09 17:30:41 root: WARN: [is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176) found no AST to IR conversion for:; Apply[annotate](; SymRef[va]; StructConstructor(; Select[toFloat64](; Select[position](; Select[locus](; SymRef[va]; ); ); ); ); ). due to the following errors:; locus<GRCh37> should be a subtype of TStruct, Select[locus](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176); is.hail.testUtils.RichMatrixTable.annotateRowsExpr(RichMatrixTable.scala:57); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:107); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); ```. With multiple failures:. ```; 2018-05-09 17:43:03 root: WARN: [is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810) found no AST to IR conversion for:; StructConstructor(; ApplyMethod[sum](; ApplyMethod[map](; SymRef[AGG]; Lambda[g](; Apply[*](; Select[weight](; SymRef[va]; ); Select[toFloat64](; ApplyMethod[nNonRefAlleles](; Select[GT](; SymRef[g]; ); ); ); ); ); ); ); ). due to the following errors:; float64 should be a subtype of TStruct, Select[weight](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); call should be a subtype of TStruct, Select[GT](SymRef[g] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:113); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3545
https://github.com/hail-is/hail/pull/3545:821,Availability,failure,failures,821,"Example:. ```; 2018-05-09 17:30:41 root: WARN: [is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176) found no AST to IR conversion for:; Apply[annotate](; SymRef[va]; StructConstructor(; Select[toFloat64](; Select[position](; Select[locus](; SymRef[va]; ); ); ); ); ). due to the following errors:; locus<GRCh37> should be a subtype of TStruct, Select[locus](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176); is.hail.testUtils.RichMatrixTable.annotateRowsExpr(RichMatrixTable.scala:57); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:107); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); ```. With multiple failures:. ```; 2018-05-09 17:43:03 root: WARN: [is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810) found no AST to IR conversion for:; StructConstructor(; ApplyMethod[sum](; ApplyMethod[map](; SymRef[AGG]; Lambda[g](; Apply[*](; Select[weight](; SymRef[va]; ); Select[toFloat64](; ApplyMethod[nNonRefAlleles](; Select[GT](; SymRef[g]; ); ); ); ); ); ); ); ). due to the following errors:; float64 should be a subtype of TStruct, Select[weight](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); call should be a subtype of TStruct, Select[GT](SymRef[g] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:113); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3545
https://github.com/hail-is/hail/pull/3545:1221,Availability,error,errors,1221,"Example:. ```; 2018-05-09 17:30:41 root: WARN: [is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176) found no AST to IR conversion for:; Apply[annotate](; SymRef[va]; StructConstructor(; Select[toFloat64](; Select[position](; Select[locus](; SymRef[va]; ); ); ); ); ). due to the following errors:; locus<GRCh37> should be a subtype of TStruct, Select[locus](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176); is.hail.testUtils.RichMatrixTable.annotateRowsExpr(RichMatrixTable.scala:57); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:107); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); ```. With multiple failures:. ```; 2018-05-09 17:43:03 root: WARN: [is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810) found no AST to IR conversion for:; StructConstructor(; ApplyMethod[sum](; ApplyMethod[map](; SymRef[AGG]; Lambda[g](; Apply[*](; Select[weight](; SymRef[va]; ); Select[toFloat64](; ApplyMethod[nNonRefAlleles](; Select[GT](; SymRef[g]; ); ); ); ); ); ); ); ). due to the following errors:; float64 should be a subtype of TStruct, Select[weight](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); call should be a subtype of TStruct, Select[GT](SymRef[g] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:113); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3545
https://github.com/hail-is/hail/pull/3545:517,Testability,test,testUtils,517,"Example:. ```; 2018-05-09 17:30:41 root: WARN: [is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176) found no AST to IR conversion for:; Apply[annotate](; SymRef[va]; StructConstructor(; Select[toFloat64](; Select[position](; Select[locus](; SymRef[va]; ); ); ); ); ). due to the following errors:; locus<GRCh37> should be a subtype of TStruct, Select[locus](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176); is.hail.testUtils.RichMatrixTable.annotateRowsExpr(RichMatrixTable.scala:57); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:107); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); ```. With multiple failures:. ```; 2018-05-09 17:43:03 root: WARN: [is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810) found no AST to IR conversion for:; StructConstructor(; ApplyMethod[sum](; ApplyMethod[map](; SymRef[AGG]; Lambda[g](; Apply[*](; Select[weight](; SymRef[va]; ); Select[toFloat64](; ApplyMethod[nNonRefAlleles](; Select[GT](; SymRef[g]; ); ); ); ); ); ); ); ). due to the following errors:; float64 should be a subtype of TStruct, Select[weight](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); call should be a subtype of TStruct, Select[GT](SymRef[g] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:113); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3545
https://github.com/hail-is/hail/pull/3545:620,Testability,test,testLinregBurden,620,"Example:. ```; 2018-05-09 17:30:41 root: WARN: [is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176) found no AST to IR conversion for:; Apply[annotate](; SymRef[va]; StructConstructor(; Select[toFloat64](; Select[position](; Select[locus](; SymRef[va]; ); ); ); ); ). due to the following errors:; locus<GRCh37> should be a subtype of TStruct, Select[locus](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176); is.hail.testUtils.RichMatrixTable.annotateRowsExpr(RichMatrixTable.scala:57); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:107); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); ```. With multiple failures:. ```; 2018-05-09 17:43:03 root: WARN: [is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810) found no AST to IR conversion for:; StructConstructor(; ApplyMethod[sum](; ApplyMethod[map](; SymRef[AGG]; Lambda[g](; Apply[*](; Select[weight](; SymRef[va]; ); Select[toFloat64](; ApplyMethod[nNonRefAlleles](; Select[GT](; SymRef[g]; ); ); ); ); ); ); ); ). due to the following errors:; float64 should be a subtype of TStruct, Select[weight](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); call should be a subtype of TStruct, Select[GT](SymRef[g] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:113); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3545
https://github.com/hail-is/hail/pull/3545:1584,Testability,test,testLinregBurden,1584,"Example:. ```; 2018-05-09 17:30:41 root: WARN: [is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176) found no AST to IR conversion for:; Apply[annotate](; SymRef[va]; StructConstructor(; Select[toFloat64](; Select[position](; Select[locus](; SymRef[va]; ); ); ); ); ). due to the following errors:; locus<GRCh37> should be a subtype of TStruct, Select[locus](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1176); is.hail.testUtils.RichMatrixTable.annotateRowsExpr(RichMatrixTable.scala:57); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:107); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); ```. With multiple failures:. ```; 2018-05-09 17:43:03 root: WARN: [is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810) found no AST to IR conversion for:; StructConstructor(; ApplyMethod[sum](; ApplyMethod[map](; SymRef[AGG]; Lambda[g](; Apply[*](; Select[weight](; SymRef[va]; ); Select[toFloat64](; ApplyMethod[nNonRefAlleles](; Select[GT](; SymRef[g]; ); ); ); ); ); ); ); ). due to the following errors:; float64 should be a subtype of TStruct, Select[weight](SymRef[va] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); call should be a subtype of TStruct, Select[GT](SymRef[g] ), is.hail.expr.Select$$anonfun$toIR$1.apply(AST.scala:371); in; is.hail.variant.MatrixTable.aggregateRowsByKey(MatrixTable.scala:810); is.hail.variant.vsm.GroupBySuite.testLinregBurden(GroupBySuite.scala:113); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3545
https://github.com/hail-is/hail/pull/3546:211,Testability,test,tests,211,"Created a Relational IR for `mt.group_by_rows(...).aggregate(...)`. I've never done this before so I just copied and massaged `MatrixMapRows`. It seems a lot longer than I anticipated. Two of the `GroupBySuite` tests exercise this path. The other two use non-IR features. When more things are put into the expr IR, they'll start exercising this path too.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3546
https://github.com/hail-is/hail/pull/3552:165,Testability,log,log,165,Added the straightforward implementations to get them in the IRFunctionRegistry. They are not optimal from the computational complexity perspective (e.g. union is n log m instead of n + m). I will improve them in a separate PR.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3552
https://github.com/hail-is/hail/pull/3554:523,Modifiability,extend,extend,523,"I was assigned:; ```; size(dict<T, U>): int32; size(set<T>): int32; isEmpty(dict<T, U>): bool; isEmpty(set<T>): bool; isEmpty(array<T>): bool; head(set<T>): T; head(array<T>): T; tail(set<T>): set<T>; tail(array<T>): array<T>; sum(set<tnum>): tnum; product(set<tnum>): tnum; map(set<T>,(T) => U): set<U>; exists(set<T>,(T) => bool): bool; forall(set<T>,(T) => bool): bool; filter(set<T>,(T) => bool): set<T>; length(array<T>): int32; sort(array<T>,bool): array<T>; sort(array<T>): array<str>; append(array<T>,T): array<T>; extend(array<T>,array<T>): array<T>; ```; I didn't do `head` or `tail`, because they weren't being used. I'm not sure how/where to test the new functions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3554
https://github.com/hail-is/hail/pull/3554:654,Testability,test,test,654,"I was assigned:; ```; size(dict<T, U>): int32; size(set<T>): int32; isEmpty(dict<T, U>): bool; isEmpty(set<T>): bool; isEmpty(array<T>): bool; head(set<T>): T; head(array<T>): T; tail(set<T>): set<T>; tail(array<T>): array<T>; sum(set<tnum>): tnum; product(set<tnum>): tnum; map(set<T>,(T) => U): set<U>; exists(set<T>,(T) => bool): bool; forall(set<T>,(T) => bool): bool; filter(set<T>,(T) => bool): set<T>; length(array<T>): int32; sort(array<T>,bool): array<T>; sort(array<T>): array<str>; append(array<T>,T): array<T>; extend(array<T>,array<T>): array<T>; ```; I didn't do `head` or `tail`, because they weren't being used. I'm not sure how/where to test the new functions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3554
https://github.com/hail-is/hail/pull/3556:98,Testability,test,tests,98,"cc @jigold I'm pretty sure this works, but I'd appreciate a look just in case. . There's a set of tests in ReferenceGenomeSuite.testConstructors to test the functions, including one with a fake reference. I didn't add any more tests because those are now running through IR (I checked). But I can move the tests to python if desired.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3556
https://github.com/hail-is/hail/pull/3556:128,Testability,test,testConstructors,128,"cc @jigold I'm pretty sure this works, but I'd appreciate a look just in case. . There's a set of tests in ReferenceGenomeSuite.testConstructors to test the functions, including one with a fake reference. I didn't add any more tests because those are now running through IR (I checked). But I can move the tests to python if desired.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3556
https://github.com/hail-is/hail/pull/3556:148,Testability,test,test,148,"cc @jigold I'm pretty sure this works, but I'd appreciate a look just in case. . There's a set of tests in ReferenceGenomeSuite.testConstructors to test the functions, including one with a fake reference. I didn't add any more tests because those are now running through IR (I checked). But I can move the tests to python if desired.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3556
https://github.com/hail-is/hail/pull/3556:227,Testability,test,tests,227,"cc @jigold I'm pretty sure this works, but I'd appreciate a look just in case. . There's a set of tests in ReferenceGenomeSuite.testConstructors to test the functions, including one with a fake reference. I didn't add any more tests because those are now running through IR (I checked). But I can move the tests to python if desired.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3556
https://github.com/hail-is/hail/pull/3556:306,Testability,test,tests,306,"cc @jigold I'm pretty sure this works, but I'd appreciate a look just in case. . There's a set of tests in ReferenceGenomeSuite.testConstructors to test the functions, including one with a fake reference. I didn't add any more tests because those are now running through IR (I checked). But I can move the tests to python if desired.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3556
https://github.com/hail-is/hail/pull/3557:80,Testability,test,tests,80,"@danking this is where I was at. I think I got most of the stuff that the scala tests caught, not sure about python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3557
https://github.com/hail-is/hail/pull/3565:21,Performance,optimiz,optimization,21,I also added a small optimization that doesn't copy the column annotations into the row unless we're aggregating. I did this for both `filter_rows` and `annotate_rows`. cc: @cseed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3565
https://github.com/hail-is/hail/pull/3570:1101,Performance,optimiz,optimization,1101,"tors here: http://dev.hail.is/t/proposal-for-aggregators/93/3. Builds on: https://github.com/hail-is/hail/pull/3552. I still need to finish converting some tests from ExtractAggregatorSuite (they are currently commented out). Old-style aggregators (filter, map, flatMap) are expanded at toIR conversion time to ApplyAggOp and SeqOp. ApplyAggOp returns the result of the aggregation. ApplyAggOp's first argument, which must be of type TVoid, is the expression to run for each element being aggregated over. SeqOp merges a computed value into the RegionValueAggregator. I added a AggSignature that holds the AggOp, type being aggregated over and the RegionValueAggregator constructor argument types. This is stored by the ApplyAggOp and the SeqOp. @tpoterba I believe TAggregable is no longer used in the IR code and can go away when the AST gets ripped out. @danking I added some IR testing logic to TestUtils. Namely, `eval` evaluates an IR with environments, args and/or aggregations with a single call (and verifies that the interpret with and without optimization and compiler all agree). There are also functions for asserting the result of aggregations, for example:. ```; val aggSig = AggSignature(Sum(), TFloat64(), FastSeq()); assertEvalsTo(ApplyAggOp(; SeqOp(ApplyBinaryPrimOp(Multiply(), Ref(""a"", TFloat64()), Ref(""b"", TFloat64())), I32(0), aggSig),; FastSeq(), aggSig),; (FastIndexedSeq(Row(1.0, 10.0), Row(10.0, 10.0), Row(null, 10.0)), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),; 110.0); ```. The line:. > (FastIndexedSeq(Row(1.0, 10.0), ...), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),. is the IndexedSeq of values to aggregate over, along with their signature. The struct type is used to build the scope in which aggregators are evaluated. (A little noisy because of the aggregator syntax. It's noisier than I'd like it to be.). This nicely paves the way for aggregators with multi-argument seqOps (like takeBy) that were previously handled by lambdas and we didn't have a cl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3570
https://github.com/hail-is/hail/pull/3570:203,Testability,test,tests,203,"This implements my proposal to simplify aggregators here: http://dev.hail.is/t/proposal-for-aggregators/93/3. Builds on: https://github.com/hail-is/hail/pull/3552. I still need to finish converting some tests from ExtractAggregatorSuite (they are currently commented out). Old-style aggregators (filter, map, flatMap) are expanded at toIR conversion time to ApplyAggOp and SeqOp. ApplyAggOp returns the result of the aggregation. ApplyAggOp's first argument, which must be of type TVoid, is the expression to run for each element being aggregated over. SeqOp merges a computed value into the RegionValueAggregator. I added a AggSignature that holds the AggOp, type being aggregated over and the RegionValueAggregator constructor argument types. This is stored by the ApplyAggOp and the SeqOp. @tpoterba I believe TAggregable is no longer used in the IR code and can go away when the AST gets ripped out. @danking I added some IR testing logic to TestUtils. Namely, `eval` evaluates an IR with environments, args and/or aggregations with a single call (and verifies that the interpret with and without optimization and compiler all agree). There are also functions for asserting the result of aggregations, for example:. ```; val aggSig = AggSignature(Sum(), TFloat64(), FastSeq()); assertEvalsTo(ApplyAggOp(; SeqOp(ApplyBinaryPrimOp(Multiply(), Ref(""a"", TFloat64()), Ref(""b"", TFloat64())), I32(0), aggSig),; FastSeq(), aggSig),; (FastIndexedSeq(Row(1.0, 10.0), Row(10.0, 10.0), Row(null, 10.0)), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),; 110.0); ```. The line:. > (FastIndexedSeq(Row(1.0, 10.0), ...), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),. is the IndexedSeq of values to aggregate over, along with their signature. The struct type is used to build the scope in which aggregators are evaluated. (A little noisy because of the aggregator syntax. It's noisier than I'd like it to be.). This nicely paves the way for aggregators with multi-argument seqOps (like takeBy) that were previou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3570
https://github.com/hail-is/hail/pull/3570:929,Testability,test,testing,929,"This implements my proposal to simplify aggregators here: http://dev.hail.is/t/proposal-for-aggregators/93/3. Builds on: https://github.com/hail-is/hail/pull/3552. I still need to finish converting some tests from ExtractAggregatorSuite (they are currently commented out). Old-style aggregators (filter, map, flatMap) are expanded at toIR conversion time to ApplyAggOp and SeqOp. ApplyAggOp returns the result of the aggregation. ApplyAggOp's first argument, which must be of type TVoid, is the expression to run for each element being aggregated over. SeqOp merges a computed value into the RegionValueAggregator. I added a AggSignature that holds the AggOp, type being aggregated over and the RegionValueAggregator constructor argument types. This is stored by the ApplyAggOp and the SeqOp. @tpoterba I believe TAggregable is no longer used in the IR code and can go away when the AST gets ripped out. @danking I added some IR testing logic to TestUtils. Namely, `eval` evaluates an IR with environments, args and/or aggregations with a single call (and verifies that the interpret with and without optimization and compiler all agree). There are also functions for asserting the result of aggregations, for example:. ```; val aggSig = AggSignature(Sum(), TFloat64(), FastSeq()); assertEvalsTo(ApplyAggOp(; SeqOp(ApplyBinaryPrimOp(Multiply(), Ref(""a"", TFloat64()), Ref(""b"", TFloat64())), I32(0), aggSig),; FastSeq(), aggSig),; (FastIndexedSeq(Row(1.0, 10.0), Row(10.0, 10.0), Row(null, 10.0)), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),; 110.0); ```. The line:. > (FastIndexedSeq(Row(1.0, 10.0), ...), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),. is the IndexedSeq of values to aggregate over, along with their signature. The struct type is used to build the scope in which aggregators are evaluated. (A little noisy because of the aggregator syntax. It's noisier than I'd like it to be.). This nicely paves the way for aggregators with multi-argument seqOps (like takeBy) that were previou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3570
https://github.com/hail-is/hail/pull/3570:937,Testability,log,logic,937,"This implements my proposal to simplify aggregators here: http://dev.hail.is/t/proposal-for-aggregators/93/3. Builds on: https://github.com/hail-is/hail/pull/3552. I still need to finish converting some tests from ExtractAggregatorSuite (they are currently commented out). Old-style aggregators (filter, map, flatMap) are expanded at toIR conversion time to ApplyAggOp and SeqOp. ApplyAggOp returns the result of the aggregation. ApplyAggOp's first argument, which must be of type TVoid, is the expression to run for each element being aggregated over. SeqOp merges a computed value into the RegionValueAggregator. I added a AggSignature that holds the AggOp, type being aggregated over and the RegionValueAggregator constructor argument types. This is stored by the ApplyAggOp and the SeqOp. @tpoterba I believe TAggregable is no longer used in the IR code and can go away when the AST gets ripped out. @danking I added some IR testing logic to TestUtils. Namely, `eval` evaluates an IR with environments, args and/or aggregations with a single call (and verifies that the interpret with and without optimization and compiler all agree). There are also functions for asserting the result of aggregations, for example:. ```; val aggSig = AggSignature(Sum(), TFloat64(), FastSeq()); assertEvalsTo(ApplyAggOp(; SeqOp(ApplyBinaryPrimOp(Multiply(), Ref(""a"", TFloat64()), Ref(""b"", TFloat64())), I32(0), aggSig),; FastSeq(), aggSig),; (FastIndexedSeq(Row(1.0, 10.0), Row(10.0, 10.0), Row(null, 10.0)), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),; 110.0); ```. The line:. > (FastIndexedSeq(Row(1.0, 10.0), ...), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),. is the IndexedSeq of values to aggregate over, along with their signature. The struct type is used to build the scope in which aggregators are evaluated. (A little noisy because of the aggregator syntax. It's noisier than I'd like it to be.). This nicely paves the way for aggregators with multi-argument seqOps (like takeBy) that were previou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3570
https://github.com/hail-is/hail/pull/3570:946,Testability,Test,TestUtils,946,"This implements my proposal to simplify aggregators here: http://dev.hail.is/t/proposal-for-aggregators/93/3. Builds on: https://github.com/hail-is/hail/pull/3552. I still need to finish converting some tests from ExtractAggregatorSuite (they are currently commented out). Old-style aggregators (filter, map, flatMap) are expanded at toIR conversion time to ApplyAggOp and SeqOp. ApplyAggOp returns the result of the aggregation. ApplyAggOp's first argument, which must be of type TVoid, is the expression to run for each element being aggregated over. SeqOp merges a computed value into the RegionValueAggregator. I added a AggSignature that holds the AggOp, type being aggregated over and the RegionValueAggregator constructor argument types. This is stored by the ApplyAggOp and the SeqOp. @tpoterba I believe TAggregable is no longer used in the IR code and can go away when the AST gets ripped out. @danking I added some IR testing logic to TestUtils. Namely, `eval` evaluates an IR with environments, args and/or aggregations with a single call (and verifies that the interpret with and without optimization and compiler all agree). There are also functions for asserting the result of aggregations, for example:. ```; val aggSig = AggSignature(Sum(), TFloat64(), FastSeq()); assertEvalsTo(ApplyAggOp(; SeqOp(ApplyBinaryPrimOp(Multiply(), Ref(""a"", TFloat64()), Ref(""b"", TFloat64())), I32(0), aggSig),; FastSeq(), aggSig),; (FastIndexedSeq(Row(1.0, 10.0), Row(10.0, 10.0), Row(null, 10.0)), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),; 110.0); ```. The line:. > (FastIndexedSeq(Row(1.0, 10.0), ...), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),. is the IndexedSeq of values to aggregate over, along with their signature. The struct type is used to build the scope in which aggregators are evaluated. (A little noisy because of the aggregator syntax. It's noisier than I'd like it to be.). This nicely paves the way for aggregators with multi-argument seqOps (like takeBy) that were previou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3570
https://github.com/hail-is/hail/pull/3570:1168,Testability,assert,asserting,1168,"tp://dev.hail.is/t/proposal-for-aggregators/93/3. Builds on: https://github.com/hail-is/hail/pull/3552. I still need to finish converting some tests from ExtractAggregatorSuite (they are currently commented out). Old-style aggregators (filter, map, flatMap) are expanded at toIR conversion time to ApplyAggOp and SeqOp. ApplyAggOp returns the result of the aggregation. ApplyAggOp's first argument, which must be of type TVoid, is the expression to run for each element being aggregated over. SeqOp merges a computed value into the RegionValueAggregator. I added a AggSignature that holds the AggOp, type being aggregated over and the RegionValueAggregator constructor argument types. This is stored by the ApplyAggOp and the SeqOp. @tpoterba I believe TAggregable is no longer used in the IR code and can go away when the AST gets ripped out. @danking I added some IR testing logic to TestUtils. Namely, `eval` evaluates an IR with environments, args and/or aggregations with a single call (and verifies that the interpret with and without optimization and compiler all agree). There are also functions for asserting the result of aggregations, for example:. ```; val aggSig = AggSignature(Sum(), TFloat64(), FastSeq()); assertEvalsTo(ApplyAggOp(; SeqOp(ApplyBinaryPrimOp(Multiply(), Ref(""a"", TFloat64()), Ref(""b"", TFloat64())), I32(0), aggSig),; FastSeq(), aggSig),; (FastIndexedSeq(Row(1.0, 10.0), Row(10.0, 10.0), Row(null, 10.0)), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),; 110.0); ```. The line:. > (FastIndexedSeq(Row(1.0, 10.0), ...), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),. is the IndexedSeq of values to aggregate over, along with their signature. The struct type is used to build the scope in which aggregators are evaluated. (A little noisy because of the aggregator syntax. It's noisier than I'd like it to be.). This nicely paves the way for aggregators with multi-argument seqOps (like takeBy) that were previously handled by lambdas and we didn't have a clear plan for.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3570
https://github.com/hail-is/hail/pull/3570:1282,Testability,assert,assertEvalsTo,1282,"tp://dev.hail.is/t/proposal-for-aggregators/93/3. Builds on: https://github.com/hail-is/hail/pull/3552. I still need to finish converting some tests from ExtractAggregatorSuite (they are currently commented out). Old-style aggregators (filter, map, flatMap) are expanded at toIR conversion time to ApplyAggOp and SeqOp. ApplyAggOp returns the result of the aggregation. ApplyAggOp's first argument, which must be of type TVoid, is the expression to run for each element being aggregated over. SeqOp merges a computed value into the RegionValueAggregator. I added a AggSignature that holds the AggOp, type being aggregated over and the RegionValueAggregator constructor argument types. This is stored by the ApplyAggOp and the SeqOp. @tpoterba I believe TAggregable is no longer used in the IR code and can go away when the AST gets ripped out. @danking I added some IR testing logic to TestUtils. Namely, `eval` evaluates an IR with environments, args and/or aggregations with a single call (and verifies that the interpret with and without optimization and compiler all agree). There are also functions for asserting the result of aggregations, for example:. ```; val aggSig = AggSignature(Sum(), TFloat64(), FastSeq()); assertEvalsTo(ApplyAggOp(; SeqOp(ApplyBinaryPrimOp(Multiply(), Ref(""a"", TFloat64()), Ref(""b"", TFloat64())), I32(0), aggSig),; FastSeq(), aggSig),; (FastIndexedSeq(Row(1.0, 10.0), Row(10.0, 10.0), Row(null, 10.0)), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),; 110.0); ```. The line:. > (FastIndexedSeq(Row(1.0, 10.0), ...), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),. is the IndexedSeq of values to aggregate over, along with their signature. The struct type is used to build the scope in which aggregators are evaluated. (A little noisy because of the aggregator syntax. It's noisier than I'd like it to be.). This nicely paves the way for aggregators with multi-argument seqOps (like takeBy) that were previously handled by lambdas and we didn't have a clear plan for.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3570
https://github.com/hail-is/hail/pull/3570:31,Usability,simpl,simplify,31,"This implements my proposal to simplify aggregators here: http://dev.hail.is/t/proposal-for-aggregators/93/3. Builds on: https://github.com/hail-is/hail/pull/3552. I still need to finish converting some tests from ExtractAggregatorSuite (they are currently commented out). Old-style aggregators (filter, map, flatMap) are expanded at toIR conversion time to ApplyAggOp and SeqOp. ApplyAggOp returns the result of the aggregation. ApplyAggOp's first argument, which must be of type TVoid, is the expression to run for each element being aggregated over. SeqOp merges a computed value into the RegionValueAggregator. I added a AggSignature that holds the AggOp, type being aggregated over and the RegionValueAggregator constructor argument types. This is stored by the ApplyAggOp and the SeqOp. @tpoterba I believe TAggregable is no longer used in the IR code and can go away when the AST gets ripped out. @danking I added some IR testing logic to TestUtils. Namely, `eval` evaluates an IR with environments, args and/or aggregations with a single call (and verifies that the interpret with and without optimization and compiler all agree). There are also functions for asserting the result of aggregations, for example:. ```; val aggSig = AggSignature(Sum(), TFloat64(), FastSeq()); assertEvalsTo(ApplyAggOp(; SeqOp(ApplyBinaryPrimOp(Multiply(), Ref(""a"", TFloat64()), Ref(""b"", TFloat64())), I32(0), aggSig),; FastSeq(), aggSig),; (FastIndexedSeq(Row(1.0, 10.0), Row(10.0, 10.0), Row(null, 10.0)), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),; 110.0); ```. The line:. > (FastIndexedSeq(Row(1.0, 10.0), ...), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),. is the IndexedSeq of values to aggregate over, along with their signature. The struct type is used to build the scope in which aggregators are evaluated. (A little noisy because of the aggregator syntax. It's noisier than I'd like it to be.). This nicely paves the way for aggregators with multi-argument seqOps (like takeBy) that were previou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3570
https://github.com/hail-is/hail/pull/3570:2045,Usability,clear,clear,2045,"tp://dev.hail.is/t/proposal-for-aggregators/93/3. Builds on: https://github.com/hail-is/hail/pull/3552. I still need to finish converting some tests from ExtractAggregatorSuite (they are currently commented out). Old-style aggregators (filter, map, flatMap) are expanded at toIR conversion time to ApplyAggOp and SeqOp. ApplyAggOp returns the result of the aggregation. ApplyAggOp's first argument, which must be of type TVoid, is the expression to run for each element being aggregated over. SeqOp merges a computed value into the RegionValueAggregator. I added a AggSignature that holds the AggOp, type being aggregated over and the RegionValueAggregator constructor argument types. This is stored by the ApplyAggOp and the SeqOp. @tpoterba I believe TAggregable is no longer used in the IR code and can go away when the AST gets ripped out. @danking I added some IR testing logic to TestUtils. Namely, `eval` evaluates an IR with environments, args and/or aggregations with a single call (and verifies that the interpret with and without optimization and compiler all agree). There are also functions for asserting the result of aggregations, for example:. ```; val aggSig = AggSignature(Sum(), TFloat64(), FastSeq()); assertEvalsTo(ApplyAggOp(; SeqOp(ApplyBinaryPrimOp(Multiply(), Ref(""a"", TFloat64()), Ref(""b"", TFloat64())), I32(0), aggSig),; FastSeq(), aggSig),; (FastIndexedSeq(Row(1.0, 10.0), Row(10.0, 10.0), Row(null, 10.0)), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),; 110.0); ```. The line:. > (FastIndexedSeq(Row(1.0, 10.0), ...), TStruct(""a"" -> TFloat64(), ""b"" -> TFloat64())),. is the IndexedSeq of values to aggregate over, along with their signature. The struct type is used to build the scope in which aggregators are evaluated. (A little noisy because of the aggregator syntax. It's noisier than I'd like it to be.). This nicely paves the way for aggregators with multi-argument seqOps (like takeBy) that were previously handled by lambdas and we didn't have a clear plan for.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3570
https://github.com/hail-is/hail/pull/3573:422,Modifiability,variab,variable,422,"This commit changes the tracking of Joins on Python expressions; in two ways:. First, joins are switched from being tracked explicitly; on Expression to being a part of the expr AST. Second, broadcasts are split off as a separate AST node, which; lets us make them significantly simpler. It also made it easy; to collapse all broadcasts for a given MT/Table operation into; one MapGlobals IR, instead of one per broadcast variable. In the future, all the metadata on Expression should be tracked; on AST (type, aggregations, indices). This is a first incremental; step in getting there.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3573
https://github.com/hail-is/hail/pull/3573:279,Usability,simpl,simpler,279,"This commit changes the tracking of Joins on Python expressions; in two ways:. First, joins are switched from being tracked explicitly; on Expression to being a part of the expr AST. Second, broadcasts are split off as a separate AST node, which; lets us make them significantly simpler. It also made it easy; to collapse all broadcasts for a given MT/Table operation into; one MapGlobals IR, instead of one per broadcast variable. In the future, all the metadata on Expression should be tracked; on AST (type, aggregations, indices). This is a first incremental; step in getting there.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3573
https://github.com/hail-is/hail/pull/3575:461,Usability,clear,clearer,461,"Proposed change for `ld_prune` motivated by user difficulty importing and pruning 1kg. As we don't have near-term plans for a version that exploits phasing, I think it's best to make this runnable regardless of phasing, while documenting that the algorithm ignores phasing. I've added this description, fixed a doc bug (unit is bases, not kilobases) and changed the parameter `window` to `bp_window_size`, which is consistent with `window_by_locus` and I think clearer. (I'll note it on our breaking changes dev post). If we add a method that incorporates phasing in 0.2 lifetime, it can be a separate method or we can add a parameter `use_phasing` with default value `False`. (actually, it should be a separate method since r^2 correlation won't be the right measure for that).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3575
https://github.com/hail-is/hail/pull/3576:99,Safety,avoid,avoid,99,ExtractAggregators uses a `Ref` with a null type (which is set before; returning to its callee) to avoid traversing the IR tree twice because; the type of the aggregation result is not known until all; the aggregations have been seen. A non lazy call to `typ` will see the null type (and blow up with an; NPE) when an `ApplySpecial`/`Apply` IR node is copied to replace a; leaf IR aggregation node with a `Ref` to the aggregation result; struct.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3576
https://github.com/hail-is/hail/pull/3579:1175,Availability,alive,alive,1175,"Will merge cleanly when https://github.com/hail-is/hail/pull/3560 lands. I needed to remove `RegionValue.copy` and `Region.copy` because they necessarily create regions that aren't managed by an `RVDContext`. `RegionValue.copy` is only used in three places. . - `Table.toMatrixTable`: Here, I took the somewhat inefficient choice of creating `SafeRow`s. If `toMatrixTable` is a performance bottleneck, we might want to reconsider this. It's not totally obvious how to do this. I think I'd need to explicitly serialize/deserialize these values and modify `reduceByKey` to explicitly provide the `RVDContext`. Anyway, this works and I don't think it's _that_ slow. (I guess I should check that). - `OrderedRVD.localKeySort` & `LocalLDPrune.pruneLocal`: in both cases we need keep a handful of region values around per-partition. This does not lend itself to region-based-allocation. I solve this with two copies and a fresh region per value. Putting a value into `localKeySort`'s queue requires copying it into a fresh region. Taking a value out of the queue requires copying it into the consumer's region and closing/freeing the region it was living in. There fresh region is alive as long as the value is in the queue. I had to modify `RVDContext` to track `Region`s that get closed early. This seems a bit inefficient. Maybe I should track children as a `Set`?. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3579
https://github.com/hail-is/hail/pull/3579:555,Energy Efficiency,reduce,reduceByKey,555,"Will merge cleanly when https://github.com/hail-is/hail/pull/3560 lands. I needed to remove `RegionValue.copy` and `Region.copy` because they necessarily create regions that aren't managed by an `RVDContext`. `RegionValue.copy` is only used in three places. . - `Table.toMatrixTable`: Here, I took the somewhat inefficient choice of creating `SafeRow`s. If `toMatrixTable` is a performance bottleneck, we might want to reconsider this. It's not totally obvious how to do this. I think I'd need to explicitly serialize/deserialize these values and modify `reduceByKey` to explicitly provide the `RVDContext`. Anyway, this works and I don't think it's _that_ slow. (I guess I should check that). - `OrderedRVD.localKeySort` & `LocalLDPrune.pruneLocal`: in both cases we need keep a handful of region values around per-partition. This does not lend itself to region-based-allocation. I solve this with two copies and a fresh region per value. Putting a value into `localKeySort`'s queue requires copying it into a fresh region. Taking a value out of the queue requires copying it into the consumer's region and closing/freeing the region it was living in. There fresh region is alive as long as the value is in the queue. I had to modify `RVDContext` to track `Region`s that get closed early. This seems a bit inefficient. Maybe I should track children as a `Set`?. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3579
https://github.com/hail-is/hail/pull/3579:378,Performance,perform,performance,378,"Will merge cleanly when https://github.com/hail-is/hail/pull/3560 lands. I needed to remove `RegionValue.copy` and `Region.copy` because they necessarily create regions that aren't managed by an `RVDContext`. `RegionValue.copy` is only used in three places. . - `Table.toMatrixTable`: Here, I took the somewhat inefficient choice of creating `SafeRow`s. If `toMatrixTable` is a performance bottleneck, we might want to reconsider this. It's not totally obvious how to do this. I think I'd need to explicitly serialize/deserialize these values and modify `reduceByKey` to explicitly provide the `RVDContext`. Anyway, this works and I don't think it's _that_ slow. (I guess I should check that). - `OrderedRVD.localKeySort` & `LocalLDPrune.pruneLocal`: in both cases we need keep a handful of region values around per-partition. This does not lend itself to region-based-allocation. I solve this with two copies and a fresh region per value. Putting a value into `localKeySort`'s queue requires copying it into a fresh region. Taking a value out of the queue requires copying it into the consumer's region and closing/freeing the region it was living in. There fresh region is alive as long as the value is in the queue. I had to modify `RVDContext` to track `Region`s that get closed early. This seems a bit inefficient. Maybe I should track children as a `Set`?. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3579
https://github.com/hail-is/hail/pull/3579:390,Performance,bottleneck,bottleneck,390,"Will merge cleanly when https://github.com/hail-is/hail/pull/3560 lands. I needed to remove `RegionValue.copy` and `Region.copy` because they necessarily create regions that aren't managed by an `RVDContext`. `RegionValue.copy` is only used in three places. . - `Table.toMatrixTable`: Here, I took the somewhat inefficient choice of creating `SafeRow`s. If `toMatrixTable` is a performance bottleneck, we might want to reconsider this. It's not totally obvious how to do this. I think I'd need to explicitly serialize/deserialize these values and modify `reduceByKey` to explicitly provide the `RVDContext`. Anyway, this works and I don't think it's _that_ slow. (I guess I should check that). - `OrderedRVD.localKeySort` & `LocalLDPrune.pruneLocal`: in both cases we need keep a handful of region values around per-partition. This does not lend itself to region-based-allocation. I solve this with two copies and a fresh region per value. Putting a value into `localKeySort`'s queue requires copying it into a fresh region. Taking a value out of the queue requires copying it into the consumer's region and closing/freeing the region it was living in. There fresh region is alive as long as the value is in the queue. I had to modify `RVDContext` to track `Region`s that get closed early. This seems a bit inefficient. Maybe I should track children as a `Set`?. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3579
https://github.com/hail-is/hail/pull/3579:978,Performance,queue,queue,978,"Will merge cleanly when https://github.com/hail-is/hail/pull/3560 lands. I needed to remove `RegionValue.copy` and `Region.copy` because they necessarily create regions that aren't managed by an `RVDContext`. `RegionValue.copy` is only used in three places. . - `Table.toMatrixTable`: Here, I took the somewhat inefficient choice of creating `SafeRow`s. If `toMatrixTable` is a performance bottleneck, we might want to reconsider this. It's not totally obvious how to do this. I think I'd need to explicitly serialize/deserialize these values and modify `reduceByKey` to explicitly provide the `RVDContext`. Anyway, this works and I don't think it's _that_ slow. (I guess I should check that). - `OrderedRVD.localKeySort` & `LocalLDPrune.pruneLocal`: in both cases we need keep a handful of region values around per-partition. This does not lend itself to region-based-allocation. I solve this with two copies and a fresh region per value. Putting a value into `localKeySort`'s queue requires copying it into a fresh region. Taking a value out of the queue requires copying it into the consumer's region and closing/freeing the region it was living in. There fresh region is alive as long as the value is in the queue. I had to modify `RVDContext` to track `Region`s that get closed early. This seems a bit inefficient. Maybe I should track children as a `Set`?. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3579
https://github.com/hail-is/hail/pull/3579:1051,Performance,queue,queue,1051,"Will merge cleanly when https://github.com/hail-is/hail/pull/3560 lands. I needed to remove `RegionValue.copy` and `Region.copy` because they necessarily create regions that aren't managed by an `RVDContext`. `RegionValue.copy` is only used in three places. . - `Table.toMatrixTable`: Here, I took the somewhat inefficient choice of creating `SafeRow`s. If `toMatrixTable` is a performance bottleneck, we might want to reconsider this. It's not totally obvious how to do this. I think I'd need to explicitly serialize/deserialize these values and modify `reduceByKey` to explicitly provide the `RVDContext`. Anyway, this works and I don't think it's _that_ slow. (I guess I should check that). - `OrderedRVD.localKeySort` & `LocalLDPrune.pruneLocal`: in both cases we need keep a handful of region values around per-partition. This does not lend itself to region-based-allocation. I solve this with two copies and a fresh region per value. Putting a value into `localKeySort`'s queue requires copying it into a fresh region. Taking a value out of the queue requires copying it into the consumer's region and closing/freeing the region it was living in. There fresh region is alive as long as the value is in the queue. I had to modify `RVDContext` to track `Region`s that get closed early. This seems a bit inefficient. Maybe I should track children as a `Set`?. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3579
https://github.com/hail-is/hail/pull/3579:1212,Performance,queue,queue,1212,"Will merge cleanly when https://github.com/hail-is/hail/pull/3560 lands. I needed to remove `RegionValue.copy` and `Region.copy` because they necessarily create regions that aren't managed by an `RVDContext`. `RegionValue.copy` is only used in three places. . - `Table.toMatrixTable`: Here, I took the somewhat inefficient choice of creating `SafeRow`s. If `toMatrixTable` is a performance bottleneck, we might want to reconsider this. It's not totally obvious how to do this. I think I'd need to explicitly serialize/deserialize these values and modify `reduceByKey` to explicitly provide the `RVDContext`. Anyway, this works and I don't think it's _that_ slow. (I guess I should check that). - `OrderedRVD.localKeySort` & `LocalLDPrune.pruneLocal`: in both cases we need keep a handful of region values around per-partition. This does not lend itself to region-based-allocation. I solve this with two copies and a fresh region per value. Putting a value into `localKeySort`'s queue requires copying it into a fresh region. Taking a value out of the queue requires copying it into the consumer's region and closing/freeing the region it was living in. There fresh region is alive as long as the value is in the queue. I had to modify `RVDContext` to track `Region`s that get closed early. This seems a bit inefficient. Maybe I should track children as a `Set`?. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3579
https://github.com/hail-is/hail/pull/3579:343,Safety,Safe,SafeRow,343,"Will merge cleanly when https://github.com/hail-is/hail/pull/3560 lands. I needed to remove `RegionValue.copy` and `Region.copy` because they necessarily create regions that aren't managed by an `RVDContext`. `RegionValue.copy` is only used in three places. . - `Table.toMatrixTable`: Here, I took the somewhat inefficient choice of creating `SafeRow`s. If `toMatrixTable` is a performance bottleneck, we might want to reconsider this. It's not totally obvious how to do this. I think I'd need to explicitly serialize/deserialize these values and modify `reduceByKey` to explicitly provide the `RVDContext`. Anyway, this works and I don't think it's _that_ slow. (I guess I should check that). - `OrderedRVD.localKeySort` & `LocalLDPrune.pruneLocal`: in both cases we need keep a handful of region values around per-partition. This does not lend itself to region-based-allocation. I solve this with two copies and a fresh region per value. Putting a value into `localKeySort`'s queue requires copying it into a fresh region. Taking a value out of the queue requires copying it into the consumer's region and closing/freeing the region it was living in. There fresh region is alive as long as the value is in the queue. I had to modify `RVDContext` to track `Region`s that get closed early. This seems a bit inefficient. Maybe I should track children as a `Set`?. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3579
https://github.com/hail-is/hail/issues/3583:778,Availability,error,error,778,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-c8ca698. ### What you did:; ```; mt_path = ""gs://gnomad-berylc/myoseq/qc/variant_qc/MYOSEQ.variant.qc.filtered.040318.mt""; myoseq = hl.read_matrix_table(mt_path); qc_mt_common = myoseq.filter_rows(myoseq.va_qc.AF > 0.05). qc_mt_common_syn = qc_mt_common.filter_rows(qc_mt_common.worst_csq == ""synonymous""); results_per_gene_common = (qc_mt_common_syn.group_rows_by(qc_mt_common_syn.vep.ensg_with_most_severe_csq).aggregate(n_non_ref=agg.count_where((qc_mt_common_syn.GT.is_non_ref())))); ```. ### What went wrong (all error messages here, including the full java stack trace):; 2018-05-14 23:45:22 Hail: WARN: modified row key, rescanning to compute ordering...; [Stage 9:=====> (163 + 76) / 1526]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-12-cefffae1d4ba> in <module>(); ----> 1 results_per_gene_common = (qc_mt_common_syn.group_rows_by(qc_mt_common_syn.vep.ensg_with_most_severe_csq).aggregate(n_non_ref=agg.count_where((qc_mt_common_syn.GT.is_non_ref())))). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-c8ca698c6ed5.zip/hail/matrixtable.py in aggregate(self, **named_exprs); 349 rest_of_key = {k: self._row_keys[k] for k in self._row_keys.keys() if k not in self._partition_key}; 350 base = MatrixTable(; --> 351 base._key_rows_by(""GroupedMatrixTable.group_rows_by"", pk, rest_of_key)._jvds.aggregateRowsByKey(; 352 ',\n'.join(strs))); 353 else:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-c8ca698c6ed5.zip/hail/matrixtable.py in _key_rows_by(self, caller, pk_dict, rest_of_keys_dict); 706 key_fields = dict(pk_dict, **rest_of_keys_dict); 707; --> 708 return self._select_rows(caller, key_struct=hl.struct(**key_fields), pk_si",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:3340,Availability,Error,Error,3340,"gs, **kwargs); 505 def _typecheck(__orig_func__, *args, **kwargs):; 506 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=True); --> 507 return __orig_func__(*args_, **kwargs_); 508; 509 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-c8ca698c6ed5.zip/hail/matrixtable.py in _select_rows(self, caller, key_struct, value_struct, pk_size); 2814; 2815 return cleanup(MatrixTable(base._jvds.selectRows(row._ast.to_hql(),; -> 2816 new_key))); 2817; 2818 @typecheck_method(caller=str,. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-c8ca698c6ed5.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 9.0 failed 20 times, most recent failure: Lost task 24.19 in stage 9.0 (TID 2874, berylc-sw-68wx.c.broad-mpg-gnomad.internal, executor 39): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:140); 	at is.hail.annotations.Region.allocate(Region.scala:153); 	at is.hail.annotations.Region.allocate(Region.scala:160); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:650); 	at is.hail.HailContext$$anon$2.next(HailCo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:3647,Availability,failure,failure,3647,"elect_rows(self, caller, key_struct, value_struct, pk_size); 2814; 2815 return cleanup(MatrixTable(base._jvds.selectRows(row._ast.to_hql(),; -> 2816 new_key))); 2817; 2818 @typecheck_method(caller=str,. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-c8ca698c6ed5.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 9.0 failed 20 times, most recent failure: Lost task 24.19 in stage 9.0 (TID 2874, berylc-sw-68wx.c.broad-mpg-gnomad.internal, executor 39): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:140); 	at is.hail.annotations.Region.allocate(Region.scala:153); 	at is.hail.annotations.Region.allocate(Region.scala:160); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:650); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:245); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:218); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:333); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:3706,Availability,failure,failure,3706,"2814; 2815 return cleanup(MatrixTable(base._jvds.selectRows(row._ast.to_hql(),; -> 2816 new_key))); 2817; 2818 @typecheck_method(caller=str,. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-c8ca698c6ed5.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 9.0 failed 20 times, most recent failure: Lost task 24.19 in stage 9.0 (TID 2874, berylc-sw-68wx.c.broad-mpg-gnomad.internal, executor 39): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:140); 	at is.hail.annotations.Region.allocate(Region.scala:153); 	at is.hail.annotations.Region.allocate(Region.scala:160); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:650); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:245); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:218); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:333); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:915); 	at is.hail.rvd.Ordered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:15798,Availability,Error,Error,15798,anonfun$apply$22.apply(ContextRDD.scala:308); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c8ca698; Error summary: NegativeArraySizeException: null,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:3986,Energy Efficiency,allocate,allocate,3986,"132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-c8ca698c6ed5.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 9.0 failed 20 times, most recent failure: Lost task 24.19 in stage 9.0 (TID 2874, berylc-sw-68wx.c.broad-mpg-gnomad.internal, executor 39): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:140); 	at is.hail.annotations.Region.allocate(Region.scala:153); 	at is.hail.annotations.Region.allocate(Region.scala:160); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:650); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:245); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:218); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:333); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:915); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:909); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonf",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:4045,Energy Efficiency,allocate,allocate,4045,"gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-c8ca698c6ed5.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 9.0 failed 20 times, most recent failure: Lost task 24.19 in stage 9.0 (TID 2874, berylc-sw-68wx.c.broad-mpg-gnomad.internal, executor 39): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:140); 	at is.hail.annotations.Region.allocate(Region.scala:153); 	at is.hail.annotations.Region.allocate(Region.scala:160); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:650); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:245); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:218); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:333); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:915); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:909); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:912); 	at scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:7850,Energy Efficiency,schedul,scheduler,7850,ic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:7922,Energy Efficiency,schedul,scheduler,7922,.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:8287,Energy Efficiency,schedul,scheduler,8287,ollection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:8327,Energy Efficiency,schedul,scheduler,8327,at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:8426,Energy Efficiency,schedul,scheduler,8426,t scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:8524,Energy Efficiency,schedul,scheduler,8524,onfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:8778,Energy Efficiency,schedul,scheduler,8778,nfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:8859,Energy Efficiency,schedul,scheduler,8859,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:8965,Energy Efficiency,schedul,scheduler,8965,he.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:9115,Energy Efficiency,schedul,scheduler,9115,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:9204,Energy Efficiency,schedul,scheduler,9204,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:9302,Energy Efficiency,schedul,scheduler,9302,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:143); 	at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:9398,Energy Efficiency,schedul,scheduler,9398,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:143); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:542); 	at is.hail.rvd.OrderedRVD$.coerce,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:9563,Energy Efficiency,schedul,scheduler,9563,.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:143); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:542); 	at is.hail.rvd.OrderedRVD$.coerce(OrderedRVD.scala:635); 	at is.hail.rvd.OrderedRVD$.coerce(OrderedRVD.scala:569); 	at is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1229); 	at is.hail.var,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:11509,Energy Efficiency,allocate,allocate,11509,selectRows(MatrixTable.scala:1229); 	at is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1168); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.NegativeArraySizeException: null; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:140); 	at is.hail.annotations.Region.allocate(Region.scala:153); 	at is.hail.annotations.Region.allocate(Region.scala:160); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:650); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:245); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:218); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:333); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:915); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:909); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:11568,Energy Efficiency,allocate,allocate,11568,rixTable.selectRows(MatrixTable.scala:1168); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.NegativeArraySizeException: null; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:140); 	at is.hail.annotations.Region.allocate(Region.scala:153); 	at is.hail.annotations.Region.allocate(Region.scala:160); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:650); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:245); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:218); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:333); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:915); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:909); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:912); 	at scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:15373,Energy Efficiency,schedul,scheduler,15373,anonfun$apply$22.apply(ContextRDD.scala:308); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c8ca698; Error summary: NegativeArraySizeException: null,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:15445,Energy Efficiency,schedul,scheduler,15445,anonfun$apply$22.apply(ContextRDD.scala:308); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c8ca698; Error summary: NegativeArraySizeException: null,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:784,Integrability,message,messages,784,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-c8ca698. ### What you did:; ```; mt_path = ""gs://gnomad-berylc/myoseq/qc/variant_qc/MYOSEQ.variant.qc.filtered.040318.mt""; myoseq = hl.read_matrix_table(mt_path); qc_mt_common = myoseq.filter_rows(myoseq.va_qc.AF > 0.05). qc_mt_common_syn = qc_mt_common.filter_rows(qc_mt_common.worst_csq == ""synonymous""); results_per_gene_common = (qc_mt_common_syn.group_rows_by(qc_mt_common_syn.vep.ensg_with_most_severe_csq).aggregate(n_non_ref=agg.count_where((qc_mt_common_syn.GT.is_non_ref())))); ```. ### What went wrong (all error messages here, including the full java stack trace):; 2018-05-14 23:45:22 Hail: WARN: modified row key, rescanning to compute ordering...; [Stage 9:=====> (163 + 76) / 1526]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-12-cefffae1d4ba> in <module>(); ----> 1 results_per_gene_common = (qc_mt_common_syn.group_rows_by(qc_mt_common_syn.vep.ensg_with_most_severe_csq).aggregate(n_non_ref=agg.count_where((qc_mt_common_syn.GT.is_non_ref())))). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-c8ca698c6ed5.zip/hail/matrixtable.py in aggregate(self, **named_exprs); 349 rest_of_key = {k: self._row_keys[k] for k in self._row_keys.keys() if k not in self._partition_key}; 350 base = MatrixTable(; --> 351 base._key_rows_by(""GroupedMatrixTable.group_rows_by"", pk, rest_of_key)._jvds.aggregateRowsByKey(; 352 ',\n'.join(strs))); 353 else:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-c8ca698c6ed5.zip/hail/matrixtable.py in _key_rows_by(self, caller, pk_dict, rest_of_keys_dict); 706 key_fields = dict(pk_dict, **rest_of_keys_dict); 707; --> 708 return self._select_rows(caller, key_struct=hl.struct(**key_fields), pk_si",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:8047,Performance,concurren,concurrent,8047,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:8132,Performance,concurren,concurrent,8132, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:15570,Performance,concurren,concurrent,15570,anonfun$apply$22.apply(ContextRDD.scala:308); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c8ca698; Error summary: NegativeArraySizeException: null,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:15655,Performance,concurren,concurrent,15655,anonfun$apply$22.apply(ContextRDD.scala:308); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-c8ca698; Error summary: NegativeArraySizeException: null,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:3626,Safety,abort,aborted,3626,"elect_rows(self, caller, key_struct, value_struct, pk_size); 2814; 2815 return cleanup(MatrixTable(base._jvds.selectRows(row._ast.to_hql(),; -> 2816 new_key))); 2817; 2818 @typecheck_method(caller=str,. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-c8ca698c6ed5.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 9.0 failed 20 times, most recent failure: Lost task 24.19 in stage 9.0 (TID 2874, berylc-sw-68wx.c.broad-mpg-gnomad.internal, executor 39): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:140); 	at is.hail.annotations.Region.allocate(Region.scala:153); 	at is.hail.annotations.Region.allocate(Region.scala:160); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.codegen.generated.C11.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:650); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:245); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:218); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:333); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:8458,Safety,abort,abortStage,8458,tractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:8556,Safety,abort,abortStage,8556,un$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3583:8801,Safety,abort,abortStage,8801,xt.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3583
https://github.com/hail-is/hail/issues/3585:492,Availability,down,downloaded,492,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; hail: `hail-devel-c8ca698`; ### What you did:; ```; hl.init(); ds = hl.import_plink('gs://testdata/1kg_phase1_chr22.bed',; 'gs://testdata/1kg_phase1_chr22.bim',; 'gs://testdata/1kg_phase1_chr22.fam'); ds = hl.sample_qc(ds); ```; Data was downloaded from [here](https://www.cog-genomics.org/plink/1.9/resources).; ; ### What went wrong (all error messages here, including the full java stack trace):; Spark jobs fail at `treeReduce at SampleQC.scala:206`.; Java Error: `java.lang.NegativeArraySizeException`. Full stack:; See file attached.; [error_message.txt](https://github.com/hail-is/hail/files/2003301/error_message.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3585
https://github.com/hail-is/hail/issues/3585:594,Availability,error,error,594,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; hail: `hail-devel-c8ca698`; ### What you did:; ```; hl.init(); ds = hl.import_plink('gs://testdata/1kg_phase1_chr22.bed',; 'gs://testdata/1kg_phase1_chr22.bim',; 'gs://testdata/1kg_phase1_chr22.fam'); ds = hl.sample_qc(ds); ```; Data was downloaded from [here](https://www.cog-genomics.org/plink/1.9/resources).; ; ### What went wrong (all error messages here, including the full java stack trace):; Spark jobs fail at `treeReduce at SampleQC.scala:206`.; Java Error: `java.lang.NegativeArraySizeException`. Full stack:; See file attached.; [error_message.txt](https://github.com/hail-is/hail/files/2003301/error_message.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3585
https://github.com/hail-is/hail/issues/3585:715,Availability,Error,Error,715,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; hail: `hail-devel-c8ca698`; ### What you did:; ```; hl.init(); ds = hl.import_plink('gs://testdata/1kg_phase1_chr22.bed',; 'gs://testdata/1kg_phase1_chr22.bim',; 'gs://testdata/1kg_phase1_chr22.fam'); ds = hl.sample_qc(ds); ```; Data was downloaded from [here](https://www.cog-genomics.org/plink/1.9/resources).; ; ### What went wrong (all error messages here, including the full java stack trace):; Spark jobs fail at `treeReduce at SampleQC.scala:206`.; Java Error: `java.lang.NegativeArraySizeException`. Full stack:; See file attached.; [error_message.txt](https://github.com/hail-is/hail/files/2003301/error_message.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3585
https://github.com/hail-is/hail/issues/3585:600,Integrability,message,messages,600,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; hail: `hail-devel-c8ca698`; ### What you did:; ```; hl.init(); ds = hl.import_plink('gs://testdata/1kg_phase1_chr22.bed',; 'gs://testdata/1kg_phase1_chr22.bim',; 'gs://testdata/1kg_phase1_chr22.fam'); ds = hl.sample_qc(ds); ```; Data was downloaded from [here](https://www.cog-genomics.org/plink/1.9/resources).; ; ### What went wrong (all error messages here, including the full java stack trace):; Spark jobs fail at `treeReduce at SampleQC.scala:206`.; Java Error: `java.lang.NegativeArraySizeException`. Full stack:; See file attached.; [error_message.txt](https://github.com/hail-is/hail/files/2003301/error_message.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3585
https://github.com/hail-is/hail/issues/3585:344,Testability,test,testdata,344,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; hail: `hail-devel-c8ca698`; ### What you did:; ```; hl.init(); ds = hl.import_plink('gs://testdata/1kg_phase1_chr22.bed',; 'gs://testdata/1kg_phase1_chr22.bim',; 'gs://testdata/1kg_phase1_chr22.fam'); ds = hl.sample_qc(ds); ```; Data was downloaded from [here](https://www.cog-genomics.org/plink/1.9/resources).; ; ### What went wrong (all error messages here, including the full java stack trace):; Spark jobs fail at `treeReduce at SampleQC.scala:206`.; Java Error: `java.lang.NegativeArraySizeException`. Full stack:; See file attached.; [error_message.txt](https://github.com/hail-is/hail/files/2003301/error_message.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3585
https://github.com/hail-is/hail/issues/3585:383,Testability,test,testdata,383,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; hail: `hail-devel-c8ca698`; ### What you did:; ```; hl.init(); ds = hl.import_plink('gs://testdata/1kg_phase1_chr22.bed',; 'gs://testdata/1kg_phase1_chr22.bim',; 'gs://testdata/1kg_phase1_chr22.fam'); ds = hl.sample_qc(ds); ```; Data was downloaded from [here](https://www.cog-genomics.org/plink/1.9/resources).; ; ### What went wrong (all error messages here, including the full java stack trace):; Spark jobs fail at `treeReduce at SampleQC.scala:206`.; Java Error: `java.lang.NegativeArraySizeException`. Full stack:; See file attached.; [error_message.txt](https://github.com/hail-is/hail/files/2003301/error_message.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3585
https://github.com/hail-is/hail/issues/3585:422,Testability,test,testdata,422,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; hail: `hail-devel-c8ca698`; ### What you did:; ```; hl.init(); ds = hl.import_plink('gs://testdata/1kg_phase1_chr22.bed',; 'gs://testdata/1kg_phase1_chr22.bim',; 'gs://testdata/1kg_phase1_chr22.fam'); ds = hl.sample_qc(ds); ```; Data was downloaded from [here](https://www.cog-genomics.org/plink/1.9/resources).; ; ### What went wrong (all error messages here, including the full java stack trace):; Spark jobs fail at `treeReduce at SampleQC.scala:206`.; Java Error: `java.lang.NegativeArraySizeException`. Full stack:; See file attached.; [error_message.txt](https://github.com/hail-is/hail/files/2003301/error_message.txt)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3585
https://github.com/hail-is/hail/pull/3587:168,Usability,clear,clears,168,"cc: @tpoterba . Some greek yogurt to cool off that spicy meatball. Large filters, or filters with large globals will clutter up the region unnecessarily. This fix adds clears to the two matrix row filters and the table filter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3587
https://github.com/hail-is/hail/pull/3590:218,Availability,alive,alive,218,"No one has complained about this yet, but I suspect there's a lurking issue in `linreg`. On line 75 of LinearRegression.scala, we create a writable region value using a context managed region. This region will be kept alive (in the garbage collection sense) by the context until the end of the Task (which I believe is the end of processing one partition). As such, `linreg` will generate a bunch of garbage. We close the child as soon as we know it is no longer used, thus saving memory use, at the cost of copying the results. In a future where we can reference-count regions then we could avoid the copy and simply return the writable region value. This future is not here yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3590
https://github.com/hail-is/hail/pull/3590:592,Safety,avoid,avoid,592,"No one has complained about this yet, but I suspect there's a lurking issue in `linreg`. On line 75 of LinearRegression.scala, we create a writable region value using a context managed region. This region will be kept alive (in the garbage collection sense) by the context until the end of the Task (which I believe is the end of processing one partition). As such, `linreg` will generate a bunch of garbage. We close the child as soon as we know it is no longer used, thus saving memory use, at the cost of copying the results. In a future where we can reference-count regions then we could avoid the copy and simply return the writable region value. This future is not here yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3590
https://github.com/hail-is/hail/pull/3590:611,Usability,simpl,simply,611,"No one has complained about this yet, but I suspect there's a lurking issue in `linreg`. On line 75 of LinearRegression.scala, we create a writable region value using a context managed region. This region will be kept alive (in the garbage collection sense) by the context until the end of the Task (which I believe is the end of processing one partition). As such, `linreg` will generate a bunch of garbage. We close the child as soon as we know it is no longer used, thus saving memory use, at the cost of copying the results. In a future where we can reference-count regions then we could avoid the copy and simply return the writable region value. This future is not here yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3590
https://github.com/hail-is/hail/pull/3593:20,Deployability,deploy,deployed,20,with the version of deployed jars / zips / distros,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3593
https://github.com/hail-is/hail/pull/3595:245,Performance,load,loading,245,"This is the first chunk of the C++ support, giving Scala ref-counted pointers to C++ objects. On its own, this doesn't do anything very useful. But we need this infrastructure to support; off-heap Region. And a later PR will support compilation/loading of Scala-generated C++; functions (which also needs this infrastructure).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3595
https://github.com/hail-is/hail/pull/3598:2,Testability,test,tested,2,I tested this on #3562 and `test_call_stats` passes now.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3598
https://github.com/hail-is/hail/pull/3600:328,Integrability,interface,interface,328,"Per conversation with @konradjk @danking regarding user confusion, this PR emphasizes that ld_prune returns a table of pruned variants rather than pruning the matrix table itself. Also corrected but about parallelism. @danking I could go farther and change the method name to `ld_pruned_variants`, but I'd rather leave whatever interface changes seem optimal for when you add your planned version that actually does the pruning.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3600
https://github.com/hail-is/hail/pull/3601:53,Safety,unsafe,unsafe,53,I think there's now 4 compiler warnings (besides the unsafe warnings) that I wasn't sure what to do with.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3601
https://github.com/hail-is/hail/pull/3616:71,Usability,clear,clearing,71,"I am not sure why I thought my previous fix fixed anything. I was; not clearing the correct region. This fix clears the region; appropriately, i.e., clears it whenever it is finished with; a value from the producers region.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3616
https://github.com/hail-is/hail/pull/3616:109,Usability,clear,clears,109,"I am not sure why I thought my previous fix fixed anything. I was; not clearing the correct region. This fix clears the region; appropriately, i.e., clears it whenever it is finished with; a value from the producers region.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3616
https://github.com/hail-is/hail/pull/3616:149,Usability,clear,clears,149,"I am not sure why I thought my previous fix fixed anything. I was; not clearing the correct region. This fix clears the region; appropriately, i.e., clears it whenever it is finished with; a value from the producers region.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3616
https://github.com/hail-is/hail/pull/3622:634,Deployability,update,update,634,"This PR changes the semantics of TableMapRows in the IR and Table.select. They are now required to preserve the key ordering. In other words, applying TableMapRows to an ordered table can produce an ordered table without shuffling. I put an assert to verify that the produced OrderedRVD is really ordered, but that might be too expensive an assertion. Before, `newKey = None` meant ""keep the old key"", but that left no room to ask for the result to be unkeyed. Now `newKey` is just the key of the resulting table. If any partition key fields are modified, then even if the ordering is preserved, the result will need to be scanned to update the partition bounds. To avoid this scan when unnecessary, I added the `preservedKeyFields` parameter, which is the length of the prefix of key fields which are not modified. Thus, if the number of partition keys of the underlying OrderedRVD is less than or equal to `preservedKeyFields`, the partition bounds will remain valid. This feels pretty clunky, and I welcome better ideas. In particular, is there a way to compute this from the `newRow` IR? That's what I did on the Python side in `_select`, but it wasn't obvious to me how to do the same with the Scala IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3622
https://github.com/hail-is/hail/pull/3622:666,Safety,avoid,avoid,666,"This PR changes the semantics of TableMapRows in the IR and Table.select. They are now required to preserve the key ordering. In other words, applying TableMapRows to an ordered table can produce an ordered table without shuffling. I put an assert to verify that the produced OrderedRVD is really ordered, but that might be too expensive an assertion. Before, `newKey = None` meant ""keep the old key"", but that left no room to ask for the result to be unkeyed. Now `newKey` is just the key of the resulting table. If any partition key fields are modified, then even if the ordering is preserved, the result will need to be scanned to update the partition bounds. To avoid this scan when unnecessary, I added the `preservedKeyFields` parameter, which is the length of the prefix of key fields which are not modified. Thus, if the number of partition keys of the underlying OrderedRVD is less than or equal to `preservedKeyFields`, the partition bounds will remain valid. This feels pretty clunky, and I welcome better ideas. In particular, is there a way to compute this from the `newRow` IR? That's what I did on the Python side in `_select`, but it wasn't obvious to me how to do the same with the Scala IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3622
https://github.com/hail-is/hail/pull/3622:241,Testability,assert,assert,241,"This PR changes the semantics of TableMapRows in the IR and Table.select. They are now required to preserve the key ordering. In other words, applying TableMapRows to an ordered table can produce an ordered table without shuffling. I put an assert to verify that the produced OrderedRVD is really ordered, but that might be too expensive an assertion. Before, `newKey = None` meant ""keep the old key"", but that left no room to ask for the result to be unkeyed. Now `newKey` is just the key of the resulting table. If any partition key fields are modified, then even if the ordering is preserved, the result will need to be scanned to update the partition bounds. To avoid this scan when unnecessary, I added the `preservedKeyFields` parameter, which is the length of the prefix of key fields which are not modified. Thus, if the number of partition keys of the underlying OrderedRVD is less than or equal to `preservedKeyFields`, the partition bounds will remain valid. This feels pretty clunky, and I welcome better ideas. In particular, is there a way to compute this from the `newRow` IR? That's what I did on the Python side in `_select`, but it wasn't obvious to me how to do the same with the Scala IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3622
https://github.com/hail-is/hail/pull/3622:341,Testability,assert,assertion,341,"This PR changes the semantics of TableMapRows in the IR and Table.select. They are now required to preserve the key ordering. In other words, applying TableMapRows to an ordered table can produce an ordered table without shuffling. I put an assert to verify that the produced OrderedRVD is really ordered, but that might be too expensive an assertion. Before, `newKey = None` meant ""keep the old key"", but that left no room to ask for the result to be unkeyed. Now `newKey` is just the key of the resulting table. If any partition key fields are modified, then even if the ordering is preserved, the result will need to be scanned to update the partition bounds. To avoid this scan when unnecessary, I added the `preservedKeyFields` parameter, which is the length of the prefix of key fields which are not modified. Thus, if the number of partition keys of the underlying OrderedRVD is less than or equal to `preservedKeyFields`, the partition bounds will remain valid. This feels pretty clunky, and I welcome better ideas. In particular, is there a way to compute this from the `newRow` IR? That's what I did on the Python side in `_select`, but it wasn't obvious to me how to do the same with the Scala IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3622
https://github.com/hail-is/hail/pull/3623:51,Deployability,update,updated,51,"I'll add to the log of breaking changes. I've also updated the name from `position_morgan` to `cm_position`, as Plink 2.0 has settled on centimorgans: https://www.cog-genomics.org/plink/2.0/formats#bim. With this change, I can again import 1kg plink files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3623
https://github.com/hail-is/hail/pull/3623:16,Testability,log,log,16,"I'll add to the log of breaking changes. I've also updated the name from `position_morgan` to `cm_position`, as Plink 2.0 has settled on centimorgans: https://www.cog-genomics.org/plink/2.0/formats#bim. With this change, I can again import 1kg plink files.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3623
https://github.com/hail-is/hail/pull/3624:19,Security,access,access,19,"Scala's reflective access warning refers to referencing a member of an anonymous class that is not also a member of the implemented int.rface / super-class. Because there is no concrete, non-anonymous type that contains that member, there is no normal means to access / call that member. Scala hacks around this issue by inserting a use of Java's reflection library to look up the member at run-time. The solution is simple: make the class named / non-anonymous/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3624
https://github.com/hail-is/hail/pull/3624:261,Security,access,access,261,"Scala's reflective access warning refers to referencing a member of an anonymous class that is not also a member of the implemented int.rface / super-class. Because there is no concrete, non-anonymous type that contains that member, there is no normal means to access / call that member. Scala hacks around this issue by inserting a use of Java's reflection library to look up the member at run-time. The solution is simple: make the class named / non-anonymous/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3624
https://github.com/hail-is/hail/pull/3624:417,Usability,simpl,simple,417,"Scala's reflective access warning refers to referencing a member of an anonymous class that is not also a member of the implemented int.rface / super-class. Because there is no concrete, non-anonymous type that contains that member, there is no normal means to access / call that member. Scala hacks around this issue by inserting a use of Java's reflection library to look up the member at run-time. The solution is simple: make the class named / non-anonymous/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3624
https://github.com/hail-is/hail/pull/3626:123,Performance,load,load,123,"A small PR for you, @cseed. . If we are immediately looking at the rows or cols table of; a MatrixTable, then we need not load the entries in either case.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3626
https://github.com/hail-is/hail/pull/3627:61,Deployability,pipeline,pipeline,61,"This seems to save like 20 to 30 seconds out of a 2.5 minute pipeline, so like 1/6 to 1/5 saved. I'm testing this PR and the SplitMulti one (both separately and with changes combined) with:; ```; In [1]: import hail as hl; In [2]: hl.init(); In [3]: %%time ; ...: hl.split_multi_hts(hl.read_matrix_table('/Users/dking/projects/hail-data/profile.mt').select_entries('GT', 'AD', 'DP', 'GQ', 'PL'))._force_count_rows(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3627
https://github.com/hail-is/hail/pull/3627:101,Testability,test,testing,101,"This seems to save like 20 to 30 seconds out of a 2.5 minute pipeline, so like 1/6 to 1/5 saved. I'm testing this PR and the SplitMulti one (both separately and with changes combined) with:; ```; In [1]: import hail as hl; In [2]: hl.init(); In [3]: %%time ; ...: hl.split_multi_hts(hl.read_matrix_table('/Users/dking/projects/hail-data/profile.mt').select_entries('GT', 'AD', 'DP', 'GQ', 'PL'))._force_count_rows(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3627
https://github.com/hail-is/hail/pull/3636:246,Availability,error,error,246,"Depends on #3539. (first new commit is ""filled out block-sparse matrix support""). Third and final major PR to introduce block-sparse matrices. All block matrix operations are now supported apart from the following cases that are most likely user error (and even these can be forced by applying the new method `densify` first):; - Division between two block matrices.; - Multiplication by a scalar or broadcasted vector which includes an infinite or ``nan`` value.; - Division by a scalar or broadcasted vector which includes a zero, infinite or ``nan`` value.; - Division of a scalar or broadcasted vector by a block matrix.; - Exponentiation by a negative exponent. The following operations are newly supported:; - Addition and subtraction of block matrices, resulting in ""union"" of realized blocks.; - Addition and subtraction of a scalar or broadcasted vector, resulting in a block-dense matrix.; - Slicing, and more generally row/column filtering, resulting in a block-dense matrix. New infrastructure includes:; - `supersetPartitions` in RichRDD, analogue of `subsetPartitions`, used to densify/realizeBlocks; - `BlockMatrixUnionOpRDD`, so far applied for `+` and `-` but able to support more general ops. Throughout I've aimed to support the block-sparse case while minimizing overhead on the more common block-dense case, particularly with `maybeSparse = None` when block-dense. Docs and tests updated accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3636
https://github.com/hail-is/hail/pull/3636:1401,Deployability,update,updated,1401,"Depends on #3539. (first new commit is ""filled out block-sparse matrix support""). Third and final major PR to introduce block-sparse matrices. All block matrix operations are now supported apart from the following cases that are most likely user error (and even these can be forced by applying the new method `densify` first):; - Division between two block matrices.; - Multiplication by a scalar or broadcasted vector which includes an infinite or ``nan`` value.; - Division by a scalar or broadcasted vector which includes a zero, infinite or ``nan`` value.; - Division of a scalar or broadcasted vector by a block matrix.; - Exponentiation by a negative exponent. The following operations are newly supported:; - Addition and subtraction of block matrices, resulting in ""union"" of realized blocks.; - Addition and subtraction of a scalar or broadcasted vector, resulting in a block-dense matrix.; - Slicing, and more generally row/column filtering, resulting in a block-dense matrix. New infrastructure includes:; - `supersetPartitions` in RichRDD, analogue of `subsetPartitions`, used to densify/realizeBlocks; - `BlockMatrixUnionOpRDD`, so far applied for `+` and `-` but able to support more general ops. Throughout I've aimed to support the block-sparse case while minimizing overhead on the more common block-dense case, particularly with `maybeSparse = None` when block-dense. Docs and tests updated accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3636
https://github.com/hail-is/hail/pull/3636:0,Integrability,Depend,Depends,0,"Depends on #3539. (first new commit is ""filled out block-sparse matrix support""). Third and final major PR to introduce block-sparse matrices. All block matrix operations are now supported apart from the following cases that are most likely user error (and even these can be forced by applying the new method `densify` first):; - Division between two block matrices.; - Multiplication by a scalar or broadcasted vector which includes an infinite or ``nan`` value.; - Division by a scalar or broadcasted vector which includes a zero, infinite or ``nan`` value.; - Division of a scalar or broadcasted vector by a block matrix.; - Exponentiation by a negative exponent. The following operations are newly supported:; - Addition and subtraction of block matrices, resulting in ""union"" of realized blocks.; - Addition and subtraction of a scalar or broadcasted vector, resulting in a block-dense matrix.; - Slicing, and more generally row/column filtering, resulting in a block-dense matrix. New infrastructure includes:; - `supersetPartitions` in RichRDD, analogue of `subsetPartitions`, used to densify/realizeBlocks; - `BlockMatrixUnionOpRDD`, so far applied for `+` and `-` but able to support more general ops. Throughout I've aimed to support the block-sparse case while minimizing overhead on the more common block-dense case, particularly with `maybeSparse = None` when block-dense. Docs and tests updated accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3636
https://github.com/hail-is/hail/pull/3636:1395,Testability,test,tests,1395,"Depends on #3539. (first new commit is ""filled out block-sparse matrix support""). Third and final major PR to introduce block-sparse matrices. All block matrix operations are now supported apart from the following cases that are most likely user error (and even these can be forced by applying the new method `densify` first):; - Division between two block matrices.; - Multiplication by a scalar or broadcasted vector which includes an infinite or ``nan`` value.; - Division by a scalar or broadcasted vector which includes a zero, infinite or ``nan`` value.; - Division of a scalar or broadcasted vector by a block matrix.; - Exponentiation by a negative exponent. The following operations are newly supported:; - Addition and subtraction of block matrices, resulting in ""union"" of realized blocks.; - Addition and subtraction of a scalar or broadcasted vector, resulting in a block-dense matrix.; - Slicing, and more generally row/column filtering, resulting in a block-dense matrix. New infrastructure includes:; - `supersetPartitions` in RichRDD, analogue of `subsetPartitions`, used to densify/realizeBlocks; - `BlockMatrixUnionOpRDD`, so far applied for `+` and `-` but able to support more general ops. Throughout I've aimed to support the block-sparse case while minimizing overhead on the more common block-dense case, particularly with `maybeSparse = None` when block-dense. Docs and tests updated accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3636
https://github.com/hail-is/hail/issues/3639:749,Testability,Assert,AssertionError,749,"table1:; ```; 'Table{global:+Struct{},key:[s],row:Struct{s:String,PC1:Float64,PC2:Float64,PC3:Float64,PC4:Float64,PC5:Float64,PC6:Float64,PC7:Float64,PC8:Float64,PC9:Float64,PC10:Float64,PC11:Float64,PC12:Float64,PC13:Float64,PC14:Float64,PC15:Float64,PC16:Float64,PC17:Float64,PC18:Float64,PC19:Float64,PC20:Float64,source:String}}'; ```. table2:; ```; 'Table{global:Struct{},key:[s],row:Struct{s:String,PC1:Float64,PC2:Float64,PC3:Float64,PC4:Float64,PC5:Float64,PC6:Float64,PC7:Float64,PC8:Float64,PC9:Float64,PC10:Float64,PC11:Float64,PC12:Float64,PC13:Float64,PC14:Float64,PC15:Float64,PC16:Float64,PC17:Float64,PC18:Float64,PC19:Float64,PC20:Float64,source:String}}'; ```. `table1.union(table2)` fails with:; ```; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.TableUnion.<init>(Relational.scala:1996); at is.hail.table.Table.union(Table.scala:947); at is.hail.table.Table.union(Table.scala:945); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3639
https://github.com/hail-is/hail/issues/3639:765,Testability,assert,assertion,765,"table1:; ```; 'Table{global:+Struct{},key:[s],row:Struct{s:String,PC1:Float64,PC2:Float64,PC3:Float64,PC4:Float64,PC5:Float64,PC6:Float64,PC7:Float64,PC8:Float64,PC9:Float64,PC10:Float64,PC11:Float64,PC12:Float64,PC13:Float64,PC14:Float64,PC15:Float64,PC16:Float64,PC17:Float64,PC18:Float64,PC19:Float64,PC20:Float64,source:String}}'; ```. table2:; ```; 'Table{global:Struct{},key:[s],row:Struct{s:String,PC1:Float64,PC2:Float64,PC3:Float64,PC4:Float64,PC5:Float64,PC6:Float64,PC7:Float64,PC8:Float64,PC9:Float64,PC10:Float64,PC11:Float64,PC12:Float64,PC13:Float64,PC14:Float64,PC15:Float64,PC16:Float64,PC17:Float64,PC18:Float64,PC19:Float64,PC20:Float64,source:String}}'; ```. `table1.union(table2)` fails with:; ```; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.TableUnion.<init>(Relational.scala:1996); at is.hail.table.Table.union(Table.scala:947); at is.hail.table.Table.union(Table.scala:945); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3639
https://github.com/hail-is/hail/issues/3639:800,Testability,assert,assert,800,"table1:; ```; 'Table{global:+Struct{},key:[s],row:Struct{s:String,PC1:Float64,PC2:Float64,PC3:Float64,PC4:Float64,PC5:Float64,PC6:Float64,PC7:Float64,PC8:Float64,PC9:Float64,PC10:Float64,PC11:Float64,PC12:Float64,PC13:Float64,PC14:Float64,PC15:Float64,PC16:Float64,PC17:Float64,PC18:Float64,PC19:Float64,PC20:Float64,source:String}}'; ```. table2:; ```; 'Table{global:Struct{},key:[s],row:Struct{s:String,PC1:Float64,PC2:Float64,PC3:Float64,PC4:Float64,PC5:Float64,PC6:Float64,PC7:Float64,PC8:Float64,PC9:Float64,PC10:Float64,PC11:Float64,PC12:Float64,PC13:Float64,PC14:Float64,PC15:Float64,PC16:Float64,PC17:Float64,PC18:Float64,PC19:Float64,PC20:Float64,source:String}}'; ```. `table1.union(table2)` fails with:; ```; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.TableUnion.<init>(Relational.scala:1996); at is.hail.table.Table.union(Table.scala:947); at is.hail.table.Table.union(Table.scala:945); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3639
https://github.com/hail-is/hail/issues/3648:141,Testability,test,testArrayRange,141,"hl.range behaves incorrectly when end is after start. For example:. ```; >>> hl.range(-9, -10, 3).value; [-9]; ```. The cases considered by `testArrayRange` should be expanded. (end <= start, negative step, step bigger than difference between start and end.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3648
https://github.com/hail-is/hail/pull/3649:30,Integrability,interface,interface,30,Also:; - expand assertEvalsTo interface; - typeCheck expected and result in assertEvalsTo; - fix hist expected's types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3649
https://github.com/hail-is/hail/pull/3649:16,Testability,assert,assertEvalsTo,16,Also:; - expand assertEvalsTo interface; - typeCheck expected and result in assertEvalsTo; - fix hist expected's types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3649
https://github.com/hail-is/hail/pull/3649:76,Testability,assert,assertEvalsTo,76,Also:; - expand assertEvalsTo interface; - typeCheck expected and result in assertEvalsTo; - fix hist expected's types,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3649
https://github.com/hail-is/hail/issues/3653:198,Availability,failure,failure,198,"`hl.eval_expr(hl.literal([1,2,3])/hl.literal([1,2]))`. ```; FatalError: HailException: array index out of bounds: 2 / 2. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 20 times, most recent failure: Lost task 0.19 in stage 36.0 (TID 55, exomes-w-1.c.broad-mpg-gnomad.internal, executor 3): is.hail.utils.HailException: array index out of bounds: 2 / 2; 	at is.hail.codegen.generated.C166.apply(Unknown Source); 	at is.hail.codegen.generated.C166.apply(Unknown Source); 	at is.hail.expr.TableMapRows$$anonfun$55$$anonfun$apply$29.apply(Relational.scala:1877); 	at is.hail.expr.TableMapRows$$anonfun$55$$anonfun$apply$29.apply(Relational.scala:1872); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:257,Availability,failure,failure,257,"`hl.eval_expr(hl.literal([1,2,3])/hl.literal([1,2]))`. ```; FatalError: HailException: array index out of bounds: 2 / 2. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 20 times, most recent failure: Lost task 0.19 in stage 36.0 (TID 55, exomes-w-1.c.broad-mpg-gnomad.internal, executor 3): is.hail.utils.HailException: array index out of bounds: 2 / 2; 	at is.hail.codegen.generated.C166.apply(Unknown Source); 	at is.hail.codegen.generated.C166.apply(Unknown Source); 	at is.hail.expr.TableMapRows$$anonfun$55$$anonfun$apply$29.apply(Relational.scala:1877); 	at is.hail.expr.TableMapRows$$anonfun$55$$anonfun$apply$29.apply(Relational.scala:1872); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:8048,Availability,Error,Error,8048,xt(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-425cc84a9997; Error summary: HailException: array index out of bounds: 2 / 2; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:2390,Energy Efficiency,schedul,scheduler,2390,ic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:2462,Energy Efficiency,schedul,scheduler,2462,.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:2827,Energy Efficiency,schedul,scheduler,2827,ollection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:2867,Energy Efficiency,schedul,scheduler,2867,at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:2966,Energy Efficiency,schedul,scheduler,2966,t scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:3064,Energy Efficiency,schedul,scheduler,3064,onfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:3318,Energy Efficiency,schedul,scheduler,3318,nfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:3399,Energy Efficiency,schedul,scheduler,3399,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:3505,Energy Efficiency,schedul,scheduler,3505,he.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:3655,Energy Efficiency,schedul,scheduler,3655,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:3744,Energy Efficiency,schedul,scheduler,3744,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:3842,Energy Efficiency,schedul,scheduler,3842,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.table.Table.collect(Table.scala:889); 	at is.hail.table.Table.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:3938,Energy Efficiency,schedul,scheduler,3938,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.table.Table.collect(Table.scala:889); 	at is.hail.table.Table.collectJSON(Table.scala:892); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:4103,Energy Efficiency,schedul,scheduler,4103,.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.table.Table.collect(Table.scala:889); 	at is.hail.table.Table.collectJSON(Table.scala:892); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.jav,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:7618,Energy Efficiency,schedul,scheduler,7618,xt(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-425cc84a9997; Error summary: HailException: array index out of bounds: 2 / 2; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:7690,Energy Efficiency,schedul,scheduler,7690,xt(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-425cc84a9997; Error summary: HailException: array index out of bounds: 2 / 2; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:2587,Performance,concurren,concurrent,2587,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:2672,Performance,concurren,concurrent,2672, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:7815,Performance,concurren,concurrent,7815,xt(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-425cc84a9997; Error summary: HailException: array index out of bounds: 2 / 2; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:7900,Performance,concurren,concurrent,7900,xt(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-425cc84a9997; Error summary: HailException: array index out of bounds: 2 / 2; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:177,Safety,abort,aborted,177,"`hl.eval_expr(hl.literal([1,2,3])/hl.literal([1,2]))`. ```; FatalError: HailException: array index out of bounds: 2 / 2. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 20 times, most recent failure: Lost task 0.19 in stage 36.0 (TID 55, exomes-w-1.c.broad-mpg-gnomad.internal, executor 3): is.hail.utils.HailException: array index out of bounds: 2 / 2; 	at is.hail.codegen.generated.C166.apply(Unknown Source); 	at is.hail.codegen.generated.C166.apply(Unknown Source); 	at is.hail.expr.TableMapRows$$anonfun$55$$anonfun$apply$29.apply(Relational.scala:1877); 	at is.hail.expr.TableMapRows$$anonfun$55$$anonfun$apply$29.apply(Relational.scala:1872); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:2998,Safety,abort,abortStage,2998,tractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:3096,Safety,abort,abortStage,3096,un$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/issues/3653:3341,Safety,abort,abortStage,3341,xt.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3653
https://github.com/hail-is/hail/pull/3655:100,Energy Efficiency,allocate,allocate,100,"This change is rather small actually. Instead of allocating a JVM-managed `Array[Byte]` we directly allocate memory with `Memory.malloc` which calls into `sun.misc.Unsafe`'s allocation functions. This gives us a non-managed (i.e. non-GC'ed) block of memory. We must free this memory. Regions created by `RVDContext`'s are handled by `ContextRDD`. `ContextRDD` uses Spark's `TaskContext.addTaskCompletionListener` to ensure memory is free'd when the task is finished. By virtue of returning pointers instead of offsets, this change converts every use of offset in the region value world to a use of a pointer. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3655
https://github.com/hail-is/hail/pull/3655:164,Safety,Unsafe,Unsafe,164,"This change is rather small actually. Instead of allocating a JVM-managed `Array[Byte]` we directly allocate memory with `Memory.malloc` which calls into `sun.misc.Unsafe`'s allocation functions. This gives us a non-managed (i.e. non-GC'ed) block of memory. We must free this memory. Regions created by `RVDContext`'s are handled by `ContextRDD`. `ContextRDD` uses Spark's `TaskContext.addTaskCompletionListener` to ensure memory is free'd when the task is finished. By virtue of returning pointers instead of offsets, this change converts every use of offset in the region value world to a use of a pointer. cc: @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3655
https://github.com/hail-is/hail/pull/3660:146,Energy Efficiency,allocate,allocates,146,"Relies on #3655. Since #3655 replaces offsets with true machine pointers, the IR can operate on values from arbitrary regions (although it always allocates new things in one region). This change exploits this to keep globals and columns in per-partition regions (rather than copying them per-row).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3660
https://github.com/hail-is/hail/pull/3667:0,Testability,Test,Tested,0,Tested with check.count = 1000. Will easily plug into read once prune fields goes in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3667
https://github.com/hail-is/hail/issues/3668:1766,Availability,Error,Error,1766,"heck.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/matrixtable.py in from_rows_table(cls, table, partition_key); 3107 elif list(table.key)[:len(partition_key)] != partition_key:; 3108 raise ValueError('partition_key must be a prefix of table key'); -> 3109 jmt = scala_object(Env.hail().variant, 'MatrixTable').fromRowsTable(table._jt, partition_key); 3110 return MatrixTable(jmt); 3111 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: ArrayIndexOutOfBoundsException: 0. Java stack trace:; java.lang.ArrayIndexOutOfBoundsException: 0; 	at org.apache.spark.sql.catalyst.expressions.GenericRow.get(rows.scala:173); 	at org.apache.spark.sql.Row$class.apply(Row.scala:163); 	at org.apache.spark.sql.catalyst.expressions.GenericRow.apply(rows.scala:165); 	at is.hail.annotations.Annotation$$anonfun$isSafe$1.apply$mcZI$sp(Annotation.scala:70); 	at is.hail.annotations.Annotation$$anonfun$isSafe$1.apply(Annotation.scala:70); 	at is.hail.annotations.Annotation$$anonfun$isSafe$1.apply(Annotation.scala:70); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3668
https://github.com/hail-is/hail/issues/3668:3730,Availability,Error,Error,3730,"' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: ArrayIndexOutOfBoundsException: 0. Java stack trace:; java.lang.ArrayIndexOutOfBoundsException: 0; 	at org.apache.spark.sql.catalyst.expressions.GenericRow.get(rows.scala:173); 	at org.apache.spark.sql.Row$class.apply(Row.scala:163); 	at org.apache.spark.sql.catalyst.expressions.GenericRow.apply(rows.scala:165); 	at is.hail.annotations.Annotation$$anonfun$isSafe$1.apply$mcZI$sp(Annotation.scala:70); 	at is.hail.annotations.Annotation$$anonfun$isSafe$1.apply(Annotation.scala:70); 	at is.hail.annotations.Annotation$$anonfun$isSafe$1.apply(Annotation.scala:70); 	at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38); 	at scala.collection.IndexedSeqOptimized$class.forall(IndexedSeqOptimized.scala:43); 	at scala.collection.mutable.ArrayOps$ofInt.forall(ArrayOps.scala:234); 	at is.hail.annotations.Annotation$.isSafe(Annotation.scala:70); 	at is.hail.annotations.BroadcastRow.<init>(BroadcastValue.scala:27); 	at is.hail.variant.MatrixTable$.fromRowsTable(MatrixTable.scala:462); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-1ea0ab823b1f; Error summary: ArrayIndexOutOfBoundsException: 0; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3668
https://github.com/hail-is/hail/issues/3668:739,Integrability,wrap,wrapper,739,"```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-2-84a33bbdd0f1> in <module>(); 1 pre_process_data(annotations_ht_path('genomes', 'frequencies'),; ----> 2 split_context_mt_path, processed_genomes_ht_path, True). /home/konradk/constraint_utils.py in pre_process_data(ht_path, split_context_mt_path, output_ht_path, overwrite); 126 output_ht_path: str, overwrite: bool = False) -> None:; 127 ht = hl.read_table(ht_path); --> 128 mt = hl.MatrixTable.from_rows_table(ht, partition_key='locus'); 129 mt.write(ht_path.replace('.ht', '.mt')); 130 mt = hl.read_matrix_table(ht_path.replace('.ht', '.mt')). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/matrixtable.py in from_rows_table(cls, table, partition_key); 3107 elif list(table.key)[:len(partition_key)] != partition_key:; 3108 raise ValueError('partition_key must be a prefix of table key'); -> 3109 jmt = scala_object(Env.hail().variant, 'MatrixTable').fromRowsTable(table._jt, partition_key); 3110 return MatrixTable(jmt); 3111 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: ArrayIndexOutOfBoundsException: 0. Java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3668
https://github.com/hail-is/hail/issues/3668:773,Integrability,wrap,wrapper,773,"```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-2-84a33bbdd0f1> in <module>(); 1 pre_process_data(annotations_ht_path('genomes', 'frequencies'),; ----> 2 split_context_mt_path, processed_genomes_ht_path, True). /home/konradk/constraint_utils.py in pre_process_data(ht_path, split_context_mt_path, output_ht_path, overwrite); 126 output_ht_path: str, overwrite: bool = False) -> None:; 127 ht = hl.read_table(ht_path); --> 128 mt = hl.MatrixTable.from_rows_table(ht, partition_key='locus'); 129 mt.write(ht_path.replace('.ht', '.mt')); 130 mt = hl.read_matrix_table(ht_path.replace('.ht', '.mt')). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/matrixtable.py in from_rows_table(cls, table, partition_key); 3107 elif list(table.key)[:len(partition_key)] != partition_key:; 3108 raise ValueError('partition_key must be a prefix of table key'); -> 3109 jmt = scala_object(Env.hail().variant, 'MatrixTable').fromRowsTable(table._jt, partition_key); 3110 return MatrixTable(jmt); 3111 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: ArrayIndexOutOfBoundsException: 0. Java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3668
https://github.com/hail-is/hail/issues/3668:942,Integrability,wrap,wrapper,942,"```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-2-84a33bbdd0f1> in <module>(); 1 pre_process_data(annotations_ht_path('genomes', 'frequencies'),; ----> 2 split_context_mt_path, processed_genomes_ht_path, True). /home/konradk/constraint_utils.py in pre_process_data(ht_path, split_context_mt_path, output_ht_path, overwrite); 126 output_ht_path: str, overwrite: bool = False) -> None:; 127 ht = hl.read_table(ht_path); --> 128 mt = hl.MatrixTable.from_rows_table(ht, partition_key='locus'); 129 mt.write(ht_path.replace('.ht', '.mt')); 130 mt = hl.read_matrix_table(ht_path.replace('.ht', '.mt')). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/matrixtable.py in from_rows_table(cls, table, partition_key); 3107 elif list(table.key)[:len(partition_key)] != partition_key:; 3108 raise ValueError('partition_key must be a prefix of table key'); -> 3109 jmt = scala_object(Env.hail().variant, 'MatrixTable').fromRowsTable(table._jt, partition_key); 3110 return MatrixTable(jmt); 3111 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: ArrayIndexOutOfBoundsException: 0. Java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3668
https://github.com/hail-is/hail/pull/3669:158,Safety,avoid,avoid,158,"I need to use in decoder builder which doesn't currently chunk struct construction, will use there after https://github.com/hail-is/hail/pull/3667 goes in to avoid conflicts. This introduces 1 change in behavior: code with estimated size > 100 will be put out in its own method, even it fits in one block. If I read it correctly, the old code would put any amount of code inline up to the method limit target, so several medium-sized structs would potentially blow out the method bytecode limit. Also fixed a bug in getChunkBounds where items on the boundary weren't counted in the size.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3669
https://github.com/hail-is/hail/pull/3672:257,Energy Efficiency,reduce,reduce,257,"Added to Python BlockMatrix:; - `fill` w/ test; - `sum` w/ test (useful for LD scores among other things). Added to Scala BlockMatrix:; - `fill`; - `apply` to factor out common motif in `fromBreezeMatrix`, `random`, `fill`; - `sum`, `rowSum`, `colSum` via `reduce`, `rowReduce`, `colReduce`. Added `maybeBlockRows` and `maybeBlockCols` to GridPartitioner, and changed `maybeSparse` to the more descriptive `maybeBlocks` to match. Regarding the implementations of `sum`, `rowSum`, `colSum`: I tested Breeze op `sum(a(::, *))` against Breeze matrix multiplication by a vector of ones at the block level, and found the latter to be four times faster on laptop (17s vs 4.5s for 1000 iterations). ```; @Test ; def sum() {; val a = BDM.rand[Double](4096, 4096); printTime {; var i = 0; while (i < 1000) {; //val c = breeze.linalg.sum(a(*, ::)); val b = BDV.ones[Double](4096); val c = a * b; i += 1; }; }; }; ```. However, I didn't implement `sum` over (say) rows using distributed multiply because the parallelism would be reduced from the number of blocks to the number of blockCols. Two other small changes:; - added a suggestion in the `from_entry_expr` doc based on seeing a user make this mistake.; - changed `random` to multiply the partition index by a large prime so that consecutive seeds don't produce the same blocks shifted over.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3672
https://github.com/hail-is/hail/pull/3672:1018,Energy Efficiency,reduce,reduced,1018,"Added to Python BlockMatrix:; - `fill` w/ test; - `sum` w/ test (useful for LD scores among other things). Added to Scala BlockMatrix:; - `fill`; - `apply` to factor out common motif in `fromBreezeMatrix`, `random`, `fill`; - `sum`, `rowSum`, `colSum` via `reduce`, `rowReduce`, `colReduce`. Added `maybeBlockRows` and `maybeBlockCols` to GridPartitioner, and changed `maybeSparse` to the more descriptive `maybeBlocks` to match. Regarding the implementations of `sum`, `rowSum`, `colSum`: I tested Breeze op `sum(a(::, *))` against Breeze matrix multiplication by a vector of ones at the block level, and found the latter to be four times faster on laptop (17s vs 4.5s for 1000 iterations). ```; @Test ; def sum() {; val a = BDM.rand[Double](4096, 4096); printTime {; var i = 0; while (i < 1000) {; //val c = breeze.linalg.sum(a(*, ::)); val b = BDV.ones[Double](4096); val c = a * b; i += 1; }; }; }; ```. However, I didn't implement `sum` over (say) rows using distributed multiply because the parallelism would be reduced from the number of blocks to the number of blockCols. Two other small changes:; - added a suggestion in the `from_entry_expr` doc based on seeing a user make this mistake.; - changed `random` to multiply the partition index by a large prime so that consecutive seeds don't produce the same blocks shifted over.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3672
https://github.com/hail-is/hail/pull/3672:42,Testability,test,test,42,"Added to Python BlockMatrix:; - `fill` w/ test; - `sum` w/ test (useful for LD scores among other things). Added to Scala BlockMatrix:; - `fill`; - `apply` to factor out common motif in `fromBreezeMatrix`, `random`, `fill`; - `sum`, `rowSum`, `colSum` via `reduce`, `rowReduce`, `colReduce`. Added `maybeBlockRows` and `maybeBlockCols` to GridPartitioner, and changed `maybeSparse` to the more descriptive `maybeBlocks` to match. Regarding the implementations of `sum`, `rowSum`, `colSum`: I tested Breeze op `sum(a(::, *))` against Breeze matrix multiplication by a vector of ones at the block level, and found the latter to be four times faster on laptop (17s vs 4.5s for 1000 iterations). ```; @Test ; def sum() {; val a = BDM.rand[Double](4096, 4096); printTime {; var i = 0; while (i < 1000) {; //val c = breeze.linalg.sum(a(*, ::)); val b = BDV.ones[Double](4096); val c = a * b; i += 1; }; }; }; ```. However, I didn't implement `sum` over (say) rows using distributed multiply because the parallelism would be reduced from the number of blocks to the number of blockCols. Two other small changes:; - added a suggestion in the `from_entry_expr` doc based on seeing a user make this mistake.; - changed `random` to multiply the partition index by a large prime so that consecutive seeds don't produce the same blocks shifted over.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3672
https://github.com/hail-is/hail/pull/3672:59,Testability,test,test,59,"Added to Python BlockMatrix:; - `fill` w/ test; - `sum` w/ test (useful for LD scores among other things). Added to Scala BlockMatrix:; - `fill`; - `apply` to factor out common motif in `fromBreezeMatrix`, `random`, `fill`; - `sum`, `rowSum`, `colSum` via `reduce`, `rowReduce`, `colReduce`. Added `maybeBlockRows` and `maybeBlockCols` to GridPartitioner, and changed `maybeSparse` to the more descriptive `maybeBlocks` to match. Regarding the implementations of `sum`, `rowSum`, `colSum`: I tested Breeze op `sum(a(::, *))` against Breeze matrix multiplication by a vector of ones at the block level, and found the latter to be four times faster on laptop (17s vs 4.5s for 1000 iterations). ```; @Test ; def sum() {; val a = BDM.rand[Double](4096, 4096); printTime {; var i = 0; while (i < 1000) {; //val c = breeze.linalg.sum(a(*, ::)); val b = BDV.ones[Double](4096); val c = a * b; i += 1; }; }; }; ```. However, I didn't implement `sum` over (say) rows using distributed multiply because the parallelism would be reduced from the number of blocks to the number of blockCols. Two other small changes:; - added a suggestion in the `from_entry_expr` doc based on seeing a user make this mistake.; - changed `random` to multiply the partition index by a large prime so that consecutive seeds don't produce the same blocks shifted over.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3672
https://github.com/hail-is/hail/pull/3672:492,Testability,test,tested,492,"Added to Python BlockMatrix:; - `fill` w/ test; - `sum` w/ test (useful for LD scores among other things). Added to Scala BlockMatrix:; - `fill`; - `apply` to factor out common motif in `fromBreezeMatrix`, `random`, `fill`; - `sum`, `rowSum`, `colSum` via `reduce`, `rowReduce`, `colReduce`. Added `maybeBlockRows` and `maybeBlockCols` to GridPartitioner, and changed `maybeSparse` to the more descriptive `maybeBlocks` to match. Regarding the implementations of `sum`, `rowSum`, `colSum`: I tested Breeze op `sum(a(::, *))` against Breeze matrix multiplication by a vector of ones at the block level, and found the latter to be four times faster on laptop (17s vs 4.5s for 1000 iterations). ```; @Test ; def sum() {; val a = BDM.rand[Double](4096, 4096); printTime {; var i = 0; while (i < 1000) {; //val c = breeze.linalg.sum(a(*, ::)); val b = BDV.ones[Double](4096); val c = a * b; i += 1; }; }; }; ```. However, I didn't implement `sum` over (say) rows using distributed multiply because the parallelism would be reduced from the number of blocks to the number of blockCols. Two other small changes:; - added a suggestion in the `from_entry_expr` doc based on seeing a user make this mistake.; - changed `random` to multiply the partition index by a large prime so that consecutive seeds don't produce the same blocks shifted over.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3672
https://github.com/hail-is/hail/pull/3672:698,Testability,Test,Test,698,"Added to Python BlockMatrix:; - `fill` w/ test; - `sum` w/ test (useful for LD scores among other things). Added to Scala BlockMatrix:; - `fill`; - `apply` to factor out common motif in `fromBreezeMatrix`, `random`, `fill`; - `sum`, `rowSum`, `colSum` via `reduce`, `rowReduce`, `colReduce`. Added `maybeBlockRows` and `maybeBlockCols` to GridPartitioner, and changed `maybeSparse` to the more descriptive `maybeBlocks` to match. Regarding the implementations of `sum`, `rowSum`, `colSum`: I tested Breeze op `sum(a(::, *))` against Breeze matrix multiplication by a vector of ones at the block level, and found the latter to be four times faster on laptop (17s vs 4.5s for 1000 iterations). ```; @Test ; def sum() {; val a = BDM.rand[Double](4096, 4096); printTime {; var i = 0; while (i < 1000) {; //val c = breeze.linalg.sum(a(*, ::)); val b = BDV.ones[Double](4096); val c = a * b; i += 1; }; }; }; ```. However, I didn't implement `sum` over (say) rows using distributed multiply because the parallelism would be reduced from the number of blocks to the number of blockCols. Two other small changes:; - added a suggestion in the `from_entry_expr` doc based on seeing a user make this mistake.; - changed `random` to multiply the partition index by a large prime so that consecutive seeds don't produce the same blocks shifted over.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3672
https://github.com/hail-is/hail/issues/3673:76,Availability,error,error,76,"I'd think this: `ht.transmute(ref=ht.alleles[0], alt=ht.alleles[1])` should error out since it's modifying a key field, but it doesn't seem to.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3673
https://github.com/hail-is/hail/issues/3685:175,Performance,load,loads,175,"Konrad is seeing bookend problems related to table joins with different data ranges. For example, trying to left join a table with only chr22 and a table with all chromosomes loads all chr1-21 partitions with the first partition of the left. This is a problem for any type of join, but we can easily optimize for left/right by removing partitions from the opposite table that cannot possibly overlap any partition of the result.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3685
https://github.com/hail-is/hail/issues/3685:300,Performance,optimiz,optimize,300,"Konrad is seeing bookend problems related to table joins with different data ranges. For example, trying to left join a table with only chr22 and a table with all chromosomes loads all chr1-21 partitions with the first partition of the left. This is a problem for any type of join, but we can easily optimize for left/right by removing partitions from the opposite table that cannot possibly overlap any partition of the result.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3685
https://github.com/hail-is/hail/issues/3690:1888,Integrability,wrap,wrapper,1888,"oing from `x = ht.aggregate(hl.agg.counter(ht.row_field))` - that once `row_field` has any missing data, you can't `hl.literal(x)` without some more work. ```; Traceback (most recent call last):; File ""/tmp/79408a38ad73483d9f4c5afc91adbb19/hail-devel-72603c5e840e.zip/hail/expr/functions.py"", line 98, in literal; dtype._typecheck(x); File ""/tmp/79408a38ad73483d9f4c5afc91adbb19/hail-devel-72603c5e840e.zip/hail/expr/types.py"", line 641, in _typecheck; self.key_type._typecheck(k); File ""/tmp/79408a38ad73483d9f4c5afc91adbb19/hail-devel-72603c5e840e.zip/hail/expr/types.py"", line 730, in _typecheck; t._typecheck(annotation.get(f)); File ""/tmp/79408a38ad73483d9f4c5afc91adbb19/hail-devel-72603c5e840e.zip/hail/expr/types.py"", line 231, in _typecheck; if not self.min_value <= annotation <= self.max_value:; TypeError: '<=' not supported between instances of 'int' and 'NoneType'. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/tmp/79408a38ad73483d9f4c5afc91adbb19/pyscripts_gYULqI.zip/gnomad_hail/utils/slack.py"", line 95, in try_slack; func(*args); File ""/tmp/79408a38ad73483d9f4c5afc91adbb19/constraint2.py"", line 70, in main; coverage_ht = get_proportion_observed_by_coverage(rare_autosomes_exome_ht, autosomes_context_ht, mutation_ht, False); File ""/tmp/79408a38ad73483d9f4c5afc91adbb19/constraint2.py"", line 32, in get_proportion_observed_by_coverage; ht = ht.annotate(possible_variants=hl.literal(possible_variants.variant_count, dtype=variant_counts_dtype)[ht.key],; File ""/tmp/79408a38ad73483d9f4c5afc91adbb19/hail-devel-72603c5e840e.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/tmp/79408a38ad73483d9f4c5afc91adbb19/hail-devel-72603c5e840e.zip/hail/expr/functions.py"", line 101, in literal; .format(dtype)) from e; TypeError: 'literal': object did not match the passed type 'dict<struct{context: str, ref: str, alt: str, methylation_level: int32, exome_coverage: int32}, int64>'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3690
https://github.com/hail-is/hail/pull/3694:56,Testability,log,logic,56,"Fixes #3692. I also factored out some of the TableKeyBy logic and tweaked it. If a table is already backed by an OrderedRVD with key (""a"", ""b"", ""c"") and partition key (""a""), and we do a keyBy([""a"", ""b""], [""a"", ""b""]), then no work needs to be done, but we want the OrderedRVDType of the underlying OrderedRVD to remember the stronger invariants it satisfies. That way if we later keyBy([""a"", ""b"", ""c""], [""a""]), we don't have to do any work.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3694
https://github.com/hail-is/hail/pull/3695:25,Modifiability,refactor,refactored,25,- Fixes #3691 ; - I also refactored the python tests to move all of the import/export tests to test_io.py,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3695
https://github.com/hail-is/hail/pull/3695:47,Testability,test,tests,47,- Fixes #3691 ; - I also refactored the python tests to move all of the import/export tests to test_io.py,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3695
https://github.com/hail-is/hail/pull/3695:86,Testability,test,tests,86,- Fixes #3691 ; - I also refactored the python tests to move all of the import/export tests to test_io.py,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3695
https://github.com/hail-is/hail/issues/3696:208,Availability,error,error,208,"### Hail version:; `6d4d50458`; ### What you did:; ```; label_expr = (hl.case(missing_false=True); .when(ht[tp_col] & ~ht[fp_col], ""TP""); .when(ht[fp_col] & ~ht[tp_col], ""FP)); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Im getting an error that reads,; Traceback (most recent call last):; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 567, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/pyscripts_SuuLaL.zip/gnomad_hail/utils/slack.py"", line 112, in try_slack; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/pyscripts_SuuLaL.zip/gnomad_hail/utils/slack.py"", line 95, in try_slack; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 508, in main; run_hash = train_rf(data_type, args) if args.train_rf else args.run_hash; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 422, in train_rf; fp_to_tp=args.fp_to_tp); File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 240, in sample_rf_training_examples; train_col: train_expr}); File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/table.py"", line 720, in annotate; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/utils/misc.py"", line 336, in get_annotate_exprs; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/utils/misc.py"", line 336, in <dictcomp>; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/expr/expressions/base_expression.py"", line 102, in to_expr; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/expr/expressions/base_expression.py"", line 93, in impute_type; hail.expr.expressions.base_expression.ExpressionException: Hail cannot automatically impute type of <class 'hail.expr.builders.CaseBuilder'>: <hail.expr.builders.CaseBuilder object at 0x7f2ad03c74e0>; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3696
https://github.com/hail-is/hail/issues/3696:288,Availability,error,error,288,"### Hail version:; `6d4d50458`; ### What you did:; ```; label_expr = (hl.case(missing_false=True); .when(ht[tp_col] & ~ht[fp_col], ""TP""); .when(ht[fp_col] & ~ht[tp_col], ""FP)); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Im getting an error that reads,; Traceback (most recent call last):; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 567, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/pyscripts_SuuLaL.zip/gnomad_hail/utils/slack.py"", line 112, in try_slack; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/pyscripts_SuuLaL.zip/gnomad_hail/utils/slack.py"", line 95, in try_slack; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 508, in main; run_hash = train_rf(data_type, args) if args.train_rf else args.run_hash; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 422, in train_rf; fp_to_tp=args.fp_to_tp); File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 240, in sample_rf_training_examples; train_col: train_expr}); File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/table.py"", line 720, in annotate; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/utils/misc.py"", line 336, in get_annotate_exprs; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/utils/misc.py"", line 336, in <dictcomp>; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/expr/expressions/base_expression.py"", line 102, in to_expr; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/expr/expressions/base_expression.py"", line 93, in impute_type; hail.expr.expressions.base_expression.ExpressionException: Hail cannot automatically impute type of <class 'hail.expr.builders.CaseBuilder'>: <hail.expr.builders.CaseBuilder object at 0x7f2ad03c74e0>; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3696
https://github.com/hail-is/hail/issues/3696:214,Integrability,message,messages,214,"### Hail version:; `6d4d50458`; ### What you did:; ```; label_expr = (hl.case(missing_false=True); .when(ht[tp_col] & ~ht[fp_col], ""TP""); .when(ht[fp_col] & ~ht[tp_col], ""FP)); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Im getting an error that reads,; Traceback (most recent call last):; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 567, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/pyscripts_SuuLaL.zip/gnomad_hail/utils/slack.py"", line 112, in try_slack; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/pyscripts_SuuLaL.zip/gnomad_hail/utils/slack.py"", line 95, in try_slack; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 508, in main; run_hash = train_rf(data_type, args) if args.train_rf else args.run_hash; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 422, in train_rf; fp_to_tp=args.fp_to_tp); File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/variantqc.py"", line 240, in sample_rf_training_examples; train_col: train_expr}); File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/table.py"", line 720, in annotate; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/utils/misc.py"", line 336, in get_annotate_exprs; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/utils/misc.py"", line 336, in <dictcomp>; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/expr/expressions/base_expression.py"", line 102, in to_expr; File ""/tmp/99ecf728f92847f2bfddb1e75cbd0a4e/hail-6d4d50458.zip/hail/expr/expressions/base_expression.py"", line 93, in impute_type; hail.expr.expressions.base_expression.ExpressionException: Hail cannot automatically impute type of <class 'hail.expr.builders.CaseBuilder'>: <hail.expr.builders.CaseBuilder object at 0x7f2ad03c74e0>; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3696
https://github.com/hail-is/hail/pull/3698:30,Deployability,pipeline,pipeline,30,"Caitlin uses these three in a pipeline she's trying to run, so this should get her a little closer to being 100% IR. I added three new IR pieces:; - `str` on ints and floats (implemented using Scala string method); - `StringSlice` IR node corresponding to java.lang.String.substring (the python-like negative indexing is implemented in the function registry); - `StringLength` IR node to calculate number of bytes present in the String (in particular, this incorrectly reports length 4 for the poop emoji). I added two suites to test these new IR nodes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3698
https://github.com/hail-is/hail/pull/3698:529,Testability,test,test,529,"Caitlin uses these three in a pipeline she's trying to run, so this should get her a little closer to being 100% IR. I added three new IR pieces:; - `str` on ints and floats (implemented using Scala string method); - `StringSlice` IR node corresponding to java.lang.String.substring (the python-like negative indexing is implemented in the function registry); - `StringLength` IR node to calculate number of bytes present in the String (in particular, this incorrectly reports length 4 for the poop emoji). I added two suites to test these new IR nodes.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3698
https://github.com/hail-is/hail/issues/3699:151,Availability,error,error,151,"### Hail version:; 6195693b3; ### What you did:; ```mt.filter_cols(hl.literal(set(max_downsample1)).contains(mt.col_idx))```. ### What went wrong (all error messages here, including the full java stack trace):; ```Hail cannot automatically impute type of <class 'numpy.int64'>: 129```. Hail should understand the usual numpy int types.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3699
https://github.com/hail-is/hail/issues/3699:157,Integrability,message,messages,157,"### Hail version:; 6195693b3; ### What you did:; ```mt.filter_cols(hl.literal(set(max_downsample1)).contains(mt.col_idx))```. ### What went wrong (all error messages here, including the full java stack trace):; ```Hail cannot automatically impute type of <class 'numpy.int64'>: 129```. Hail should understand the usual numpy int types.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3699
https://github.com/hail-is/hail/pull/3703:105,Testability,test,tested,105,"I forgot to add the restructured text files to the new doctest framework, so right now they aren't being tested. This fix will test the python commands in all of the `.rst` files in `python/hail`. Also, I removed the obsolete doctest directive strings `.. doctest::` and `.. testsetup::`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3703
https://github.com/hail-is/hail/pull/3703:127,Testability,test,test,127,"I forgot to add the restructured text files to the new doctest framework, so right now they aren't being tested. This fix will test the python commands in all of the `.rst` files in `python/hail`. Also, I removed the obsolete doctest directive strings `.. doctest::` and `.. testsetup::`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3703
https://github.com/hail-is/hail/pull/3703:275,Testability,test,testsetup,275,"I forgot to add the restructured text files to the new doctest framework, so right now they aren't being tested. This fix will test the python commands in all of the `.rst` files in `python/hail`. Also, I removed the obsolete doctest directive strings `.. doctest::` and `.. testsetup::`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3703
https://github.com/hail-is/hail/pull/3704:901,Availability,error,error,901,"- added `keep_higher_maf` option (`true` by default) to `ld_prune` to prefer to keep higher MAF variants in the global (MIS) stage.; - improved the `ld_prune` flow to reduce duplicated work; - set BlockMatrix.entries to set `i` and `j` as key fields and improved its doc; - corrected references to standard deviation that are actually n times standard deviation, i.e. centered length; - switched `computeCoverByUpperTriangularBlocks` to use the newer`rowIntervalsBlocks` rather than `rectanglesBlocks` directly. I've tested `keep_higher_maf` in notebooks, will add a test of MAF soon and then assign. @danking rather than:; ```; def tie_breaker(l, r):; return hl.cond(l.twice_maf > r.twice_maf,; -1,; hl.cond(l.twice_maf < r.twice_maf,; 1,; 0)); ```; I'd prefer:; ```; def tie_breaker(l, r):; return hl.signum(r.twice_maf - l.twice_maf); ```; I'm having trouble figuring out why the latter throws the error below. Your `tie_breaker` code looks like it should work with Int32Expressions. Any idea what's going on?; ```; FatalError: ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.Row. Java stack trace:; java.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.Row; 	at is.hail.codegen.generated.C46.apply(Unknown Source); 	at is.hail.codegen.generated.C46.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/pull/3704:167,Energy Efficiency,reduce,reduce,167,"- added `keep_higher_maf` option (`true` by default) to `ld_prune` to prefer to keep higher MAF variants in the global (MIS) stage.; - improved the `ld_prune` flow to reduce duplicated work; - set BlockMatrix.entries to set `i` and `j` as key fields and improved its doc; - corrected references to standard deviation that are actually n times standard deviation, i.e. centered length; - switched `computeCoverByUpperTriangularBlocks` to use the newer`rowIntervalsBlocks` rather than `rectanglesBlocks` directly. I've tested `keep_higher_maf` in notebooks, will add a test of MAF soon and then assign. @danking rather than:; ```; def tie_breaker(l, r):; return hl.cond(l.twice_maf > r.twice_maf,; -1,; hl.cond(l.twice_maf < r.twice_maf,; 1,; 0)); ```; I'd prefer:; ```; def tie_breaker(l, r):; return hl.signum(r.twice_maf - l.twice_maf); ```; I'm having trouble figuring out why the latter throws the error below. Your `tie_breaker` code looks like it should work with Int32Expressions. Any idea what's going on?; ```; FatalError: ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.Row. Java stack trace:; java.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.Row; 	at is.hail.codegen.generated.C46.apply(Unknown Source); 	at is.hail.codegen.generated.C46.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/pull/3704:2339,Security,Hash,HashMap,2339,$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:101); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:100); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:100); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:86); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at p,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/pull/3704:2372,Security,Hash,HashMap,2372,edValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:101); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:100); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:100); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:86); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.Abstrac,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/pull/3704:2420,Security,Hash,HashMap,2420,runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:101); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:100); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:100); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:86); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.com,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/pull/3704:2453,Security,Hash,HashMap,2453,1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:101); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:100); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:100); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:86); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.ex,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/pull/3704:2501,Security,Hash,HashTable,2501,is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:101); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:100); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:100); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:86); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/pull/3704:2530,Security,Hash,HashTable,2530,$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:101); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:100); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:100); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:86); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnectio,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/pull/3704:2581,Security,Hash,HashMap,2581,xpr.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:101); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:100); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:100); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:86); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/pull/3704:2602,Security,Hash,HashMap,2602,.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:101); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:100); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:100); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:86); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/pull/3704:2650,Security,Hash,HashMap,2650,.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:101); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:100); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:100); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:86); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/pull/3704:2666,Security,Hash,HashMap,2666,.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16); 	at is.hail.utils.BinaryHeap.is$hail$utils$BinaryHeap$$bubbleUp(BinaryHeap.scala:161); 	at is.hail.utils.BinaryHeap.insert(BinaryHeap.scala:40); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:101); 	at is.hail.utils.Graph$$anonfun$maximalIndependentSet$1.apply(Graph.scala:100); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:100); 	at is.hail.utils.Graph$.maximalIndependentSet(Graph.scala:86); 	at is.hail.utils.Graph.maximalIndependentSet(Graph.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/pull/3704:517,Testability,test,tested,517,"- added `keep_higher_maf` option (`true` by default) to `ld_prune` to prefer to keep higher MAF variants in the global (MIS) stage.; - improved the `ld_prune` flow to reduce duplicated work; - set BlockMatrix.entries to set `i` and `j` as key fields and improved its doc; - corrected references to standard deviation that are actually n times standard deviation, i.e. centered length; - switched `computeCoverByUpperTriangularBlocks` to use the newer`rowIntervalsBlocks` rather than `rectanglesBlocks` directly. I've tested `keep_higher_maf` in notebooks, will add a test of MAF soon and then assign. @danking rather than:; ```; def tie_breaker(l, r):; return hl.cond(l.twice_maf > r.twice_maf,; -1,; hl.cond(l.twice_maf < r.twice_maf,; 1,; 0)); ```; I'd prefer:; ```; def tie_breaker(l, r):; return hl.signum(r.twice_maf - l.twice_maf); ```; I'm having trouble figuring out why the latter throws the error below. Your `tie_breaker` code looks like it should work with Int32Expressions. Any idea what's going on?; ```; FatalError: ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.Row. Java stack trace:; java.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.Row; 	at is.hail.codegen.generated.C46.apply(Unknown Source); 	at is.hail.codegen.generated.C46.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/pull/3704:567,Testability,test,test,567,"- added `keep_higher_maf` option (`true` by default) to `ld_prune` to prefer to keep higher MAF variants in the global (MIS) stage.; - improved the `ld_prune` flow to reduce duplicated work; - set BlockMatrix.entries to set `i` and `j` as key fields and improved its doc; - corrected references to standard deviation that are actually n times standard deviation, i.e. centered length; - switched `computeCoverByUpperTriangularBlocks` to use the newer`rowIntervalsBlocks` rather than `rectanglesBlocks` directly. I've tested `keep_higher_maf` in notebooks, will add a test of MAF soon and then assign. @danking rather than:; ```; def tie_breaker(l, r):; return hl.cond(l.twice_maf > r.twice_maf,; -1,; hl.cond(l.twice_maf < r.twice_maf,; 1,; 0)); ```; I'd prefer:; ```; def tie_breaker(l, r):; return hl.signum(r.twice_maf - l.twice_maf); ```; I'm having trouble figuring out why the latter throws the error below. Your `tie_breaker` code looks like it should work with Int32Expressions. Any idea what's going on?; ```; FatalError: ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.Row. Java stack trace:; java.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.spark.sql.Row; 	at is.hail.codegen.generated.C46.apply(Unknown Source); 	at is.hail.codegen.generated.C46.apply(Unknown Source); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$parseTypedExpr$1.apply(Parser.scala:102); 	at scala.Function0$class.apply$mcJ$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcJ$sp(AbstractFunction0.scala:12); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:81); 	at is.hail.utils.Graph$$anonfun$2$$anonfun$apply$4.apply(Graph.scala:79); 	at is.hail.utils.BinaryHeap.isLeftFavoredTie(BinaryHeap.scala:16)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3704
https://github.com/hail-is/hail/issues/3705:503,Availability,error,error,503,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:. 1. edited build.gradle to it will accept my spark version like this if (sparkVersion == '2.2.0.2.6.3.0-235') {. 2. Then I ran ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip command. ### What went wrong (all error messages here, including the full java stack trace):; bild fails. ```; [luffy@wp-hdp-ctrl03 hail]$ ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip --stacktrace; 1f253167d53c; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.0-rc2.tar.gz; g++ -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror -fPIC -ggdb -c -o ibs.o ibs.cpp; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [ibs.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --info or --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':nativeLib'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:66); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:905,Availability,error,error,905,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:. 1. edited build.gradle to it will accept my spark version like this if (sparkVersion == '2.2.0.2.6.3.0-235') {. 2. Then I ran ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip command. ### What went wrong (all error messages here, including the full java stack trace):; bild fails. ```; [luffy@wp-hdp-ctrl03 hail]$ ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip --stacktrace; 1f253167d53c; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.0-rc2.tar.gz; g++ -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror -fPIC -ggdb -c -o ibs.o ibs.cpp; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [ibs.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --info or --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':nativeLib'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:66); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:977,Availability,Error,Error,977,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:. 1. edited build.gradle to it will accept my spark version like this if (sparkVersion == '2.2.0.2.6.3.0-235') {. 2. Then I ran ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip command. ### What went wrong (all error messages here, including the full java stack trace):; bild fails. ```; [luffy@wp-hdp-ctrl03 hail]$ ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip --stacktrace; 1f253167d53c; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.0-rc2.tar.gz; g++ -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror -fPIC -ggdb -c -o ibs.o ibs.cpp; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [ibs.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --info or --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':nativeLib'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:66); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:1005,Availability,FAILURE,FAILURE,1005," the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:. 1. edited build.gradle to it will accept my spark version like this if (sparkVersion == '2.2.0.2.6.3.0-235') {. 2. Then I ran ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip command. ### What went wrong (all error messages here, including the full java stack trace):; bild fails. ```; [luffy@wp-hdp-ctrl03 hail]$ ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip --stacktrace; 1f253167d53c; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.0-rc2.tar.gz; g++ -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror -fPIC -ggdb -c -o ibs.o ibs.cpp; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [ibs.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --info or --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':nativeLib'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:66); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:5401,Deployability,Continuous,ContinuousBuildActionExecuter,5401,reate(DefaultGradleLauncher.java:92); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:92); at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:83); at org.gradle.launcher.exec.InProcessBuildActionExecuter$DefaultBuildController.run(InProcessBuildActionExecuter.java:99); at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:48); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:30); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:81); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:46); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:173); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:239); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:212); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:5439,Deployability,Continuous,ContinuousBuildActionExecuter,5439,2); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:92); at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:83); at org.gradle.launcher.exec.InProcessBuildActionExecuter$DefaultBuildController.run(InProcessBuildActionExecuter.java:99); at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:48); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:30); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:81); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:46); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:173); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:239); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:212); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:5507,Deployability,Continuous,ContinuousBuildActionExecuter,5507,faultBuildOperationExecutor.java:91); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:92); at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:83); at org.gradle.launcher.exec.InProcessBuildActionExecuter$DefaultBuildController.run(InProcessBuildActionExecuter.java:99); at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:48); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:30); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:81); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:46); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:173); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:239); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:212); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33); a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:5545,Deployability,Continuous,ContinuousBuildActionExecuter,5545,1); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:92); at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:83); at org.gradle.launcher.exec.InProcessBuildActionExecuter$DefaultBuildController.run(InProcessBuildActionExecuter.java:99); at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:48); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:30); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:81); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:46); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:173); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:239); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:212); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33); at org.gradle.launcher.cli.Exceptio,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:509,Integrability,message,messages,509,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2; ### What you did:. 1. edited build.gradle to it will accept my spark version like this if (sparkVersion == '2.2.0.2.6.3.0-235') {. 2. Then I ran ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip command. ### What went wrong (all error messages here, including the full java stack trace):; bild fails. ```; [luffy@wp-hdp-ctrl03 hail]$ ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip --stacktrace; 1f253167d53c; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.0-rc2.tar.gz; g++ -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror -fPIC -ggdb -c -o ibs.o ibs.cpp; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [ibs.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --info or --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':nativeLib'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:66); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.e",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:7195,Integrability,wrap,wrapper,7195,ActionFactory.java:212); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33); at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:22); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:205); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:169); at org.gradle.launcher.Main.doAction(Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:55); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:36); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.process.internal.ExecException: Process 'command 'make'' finished with non-zero exit value 2; at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:367); at org.gradle.process.internal.DefaultExecAction.execute(DefaultExecAction.java:31); at org.gradle.api.tasks.AbstractExecTask.exec(AbstractExecTask.java:54); at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:75); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.doExecute(AnnotationProcessingTaskFactory.java:228); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:221); at org.gradle.api.internal.projec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:7275,Integrability,wrap,wrapper,7275,execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33); at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:22); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:205); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:169); at org.gradle.launcher.Main.doAction(Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:55); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:36); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.process.internal.ExecException: Process 'command 'make'' finished with non-zero exit value 2; at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:367); at org.gradle.process.internal.DefaultExecAction.execute(DefaultExecAction.java:31); at org.gradle.api.tasks.AbstractExecTask.exec(AbstractExecTask.java:54); at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:75); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.doExecute(AnnotationProcessingTaskFactory.java:228); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:221); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(Annotat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:7283,Integrability,Wrap,WrapperExecutor,7283,RuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33); at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:22); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:205); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:169); at org.gradle.launcher.Main.doAction(Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:55); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:36); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.process.internal.ExecException: Process 'command 'make'' finished with non-zero exit value 2; at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:367); at org.gradle.process.internal.DefaultExecAction.execute(DefaultExecAction.java:31); at org.gradle.api.tasks.AbstractExecTask.exec(AbstractExecTask.java:54); at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:75); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.doExecute(AnnotationProcessingTaskFactory.java:228); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:221); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessin,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:7307,Integrability,Wrap,WrapperExecutor,7307,ion.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33); at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:22); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:205); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:169); at org.gradle.launcher.Main.doAction(Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:55); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:36); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.process.internal.ExecException: Process 'command 'make'' finished with non-zero exit value 2; at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:367); at org.gradle.process.internal.DefaultExecAction.execute(DefaultExecAction.java:31); at org.gradle.api.tasks.AbstractExecTask.exec(AbstractExecTask.java:54); at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:75); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.doExecute(AnnotationProcessingTaskFactory.java:228); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:221); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:21,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:7348,Integrability,wrap,wrapper,7348,JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33); at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:22); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:205); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:169); at org.gradle.launcher.Main.doAction(Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:55); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:36); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.process.internal.ExecException: Process 'command 'make'' finished with non-zero exit value 2; at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:367); at org.gradle.process.internal.DefaultExecAction.execute(DefaultExecAction.java:31); at org.gradle.api.tasks.AbstractExecTask.exec(AbstractExecTask.java:54); at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:75); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.doExecute(AnnotationProcessingTaskFactory.java:228); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:221); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:210); at org.gradle.api.internal.AbstractTa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:1860,Security,Validat,ValidatingTaskExecuter,1860,"gdb -c -o ibs.o ibs.cpp; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [ibs.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --info or --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':nativeLib'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:66); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:53); at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:203); at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:185); at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:66); at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPla",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:1891,Security,Validat,ValidatingTaskExecuter,1891,"1plus: error: unrecognized command line option ""-std=c++11""; make: *** [ibs.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --info or --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':nativeLib'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:66); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:53); at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:203); at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:185); at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:66); at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:50); at org.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:3322,Security,access,access,3322,eTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43); at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:203); at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:185); at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:66); at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:50); at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:25); at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:110); at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37); at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); at org.gradle.initialization.DefaultGradleLauncher$4.run(DefaultGradleLauncher.java:153); at org.gradle.internal.Factories$1.create(Factories.java:22); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:150); at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:32); at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:4227,Security,access,access,4227,te(DefaultBuildExecuter.java:37); at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23); at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43); at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32); at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37); at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30); at org.gradle.initialization.DefaultGradleLauncher$4.run(DefaultGradleLauncher.java:153); at org.gradle.internal.Factories$1.create(Factories.java:22); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:53); at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:150); at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:32); at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:98); at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:92); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:92); at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:83); at org.gradle.launcher.exec.InProcessBuildActionExecuter$DefaultBuildController.run(InProcessBuildActionExecuter.java:99); at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuild,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:1225,Testability,log,log,1225,"------------------------------. ### Hail version:; 0.2; ### What you did:. 1. edited build.gradle to it will accept my spark version like this if (sparkVersion == '2.2.0.2.6.3.0-235') {. 2. Then I ran ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip command. ### What went wrong (all error messages here, including the full java stack trace):; bild fails. ```; [luffy@wp-hdp-ctrl03 hail]$ ./gradlew -Dspark.version=2.2.0.2.6.3.0-235 shadowJar archiveZip --stacktrace; 1f253167d53c; :compileJava UP-TO-DATE; :generateBuildInfo; :nativeLib; tar -xzf libsimdpp-2.0-rc2.tar.gz; g++ -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror -fPIC -ggdb -c -o ibs.o ibs.cpp; cc1plus: error: unrecognized command line option ""-std=c++11""; make: *** [ibs.o] Error 1; :nativeLib FAILED. FAILURE: Build failed with an exception. * What went wrong:; Execution failed for task ':nativeLib'.; > Process 'command 'make'' finished with non-zero exit value 2. * Try:; Run with --info or --debug option to get more log output. * Exception is:; org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':nativeLib'.; at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46); at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35); at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:66); at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58); at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:52); at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52); at org.gradle.api.internal.task",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3705:7587,Testability,assert,assertNormalExitValue,7587,ava:22); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:205); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:169); at org.gradle.launcher.Main.doAction(Main.java:33); at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45); at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:55); at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:36); at org.gradle.launcher.GradleMain.main(GradleMain.java:23); at org.gradle.wrapper.BootstrapMainStarter.start(BootstrapMainStarter.java:30); at org.gradle.wrapper.WrapperExecutor.execute(WrapperExecutor.java:129); at org.gradle.wrapper.GradleWrapperMain.main(GradleWrapperMain.java:61); Caused by: org.gradle.process.internal.ExecException: Process 'command 'make'' finished with non-zero exit value 2; at org.gradle.process.internal.DefaultExecHandle$ExecResultImpl.assertNormalExitValue(DefaultExecHandle.java:367); at org.gradle.process.internal.DefaultExecAction.execute(DefaultExecAction.java:31); at org.gradle.api.tasks.AbstractExecTask.exec(AbstractExecTask.java:54); at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:75); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.doExecute(AnnotationProcessingTaskFactory.java:228); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:221); at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:210); at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:621); at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:604); at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.j,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3705
https://github.com/hail-is/hail/issues/3706:234,Availability,error,error,234,"### Hail version:; 1f253167d; ### What you did:; ```; import hail as hl; t = hl.utils.range_table(10); hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)) ; ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-8-b09a26588332> in <module>(); ----> 1 hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 139 .select()); 140 ; --> 141 edges = t.key_by(None).select('i', 'j'); 142 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_hql)); 143 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 863 row = get_select_exprs('Table.select',; 864 exprs, named_exprs, self._row_indices,; --> 865 protect_keys=True); 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 . ~/projects/hail/python/hail/utils/misc.py in get_select_exprs(caller, exprs, named_exprs, indices, protect_keys); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_keys=True):; 315 from hail.expr.expressions import to_expr, ExpressionException, TopLevelReference, Select; --> 316 exprs = [to_expr(e) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3706
https://github.com/hail-is/hail/issues/3706:240,Integrability,message,messages,240,"### Hail version:; 1f253167d; ### What you did:; ```; import hail as hl; t = hl.utils.range_table(10); hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)) ; ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-8-b09a26588332> in <module>(); ----> 1 hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 139 .select()); 140 ; --> 141 edges = t.key_by(None).select('i', 'j'); 142 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_hql)); 143 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 863 row = get_select_exprs('Table.select',; 864 exprs, named_exprs, self._row_indices,; --> 865 protect_keys=True); 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 . ~/projects/hail/python/hail/utils/misc.py in get_select_exprs(caller, exprs, named_exprs, indices, protect_keys); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_keys=True):; 315 from hail.expr.expressions import to_expr, ExpressionException, TopLevelReference, Select; --> 316 exprs = [to_expr(e) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3706
https://github.com/hail-is/hail/issues/3706:627,Integrability,wrap,wrapper,627,"### Hail version:; 1f253167d; ### What you did:; ```; import hail as hl; t = hl.utils.range_table(10); hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)) ; ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-8-b09a26588332> in <module>(); ----> 1 hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 139 .select()); 140 ; --> 141 edges = t.key_by(None).select('i', 'j'); 142 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_hql)); 143 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 863 row = get_select_exprs('Table.select',; 864 exprs, named_exprs, self._row_indices,; --> 865 protect_keys=True); 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 . ~/projects/hail/python/hail/utils/misc.py in get_select_exprs(caller, exprs, named_exprs, indices, protect_keys); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_keys=True):; 315 from hail.expr.expressions import to_expr, ExpressionException, TopLevelReference, Select; --> 316 exprs = [to_expr(e) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3706
https://github.com/hail-is/hail/issues/3706:661,Integrability,wrap,wrapper,661,"### Hail version:; 1f253167d; ### What you did:; ```; import hail as hl; t = hl.utils.range_table(10); hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)) ; ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-8-b09a26588332> in <module>(); ----> 1 hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 139 .select()); 140 ; --> 141 edges = t.key_by(None).select('i', 'j'); 142 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_hql)); 143 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 863 row = get_select_exprs('Table.select',; 864 exprs, named_exprs, self._row_indices,; --> 865 protect_keys=True); 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 . ~/projects/hail/python/hail/utils/misc.py in get_select_exprs(caller, exprs, named_exprs, indices, protect_keys); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_keys=True):; 315 from hail.expr.expressions import to_expr, ExpressionException, TopLevelReference, Select; --> 316 exprs = [to_expr(e) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3706
https://github.com/hail-is/hail/issues/3706:830,Integrability,wrap,wrapper,830,"### Hail version:; 1f253167d; ### What you did:; ```; import hail as hl; t = hl.utils.range_table(10); hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)) ; ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-8-b09a26588332> in <module>(); ----> 1 hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 139 .select()); 140 ; --> 141 edges = t.key_by(None).select('i', 'j'); 142 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_hql)); 143 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 863 row = get_select_exprs('Table.select',; 864 exprs, named_exprs, self._row_indices,; --> 865 protect_keys=True); 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 . ~/projects/hail/python/hail/utils/misc.py in get_select_exprs(caller, exprs, named_exprs, indices, protect_keys); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_keys=True):; 315 from hail.expr.expressions import to_expr, ExpressionException, TopLevelReference, Select; --> 316 exprs = [to_expr(e) ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3706
https://github.com/hail-is/hail/issues/3706:1194,Integrability,wrap,wrapper,1194," ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-8-b09a26588332> in <module>(); ----> 1 hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 139 .select()); 140 ; --> 141 edges = t.key_by(None).select('i', 'j'); 142 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_hql)); 143 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 863 row = get_select_exprs('Table.select',; 864 exprs, named_exprs, self._row_indices,; --> 865 protect_keys=True); 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 . ~/projects/hail/python/hail/utils/misc.py in get_select_exprs(caller, exprs, named_exprs, indices, protect_keys); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_keys=True):; 315 from hail.expr.expressions import to_expr, ExpressionException, TopLevelReference, Select; --> 316 exprs = [to_expr(e) if not isinstance(e, str) else indices.source[e] for e in exprs]; 317 named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; 318 assignments = OrderedDict(). ~/projects/hail/python/hail/utils/misc.py in <listcomp>(.0); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_k",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3706
https://github.com/hail-is/hail/issues/3706:1228,Integrability,wrap,wrapper,1228," ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-8-b09a26588332> in <module>(); ----> 1 hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 139 .select()); 140 ; --> 141 edges = t.key_by(None).select('i', 'j'); 142 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_hql)); 143 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 863 row = get_select_exprs('Table.select',; 864 exprs, named_exprs, self._row_indices,; --> 865 protect_keys=True); 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 . ~/projects/hail/python/hail/utils/misc.py in get_select_exprs(caller, exprs, named_exprs, indices, protect_keys); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_keys=True):; 315 from hail.expr.expressions import to_expr, ExpressionException, TopLevelReference, Select; --> 316 exprs = [to_expr(e) if not isinstance(e, str) else indices.source[e] for e in exprs]; 317 named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; 318 assignments = OrderedDict(). ~/projects/hail/python/hail/utils/misc.py in <listcomp>(.0); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_k",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3706
https://github.com/hail-is/hail/issues/3706:1397,Integrability,wrap,wrapper,1397," ---------------------------------------------------------------------------; LookupError Traceback (most recent call last); <ipython-input-8-b09a26588332> in <module>(); ----> 1 hl.methods.maximal_independent_set(t.idx, t.idx // 2, tie_breaker = lambda i, j: hl.signum(i - j)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/methods/misc.py in maximal_independent_set(i, j, keep, tie_breaker); 139 .select()); 140 ; --> 141 edges = t.key_by(None).select('i', 'j'); 142 nodes_in_set = Env.hail().utils.Graph.maximalIndependentSet(edges._jt.collect(), node_t._jtype, joption(tie_breaker_hql)); 143 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 863 row = get_select_exprs('Table.select',; 864 exprs, named_exprs, self._row_indices,; --> 865 protect_keys=True); 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 . ~/projects/hail/python/hail/utils/misc.py in get_select_exprs(caller, exprs, named_exprs, indices, protect_keys); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_keys=True):; 315 from hail.expr.expressions import to_expr, ExpressionException, TopLevelReference, Select; --> 316 exprs = [to_expr(e) if not isinstance(e, str) else indices.source[e] for e in exprs]; 317 named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; 318 assignments = OrderedDict(). ~/projects/hail/python/hail/utils/misc.py in <listcomp>(.0); 314 def get_select_exprs(caller, exprs, named_exprs, indices, protect_k",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3706
https://github.com/hail-is/hail/pull/3707:51,Performance,load,loader,51,This lets me run tests in SBT. SBT sets up a class loader that it; uses to load freshly compiled test clases and execute tests. This; makes the code-compile-test loop less time consuming. I will check it against a cluster to ensure that it does not; introduce new class loader issues when code is shipped.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3707
https://github.com/hail-is/hail/pull/3707:75,Performance,load,load,75,This lets me run tests in SBT. SBT sets up a class loader that it; uses to load freshly compiled test clases and execute tests. This; makes the code-compile-test loop less time consuming. I will check it against a cluster to ensure that it does not; introduce new class loader issues when code is shipped.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3707
https://github.com/hail-is/hail/pull/3707:270,Performance,load,loader,270,This lets me run tests in SBT. SBT sets up a class loader that it; uses to load freshly compiled test clases and execute tests. This; makes the code-compile-test loop less time consuming. I will check it against a cluster to ensure that it does not; introduce new class loader issues when code is shipped.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3707
https://github.com/hail-is/hail/pull/3707:17,Testability,test,tests,17,This lets me run tests in SBT. SBT sets up a class loader that it; uses to load freshly compiled test clases and execute tests. This; makes the code-compile-test loop less time consuming. I will check it against a cluster to ensure that it does not; introduce new class loader issues when code is shipped.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3707
https://github.com/hail-is/hail/pull/3707:97,Testability,test,test,97,This lets me run tests in SBT. SBT sets up a class loader that it; uses to load freshly compiled test clases and execute tests. This; makes the code-compile-test loop less time consuming. I will check it against a cluster to ensure that it does not; introduce new class loader issues when code is shipped.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3707
https://github.com/hail-is/hail/pull/3707:121,Testability,test,tests,121,This lets me run tests in SBT. SBT sets up a class loader that it; uses to load freshly compiled test clases and execute tests. This; makes the code-compile-test loop less time consuming. I will check it against a cluster to ensure that it does not; introduce new class loader issues when code is shipped.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3707
https://github.com/hail-is/hail/pull/3707:157,Testability,test,test,157,This lets me run tests in SBT. SBT sets up a class loader that it; uses to load freshly compiled test clases and execute tests. This; makes the code-compile-test loop less time consuming. I will check it against a cluster to ensure that it does not; introduce new class loader issues when code is shipped.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3707
https://github.com/hail-is/hail/issues/3708:169,Availability,error,error,169,"### Hail version:; 1f253167d53c; ### What you did:; ```; hl.eval_expr(hl.literal(hl.tuple([3]))); hl.eval_expr(hl.literal(hl.literal(3))); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-4a1008efb336> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.tuple([3]))[0]). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 136 Result of evaluating `expression`.; 137 """"""; --> 138 return eval_expr_typed(expression)[0]; 139 ; 140 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173 ; --> 174 return expression.collect()[0], expression.dtype; 175 ; 176 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in collect(self); 762 """"""; 763 uid = Env.get_uid(); --> 764 t = self._to_table(uid); 765 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:175,Integrability,message,messages,175,"### Hail version:; 1f253167d53c; ### What you did:; ```; hl.eval_expr(hl.literal(hl.tuple([3]))); hl.eval_expr(hl.literal(hl.literal(3))); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-4a1008efb336> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.tuple([3]))[0]). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 136 Result of evaluating `expression`.; 137 """"""; --> 138 return eval_expr_typed(expression)[0]; 139 ; 140 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173 ; --> 174 return expression.collect()[0], expression.dtype; 175 ; 176 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in collect(self); 762 """"""; 763 uid = Env.get_uid(); --> 764 t = self._to_table(uid); 765 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:504,Integrability,wrap,wrapper,504,"### Hail version:; 1f253167d53c; ### What you did:; ```; hl.eval_expr(hl.literal(hl.tuple([3]))); hl.eval_expr(hl.literal(hl.literal(3))); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-4a1008efb336> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.tuple([3]))[0]). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 136 Result of evaluating `expression`.; 137 """"""; --> 138 return eval_expr_typed(expression)[0]; 139 ; 140 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173 ; --> 174 return expression.collect()[0], expression.dtype; 175 ; 176 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in collect(self); 762 """"""; 763 uid = Env.get_uid(); --> 764 t = self._to_table(uid); 765 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:538,Integrability,wrap,wrapper,538,"### Hail version:; 1f253167d53c; ### What you did:; ```; hl.eval_expr(hl.literal(hl.tuple([3]))); hl.eval_expr(hl.literal(hl.literal(3))); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-4a1008efb336> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.tuple([3]))[0]). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 136 Result of evaluating `expression`.; 137 """"""; --> 138 return eval_expr_typed(expression)[0]; 139 ; 140 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173 ; --> 174 return expression.collect()[0], expression.dtype; 175 ; 176 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in collect(self); 762 """"""; 763 uid = Env.get_uid(); --> 764 t = self._to_table(uid); 765 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:707,Integrability,wrap,wrapper,707,"### Hail version:; 1f253167d53c; ### What you did:; ```; hl.eval_expr(hl.literal(hl.tuple([3]))); hl.eval_expr(hl.literal(hl.literal(3))); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-4a1008efb336> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.tuple([3]))[0]). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 136 Result of evaluating `expression`.; 137 """"""; --> 138 return eval_expr_typed(expression)[0]; 139 ; 140 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173 ; --> 174 return expression.collect()[0], expression.dtype; 175 ; 176 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in collect(self); 762 """"""; 763 uid = Env.get_uid(); --> 764 t = self._to_table(uid); 765 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:969,Integrability,wrap,wrapper,969,"literal(hl.tuple([3]))); hl.eval_expr(hl.literal(hl.literal(3))); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-4a1008efb336> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.tuple([3]))[0]). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 136 Result of evaluating `expression`.; 137 """"""; --> 138 return eval_expr_typed(expression)[0]; 139 ; 140 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173 ; --> 174 return expression.collect()[0], expression.dtype; 175 ; 176 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in collect(self); 762 """"""; 763 uid = Env.get_uid(); --> 764 t = self._to_table(uid); 765 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, ch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:1003,Integrability,wrap,wrapper,1003,"literal(hl.tuple([3]))); hl.eval_expr(hl.literal(hl.literal(3))); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-4a1008efb336> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.tuple([3]))[0]). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 136 Result of evaluating `expression`.; 137 """"""; --> 138 return eval_expr_typed(expression)[0]; 139 ; 140 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173 ; --> 174 return expression.collect()[0], expression.dtype; 175 ; 176 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in collect(self); 762 """"""; 763 uid = Env.get_uid(); --> 764 t = self._to_table(uid); 765 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, ch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:1172,Integrability,wrap,wrapper,1172,"literal(hl.tuple([3]))); hl.eval_expr(hl.literal(hl.literal(3))); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-3-4a1008efb336> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.tuple([3]))[0]). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 136 Result of evaluating `expression`.; 137 """"""; --> 138 return eval_expr_typed(expression)[0]; 139 ; 140 . ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173 ; --> 174 return expression.collect()[0], expression.dtype; 175 ; 176 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in collect(self); 762 """"""; 763 uid = Env.get_uid(); --> 764 t = self._to_table(uid); 765 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, ch",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:1963,Integrability,wrap,wrapper,1963,"s, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173 ; --> 174 return expression.collect()[0], expression.dtype; 175 ; 176 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in collect(self); 762 """"""; 763 uid = Env.get_uid(); --> 764 t = self._to_table(uid); 765 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . ~/projects/hail/python/hail/table.py in _process_joins(self, *exprs); 1463 def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:1997,Integrability,wrap,wrapper,1997,"s, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173 ; --> 174 return expression.collect()[0], expression.dtype; 175 ; 176 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in collect(self); 762 """"""; 763 uid = Env.get_uid(); --> 764 t = self._to_table(uid); 765 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . ~/projects/hail/python/hail/table.py in _process_joins(self, *exprs); 1463 def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:2166,Integrability,wrap,wrapper,2166,"s, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 172 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 173 ; --> 174 return expression.collect()[0], expression.dtype; 175 ; 176 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in collect(self); 762 """"""; 763 uid = Env.get_uid(); --> 764 t = self._to_table(uid); 765 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . ~/projects/hail/python/hail/table.py in _process_joins(self, *exprs); 1463 def",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:2509,Integrability,wrap,wrapper,2509,"in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . ~/projects/hail/python/hail/table.py in _process_joins(self, *exprs); 1463 def broadcast_f(left, data, jt):; 1464 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1465 return process_joins(self, exprs, broadcast_f); 1466 ; 1467 def cache(self):. ~/projects/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 365 all_uids.extend(list(t)); 366 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); --> 367 data_json = t._to_json(data); 368 left = broadcast_f(left, data_json, t._jtype); 369 . ~/projects/hail/python/hail/expr/types.py in _to_json(self, x); 176 def _to_json(self, x):; 177 c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:2543,Integrability,wrap,wrapper,2543,"in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . ~/projects/hail/python/hail/table.py in _process_joins(self, *exprs); 1463 def broadcast_f(left, data, jt):; 1464 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1465 return process_joins(self, exprs, broadcast_f); 1466 ; 1467 def cache(self):. ~/projects/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 365 all_uids.extend(list(t)); 366 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); --> 367 data_json = t._to_json(data); 368 left = broadcast_f(left, data_json, t._jtype); 369 . ~/projects/hail/python/hail/expr/types.py in _to_json(self, x); 176 def _to_json(self, x):; 177 c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:2712,Integrability,wrap,wrapper,2712,"in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 766 . ~/projects/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 582 # scalar expression; 583 df = Env.dummy_table(); --> 584 df = df.select(**{name: self}); 585 return df; 586 elif len(axes) == 0:. ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . ~/projects/hail/python/hail/table.py in _process_joins(self, *exprs); 1463 def broadcast_f(left, data, jt):; 1464 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1465 return process_joins(self, exprs, broadcast_f); 1466 ; 1467 def cache(self):. ~/projects/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 365 all_uids.extend(list(t)); 366 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); --> 367 data_json = t._to_json(data); 368 left = broadcast_f(left, data_json, t._jtype); 369 . ~/projects/hail/python/hail/expr/types.py in _to_json(self, x); 176 def _to_json(self, x):; 177 c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:5313,Integrability,wrap,wrapper,5313,"rcular, allow_nan, cls, indent, separators, default, sort_keys, **kw); 229 cls is None and indent is None and separators is None and; 230 default is None and not sort_keys and not kw):; --> 231 return _default_encoder.encode(obj); 232 if cls is None:; 233 cls = JSONEncoder. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in encode(self, o); 197 # exceptions aren't as detailed. The list call should be roughly; 198 # equivalent to the PySequence_Fast that ''.join() would do.; --> 199 chunks = self.iterencode(o, _one_shot=True); 200 if not isinstance(chunks, (list, tuple)):; 201 chunks = list(chunks). ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in iterencode(self, o, _one_shot); 255 self.key_separator, self.item_separator, self.sort_keys,; 256 self.skipkeys, _one_shot); --> 257 return _iterencode(o, 0); 258 ; 259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in default(self, o); 178 """"""; 179 raise TypeError(""Object of type '%s' is not JSON serializable"" %; --> 180 o.__class__.__name__); 181 ; 182 def encode(self, o):. TypeError: Object of type 'Int32Expression' is not JSON serializable; ```; and; ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-7-d049e8be5f76> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.literal(3))). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/functions.py in literal(x, dtype); 115 elif is_primitive(dtype):; 116 if dtype == tint32:; --> 117 assert isinstance(x, builtins.int); 118 assert tint32.min_value <= x <= tint32.max_value; 119 return construct_expr(Literal('i32#{}'.format(x)), tint32). AssertionError: ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:5347,Integrability,wrap,wrapper,5347,"rcular, allow_nan, cls, indent, separators, default, sort_keys, **kw); 229 cls is None and indent is None and separators is None and; 230 default is None and not sort_keys and not kw):; --> 231 return _default_encoder.encode(obj); 232 if cls is None:; 233 cls = JSONEncoder. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in encode(self, o); 197 # exceptions aren't as detailed. The list call should be roughly; 198 # equivalent to the PySequence_Fast that ''.join() would do.; --> 199 chunks = self.iterencode(o, _one_shot=True); 200 if not isinstance(chunks, (list, tuple)):; 201 chunks = list(chunks). ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in iterencode(self, o, _one_shot); 255 self.key_separator, self.item_separator, self.sort_keys,; 256 self.skipkeys, _one_shot); --> 257 return _iterencode(o, 0); 258 ; 259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in default(self, o); 178 """"""; 179 raise TypeError(""Object of type '%s' is not JSON serializable"" %; --> 180 o.__class__.__name__); 181 ; 182 def encode(self, o):. TypeError: Object of type 'Int32Expression' is not JSON serializable; ```; and; ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-7-d049e8be5f76> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.literal(3))). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/functions.py in literal(x, dtype); 115 elif is_primitive(dtype):; 116 if dtype == tint32:; --> 117 assert isinstance(x, builtins.int); 118 assert tint32.min_value <= x <= tint32.max_value; 119 return construct_expr(Literal('i32#{}'.format(x)), tint32). AssertionError: ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:5516,Integrability,wrap,wrapper,5516,"rcular, allow_nan, cls, indent, separators, default, sort_keys, **kw); 229 cls is None and indent is None and separators is None and; 230 default is None and not sort_keys and not kw):; --> 231 return _default_encoder.encode(obj); 232 if cls is None:; 233 cls = JSONEncoder. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in encode(self, o); 197 # exceptions aren't as detailed. The list call should be roughly; 198 # equivalent to the PySequence_Fast that ''.join() would do.; --> 199 chunks = self.iterencode(o, _one_shot=True); 200 if not isinstance(chunks, (list, tuple)):; 201 chunks = list(chunks). ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in iterencode(self, o, _one_shot); 255 self.key_separator, self.item_separator, self.sort_keys,; 256 self.skipkeys, _one_shot); --> 257 return _iterencode(o, 0); 258 ; 259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in default(self, o); 178 """"""; 179 raise TypeError(""Object of type '%s' is not JSON serializable"" %; --> 180 o.__class__.__name__); 181 ; 182 def encode(self, o):. TypeError: Object of type 'Int32Expression' is not JSON serializable; ```; and; ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-7-d049e8be5f76> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.literal(3))). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/functions.py in literal(x, dtype); 115 elif is_primitive(dtype):; 116 if dtype == tint32:; --> 117 assert isinstance(x, builtins.int); 118 assert tint32.min_value <= x <= tint32.max_value; 119 return construct_expr(Literal('i32#{}'.format(x)), tint32). AssertionError: ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:3341,Modifiability,extend,extend,3341,"e.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . ~/projects/hail/python/hail/table.py in _process_joins(self, *exprs); 1463 def broadcast_f(left, data, jt):; 1464 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1465 return process_joins(self, exprs, broadcast_f); 1466 ; 1467 def cache(self):. ~/projects/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 365 all_uids.extend(list(t)); 366 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); --> 367 data_json = t._to_json(data); 368 left = broadcast_f(left, data_json, t._jtype); 369 . ~/projects/hail/python/hail/expr/types.py in _to_json(self, x); 176 def _to_json(self, x):; 177 converted = self._convert_to_json_na(x); --> 178 return json.dumps(converted); 179 ; 180 def _convert_to_json_na(self, x):. ~/anaconda2/envs/hail/lib/python3.6/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw); 229 cls is None and indent is None and separators is None and; 230 default is None and not sort_keys and not kw):; --> 231 return _default_encoder.encode(obj); 232 if cls is None:; 233 cls = JSONEncoder. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in encode(self, o); 197 # exceptions aren't as detailed. The list call should be roughly; 198 # equivalent to the PySequence_Fast that ''.join() would do.; --> 199 chunks = self.iterencode(o, _on",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:3229,Performance,cache,cache,3229,"jects/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . ~/projects/hail/python/hail/table.py in _process_joins(self, *exprs); 1463 def broadcast_f(left, data, jt):; 1464 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1465 return process_joins(self, exprs, broadcast_f); 1466 ; 1467 def cache(self):. ~/projects/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 365 all_uids.extend(list(t)); 366 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); --> 367 data_json = t._to_json(data); 368 left = broadcast_f(left, data_json, t._jtype); 369 . ~/projects/hail/python/hail/expr/types.py in _to_json(self, x); 176 def _to_json(self, x):; 177 converted = self._convert_to_json_na(x); --> 178 return json.dumps(converted); 179 ; 180 def _convert_to_json_na(self, x):. ~/anaconda2/envs/hail/lib/python3.6/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw); 229 cls is None and indent is None and separators is None and; 230 default is None and not sort_keys and not kw):; --> 231 return _default_encoder.encode(obj); 232 if cls is None:; 233 cls = JSONEncoder. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in encode(self, o); 1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:5118,Testability,Assert,AssertionError,5118,"rcular, allow_nan, cls, indent, separators, default, sort_keys, **kw); 229 cls is None and indent is None and separators is None and; 230 default is None and not sort_keys and not kw):; --> 231 return _default_encoder.encode(obj); 232 if cls is None:; 233 cls = JSONEncoder. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in encode(self, o); 197 # exceptions aren't as detailed. The list call should be roughly; 198 # equivalent to the PySequence_Fast that ''.join() would do.; --> 199 chunks = self.iterencode(o, _one_shot=True); 200 if not isinstance(chunks, (list, tuple)):; 201 chunks = list(chunks). ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in iterencode(self, o, _one_shot); 255 self.key_separator, self.item_separator, self.sort_keys,; 256 self.skipkeys, _one_shot); --> 257 return _iterencode(o, 0); 258 ; 259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in default(self, o); 178 """"""; 179 raise TypeError(""Object of type '%s' is not JSON serializable"" %; --> 180 o.__class__.__name__); 181 ; 182 def encode(self, o):. TypeError: Object of type 'Int32Expression' is not JSON serializable; ```; and; ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-7-d049e8be5f76> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.literal(3))). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/functions.py in literal(x, dtype); 115 elif is_primitive(dtype):; 116 if dtype == tint32:; --> 117 assert isinstance(x, builtins.int); 118 assert tint32.min_value <= x <= tint32.max_value; 119 return construct_expr(Literal('i32#{}'.format(x)), tint32). AssertionError: ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:5661,Testability,assert,assert,5661,"rcular, allow_nan, cls, indent, separators, default, sort_keys, **kw); 229 cls is None and indent is None and separators is None and; 230 default is None and not sort_keys and not kw):; --> 231 return _default_encoder.encode(obj); 232 if cls is None:; 233 cls = JSONEncoder. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in encode(self, o); 197 # exceptions aren't as detailed. The list call should be roughly; 198 # equivalent to the PySequence_Fast that ''.join() would do.; --> 199 chunks = self.iterencode(o, _one_shot=True); 200 if not isinstance(chunks, (list, tuple)):; 201 chunks = list(chunks). ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in iterencode(self, o, _one_shot); 255 self.key_separator, self.item_separator, self.sort_keys,; 256 self.skipkeys, _one_shot); --> 257 return _iterencode(o, 0); 258 ; 259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in default(self, o); 178 """"""; 179 raise TypeError(""Object of type '%s' is not JSON serializable"" %; --> 180 o.__class__.__name__); 181 ; 182 def encode(self, o):. TypeError: Object of type 'Int32Expression' is not JSON serializable; ```; and; ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-7-d049e8be5f76> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.literal(3))). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/functions.py in literal(x, dtype); 115 elif is_primitive(dtype):; 116 if dtype == tint32:; --> 117 assert isinstance(x, builtins.int); 118 assert tint32.min_value <= x <= tint32.max_value; 119 return construct_expr(Literal('i32#{}'.format(x)), tint32). AssertionError: ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:5701,Testability,assert,assert,5701,"rcular, allow_nan, cls, indent, separators, default, sort_keys, **kw); 229 cls is None and indent is None and separators is None and; 230 default is None and not sort_keys and not kw):; --> 231 return _default_encoder.encode(obj); 232 if cls is None:; 233 cls = JSONEncoder. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in encode(self, o); 197 # exceptions aren't as detailed. The list call should be roughly; 198 # equivalent to the PySequence_Fast that ''.join() would do.; --> 199 chunks = self.iterencode(o, _one_shot=True); 200 if not isinstance(chunks, (list, tuple)):; 201 chunks = list(chunks). ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in iterencode(self, o, _one_shot); 255 self.key_separator, self.item_separator, self.sort_keys,; 256 self.skipkeys, _one_shot); --> 257 return _iterencode(o, 0); 258 ; 259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in default(self, o); 178 """"""; 179 raise TypeError(""Object of type '%s' is not JSON serializable"" %; --> 180 o.__class__.__name__); 181 ; 182 def encode(self, o):. TypeError: Object of type 'Int32Expression' is not JSON serializable; ```; and; ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-7-d049e8be5f76> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.literal(3))). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/functions.py in literal(x, dtype); 115 elif is_primitive(dtype):; 116 if dtype == tint32:; --> 117 assert isinstance(x, builtins.int); 118 assert tint32.min_value <= x <= tint32.max_value; 119 return construct_expr(Literal('i32#{}'.format(x)), tint32). AssertionError: ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3708:5815,Testability,Assert,AssertionError,5815,"rcular, allow_nan, cls, indent, separators, default, sort_keys, **kw); 229 cls is None and indent is None and separators is None and; 230 default is None and not sort_keys and not kw):; --> 231 return _default_encoder.encode(obj); 232 if cls is None:; 233 cls = JSONEncoder. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in encode(self, o); 197 # exceptions aren't as detailed. The list call should be roughly; 198 # equivalent to the PySequence_Fast that ''.join() would do.; --> 199 chunks = self.iterencode(o, _one_shot=True); 200 if not isinstance(chunks, (list, tuple)):; 201 chunks = list(chunks). ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in iterencode(self, o, _one_shot); 255 self.key_separator, self.item_separator, self.sort_keys,; 256 self.skipkeys, _one_shot); --> 257 return _iterencode(o, 0); 258 ; 259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,. ~/anaconda2/envs/hail/lib/python3.6/json/encoder.py in default(self, o); 178 """"""; 179 raise TypeError(""Object of type '%s' is not JSON serializable"" %; --> 180 o.__class__.__name__); 181 ; 182 def encode(self, o):. TypeError: Object of type 'Int32Expression' is not JSON serializable; ```; and; ```; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); <ipython-input-7-d049e8be5f76> in <module>(); ----> 1 hl.eval_expr(hl.literal(hl.literal(3))). ~/projects/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/projects/hail/python/hail/expr/functions.py in literal(x, dtype); 115 elif is_primitive(dtype):; 116 if dtype == tint32:; --> 117 assert isinstance(x, builtins.int); 118 assert tint32.min_value <= x <= tint32.max_value; 119 return construct_expr(Literal('i32#{}'.format(x)), tint32). AssertionError: ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3708
https://github.com/hail-is/hail/issues/3709:75,Availability,error,error,75,"### Hail version:; 2018-06-04; ### What you did:; ### What went wrong (all error messages here, including the full java stack trace):; The docs for [`liftover`](https://hail.is/docs/devel/functions/genetics.html?highlight=liftover#hail.expr.functions.liftover) confusingly use `.value` to convert an expression to a value. Users are apt to copy paste them and not understand what the `.value` is doing. We should uniformly use `hl.eval_expr` which is less surprising.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3709
https://github.com/hail-is/hail/issues/3709:81,Integrability,message,messages,81,"### Hail version:; 2018-06-04; ### What you did:; ### What went wrong (all error messages here, including the full java stack trace):; The docs for [`liftover`](https://hail.is/docs/devel/functions/genetics.html?highlight=liftover#hail.expr.functions.liftover) confusingly use `.value` to convert an expression to a value. Users are apt to copy paste them and not understand what the `.value` is doing. We should uniformly use `hl.eval_expr` which is less surprising.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3709
https://github.com/hail-is/hail/issues/3712:77,Availability,error,error,77,"### Hail version:. 9e4ca83b6b0d. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):. `hl.eval_expr(hl.is_snp(hl.literal('A'), hl.literal('A')))` should return false, i.e. ['A', 'A'] shouldn't be considered a snp.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3712
https://github.com/hail-is/hail/issues/3712:83,Integrability,message,messages,83,"### Hail version:. 9e4ca83b6b0d. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):. `hl.eval_expr(hl.is_snp(hl.literal('A'), hl.literal('A')))` should return false, i.e. ['A', 'A'] shouldn't be considered a snp.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3712
https://github.com/hail-is/hail/issues/3713:104,Availability,error,error,104,"### Hail version:; `a230321`; ### What you did:; `mt.GT[1]` on a haploid call; ### What went wrong (all error messages here, including the full java stack trace):; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 3.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 3.0 (TID 10515, pbt-sw-nkpn.c.broad-mpg-gnomad.internal, executor 306): java.lang.IllegalArgumentException: requirement failed; at scala.Predef$.require(Predef.scala:212); at is.hail.variant.Call$.alleleByIndex(Call.scala:128); at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.BinaryFun.apply(Fun.scala:122); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3713
https://github.com/hail-is/hail/issues/3713:227,Availability,failure,failure,227,"### Hail version:; `a230321`; ### What you did:; `mt.GT[1]` on a haploid call; ### What went wrong (all error messages here, including the full java stack trace):; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 3.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 3.0 (TID 10515, pbt-sw-nkpn.c.broad-mpg-gnomad.internal, executor 306): java.lang.IllegalArgumentException: requirement failed; at scala.Predef$.require(Predef.scala:212); at is.hail.variant.Call$.alleleByIndex(Call.scala:128); at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.BinaryFun.apply(Fun.scala:122); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3713
https://github.com/hail-is/hail/issues/3713:288,Availability,failure,failure,288,"### Hail version:; `a230321`; ### What you did:; `mt.GT[1]` on a haploid call; ### What went wrong (all error messages here, including the full java stack trace):; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 3.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 3.0 (TID 10515, pbt-sw-nkpn.c.broad-mpg-gnomad.internal, executor 306): java.lang.IllegalArgumentException: requirement failed; at scala.Predef$.require(Predef.scala:212); at is.hail.variant.Call$.alleleByIndex(Call.scala:128); at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.BinaryFun.apply(Fun.scala:122); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3713
https://github.com/hail-is/hail/issues/3713:110,Integrability,message,messages,110,"### Hail version:; `a230321`; ### What you did:; `mt.GT[1]` on a haploid call; ### What went wrong (all error messages here, including the full java stack trace):; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 3.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 3.0 (TID 10515, pbt-sw-nkpn.c.broad-mpg-gnomad.internal, executor 306): java.lang.IllegalArgumentException: requirement failed; at scala.Predef$.require(Predef.scala:212); at is.hail.variant.Call$.alleleByIndex(Call.scala:128); at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.BinaryFun.apply(Fun.scala:122); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3713
https://github.com/hail-is/hail/issues/3713:206,Safety,abort,aborted,206,"### Hail version:; `a230321`; ### What you did:; `mt.GT[1]` on a haploid call; ### What went wrong (all error messages here, including the full java stack trace):; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 9716 in stage 3.0 failed 20 times, most recent failure: Lost task 9716.19 in stage 3.0 (TID 10515, pbt-sw-nkpn.c.broad-mpg-gnomad.internal, executor 306): java.lang.IllegalArgumentException: requirement failed; at scala.Predef$.require(Predef.scala:212); at is.hail.variant.Call$.alleleByIndex(Call.scala:128); at is.hail.expr.FunctionRegistry$$anonfun$11.apply$mcIII$sp(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.FunctionRegistry$$anonfun$11.apply(FunctionRegistry.scala:682); at is.hail.expr.BinaryFun.apply(Fun.scala:122); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3713
https://github.com/hail-is/hail/pull/3715:1522,Performance,bottleneck,bottleneck,1522,"This PR Python-izes all but the LocalLDPrune step of `ld_prune` while adding modular functionality for window indices that also simplifies computing big banded correlation (where the band is in terms of position, centimorgans, or any other non-decreasing value per contig). I later plan to simplify computing correlation, but basically:; ```; ld_matrix = ... BlockMatrix stuff starting from mt ...; starts, stops = hl.locus_windows(mt.rows(), radius=1e6); banded_ld_matrix = ld_matrix.sparsify_row_intervals(starts, stops); ```; Or if centimorgan coordinates are a row field (via annotation or import from plink format):; ```; ht = mt.rows(); starts, stops = hl.locus_windows(ht, radius=1e6, value_expr=ht.centimorgans); ```. Changes:; - added `array_windows` to methods/misc, with docs and test.; - added `locus_windows` to methods/genetics, with docs and test.; - reworked `ld_prune` to use `locus_windows` and `sparsify_row_intervals`, moved squaring op from expression language to block matrix `r2_bm`, moved arg checks from scala to python.; - then deleted `UpperIndexBounds`, `UpperIndexBoundsSuite`, `BlockMatrix.filteredEntriesTable` and tests.; - improved `test_ld_prune` and modified `ldprune.vcf` to make it much smaller and span three chromosomes instead of one for better testing; - allowed `sparsify_row_intervals` to accept ndarrays so user need not convert output of `locus_windows` (which should naturally be an ndarray) with `[int(s) for s in starts]`. If the arg checking or py4j communication become a bottleneck, we can add passage through file similar to what we do for arrays of doubles.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3715
https://github.com/hail-is/hail/pull/3715:791,Testability,test,test,791,"This PR Python-izes all but the LocalLDPrune step of `ld_prune` while adding modular functionality for window indices that also simplifies computing big banded correlation (where the band is in terms of position, centimorgans, or any other non-decreasing value per contig). I later plan to simplify computing correlation, but basically:; ```; ld_matrix = ... BlockMatrix stuff starting from mt ...; starts, stops = hl.locus_windows(mt.rows(), radius=1e6); banded_ld_matrix = ld_matrix.sparsify_row_intervals(starts, stops); ```; Or if centimorgan coordinates are a row field (via annotation or import from plink format):; ```; ht = mt.rows(); starts, stops = hl.locus_windows(ht, radius=1e6, value_expr=ht.centimorgans); ```. Changes:; - added `array_windows` to methods/misc, with docs and test.; - added `locus_windows` to methods/genetics, with docs and test.; - reworked `ld_prune` to use `locus_windows` and `sparsify_row_intervals`, moved squaring op from expression language to block matrix `r2_bm`, moved arg checks from scala to python.; - then deleted `UpperIndexBounds`, `UpperIndexBoundsSuite`, `BlockMatrix.filteredEntriesTable` and tests.; - improved `test_ld_prune` and modified `ldprune.vcf` to make it much smaller and span three chromosomes instead of one for better testing; - allowed `sparsify_row_intervals` to accept ndarrays so user need not convert output of `locus_windows` (which should naturally be an ndarray) with `[int(s) for s in starts]`. If the arg checking or py4j communication become a bottleneck, we can add passage through file similar to what we do for arrays of doubles.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3715
https://github.com/hail-is/hail/pull/3715:857,Testability,test,test,857,"This PR Python-izes all but the LocalLDPrune step of `ld_prune` while adding modular functionality for window indices that also simplifies computing big banded correlation (where the band is in terms of position, centimorgans, or any other non-decreasing value per contig). I later plan to simplify computing correlation, but basically:; ```; ld_matrix = ... BlockMatrix stuff starting from mt ...; starts, stops = hl.locus_windows(mt.rows(), radius=1e6); banded_ld_matrix = ld_matrix.sparsify_row_intervals(starts, stops); ```; Or if centimorgan coordinates are a row field (via annotation or import from plink format):; ```; ht = mt.rows(); starts, stops = hl.locus_windows(ht, radius=1e6, value_expr=ht.centimorgans); ```. Changes:; - added `array_windows` to methods/misc, with docs and test.; - added `locus_windows` to methods/genetics, with docs and test.; - reworked `ld_prune` to use `locus_windows` and `sparsify_row_intervals`, moved squaring op from expression language to block matrix `r2_bm`, moved arg checks from scala to python.; - then deleted `UpperIndexBounds`, `UpperIndexBoundsSuite`, `BlockMatrix.filteredEntriesTable` and tests.; - improved `test_ld_prune` and modified `ldprune.vcf` to make it much smaller and span three chromosomes instead of one for better testing; - allowed `sparsify_row_intervals` to accept ndarrays so user need not convert output of `locus_windows` (which should naturally be an ndarray) with `[int(s) for s in starts]`. If the arg checking or py4j communication become a bottleneck, we can add passage through file similar to what we do for arrays of doubles.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3715
https://github.com/hail-is/hail/pull/3715:1146,Testability,test,tests,1146,"This PR Python-izes all but the LocalLDPrune step of `ld_prune` while adding modular functionality for window indices that also simplifies computing big banded correlation (where the band is in terms of position, centimorgans, or any other non-decreasing value per contig). I later plan to simplify computing correlation, but basically:; ```; ld_matrix = ... BlockMatrix stuff starting from mt ...; starts, stops = hl.locus_windows(mt.rows(), radius=1e6); banded_ld_matrix = ld_matrix.sparsify_row_intervals(starts, stops); ```; Or if centimorgan coordinates are a row field (via annotation or import from plink format):; ```; ht = mt.rows(); starts, stops = hl.locus_windows(ht, radius=1e6, value_expr=ht.centimorgans); ```. Changes:; - added `array_windows` to methods/misc, with docs and test.; - added `locus_windows` to methods/genetics, with docs and test.; - reworked `ld_prune` to use `locus_windows` and `sparsify_row_intervals`, moved squaring op from expression language to block matrix `r2_bm`, moved arg checks from scala to python.; - then deleted `UpperIndexBounds`, `UpperIndexBoundsSuite`, `BlockMatrix.filteredEntriesTable` and tests.; - improved `test_ld_prune` and modified `ldprune.vcf` to make it much smaller and span three chromosomes instead of one for better testing; - allowed `sparsify_row_intervals` to accept ndarrays so user need not convert output of `locus_windows` (which should naturally be an ndarray) with `[int(s) for s in starts]`. If the arg checking or py4j communication become a bottleneck, we can add passage through file similar to what we do for arrays of doubles.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3715
https://github.com/hail-is/hail/pull/3715:1285,Testability,test,testing,1285,"This PR Python-izes all but the LocalLDPrune step of `ld_prune` while adding modular functionality for window indices that also simplifies computing big banded correlation (where the band is in terms of position, centimorgans, or any other non-decreasing value per contig). I later plan to simplify computing correlation, but basically:; ```; ld_matrix = ... BlockMatrix stuff starting from mt ...; starts, stops = hl.locus_windows(mt.rows(), radius=1e6); banded_ld_matrix = ld_matrix.sparsify_row_intervals(starts, stops); ```; Or if centimorgan coordinates are a row field (via annotation or import from plink format):; ```; ht = mt.rows(); starts, stops = hl.locus_windows(ht, radius=1e6, value_expr=ht.centimorgans); ```. Changes:; - added `array_windows` to methods/misc, with docs and test.; - added `locus_windows` to methods/genetics, with docs and test.; - reworked `ld_prune` to use `locus_windows` and `sparsify_row_intervals`, moved squaring op from expression language to block matrix `r2_bm`, moved arg checks from scala to python.; - then deleted `UpperIndexBounds`, `UpperIndexBoundsSuite`, `BlockMatrix.filteredEntriesTable` and tests.; - improved `test_ld_prune` and modified `ldprune.vcf` to make it much smaller and span three chromosomes instead of one for better testing; - allowed `sparsify_row_intervals` to accept ndarrays so user need not convert output of `locus_windows` (which should naturally be an ndarray) with `[int(s) for s in starts]`. If the arg checking or py4j communication become a bottleneck, we can add passage through file similar to what we do for arrays of doubles.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3715
https://github.com/hail-is/hail/pull/3715:128,Usability,simpl,simplifies,128,"This PR Python-izes all but the LocalLDPrune step of `ld_prune` while adding modular functionality for window indices that also simplifies computing big banded correlation (where the band is in terms of position, centimorgans, or any other non-decreasing value per contig). I later plan to simplify computing correlation, but basically:; ```; ld_matrix = ... BlockMatrix stuff starting from mt ...; starts, stops = hl.locus_windows(mt.rows(), radius=1e6); banded_ld_matrix = ld_matrix.sparsify_row_intervals(starts, stops); ```; Or if centimorgan coordinates are a row field (via annotation or import from plink format):; ```; ht = mt.rows(); starts, stops = hl.locus_windows(ht, radius=1e6, value_expr=ht.centimorgans); ```. Changes:; - added `array_windows` to methods/misc, with docs and test.; - added `locus_windows` to methods/genetics, with docs and test.; - reworked `ld_prune` to use `locus_windows` and `sparsify_row_intervals`, moved squaring op from expression language to block matrix `r2_bm`, moved arg checks from scala to python.; - then deleted `UpperIndexBounds`, `UpperIndexBoundsSuite`, `BlockMatrix.filteredEntriesTable` and tests.; - improved `test_ld_prune` and modified `ldprune.vcf` to make it much smaller and span three chromosomes instead of one for better testing; - allowed `sparsify_row_intervals` to accept ndarrays so user need not convert output of `locus_windows` (which should naturally be an ndarray) with `[int(s) for s in starts]`. If the arg checking or py4j communication become a bottleneck, we can add passage through file similar to what we do for arrays of doubles.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3715
https://github.com/hail-is/hail/pull/3715:290,Usability,simpl,simplify,290,"This PR Python-izes all but the LocalLDPrune step of `ld_prune` while adding modular functionality for window indices that also simplifies computing big banded correlation (where the band is in terms of position, centimorgans, or any other non-decreasing value per contig). I later plan to simplify computing correlation, but basically:; ```; ld_matrix = ... BlockMatrix stuff starting from mt ...; starts, stops = hl.locus_windows(mt.rows(), radius=1e6); banded_ld_matrix = ld_matrix.sparsify_row_intervals(starts, stops); ```; Or if centimorgan coordinates are a row field (via annotation or import from plink format):; ```; ht = mt.rows(); starts, stops = hl.locus_windows(ht, radius=1e6, value_expr=ht.centimorgans); ```. Changes:; - added `array_windows` to methods/misc, with docs and test.; - added `locus_windows` to methods/genetics, with docs and test.; - reworked `ld_prune` to use `locus_windows` and `sparsify_row_intervals`, moved squaring op from expression language to block matrix `r2_bm`, moved arg checks from scala to python.; - then deleted `UpperIndexBounds`, `UpperIndexBoundsSuite`, `BlockMatrix.filteredEntriesTable` and tests.; - improved `test_ld_prune` and modified `ldprune.vcf` to make it much smaller and span three chromosomes instead of one for better testing; - allowed `sparsify_row_intervals` to accept ndarrays so user need not convert output of `locus_windows` (which should naturally be an ndarray) with `[int(s) for s in starts]`. If the arg checking or py4j communication become a bottleneck, we can add passage through file similar to what we do for arrays of doubles.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3715
https://github.com/hail-is/hail/pull/3716:129,Testability,test,test,129,"Indexing a table by a non-unique matrixtable field was broken (only joined with one row per distinct value). For example, in the test I added, `m4.filter_cols(hl.is_defined(m4.idx)).count_cols()` previously returned 1, now returns 9.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3716
https://github.com/hail-is/hail/pull/3718:110,Energy Efficiency,allocate,allocates,110,"This is an initial implementation of the Scala Region as a reference to a C++ off-heap object. The C++ Region allocates ""small"" blocks out of 64KB chunks, and ""large"" blocks using; malloc() directly. There's a clear_but_keep_mem() which reuses all the 64KB chunks,; and the largest few individual allocations. The usefulness of this strategy is TBD. Currently all allocations are done with a JNI call to the C++, but fields of the object are; directly accessible so it's theoretically possible to try to write optimized Scala code; for the case of a small allocation which can fit in the current chunk. The other changes are mostly consequences of using absolute addresses rather than; offset-in-contiguous-buffer, and the change in the semantics of appendFoo() when a; Region's memory is in non-contiguous chunks - things which need to be located together,; such as the length of a string and its contents, now have to be within memory from a; single allocate() call.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718
https://github.com/hail-is/hail/pull/3718:952,Energy Efficiency,allocate,allocate,952,"This is an initial implementation of the Scala Region as a reference to a C++ off-heap object. The C++ Region allocates ""small"" blocks out of 64KB chunks, and ""large"" blocks using; malloc() directly. There's a clear_but_keep_mem() which reuses all the 64KB chunks,; and the largest few individual allocations. The usefulness of this strategy is TBD. Currently all allocations are done with a JNI call to the C++, but fields of the object are; directly accessible so it's theoretically possible to try to write optimized Scala code; for the case of a small allocation which can fit in the current chunk. The other changes are mostly consequences of using absolute addresses rather than; offset-in-contiguous-buffer, and the change in the semantics of appendFoo() when a; Region's memory is in non-contiguous chunks - things which need to be located together,; such as the length of a string and its contents, now have to be within memory from a; single allocate() call.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718
https://github.com/hail-is/hail/pull/3718:510,Performance,optimiz,optimized,510,"This is an initial implementation of the Scala Region as a reference to a C++ off-heap object. The C++ Region allocates ""small"" blocks out of 64KB chunks, and ""large"" blocks using; malloc() directly. There's a clear_but_keep_mem() which reuses all the 64KB chunks,; and the largest few individual allocations. The usefulness of this strategy is TBD. Currently all allocations are done with a JNI call to the C++, but fields of the object are; directly accessible so it's theoretically possible to try to write optimized Scala code; for the case of a small allocation which can fit in the current chunk. The other changes are mostly consequences of using absolute addresses rather than; offset-in-contiguous-buffer, and the change in the semantics of appendFoo() when a; Region's memory is in non-contiguous chunks - things which need to be located together,; such as the length of a string and its contents, now have to be within memory from a; single allocate() call.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718
https://github.com/hail-is/hail/pull/3718:452,Security,access,accessible,452,"This is an initial implementation of the Scala Region as a reference to a C++ off-heap object. The C++ Region allocates ""small"" blocks out of 64KB chunks, and ""large"" blocks using; malloc() directly. There's a clear_but_keep_mem() which reuses all the 64KB chunks,; and the largest few individual allocations. The usefulness of this strategy is TBD. Currently all allocations are done with a JNI call to the C++, but fields of the object are; directly accessible so it's theoretically possible to try to write optimized Scala code; for the case of a small allocation which can fit in the current chunk. The other changes are mostly consequences of using absolute addresses rather than; offset-in-contiguous-buffer, and the change in the semantics of appendFoo() when a; Region's memory is in non-contiguous chunks - things which need to be located together,; such as the length of a string and its contents, now have to be within memory from a; single allocate() call.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3718
https://github.com/hail-is/hail/pull/3719:41,Testability,log,logic,41,"I think this cleans up the array sorting logic a little bit (at least, moves the sorting logic into an actual scala function instead of keeping it in bytecode-land). The new ordering functions are to prevent the boxing of arguments and return values that happen with asmfunctions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3719
https://github.com/hail-is/hail/pull/3719:89,Testability,log,logic,89,"I think this cleans up the array sorting logic a little bit (at least, moves the sorting logic into an actual scala function instead of keeping it in bytecode-land). The new ordering functions are to prevent the boxing of arguments and return values that happen with asmfunctions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3719
https://github.com/hail-is/hail/pull/3724:217,Testability,test,tests,217,Fixed the following things:; 1. initOp was in wrong place for AggregateRows; 2. no clearing of rv aggregators between groups in AggregateRows; 3. The post-agg function wasn't being used in AggregateCols. I added more tests in Python.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3724
https://github.com/hail-is/hail/pull/3724:83,Usability,clear,clearing,83,Fixed the following things:; 1. initOp was in wrong place for AggregateRows; 2. no clearing of rv aggregators between groups in AggregateRows; 3. The post-agg function wasn't being used in AggregateCols. I added more tests in Python.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3724
https://github.com/hail-is/hail/issues/3725:428,Availability,error,error,428,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-5b299ddae758. ### What you did: ; ```; hl.utils.hadoop_is_file('gs://gnomad-tmp/pbt_parents_sparse_genotype_matrix/cols_ann.ht/_SUCCESS'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Got a `FileNotFoundException`. Based on the documentation, I was expecting `False`. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-7-5ad16e5a0601> in <module>(); 1 hl.utils.hadoop_exists('gs://gnomad-tmp/pbt_parents_sparse_genotype_matrix/cols_ann.ht/_SUCCESS'); ----> 2 hl.utils.hadoop_is_file('gs://gnomad-tmp/pbt_parents_sparse_genotype_matrix/cols_ann.ht/_SUCCESS'). /home/hail/hail.zip/hail/utils/hadoop_utils.py in hadoop_is_file(path); 164 :obj:`.bool`; 165 """"""; --> 166 return Env.jutils().isFile(path, Env.hc()._jhc); 167 ; 168 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: FileNotFoundException: File not found : gs://gnomad-tmp/pbt_parents_sparse_genotype_matrix/cols_ann.ht/_SUCCESS; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3725
https://github.com/hail-is/hail/issues/3725:1571,Availability,Error,Error,1571,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-5b299ddae758. ### What you did: ; ```; hl.utils.hadoop_is_file('gs://gnomad-tmp/pbt_parents_sparse_genotype_matrix/cols_ann.ht/_SUCCESS'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Got a `FileNotFoundException`. Based on the documentation, I was expecting `False`. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-7-5ad16e5a0601> in <module>(); 1 hl.utils.hadoop_exists('gs://gnomad-tmp/pbt_parents_sparse_genotype_matrix/cols_ann.ht/_SUCCESS'); ----> 2 hl.utils.hadoop_is_file('gs://gnomad-tmp/pbt_parents_sparse_genotype_matrix/cols_ann.ht/_SUCCESS'). /home/hail/hail.zip/hail/utils/hadoop_utils.py in hadoop_is_file(path); 164 :obj:`.bool`; 165 """"""; --> 166 return Env.jutils().isFile(path, Env.hc()._jhc); 167 ; 168 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: FileNotFoundException: File not found : gs://gnomad-tmp/pbt_parents_sparse_genotype_matrix/cols_ann.ht/_SUCCESS; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3725
https://github.com/hail-is/hail/issues/3725:434,Integrability,message,messages,434,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-5b299ddae758. ### What you did: ; ```; hl.utils.hadoop_is_file('gs://gnomad-tmp/pbt_parents_sparse_genotype_matrix/cols_ann.ht/_SUCCESS'); ```. ### What went wrong (all error messages here, including the full java stack trace):. Got a `FileNotFoundException`. Based on the documentation, I was expecting `False`. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-7-5ad16e5a0601> in <module>(); 1 hl.utils.hadoop_exists('gs://gnomad-tmp/pbt_parents_sparse_genotype_matrix/cols_ann.ht/_SUCCESS'); ----> 2 hl.utils.hadoop_is_file('gs://gnomad-tmp/pbt_parents_sparse_genotype_matrix/cols_ann.ht/_SUCCESS'). /home/hail/hail.zip/hail/utils/hadoop_utils.py in hadoop_is_file(path); 164 :obj:`.bool`; 165 """"""; --> 166 return Env.jutils().isFile(path, Env.hc()._jhc); 167 ; 168 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: FileNotFoundException: File not found : gs://gnomad-tmp/pbt_parents_sparse_genotype_matrix/cols_ann.ht/_SUCCESS; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3725
https://github.com/hail-is/hail/pull/3726:33,Testability,test,tests,33,"checked that the python liftover tests pass on a cluster, too.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3726
https://github.com/hail-is/hail/pull/3727:2022,Energy Efficiency,allocate,allocate,2022,"a MatrixMapCols~ Moved to #3751. - ~fixed a bug in `IndexBTree` wherein if the number of elements was a multiple of 1024, an unnecessary 1024 elements were added to the end of the index file (which I believe breaks the reading process which expects the number of bytes to correspond to the size of the tree)~ Moved to #3750. - ~added `IndexBTree2` which is just an in-memory list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:2308,Integrability,contract,contract,2308,"y list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:2232,Modifiability,refactor,refactoring,2232,"eve breaks the reading process which expects the number of bytes to correspond to the size of the tree)~ Moved to #3750. - ~added `IndexBTree2` which is just an in-memory list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initializ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:2455,Modifiability,refactor,refactoring,2455,"y list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:2564,Modifiability,refactor,refactoring,2564," 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I also need to check that there are tests for or write tests for:. - indexing tables doesn't cause an extra shuffle; - ~the include lid and includ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:504,Performance,load,loaded,504,"This needs tests as well as I think I need to fix indexing (so I don't blow memory on the full genome), but I wanted to share what I've been up to with y'all. Also, caitlin can use this branch to run an analysis if it comes to that. I would also appreciate some feedback on the approach. It would be much more ideal to just make joins of variant tables against BGENs smarter, but I think the infrastructure necessary for that is big. cc: @cseed. List of changes:. - added `_variants_per_file` limits the loaded variants to variants at the given array of indexes (0-indexed, same order as on disk). - ~added `row_fields` which prevents reading and allocation of LID and RSID (also improved python-type-checking for `row_fields` and `entry_fields`)~ Moved to #3779 and #3778. - ~fixed table-table joins to _not_ always coerce (thus computing partition keys of) the right-hand table~ Moved to #3723 . - ~added a check that prevents globals and sample annotations copying when they're not used in the body of a MatrixMapCols~ Moved to #3751. - ~fixed a bug in `IndexBTree` wherein if the number of elements was a multiple of 1024, an unnecessary 1024 elements were added to the end of the index file (which I believe breaks the reading process which expects the number of bytes to correspond to the size of the tree)~ Moved to #3750. - ~added `IndexBTree2` which is just an in-memory list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:1604,Performance,load,load,1604," which prevents reading and allocation of LID and RSID (also improved python-type-checking for `row_fields` and `entry_fields`)~ Moved to #3779 and #3778. - ~fixed table-table joins to _not_ always coerce (thus computing partition keys of) the right-hand table~ Moved to #3723 . - ~added a check that prevents globals and sample annotations copying when they're not used in the body of a MatrixMapCols~ Moved to #3751. - ~fixed a bug in `IndexBTree` wherein if the number of elements was a multiple of 1024, an unnecessary 1024 elements were added to the end of the index file (which I believe breaks the reading process which expects the number of bytes to correspond to the size of the tree)~ Moved to #3750. - ~added `IndexBTree2` which is just an in-memory list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:2820,Performance,load,loading,2820,"ch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I also need to check that there are tests for or write tests for:. - indexing tables doesn't cause an extra shuffle; - ~the include lid and include raid flags~ included in #3779; - the variant list flag; - `getSplits`; - the variant filtering code; - ~loading a bgen with no entries~ exists: `BGENTests.test_import_bgen_no_entries`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:3268,Performance,load,loading,3268,"ch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I also need to check that there are tests for or write tests for:. - indexing tables doesn't cause an extra shuffle; - ~the include lid and include raid flags~ included in #3779; - the variant list flag; - `getSplits`; - the variant filtering code; - ~loading a bgen with no entries~ exists: `BGENTests.test_import_bgen_no_entries`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:3628,Performance,load,loading,3628,"ch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I also need to check that there are tests for or write tests for:. - indexing tables doesn't cause an extra shuffle; - ~the include lid and include raid flags~ included in #3779; - the variant list flag; - `getSplits`; - the variant filtering code; - ~loading a bgen with no entries~ exists: `BGENTests.test_import_bgen_no_entries`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:2608,Safety,unsafe,unsafeAdvance,2608,"r every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I also need to check that there are tests for or write tests for:. - indexing tables doesn't cause an extra shuffle; - ~the include lid and include raid flags~ included in #3779; - the variant list flag; - `getSplits`; - the variant filtering code; - ~loading a bgen with no entries~ exists: `BGENTests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:2797,Safety,unsafe,unsafeAdvance,2797,"ch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I also need to check that there are tests for or write tests for:. - indexing tables doesn't cause an extra shuffle; - ~the include lid and include raid flags~ included in #3779; - the variant list flag; - `getSplits`; - the variant filtering code; - ~loading a bgen with no entries~ exists: `BGENTests.test_import_bgen_no_entries`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:11,Testability,test,tests,11,"This needs tests as well as I think I need to fix indexing (so I don't blow memory on the full genome), but I wanted to share what I've been up to with y'all. Also, caitlin can use this branch to run an analysis if it comes to that. I would also appreciate some feedback on the approach. It would be much more ideal to just make joins of variant tables against BGENs smarter, but I think the infrastructure necessary for that is big. cc: @cseed. List of changes:. - added `_variants_per_file` limits the loaded variants to variants at the given array of indexes (0-indexed, same order as on disk). - ~added `row_fields` which prevents reading and allocation of LID and RSID (also improved python-type-checking for `row_fields` and `entry_fields`)~ Moved to #3779 and #3778. - ~fixed table-table joins to _not_ always coerce (thus computing partition keys of) the right-hand table~ Moved to #3723 . - ~added a check that prevents globals and sample annotations copying when they're not used in the body of a MatrixMapCols~ Moved to #3751. - ~fixed a bug in `IndexBTree` wherein if the number of elements was a multiple of 1024, an unnecessary 1024 elements were added to the end of the index file (which I believe breaks the reading process which expects the number of bytes to correspond to the size of the tree)~ Moved to #3750. - ~added `IndexBTree2` which is just an in-memory list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:1978,Testability,assert,asserts,1978,"a MatrixMapCols~ Moved to #3751. - ~fixed a bug in `IndexBTree` wherein if the number of elements was a multiple of 1024, an unnecessary 1024 elements were added to the end of the index file (which I believe breaks the reading process which expects the number of bytes to correspond to the size of the tree)~ Moved to #3750. - ~added `IndexBTree2` which is just an in-memory list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of fil",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:3412,Testability,test,tests,3412,"ch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I also need to check that there are tests for or write tests for:. - indexing tables doesn't cause an extra shuffle; - ~the include lid and include raid flags~ included in #3779; - the variant list flag; - `getSplits`; - the variant filtering code; - ~loading a bgen with no entries~ exists: `BGENTests.test_import_bgen_no_entries`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:3431,Testability,test,tests,3431,"ch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with fatals, so as not to allocate strings~ Moved to #3771. - ~We no longer copy the genotype data into a buffer in the block reader. This was forcing the `fastKeys` to do an unnecessary data copy~ Moved to #3783 (with some substantial refactoring so it doesn't look much like this PR anymore). - ~I changed the contract on BgenRecord to require that `getValue` is called to ""consume"" the record before the next record is taken~ Irrelevant thanks to #3783 's refactoring. - ~`getValue(null)` just skips bytes (no copy, no decompression)~ Irrelevant thanks to #3783 's refactoring. - ~I added `RegionValueBuilder.unsafeAdvance` which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work.~ Moved to #3773. - ~I use `RegionValueBuilder.unsafeAdvance` to make loading a BGEN without entry fields very fast.~ Rolled into #3783. - ~I fixed `Table.index` to not trigger a partition key info gathering~ Moved to #3774. I had to ship the arrays of filtered variant indices to the workers somehow, so I shipped them as base64 encoded arrays of bytes. It's pretty groady (and that's why I added the commons-codec library). I don't know how else to initialize record readers with hadoop. Generally, I think the BGEN loading code could use a clean up, and I haven't done that here, if anything I've made it more complicated. I also need to check that there are tests for or write tests for:. - indexing tables doesn't cause an extra shuffle; - ~the include lid and include raid flags~ included in #3779; - the variant list flag; - `getSplits`; - the variant filtering code; - ~loading a bgen with no entries~ exists: `BGENTests.test_import_bgen_no_entries`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/pull/3727:262,Usability,feedback,feedback,262,"This needs tests as well as I think I need to fix indexing (so I don't blow memory on the full genome), but I wanted to share what I've been up to with y'all. Also, caitlin can use this branch to run an analysis if it comes to that. I would also appreciate some feedback on the approach. It would be much more ideal to just make joins of variant tables against BGENs smarter, but I think the infrastructure necessary for that is big. cc: @cseed. List of changes:. - added `_variants_per_file` limits the loaded variants to variants at the given array of indexes (0-indexed, same order as on disk). - ~added `row_fields` which prevents reading and allocation of LID and RSID (also improved python-type-checking for `row_fields` and `entry_fields`)~ Moved to #3779 and #3778. - ~fixed table-table joins to _not_ always coerce (thus computing partition keys of) the right-hand table~ Moved to #3723 . - ~added a check that prevents globals and sample annotations copying when they're not used in the body of a MatrixMapCols~ Moved to #3751. - ~fixed a bug in `IndexBTree` wherein if the number of elements was a multiple of 1024, an unnecessary 1024 elements were added to the end of the index file (which I believe breaks the reading process which expects the number of bytes to correspond to the size of the tree)~ Moved to #3750. - ~added `IndexBTree2` which is just an in-memory list of the variant start positions. This is a fair bit of data. Chromosome 1 has about 250 million bases, so in the worst case this is 250 * 8 million bytes = 2 GB. It occurs to me that this is actually way to much data to load on the master node in general (since I just try to open the indexes for every file). I should switch this to a disk-based index.~ Made it disk-based, called it `OnDiskBTreeIndexToValue` #3794. - each hadoop `FileSplit` now contains a possibly null (indicating no filter) list of variants (by index) to keep, in practice this should be quite small. - ~I changed several asserts to `if`'s with ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3727
https://github.com/hail-is/hail/issues/3728:404,Availability,error,error,404,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-5b299ddae758. ### What you did:. ```; print(metrics_ht['vqsr'].globals); hl.eval_expr(metrics_ht['vqsr'].globals); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; <StructExpression of type struct{}>; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-51-e812f4924948> in <module>(); 1 print(metrics_ht['vqsr'].globals); ----> 2 hl.eval_expr(metrics_ht['vqsr'].globals). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:4549,Availability,Error,Error,4549,"self, exprs, broadcast_f); 1462 ; 1463 def cache(self):. /home/hail/hail.zip/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 354 for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; 355 if j not in used_joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.TableMapGlobals.execute(Relational.scala:2158); 	at is.hail.table.Table.value$lzycompute(Table.scala:243); 	at is.hail.table.Table.value(Table.scala:238); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:246); 	at is.hail.table.Table.x$5(Table.scala:246); 	at is.hail.table.Table.globals$lzycompute(Table.scala:246); 	at is.hail.table.Table.globals(Table.scala:246); 	at is.hail.utils.Py4jUtils$class.joinGlobals(Py4jUtils.scala:137); 	at is.hail.utils.package$.joinGlobals(package.scala:26); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:6143,Availability,Error,Error,6143,"131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.TableMapGlobals.execute(Relational.scala:2158); 	at is.hail.table.Table.value$lzycompute(Table.scala:243); 	at is.hail.table.Table.value(Table.scala:238); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:246); 	at is.hail.table.Table.x$5(Table.scala:246); 	at is.hail.table.Table.globals$lzycompute(Table.scala:246); 	at is.hail.table.Table.globals(Table.scala:246); 	at is.hail.utils.Py4jUtils$class.joinGlobals(Py4jUtils.scala:137); 	at is.hail.utils.package$.joinGlobals(package.scala:26); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-5b299ddae758; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:410,Integrability,message,messages,410,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-5b299ddae758. ### What you did:. ```; print(metrics_ht['vqsr'].globals); hl.eval_expr(metrics_ht['vqsr'].globals); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; <StructExpression of type struct{}>; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-51-e812f4924948> in <module>(); 1 print(metrics_ht['vqsr'].globals); ----> 2 hl.eval_expr(metrics_ht['vqsr'].globals). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:810,Integrability,wrap,wrapper,810,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-5b299ddae758. ### What you did:. ```; print(metrics_ht['vqsr'].globals); hl.eval_expr(metrics_ht['vqsr'].globals); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; <StructExpression of type struct{}>; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-51-e812f4924948> in <module>(); 1 print(metrics_ht['vqsr'].globals); ----> 2 hl.eval_expr(metrics_ht['vqsr'].globals). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:844,Integrability,wrap,wrapper,844,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-5b299ddae758. ### What you did:. ```; print(metrics_ht['vqsr'].globals); hl.eval_expr(metrics_ht['vqsr'].globals); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; <StructExpression of type struct{}>; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-51-e812f4924948> in <module>(); 1 print(metrics_ht['vqsr'].globals); ----> 2 hl.eval_expr(metrics_ht['vqsr'].globals). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:1013,Integrability,wrap,wrapper,1013,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-5b299ddae758. ### What you did:. ```; print(metrics_ht['vqsr'].globals); hl.eval_expr(metrics_ht['vqsr'].globals); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; <StructExpression of type struct{}>; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-51-e812f4924948> in <module>(); 1 print(metrics_ht['vqsr'].globals); ----> 2 hl.eval_expr(metrics_ht['vqsr'].globals). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:1269,Integrability,wrap,wrapper,1269," ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; <StructExpression of type struct{}>; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-51-e812f4924948> in <module>(); 1 print(metrics_ht['vqsr'].globals); ----> 2 hl.eval_expr(metrics_ht['vqsr'].globals). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _to_table(self, name); 584 source = source.select_globals(**{uid: self}); 585 df = Env.dummy_table(); --> 586 df = df.select(**{name: source.index_globals()[uid]}); 587 return df; 588 elif len(axes) == 1:. /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kw",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:1303,Integrability,wrap,wrapper,1303," ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; <StructExpression of type struct{}>; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-51-e812f4924948> in <module>(); 1 print(metrics_ht['vqsr'].globals); ----> 2 hl.eval_expr(metrics_ht['vqsr'].globals). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _to_table(self, name); 584 source = source.select_globals(**{uid: self}); 585 df = Env.dummy_table(); --> 586 df = df.select(**{name: source.index_globals()[uid]}); 587 return df; 588 elif len(axes) == 1:. /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kw",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:1472,Integrability,wrap,wrapper,1472," ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; <StructExpression of type struct{}>; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-51-e812f4924948> in <module>(); 1 print(metrics_ht['vqsr'].globals); ----> 2 hl.eval_expr(metrics_ht['vqsr'].globals). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _to_table(self, name); 584 source = source.select_globals(**{uid: self}); 585 df = Env.dummy_table(); --> 586 df = df.select(**{name: source.index_globals()[uid]}); 587 return df; 588 elif len(axes) == 1:. /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kw",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:2300,Integrability,wrap,wrapper,2300,"hod); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _to_table(self, name); 584 source = source.select_globals(**{uid: self}); 585 df = Env.dummy_table(); --> 586 df = df.select(**{name: source.index_globals()[uid]}); 587 return df; 588 elif len(axes) == 1:. /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . /home/hail/hail.zip/hail/table.py in _process_joins(self, *exprs); 1459 def broadcast_f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:2334,Integrability,wrap,wrapper,2334,"hod); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _to_table(self, name); 584 source = source.select_globals(**{uid: self}); 585 df = Env.dummy_table(); --> 586 df = df.select(**{name: source.index_globals()[uid]}); 587 return df; 588 elif len(axes) == 1:. /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . /home/hail/hail.zip/hail/table.py in _process_joins(self, *exprs); 1459 def broadcast_f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:2503,Integrability,wrap,wrapper,2503,"hod); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _to_table(self, name); 584 source = source.select_globals(**{uid: self}); 585 df = Env.dummy_table(); --> 586 df = df.select(**{name: source.index_globals()[uid]}); 587 return df; 588 elif len(axes) == 1:. /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . /home/hail/hail.zip/hail/table.py in _process_joins(self, *exprs); 1459 def broadcast_f",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:2840,Integrability,wrap,wrapper,2840,"ct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _to_table(self, name); 584 source = source.select_globals(**{uid: self}); 585 df = Env.dummy_table(); --> 586 df = df.select(**{name: source.index_globals()[uid]}); 587 return df; 588 elif len(axes) == 1:. /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . /home/hail/hail.zip/hail/table.py in _process_joins(self, *exprs); 1459 def broadcast_f(left, data, jt):; 1460 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1461 return process_joins(self, exprs, broadcast_f); 1462 ; 1463 def cache(self):. /home/hail/hail.zip/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 354 for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; 355 if j not in used_joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:2874,Integrability,wrap,wrapper,2874,"ct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _to_table(self, name); 584 source = source.select_globals(**{uid: self}); 585 df = Env.dummy_table(); --> 586 df = df.select(**{name: source.index_globals()[uid]}); 587 return df; 588 elif len(axes) == 1:. /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . /home/hail/hail.zip/hail/table.py in _process_joins(self, *exprs); 1459 def broadcast_f(left, data, jt):; 1460 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1461 return process_joins(self, exprs, broadcast_f); 1462 ; 1463 def cache(self):. /home/hail/hail.zip/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 354 for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; 355 if j not in used_joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:3043,Integrability,wrap,wrapper,3043,"ct(**{uid: t[uid]})).collect()]; 759 . /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _to_table(self, name); 584 source = source.select_globals(**{uid: self}); 585 df = Env.dummy_table(); --> 586 df = df.select(**{name: source.index_globals()[uid]}); 587 return df; 588 elif len(axes) == 1:. /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . /home/hail/hail.zip/hail/table.py in _process_joins(self, *exprs); 1459 def broadcast_f(left, data, jt):; 1460 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1461 return process_joins(self, exprs, broadcast_f); 1462 ; 1463 def cache(self):. /home/hail/hail.zip/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 354 for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; 355 if j not in used_joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:3810,Modifiability,extend,extend,3810,"ail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . /home/hail/hail.zip/hail/table.py in _process_joins(self, *exprs); 1459 def broadcast_f(left, data, jt):; 1460 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1461 return process_joins(self, exprs, broadcast_f); 1462 ; 1463 def cache(self):. /home/hail/hail.zip/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 354 for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; 355 if j not in used_joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.Asserti",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:3554,Performance,cache,cache,3554,"r, f). /home/hail/hail.zip/hail/table.py in select(self, *exprs, **named_exprs); 864 exprs, named_exprs, self._row_indices,; 865 protect_keys=True); --> 866 return self._select('Table.select', value_struct=hl.struct(**row)); 867 ; 868 @typecheck_method(exprs=oneof(str, Expression)). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . /home/hail/hail.zip/hail/table.py in _process_joins(self, *exprs); 1459 def broadcast_f(left, data, jt):; 1460 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1461 return process_joins(self, exprs, broadcast_f); 1462 ; 1463 def cache(self):. /home/hail/hail.zip/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 354 for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; 355 if j not in used_joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:3921,Testability,assert,assert,3921,"(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/table.py in _select(self, caller, key_struct, value_struct); 410 row = value_struct if value_struct is not None else hl.struct(); 411 ; --> 412 base, cleanup = self._process_joins(row); 413 analyze(caller, row, self._row_indices); 414 . /home/hail/hail.zip/hail/table.py in _process_joins(self, *exprs); 1459 def broadcast_f(left, data, jt):; 1460 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1461 return process_joins(self, exprs, broadcast_f); 1462 ; 1463 def cache(self):. /home/hail/hail.zip/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 354 for j in sorted(joins, key=lambda j: j.idx): # Make sure joins happen in order; 355 if j not in used_joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.TableMapGlobals.execute(Rela",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:4745,Testability,Assert,AssertionError,4745,"joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.TableMapGlobals.execute(Relational.scala:2158); 	at is.hail.table.Table.value$lzycompute(Table.scala:243); 	at is.hail.table.Table.value(Table.scala:238); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:246); 	at is.hail.table.Table.x$5(Table.scala:246); 	at is.hail.table.Table.globals$lzycompute(Table.scala:246); 	at is.hail.table.Table.globals(Table.scala:246); 	at is.hail.utils.Py4jUtils$class.joinGlobals(Py4jUtils.scala:137); 	at is.hail.utils.package$.joinGlobals(package.scala:26); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:4761,Testability,assert,assertion,4761,"joins:; --> 356 left = j.join_func(left); 357 all_uids.extend(j.temp_vars); 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.TableMapGlobals.execute(Relational.scala:2158); 	at is.hail.table.Table.value$lzycompute(Table.scala:243); 	at is.hail.table.Table.value(Table.scala:238); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:246); 	at is.hail.table.Table.x$5(Table.scala:246); 	at is.hail.table.Table.globals$lzycompute(Table.scala:246); 	at is.hail.table.Table.globals(Table.scala:246); 	at is.hail.utils.Py4jUtils$class.joinGlobals(Py4jUtils.scala:137); 	at is.hail.utils.package$.joinGlobals(package.scala:26); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:4808,Testability,Assert,AssertionError,4808," 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.TableMapGlobals.execute(Relational.scala:2158); 	at is.hail.table.Table.value$lzycompute(Table.scala:243); 	at is.hail.table.Table.value(Table.scala:238); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:246); 	at is.hail.table.Table.x$5(Table.scala:246); 	at is.hail.table.Table.globals$lzycompute(Table.scala:246); 	at is.hail.table.Table.globals(Table.scala:246); 	at is.hail.utils.Py4jUtils$class.joinGlobals(Py4jUtils.scala:137); 	at is.hail.utils.package$.joinGlobals(package.scala:26); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:4824,Testability,assert,assertion,4824," 358 used_joins.add(j). /home/hail/hail.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.TableMapGlobals.execute(Relational.scala:2158); 	at is.hail.table.Table.value$lzycompute(Table.scala:243); 	at is.hail.table.Table.value(Table.scala:238); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:246); 	at is.hail.table.Table.x$5(Table.scala:246); 	at is.hail.table.Table.globals$lzycompute(Table.scala:246); 	at is.hail.table.Table.globals(Table.scala:246); 	at is.hail.utils.Py4jUtils$class.joinGlobals(Py4jUtils.scala:137); 	at is.hail.utils.package$.joinGlobals(package.scala:26); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:4860,Testability,assert,assert,4860,"il.zip/hail/table.py in joiner(obj); 1448 else:; 1449 assert isinstance(obj, Table); -> 1450 return Table(Env.jutils().joinGlobals(obj._jt, self._jt, uid)); 1451 ; 1452 ast = Join(Select(TopLevelReference('global', Indices()), uid),. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.TableMapGlobals.execute(Relational.scala:2158); 	at is.hail.table.Table.value$lzycompute(Table.scala:243); 	at is.hail.table.Table.value(Table.scala:238); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:246); 	at is.hail.table.Table.x$5(Table.scala:246); 	at is.hail.table.Table.globals$lzycompute(Table.scala:246); 	at is.hail.table.Table.globals(Table.scala:246); 	at is.hail.utils.Py4jUtils$class.joinGlobals(Py4jUtils.scala:137); 	at is.hail.utils.package$.joinGlobals(package.scala:26); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:2",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:6158,Testability,Assert,AssertionError,6158,"131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.TableMapGlobals.execute(Relational.scala:2158); 	at is.hail.table.Table.value$lzycompute(Table.scala:243); 	at is.hail.table.Table.value(Table.scala:238); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:246); 	at is.hail.table.Table.x$5(Table.scala:246); 	at is.hail.table.Table.globals$lzycompute(Table.scala:246); 	at is.hail.table.Table.globals(Table.scala:246); 	at is.hail.utils.Py4jUtils$class.joinGlobals(Py4jUtils.scala:137); 	at is.hail.utils.package$.joinGlobals(package.scala:26); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-5b299ddae758; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3728:6174,Testability,assert,assertion,6174,"131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.TableMapGlobals.execute(Relational.scala:2158); 	at is.hail.table.Table.value$lzycompute(Table.scala:243); 	at is.hail.table.Table.value(Table.scala:238); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:246); 	at is.hail.table.Table.x$5(Table.scala:246); 	at is.hail.table.Table.globals$lzycompute(Table.scala:246); 	at is.hail.table.Table.globals(Table.scala:246); 	at is.hail.utils.Py4jUtils$class.joinGlobals(Py4jUtils.scala:137); 	at is.hail.utils.package$.joinGlobals(package.scala:26); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-5b299ddae758; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3728
https://github.com/hail-is/hail/issues/3729:1078,Availability,error,error,1078," the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-5b299ddae758. ### What you did:. ```; conc.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37> ; 'alleles': array<str> ; 'a_index': int32 ; 'was_split': bool ; 'concordance': array<struct {; concordance_matrix: struct {; n_discordant: int64, ; concordance: array<array<int64>>; }, ; meta: dict<str, str>; }> ; 'metrics': array<struct {; score: float64, ; positive_train: bool, ; negative_train: bool, ; rank: int64, ; meta: dict<str, str>; }> ; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. conc.aggregate(hl.agg.count_where(hl.sum(conc.concordance[0].concordance_matrix.concordance[:2].map(lambda x: hl.sum(x[:2])))>0)). ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-143-c80e74dd23d2> in <module>(); ----> 1 conc.aggregate(hl.agg.count_where(hl.sum(conc.concordance[0].concordance_matrix.concordance[:2].map(lambda x: hl.sum(x[:2])))>0)). /home/hail/hail.zip/hail/table.py in aggregate(self, expr); 1107 analyze('Table.aggregate', expr, self._global_indices, {self._row_axis}); 1108 ; -> 1109 result_json = base._jt.aggregateJSON(expr._ast.to_hql()); 1110 return expr.dtype._from_json(result_json); 1111 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3729
https://github.com/hail-is/hail/issues/3729:2168,Availability,Error,Error,2168,"race):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-143-c80e74dd23d2> in <module>(); ----> 1 conc.aggregate(hl.agg.count_where(hl.sum(conc.concordance[0].concordance_matrix.concordance[:2].map(lambda x: hl.sum(x[:2])))>0)). /home/hail/hail.zip/hail/table.py in aggregate(self, expr); 1107 analyze('Table.aggregate', expr, self._global_indices, {self._row_axis}); 1108 ; -> 1109 result_json = base._jt.aggregateJSON(expr._ast.to_hql()); 1110 return expr.dtype._from_json(result_json); 1111 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: VerifyError: Bad local variable type; Exception Details:; Location:; is/hail/codegen/generated/C423.apply(Lis/hail/annotations/Region;[Lis/hail/annotations/aggregators/RegionValueAggregator;JZJZ)V @2710: iload; Reason:; Type top (current frame, locals[130]) is not assignable to integer; Current Frame:; bci: @2710; flags: { }; locals: { 'is/hail/codegen/generated/C423', 'is/hail/annotations/Region', '[Lis/hail/annotations/aggregators/RegionValueAggregator;', long, long_2nd, integer, long, long_2nd, integer, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, integer, long, long_2nd, top, integer, integer, top, top, t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3729
https://github.com/hail-is/hail/issues/3729:1084,Integrability,message,messages,1084," the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-5b299ddae758. ### What you did:. ```; conc.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'locus': locus<GRCh37> ; 'alleles': array<str> ; 'a_index': int32 ; 'was_split': bool ; 'concordance': array<struct {; concordance_matrix: struct {; n_discordant: int64, ; concordance: array<array<int64>>; }, ; meta: dict<str, str>; }> ; 'metrics': array<struct {; score: float64, ; positive_train: bool, ; negative_train: bool, ; rank: int64, ; meta: dict<str, str>; }> ; ----------------------------------------; Key: ['locus', 'alleles']; ----------------------------------------. conc.aggregate(hl.agg.count_where(hl.sum(conc.concordance[0].concordance_matrix.concordance[:2].map(lambda x: hl.sum(x[:2])))>0)). ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-143-c80e74dd23d2> in <module>(); ----> 1 conc.aggregate(hl.agg.count_where(hl.sum(conc.concordance[0].concordance_matrix.concordance[:2].map(lambda x: hl.sum(x[:2])))>0)). /home/hail/hail.zip/hail/table.py in aggregate(self, expr); 1107 analyze('Table.aggregate', expr, self._global_indices, {self._row_axis}); 1108 ; -> 1109 result_json = base._jt.aggregateJSON(expr._ast.to_hql()); 1110 return expr.dtype._from_json(result_json); 1111 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalE",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3729
https://github.com/hail-is/hail/issues/3729:2387,Modifiability,variab,variable,2387,"rdance[:2].map(lambda x: hl.sum(x[:2])))>0)). /home/hail/hail.zip/hail/table.py in aggregate(self, expr); 1107 analyze('Table.aggregate', expr, self._global_indices, {self._row_axis}); 1108 ; -> 1109 result_json = base._jt.aggregateJSON(expr._ast.to_hql()); 1110 return expr.dtype._from_json(result_json); 1111 . /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: VerifyError: Bad local variable type; Exception Details:; Location:; is/hail/codegen/generated/C423.apply(Lis/hail/annotations/Region;[Lis/hail/annotations/aggregators/RegionValueAggregator;JZJZ)V @2710: iload; Reason:; Type top (current frame, locals[130]) is not assignable to integer; Current Frame:; bci: @2710; flags: { }; locals: { 'is/hail/codegen/generated/C423', 'is/hail/annotations/Region', '[Lis/hail/annotations/aggregators/RegionValueAggregator;', long, long_2nd, integer, long, long_2nd, integer, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, top, integer, long, long_2nd, top, integer, integer, top, top, top, integer, long, long_2nd, integer, top, integer, integer, integer, long, long_2nd, integer, long, long_2nd, integer, integer, integer }; stack: { 'is/hail/codegen/generated/C423' }; Bytecode:; 0x0000000: 1508 3630 1530 9900 06a7 0008 1606 a700; 0x0000010: 0614 0020 3731 1530",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3729
https://github.com/hail-is/hail/pull/3730:40,Testability,test,tested,40,"Builds on @jigold 's fixes in #3724 . I tested the new AST branch before I added the IR branch as well as in notebook using still-un-IRed functions (like `signum`), but I'm not sure the best way to force that branch to be continue to be tested.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3730
https://github.com/hail-is/hail/pull/3730:237,Testability,test,tested,237,"Builds on @jigold 's fixes in #3724 . I tested the new AST branch before I added the IR branch as well as in notebook using still-un-IRed functions (like `signum`), but I'm not sure the best way to force that branch to be continue to be tested.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3730
https://github.com/hail-is/hail/issues/3731:11483,Availability,Error,Error,11483,Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.Recur$.apply(Recur.scala:4); 	at is.hail.expr.ir.ExtractAggregators$.extract(ExtractAggregators.scala:68); 	at is.hail.expr.ir.ExtractAggregators$.extract(ExtractAggregators.scala:52); 	at is.hail.expr.ir.ExtractAggregators$.apply(ExtractAggregators.scala:25); 	at is.hail.expr.ir.CompileWithAggregators$.apply(Compile.scala:138); 	at is.hail.expr.ir.CompileWithAggregators$.apply(Compile.scala:217); 	at is.hail.expr.MatrixMapRows.execute(Relational.scala:1272); 	at is.hail.expr.MatrixMapGlobals.execute(Relational.scala:1575); 	at is.hail.expr.MatrixRowsTable.execute(Relational.scala:2275); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:504); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:15); 	at is.hail.table.Table.write(Table.scala:905); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-258b35d26fb3; Error summary: NullPointerException: null; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3731
https://github.com/hail-is/hail/issues/3731:8563,Integrability,Wrap,WrappedArray,8563,; 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.Recur$.apply(Recur.scala:4); 	at is.hail.expr.ir.ExtractAggregators$.extract(ExtractAggregators.scala:68); 	at is.hail.expr.ir.ExtractAggregators$.is$hail$expr$ir$ExtractAggregators$$extract$1(ExtractAggregators.scala:57); 	at is.hail.expr.ir.ExtractAggregators$$anonfun$extract$1.apply(ExtractAggregators.scala:68); 	at is.hail.expr.ir.ExtractAggregators$$anonfun$extract$1.apply(ExtractAggregators.scala:68); 	at is.hail.expr.ir.Recur$$anonfun$apply$1.apply(Recur.scala:5); 	at is.hail.expr.ir.Recur$$anonfun$apply$1.apply(Recur.scala:4); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.Recur$.apply(Recur.scala:4); 	at is.hail.expr.ir.ExtractAggregators$.extract(ExtractAggregators.scala:68); 	at is.hail.expr.ir.ExtractAggregators$.is$hail$expr$ir$ExtractAggregators$$extract$1(ExtractAggregators.scala:57); 	at is.hail.expr.ir.ExtractAggregators$$anonfun$extract$1.apply(ExtractAggregators.scala:68); 	at is.hail.expr.ir.ExtractAggregators$$anonfun$extract$1.apply(ExtractAggregators.scala:68); 	at is.hail.expr.ir.Recur$$anonfun$apply$1.apply(Recur.scala:5); 	at is.hail.expr.ir.Recur$$anonfun$apply$1.apply(Recur.scala:4); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3731
https://github.com/hail-is/hail/issues/3731:8584,Integrability,Wrap,WrappedArray,8584,ction.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.Recur$.apply(Recur.scala:4); 	at is.hail.expr.ir.ExtractAggregators$.extract(ExtractAggregators.scala:68); 	at is.hail.expr.ir.ExtractAggregators$.is$hail$expr$ir$ExtractAggregators$$extract$1(ExtractAggregators.scala:57); 	at is.hail.expr.ir.ExtractAggregators$$anonfun$extract$1.apply(ExtractAggregators.scala:68); 	at is.hail.expr.ir.ExtractAggregators$$anonfun$extract$1.apply(ExtractAggregators.scala:68); 	at is.hail.expr.ir.Recur$$anonfun$apply$1.apply(Recur.scala:5); 	at is.hail.expr.ir.Recur$$anonfun$apply$1.apply(Recur.scala:4); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at is.hail.expr.ir.Recur$.apply(Recur.scala:4); 	at is.hail.expr.ir.ExtractAggregators$.extract(ExtractAggregators.scala:68); 	at is.hail.expr.ir.ExtractAggregators$.is$hail$expr$ir$ExtractAggregators$$extract$1(ExtractAggregators.scala:57); 	at is.hail.expr.ir.ExtractAggregators$$anonfun$extract$1.apply(ExtractAggregators.scala:68); 	at is.hail.expr.ir.ExtractAggregators$$anonfun$extract$1.apply(ExtractAggregators.scala:68); 	at is.hail.expr.ir.Recur$$anonfun$apply$1.apply(Recur.scala:5); 	at is.hail.expr.ir.Recur$$anonfun$apply$1.apply(Recur.scala:4); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3731
https://github.com/hail-is/hail/issues/3734:80,Deployability,pipeline,pipeline,80,"`devel-3b467d2acf9a` appears to be ~100X faster than `devel-258b35d26fb3`. Same pipeline as before:; ```; sample_group_filters = {; ""qc_samples_raw"": mt.meta.high_quality,; }; mt = mt.select_cols(**sample_group_filters); mt = mt.select_rows(). lin = 10 ** (-mt.PL / 10.0); gp0 = lin[0] / hl.sum(lin); pab = hl.binom_test(mt.AD[1], hl.sum(mt.AD), 0.5, ""two.sided""). qc_stats_expression = []; for group in sample_group_filters.keys():; criteria = mt[group]; hets = criteria & mt.GT.is_het(); non_refs = criteria & mt.GT.is_non_ref(). gq_agg = hl.agg.filter(non_refs, mt.GQ); dp_agg = hl.agg.filter(non_refs, mt.DP); pab_agg = hl.agg.filter(hets, pab). qual_agg = -10 * hl.agg.sum(hl.agg.filter(non_refs, hl.cond(; mt.PL[0] > 3000, -300, hl.log10(gp0); ))). stats_expression = hl.struct(; pab=hl.agg.stats(pab_agg),; qd=hl.min(35, qual_agg / hl.agg.sum(dp_agg)),; meta={'group': group}); qc_stats_expression.append(stats_expression); mt.annotate_rows(qc_stats=qc_stats_expression).rows().write(); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3734
https://github.com/hail-is/hail/pull/3737:27,Integrability,interface,interface,27,it uses the deprecated AST interface,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3737
https://github.com/hail-is/hail/pull/3738:28,Integrability,interface,interface,28,They use the deprecated AST interface. Rewrite in terms of Table interface.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3738
https://github.com/hail-is/hail/pull/3738:65,Integrability,interface,interface,65,They use the deprecated AST interface. Rewrite in terms of Table interface.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3738
https://github.com/hail-is/hail/pull/3738:39,Modifiability,Rewrite,Rewrite,39,They use the deprecated AST interface. Rewrite in terms of Table interface.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3738
https://github.com/hail-is/hail/pull/3739:25,Integrability,interface,interface,25,don't use deprecated AST interface,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3739
https://github.com/hail-is/hail/pull/3742:128,Testability,Test,Tests,128,"- No need to split sample.vcf, it is already biallelic.; - SplitSuite.test_split was already ported to Python, see test_methods.Tests.test_split_multi_hts.; - VariantQC and SampleQC in Scala are going away now that they've been implemented in Python, AggregatorSuite.test{Rows, Cols} don't make sense anymore, delete.; - Ported HWESuite to Python.; - Removed SplitSuite.Spec, I claim it is expensive and not very useful.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3742
https://github.com/hail-is/hail/pull/3742:267,Testability,test,test,267,"- No need to split sample.vcf, it is already biallelic.; - SplitSuite.test_split was already ported to Python, see test_methods.Tests.test_split_multi_hts.; - VariantQC and SampleQC in Scala are going away now that they've been implemented in Python, AggregatorSuite.test{Rows, Cols} don't make sense anymore, delete.; - Ported HWESuite to Python.; - Removed SplitSuite.Spec, I claim it is expensive and not very useful.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3742
https://github.com/hail-is/hail/pull/3743:65,Usability,simpl,simpler,65,Ported ImportBgenSuite to Python. I mostly did this to make this simpler.; Removed some unused funtions in RichMatrixTable. We can double-check with the community before merging.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3743
https://github.com/hail-is/hail/issues/3744:1473,Availability,error,error,1473,"l_idx, mt._row_idx]); ); # [col_idx, [row_idx]]; .group_by(lambda x: x[0]); # [[col_idx, row_idx), (col_idx, row_idx), ...], ...]; .values(); # [[row_idx, row_idx, ...], ...]; .map(lambda x: x.map(lambda y: y[0])); .flatmap(lambda x: hl.range(0, hl.len(x)); .flatmap(lambda i1: hl.range(i1 + 1, hl.len(x)); .map(lambda i2: hl.tuple([x[i1], x[i2]])))); ); ). mt.describe(). ht = mt.annotate_rows(; variant_pairs=hl.agg.take(mt.variant_pairs_entry, 1)[0]; ).rows(). ht = ht.explode('variant_pairs'); ht = ht.key_by(v1_idx=ht.variant_pairs[0], v2_idx=ht.variant_pairs[1]); return ht.select(). ht = variant_pairs_ht(mt, ['gene']; ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'v1_idx': int32 ; 'v2_idx': int32 ; ----------------------------------------; Key: ['v1_idx', 'v2_idx']; ----------------------------------------; ht.show(). ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:2509,Availability,Error,Error,2509," stack trace):; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.Emit$.apply(Emit.scala:28); at is.hail.expr.ir.Compile$.apply(Compile.scala:49); at is.hail.expr.ir.Compile$.apply(Compile.scala:31); at is.hail.expr.ir.Compile$.apply(Compile.scala:62); at is.hail.expr.TableExplode.execute(Relational.scala:2201); at is.hail.expr.TableUnkey.execute(Relational.scala:1883); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.table.Table.value$lzycompute(Table.scala:243); at is.hail.table.Table.value(Table.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:4559,Availability,Error,Error,4559,: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.Emit$.apply(Emit.scala:28); at is.hail.expr.ir.Compile$.apply(Compile.scala:49); at is.hail.expr.ir.Compile$.apply(Compile.scala:31); at is.hail.expr.ir.Compile$.apply(Compile.scala:62); at is.hail.expr.TableExplode.execute(Relational.scala:2201); at is.hail.expr.TableUnkey.execute(Relational.scala:1883); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.table.Table.value$lzycompute(Table.scala:243); at is.hail.table.Table.value(Table.scala:238); at is.hail.table.Table.x$5$lzycompute(Table.scala:246); at is.hail.table.Table.x$5(Table.scala:246); at is.hail.table.Table.rvd$lzycompute(Table.scala:246); at is.hail.table.Table.rvd(Table.scala:246); at is.hail.table.Table.take(Table.scala:961); at is.hail.table.Table.showString(Table.scala:1002); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745); Hail version: devel-10a75bb57a6f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:1833,Deployability,release,release,1833,".len(x)); .map(lambda i2: hl.tuple([x[i1], x[i2]])))); ); ). mt.describe(). ht = mt.annotate_rows(; variant_pairs=hl.agg.take(mt.variant_pairs_entry, 1)[0]; ).rows(). ht = ht.explode('variant_pairs'); ht = ht.key_by(v1_idx=ht.variant_pairs[0], v2_idx=ht.variant_pairs[1]); return ht.select(). ht = variant_pairs_ht(mt, ['gene']; ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'v1_idx': int32 ; 'v2_idx': int32 ; ----------------------------------------; Key: ['v1_idx', 'v2_idx']; ----------------------------------------; ht.show(). ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:1965,Deployability,release,release,1965,"pairs=hl.agg.take(mt.variant_pairs_entry, 1)[0]; ).rows(). ht = ht.explode('variant_pairs'); ht = ht.key_by(v1_idx=ht.variant_pairs[0], v2_idx=ht.variant_pairs[1]); return ht.select(). ht = variant_pairs_ht(mt, ['gene']; ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'v1_idx': int32 ; 'v2_idx': int32 ; ----------------------------------------; Key: ['v1_idx', 'v2_idx']; ----------------------------------------; ht.show(). ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:2101,Deployability,release,release,2101,"ariant_pairs[1]); return ht.select(). ht = variant_pairs_ht(mt, ['gene']; ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'v1_idx': int32 ; 'v2_idx': int32 ; ----------------------------------------; Key: ['v1_idx', 'v2_idx']; ----------------------------------------; ht.show(). ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.Emit$.apply(Emit.scala:28); at is.hail.expr.ir.Compile$.apply(Compile.scala:49); at is.hail.expr.ir.Compile$.apply(Compile.scala:31); at is.hail.ex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:2445,Deployability,release,release,2445,"show(). ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.Emit$.apply(Emit.scala:28); at is.hail.expr.ir.Compile$.apply(Compile.scala:49); at is.hail.expr.ir.Compile$.apply(Compile.scala:31); at is.hail.expr.ir.Compile$.apply(Compile.scala:62); at is.hail.expr.TableExplode.execute(Relational.scala:2201); at is.hail.expr.TableUnkey.execute(Relational.scala:1883); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.table",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:1479,Integrability,message,messages,1479,"l_idx, mt._row_idx]); ); # [col_idx, [row_idx]]; .group_by(lambda x: x[0]); # [[col_idx, row_idx), (col_idx, row_idx), ...], ...]; .values(); # [[row_idx, row_idx, ...], ...]; .map(lambda x: x.map(lambda y: y[0])); .flatmap(lambda x: hl.range(0, hl.len(x)); .flatmap(lambda i1: hl.range(i1 + 1, hl.len(x)); .map(lambda i2: hl.tuple([x[i1], x[i2]])))); ); ). mt.describe(). ht = mt.annotate_rows(; variant_pairs=hl.agg.take(mt.variant_pairs_entry, 1)[0]; ).rows(). ht = ht.explode('variant_pairs'); ht = ht.key_by(v1_idx=ht.variant_pairs[0], v2_idx=ht.variant_pairs[1]); return ht.select(). ht = variant_pairs_ht(mt, ['gene']; ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'v1_idx': int32 ; 'v2_idx': int32 ; ----------------------------------------; Key: ['v1_idx', 'v2_idx']; ----------------------------------------; ht.show(). ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:1895,Integrability,wrap,wrapper,1895,"pairs=hl.agg.take(mt.variant_pairs_entry, 1)[0]; ).rows(). ht = ht.explode('variant_pairs'); ht = ht.key_by(v1_idx=ht.variant_pairs[0], v2_idx=ht.variant_pairs[1]); return ht.select(). ht = variant_pairs_ht(mt, ['gene']; ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'v1_idx': int32 ; 'v2_idx': int32 ; ----------------------------------------; Key: ['v1_idx', 'v2_idx']; ----------------------------------------; ht.show(). ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:2613,Testability,Assert,AssertionError,2613,"ractiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.Emit$.apply(Emit.scala:28); at is.hail.expr.ir.Compile$.apply(Compile.scala:49); at is.hail.expr.ir.Compile$.apply(Compile.scala:31); at is.hail.expr.ir.Compile$.apply(Compile.scala:62); at is.hail.expr.TableExplode.execute(Relational.scala:2201); at is.hail.expr.TableUnkey.execute(Relational.scala:1883); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.table.Table.value$lzycompute(Table.scala:243); at is.hail.table.Table.value(Table.scala:238); at is.hail.table.Table.x$5$lzycompute(Table.scala:246); at is.hail.table.Table.x$5(Table.scala:246); at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:2629,Testability,assert,assertion,2629,"ractiveshell.py"", line 2910, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.Emit$.apply(Emit.scala:28); at is.hail.expr.ir.Compile$.apply(Compile.scala:49); at is.hail.expr.ir.Compile$.apply(Compile.scala:31); at is.hail.expr.ir.Compile$.apply(Compile.scala:62); at is.hail.expr.TableExplode.execute(Relational.scala:2201); at is.hail.expr.TableUnkey.execute(Relational.scala:1883); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.table.Table.value$lzycompute(Table.scala:243); at is.hail.table.Table.value(Table.scala:238); at is.hail.table.Table.x$5$lzycompute(Table.scala:246); at is.hail.table.Table.x$5(Table.scala:246); at is.hail.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:2676,Testability,Assert,AssertionError,2676,".user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.Emit$.apply(Emit.scala:28); at is.hail.expr.ir.Compile$.apply(Compile.scala:49); at is.hail.expr.ir.Compile$.apply(Compile.scala:31); at is.hail.expr.ir.Compile$.apply(Compile.scala:62); at is.hail.expr.TableExplode.execute(Relational.scala:2201); at is.hail.expr.TableUnkey.execute(Relational.scala:1883); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.table.Table.value$lzycompute(Table.scala:243); at is.hail.table.Table.value(Table.scala:238); at is.hail.table.Table.x$5$lzycompute(Table.scala:246); at is.hail.table.Table.x$5(Table.scala:246); at is.hail.table.Table.rvd$lzycompute(Table.scala:246); at is.hail.table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:2692,Testability,assert,assertion,2692,".user_global_ns, self.user_ns); File ""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.Emit$.apply(Emit.scala:28); at is.hail.expr.ir.Compile$.apply(Compile.scala:49); at is.hail.expr.ir.Compile$.apply(Compile.scala:31); at is.hail.expr.ir.Compile$.apply(Compile.scala:62); at is.hail.expr.TableExplode.execute(Relational.scala:2201); at is.hail.expr.TableUnkey.execute(Relational.scala:1883); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.table.Table.value$lzycompute(Table.scala:243); at is.hail.table.Table.value(Table.scala:238); at is.hail.table.Table.x$5$lzycompute(Table.scala:246); at is.hail.table.Table.x$5(Table.scala:246); at is.hail.table.Table.rvd$lzycompute(Table.scala:246); at is.hail.table.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:2727,Testability,assert,assert,2727,"""<ipython-input-53-73b5a6c78295>"", line 1, in <module>; ht.show(); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1169, in show; print(self._show(n,width, truncate, types)); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/table.py"", line 1172, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/Users/laurent/tools/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/Users/laurent/tools/hail-release/devel/hail.zip/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.Emit$.apply(Emit.scala:28); at is.hail.expr.ir.Compile$.apply(Compile.scala:49); at is.hail.expr.ir.Compile$.apply(Compile.scala:31); at is.hail.expr.ir.Compile$.apply(Compile.scala:62); at is.hail.expr.TableExplode.execute(Relational.scala:2201); at is.hail.expr.TableUnkey.execute(Relational.scala:1883); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.table.Table.value$lzycompute(Table.scala:243); at is.hail.table.Table.value(Table.scala:238); at is.hail.table.Table.x$5$lzycompute(Table.scala:246); at is.hail.table.Table.x$5(Table.scala:246); at is.hail.table.Table.rvd$lzycompute(Table.scala:246); at is.hail.table.Table.rvd(Table.scala:246); at is.ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:4574,Testability,Assert,AssertionError,4574,: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.Emit$.apply(Emit.scala:28); at is.hail.expr.ir.Compile$.apply(Compile.scala:49); at is.hail.expr.ir.Compile$.apply(Compile.scala:31); at is.hail.expr.ir.Compile$.apply(Compile.scala:62); at is.hail.expr.TableExplode.execute(Relational.scala:2201); at is.hail.expr.TableUnkey.execute(Relational.scala:1883); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.table.Table.value$lzycompute(Table.scala:243); at is.hail.table.Table.value(Table.scala:238); at is.hail.table.Table.x$5$lzycompute(Table.scala:246); at is.hail.table.Table.x$5(Table.scala:246); at is.hail.table.Table.rvd$lzycompute(Table.scala:246); at is.hail.table.Table.rvd(Table.scala:246); at is.hail.table.Table.take(Table.scala:961); at is.hail.table.Table.showString(Table.scala:1002); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745); Hail version: devel-10a75bb57a6f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3744:4590,Testability,assert,assertion,4590,: AssertionError: assertion failed; Java stack trace:; java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:78); at is.hail.expr.ir.TypeCheck$.apply(TypeCheck.scala:7); at is.hail.expr.ir.Emit$.emit(Emit.scala:42); at is.hail.expr.ir.Emit$.apply(Emit.scala:28); at is.hail.expr.ir.Compile$.apply(Compile.scala:49); at is.hail.expr.ir.Compile$.apply(Compile.scala:31); at is.hail.expr.ir.Compile$.apply(Compile.scala:62); at is.hail.expr.TableExplode.execute(Relational.scala:2201); at is.hail.expr.TableUnkey.execute(Relational.scala:1883); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); at is.hail.expr.TableMapRows.execute(Relational.scala:2090); at is.hail.table.Table.value$lzycompute(Table.scala:243); at is.hail.table.Table.value(Table.scala:238); at is.hail.table.Table.x$5$lzycompute(Table.scala:246); at is.hail.table.Table.x$5(Table.scala:246); at is.hail.table.Table.rvd$lzycompute(Table.scala:246); at is.hail.table.Table.rvd(Table.scala:246); at is.hail.table.Table.take(Table.scala:961); at is.hail.table.Table.showString(Table.scala:1002); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745); Hail version: devel-10a75bb57a6f; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3744
https://github.com/hail-is/hail/issues/3746:709,Availability,down,downsampling,709,"Trying to run a bunch of 201 aggregators + uniroot on a custom branch (https://github.com/konradjk/hail/tree/freq_filter), and getting:; ```; Traceback (most recent call last):; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/generate_frequency_data.py"", line 141, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/pyscripts_CSD0RS.zip/gnomad_hail/utils/slack.py"", line 112, in try_slack; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/pyscripts_CSD0RS.zip/gnomad_hail/utils/slack.py"", line 95, in try_slack; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/generate_frequency_data.py"", line 121, in main; mt, sample_table = generate_frequency_data(mt, args.subpop, args.downsampling, args.genomes); File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/generate_frequency_data.py"", line 102, in generate_frequency_data; mt = mt.annotate_rows(freq=frequency_expression); File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/hail-1e2e8c7e8.zip/hail/matrixtable.py"", line 905, in annotate_rows; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/hail-1e2e8c7e8.zip/hail/typecheck/check.py"", line 547, in wrapper; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/hail-1e2e8c7e8.zip/hail/matrixtable.py"", line 2893, in _select_rows; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/hail-1e2e8c7e8.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: RuntimeException: Method code too large!. Java stack trace:; java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:306); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:340); 	at is.hail.expr.CM.runWithDelayedValues(CM.scala:80); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$evalNoTypeCheck(Parser.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3746
https://github.com/hail-is/hail/issues/3746:2993,Availability,Error,Error,2993,"30/hail-1e2e8c7e8.zip/hail/typecheck/check.py"", line 547, in wrapper; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/hail-1e2e8c7e8.zip/hail/matrixtable.py"", line 2893, in _select_rows; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/hail-1e2e8c7e8.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: RuntimeException: Method code too large!. Java stack trace:; java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:306); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:340); 	at is.hail.expr.CM.runWithDelayedValues(CM.scala:80); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$evalNoTypeCheck(Parser.scala:60); 	at is.hail.expr.Parser$.eval(Parser.scala:73); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:88); 	at is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1232); 	at is.hail.variant.MatrixTable.selectRows(MatrixTable.scala:1205); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-1e2e8c7e86f3; Error summary: RuntimeException: Method code too large!; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3746
https://github.com/hail-is/hail/issues/3746:1114,Integrability,wrap,wrapper,1114,"`; Traceback (most recent call last):; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/generate_frequency_data.py"", line 141, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/pyscripts_CSD0RS.zip/gnomad_hail/utils/slack.py"", line 112, in try_slack; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/pyscripts_CSD0RS.zip/gnomad_hail/utils/slack.py"", line 95, in try_slack; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/generate_frequency_data.py"", line 121, in main; mt, sample_table = generate_frequency_data(mt, args.subpop, args.downsampling, args.genomes); File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/generate_frequency_data.py"", line 102, in generate_frequency_data; mt = mt.annotate_rows(freq=frequency_expression); File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/hail-1e2e8c7e8.zip/hail/matrixtable.py"", line 905, in annotate_rows; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/hail-1e2e8c7e8.zip/hail/typecheck/check.py"", line 547, in wrapper; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/hail-1e2e8c7e8.zip/hail/matrixtable.py"", line 2893, in _select_rows; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/9c7c1cdf3da74749a388ecb2e4365430/hail-1e2e8c7e8.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: RuntimeException: Method code too large!. Java stack trace:; java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:306); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:340); 	at is.hail.expr.CM.runWithDelayedValues(CM.scala:80); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$evalNoTypeCheck(Parser.scala:60); 	at is.hail.expr.Parser$.eval(Parser.scala:73); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:88); 	at is.hail.variant.MatrixT",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3746
https://github.com/hail-is/hail/issues/3748:329,Availability,error,error,329,"### Hail version:; devel-10a75bb57a6f; ### What you did:. ```; def test_rekey_correct_partition_key(self):; ht = hl.utils.range_table(5); ht = ht.add_index('a'); ht = ht.key_by('idx', 'a'); ht = ht.annotate(b=ht.idx); ht = ht.key_by('idx', 'b'); self.assertEqual(ht.aggregate(agg.sum(ht.idx)), 10); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVDType.<init>(OrderedRVDType.scala:21); 	at is.hail.rvd.OrderedRVDType.copy(OrderedRVDType.scala:121); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1857); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2090); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:520); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); 	at is.hail.table.Table.aggregate(Table.scala:465); 	at is.hail.table.Table.aggregate(Table.scala:446); 	at is.hail.table.Table.aggregateJSON(Table.scala:436); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3748
https://github.com/hail-is/hail/issues/3748:335,Integrability,message,messages,335,"### Hail version:; devel-10a75bb57a6f; ### What you did:. ```; def test_rekey_correct_partition_key(self):; ht = hl.utils.range_table(5); ht = ht.add_index('a'); ht = ht.key_by('idx', 'a'); ht = ht.annotate(b=ht.idx); ht = ht.key_by('idx', 'b'); self.assertEqual(ht.aggregate(agg.sum(ht.idx)), 10); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVDType.<init>(OrderedRVDType.scala:21); 	at is.hail.rvd.OrderedRVDType.copy(OrderedRVDType.scala:121); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1857); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2090); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:520); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); 	at is.hail.table.Table.aggregate(Table.scala:465); 	at is.hail.table.Table.aggregate(Table.scala:446); 	at is.hail.table.Table.aggregateJSON(Table.scala:436); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3748
https://github.com/hail-is/hail/issues/3748:251,Testability,assert,assertEqual,251,"### Hail version:; devel-10a75bb57a6f; ### What you did:. ```; def test_rekey_correct_partition_key(self):; ht = hl.utils.range_table(5); ht = ht.add_index('a'); ht = ht.key_by('idx', 'a'); ht = ht.annotate(b=ht.idx); ht = ht.key_by('idx', 'b'); self.assertEqual(ht.aggregate(agg.sum(ht.idx)), 10); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVDType.<init>(OrderedRVDType.scala:21); 	at is.hail.rvd.OrderedRVDType.copy(OrderedRVDType.scala:121); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1857); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2090); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:520); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); 	at is.hail.table.Table.aggregate(Table.scala:465); 	at is.hail.table.Table.aggregate(Table.scala:446); 	at is.hail.table.Table.aggregateJSON(Table.scala:436); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3748
https://github.com/hail-is/hail/issues/3748:423,Testability,Assert,AssertionError,423,"### Hail version:; devel-10a75bb57a6f; ### What you did:. ```; def test_rekey_correct_partition_key(self):; ht = hl.utils.range_table(5); ht = ht.add_index('a'); ht = ht.key_by('idx', 'a'); ht = ht.annotate(b=ht.idx); ht = ht.key_by('idx', 'b'); self.assertEqual(ht.aggregate(agg.sum(ht.idx)), 10); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVDType.<init>(OrderedRVDType.scala:21); 	at is.hail.rvd.OrderedRVDType.copy(OrderedRVDType.scala:121); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1857); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2090); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:520); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); 	at is.hail.table.Table.aggregate(Table.scala:465); 	at is.hail.table.Table.aggregate(Table.scala:446); 	at is.hail.table.Table.aggregateJSON(Table.scala:436); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3748
https://github.com/hail-is/hail/issues/3748:439,Testability,assert,assertion,439,"### Hail version:; devel-10a75bb57a6f; ### What you did:. ```; def test_rekey_correct_partition_key(self):; ht = hl.utils.range_table(5); ht = ht.add_index('a'); ht = ht.key_by('idx', 'a'); ht = ht.annotate(b=ht.idx); ht = ht.key_by('idx', 'b'); self.assertEqual(ht.aggregate(agg.sum(ht.idx)), 10); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVDType.<init>(OrderedRVDType.scala:21); 	at is.hail.rvd.OrderedRVDType.copy(OrderedRVDType.scala:121); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1857); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2090); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:520); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); 	at is.hail.table.Table.aggregate(Table.scala:465); 	at is.hail.table.Table.aggregate(Table.scala:446); 	at is.hail.table.Table.aggregateJSON(Table.scala:436); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3748
https://github.com/hail-is/hail/issues/3748:475,Testability,assert,assert,475,"### Hail version:; devel-10a75bb57a6f; ### What you did:. ```; def test_rekey_correct_partition_key(self):; ht = hl.utils.range_table(5); ht = ht.add_index('a'); ht = ht.key_by('idx', 'a'); ht = ht.annotate(b=ht.idx); ht = ht.key_by('idx', 'b'); self.assertEqual(ht.aggregate(agg.sum(ht.idx)), 10); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVDType.<init>(OrderedRVDType.scala:21); 	at is.hail.rvd.OrderedRVDType.copy(OrderedRVDType.scala:121); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1857); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2090); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1846); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:520); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:39); 	at is.hail.table.Table.aggregate(Table.scala:465); 	at is.hail.table.Table.aggregate(Table.scala:446); 	at is.hail.table.Table.aggregateJSON(Table.scala:436); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3748
https://github.com/hail-is/hail/pull/3750:337,Availability,error,errors,337,"To enable easy testing, I also parameterized the methods by the branchingFactor and broke generation of the byte array away from writing the byte array to a file. The key issue is that `k * 1024 % 1024 = 0` for any integer `k`, which we were interpreting as meaning that the last block needed 1024 more elements to be full. There are no errors on write. On read, we try to calculate the number of layers present in the BGEN using `calcDepth` but this fails to correctly guess the layers when the size of the file is not a positive integral power of 1024. The only real changes (the rest are restructuring/whitespace) are using `branchingFactor` in place of `1024` and replacing; ```; - // Pad last layer so last block is 1024 elements (1024*8 bytes); - val paddingRequired = 1024 - (arr.length % 1024); ```; with; ```; + // Pad last layer so last block is branchingFactor elements (branchingFactor*8 bytes); + val danglingElements = (arr.length % branchingFactor); + val paddingRequired =; + if (danglingElements == 0) 0; + else branchingFactor - danglingElements; ```. cc: @jigold one of the PRs you asked me to break out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3750
https://github.com/hail-is/hail/pull/3750:540,Energy Efficiency,power,power,540,"To enable easy testing, I also parameterized the methods by the branchingFactor and broke generation of the byte array away from writing the byte array to a file. The key issue is that `k * 1024 % 1024 = 0` for any integer `k`, which we were interpreting as meaning that the last block needed 1024 more elements to be full. There are no errors on write. On read, we try to calculate the number of layers present in the BGEN using `calcDepth` but this fails to correctly guess the layers when the size of the file is not a positive integral power of 1024. The only real changes (the rest are restructuring/whitespace) are using `branchingFactor` in place of `1024` and replacing; ```; - // Pad last layer so last block is 1024 elements (1024*8 bytes); - val paddingRequired = 1024 - (arr.length % 1024); ```; with; ```; + // Pad last layer so last block is branchingFactor elements (branchingFactor*8 bytes); + val danglingElements = (arr.length % branchingFactor); + val paddingRequired =; + if (danglingElements == 0) 0; + else branchingFactor - danglingElements; ```. cc: @jigold one of the PRs you asked me to break out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3750
https://github.com/hail-is/hail/pull/3750:31,Modifiability,parameteriz,parameterized,31,"To enable easy testing, I also parameterized the methods by the branchingFactor and broke generation of the byte array away from writing the byte array to a file. The key issue is that `k * 1024 % 1024 = 0` for any integer `k`, which we were interpreting as meaning that the last block needed 1024 more elements to be full. There are no errors on write. On read, we try to calculate the number of layers present in the BGEN using `calcDepth` but this fails to correctly guess the layers when the size of the file is not a positive integral power of 1024. The only real changes (the rest are restructuring/whitespace) are using `branchingFactor` in place of `1024` and replacing; ```; - // Pad last layer so last block is 1024 elements (1024*8 bytes); - val paddingRequired = 1024 - (arr.length % 1024); ```; with; ```; + // Pad last layer so last block is branchingFactor elements (branchingFactor*8 bytes); + val danglingElements = (arr.length % branchingFactor); + val paddingRequired =; + if (danglingElements == 0) 0; + else branchingFactor - danglingElements; ```. cc: @jigold one of the PRs you asked me to break out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3750
https://github.com/hail-is/hail/pull/3750:397,Modifiability,layers,layers,397,"To enable easy testing, I also parameterized the methods by the branchingFactor and broke generation of the byte array away from writing the byte array to a file. The key issue is that `k * 1024 % 1024 = 0` for any integer `k`, which we were interpreting as meaning that the last block needed 1024 more elements to be full. There are no errors on write. On read, we try to calculate the number of layers present in the BGEN using `calcDepth` but this fails to correctly guess the layers when the size of the file is not a positive integral power of 1024. The only real changes (the rest are restructuring/whitespace) are using `branchingFactor` in place of `1024` and replacing; ```; - // Pad last layer so last block is 1024 elements (1024*8 bytes); - val paddingRequired = 1024 - (arr.length % 1024); ```; with; ```; + // Pad last layer so last block is branchingFactor elements (branchingFactor*8 bytes); + val danglingElements = (arr.length % branchingFactor); + val paddingRequired =; + if (danglingElements == 0) 0; + else branchingFactor - danglingElements; ```. cc: @jigold one of the PRs you asked me to break out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3750
https://github.com/hail-is/hail/pull/3750:480,Modifiability,layers,layers,480,"To enable easy testing, I also parameterized the methods by the branchingFactor and broke generation of the byte array away from writing the byte array to a file. The key issue is that `k * 1024 % 1024 = 0` for any integer `k`, which we were interpreting as meaning that the last block needed 1024 more elements to be full. There are no errors on write. On read, we try to calculate the number of layers present in the BGEN using `calcDepth` but this fails to correctly guess the layers when the size of the file is not a positive integral power of 1024. The only real changes (the rest are restructuring/whitespace) are using `branchingFactor` in place of `1024` and replacing; ```; - // Pad last layer so last block is 1024 elements (1024*8 bytes); - val paddingRequired = 1024 - (arr.length % 1024); ```; with; ```; + // Pad last layer so last block is branchingFactor elements (branchingFactor*8 bytes); + val danglingElements = (arr.length % branchingFactor); + val paddingRequired =; + if (danglingElements == 0) 0; + else branchingFactor - danglingElements; ```. cc: @jigold one of the PRs you asked me to break out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3750
https://github.com/hail-is/hail/pull/3750:15,Testability,test,testing,15,"To enable easy testing, I also parameterized the methods by the branchingFactor and broke generation of the byte array away from writing the byte array to a file. The key issue is that `k * 1024 % 1024 = 0` for any integer `k`, which we were interpreting as meaning that the last block needed 1024 more elements to be full. There are no errors on write. On read, we try to calculate the number of layers present in the BGEN using `calcDepth` but this fails to correctly guess the layers when the size of the file is not a positive integral power of 1024. The only real changes (the rest are restructuring/whitespace) are using `branchingFactor` in place of `1024` and replacing; ```; - // Pad last layer so last block is 1024 elements (1024*8 bytes); - val paddingRequired = 1024 - (arr.length % 1024); ```; with; ```; + // Pad last layer so last block is branchingFactor elements (branchingFactor*8 bytes); + val danglingElements = (arr.length % branchingFactor); + val paddingRequired =; + if (danglingElements == 0) 0; + else branchingFactor - danglingElements; ```. cc: @jigold one of the PRs you asked me to break out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3750
https://github.com/hail-is/hail/pull/3751:229,Deployability,pipeline,pipeline,229,"`Mentions(ir, name)` is true if `name` is referenced by `ir`. Ergo, when it is false, there exist no references to it, so it's safe to give it some bogus value, like the `0` pointer. This was the last mile of improving Caitlin's pipeline and is obviated by the off-heap stuff. @jigold this is one of the things you requested me to break out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3751
https://github.com/hail-is/hail/pull/3751:127,Safety,safe,safe,127,"`Mentions(ir, name)` is true if `name` is referenced by `ir`. Ergo, when it is false, there exist no references to it, so it's safe to give it some bogus value, like the `0` pointer. This was the last mile of improving Caitlin's pipeline and is obviated by the off-heap stuff. @jigold this is one of the things you requested me to break out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3751
https://github.com/hail-is/hail/pull/3752:5,Security,expose,exposes,5,"this exposes `uniroot` in python. I added a new (python) AST node, `LambdaFunction` which basically mimics `LambdaClassMethod` but doesn't have a caller. cc @konradjk",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3752
https://github.com/hail-is/hail/issues/3756:90,Usability,simpl,simply,90,"I think this is the intended behavior (at the very least a warning), but at the moment it simply silently does nothing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3756
https://github.com/hail-is/hail/pull/3757:97,Testability,test,tests,97,"Builds on a few pending PRs. This shouldn't be reviewed until it is passing. Right now, about 50 tests are failing. Spot checked a few, they are all in missing aggregators: Counter, general collect (@jigold has a PR), general take, takeBy, collectAsSet and int32/float32 sum, array sum (might just be a promotion issue). @catoverdrive let's coordinate on these. I can't quite rip out all the AST eval infrastructure because the function registry method implementations are used for type inference. It will have to wait until we go directly from Python to IR, and then get ripped out along with the function registry, AST parser and AST classes. > 480 additions and 3,235 deletions. Delicious.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3757
https://github.com/hail-is/hail/issues/3760:1264,Availability,failure,failure,1264," ### What you did:. copied from gitter:. I'm using hail 0.1 on dataproc and trying to import a vcf from the local filesystem; run(""wget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh{0}/clinvar.vcf.gz -O /tmp/clinvar.vcf.gz"".format(args.genome_version)); run(""ls -l /tmp/clinvar.vcf.gz""); vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); The output is; ls -l /tmp/clinvar.vcf.gz; -rw-r--r-- 1 root root 16218805 Jun 14 20:21 /tmp/clinvar.vcf.gz; Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:1322,Availability,failure,failure,1322,"on dataproc and trying to import a vcf from the local filesystem; run(""wget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh{0}/clinvar.vcf.gz -O /tmp/clinvar.vcf.gz"".format(args.genome_version)); run(""ls -l /tmp/clinvar.vcf.gz""); vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); The output is; ls -l /tmp/clinvar.vcf.gz; -rw-r--r-- 1 root root 16218805 Jun 14 20:21 /tmp/clinvar.vcf.gz; Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:4493,Availability,error,error,4493,"Iterator$1.apply(BlockManager.scala:935); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but \_()_/). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:5099,Availability,failure,failure,5099,".RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but \_()_/). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:5157,Availability,failure,failure,5157,"park.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but \_()_/). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); 	at org.apache.spark.rdd.HadoopRDD$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:14543,Availability,Error,Error,14543,; 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-74bf1eb; Error summary: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:8083,Energy Efficiency,schedul,scheduler,8083,ompute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:8155,Energy Efficiency,schedul,scheduler,8155,rReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSchedu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:8519,Energy Efficiency,schedul,scheduler,8519,he.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:8559,Energy Efficiency,schedul,scheduler,8559,; 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:8658,Energy Efficiency,schedul,scheduler,8658,); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:8756,Energy Efficiency,schedul,scheduler,8756,he.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:9010,Energy Efficiency,schedul,scheduler,9010,.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:9091,Energy Efficiency,schedul,scheduler,9091,ultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:9197,Energy Efficiency,schedul,scheduler,9197,he.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:9347,Energy Efficiency,schedul,scheduler,9347,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtil,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:9436,Energy Efficiency,schedul,scheduler,9436,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:29); 	at is.hail.utils.richUtils.RichRDD$.foral,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:9534,Energy Efficiency,schedul,scheduler,9534,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:29); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:25); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.H,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:9630,Energy Efficiency,schedul,scheduler,9630,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:29); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:25); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:557); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(N,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:9795,Energy Efficiency,schedul,scheduler,9795,.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:29); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:25); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:557); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMetho,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:14121,Energy Efficiency,schedul,scheduler,14121,; 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-74bf1eb; Error summary: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:14193,Energy Efficiency,schedul,scheduler,14193,; 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-74bf1eb; Error summary: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:4499,Integrability,message,messages,4499,"Iterator$1.apply(BlockManager.scala:935); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but \_()_/). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:4354,Performance,load,load,4354,".rdd.RDD$$anonfun$8.apply(RDD.scala:332); at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but \_()_/). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hado",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:8279,Performance,concurren,concurrent,8279,anonfun$8.apply(RDD.scala:330); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:8364,Performance,concurren,concurrent,8364,PutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:10491,Performance,Load,LoadVCF,10491,uler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:29); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:25); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:557); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.Ra,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:10506,Performance,Load,LoadVCF,10506,667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:29); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:25); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:557); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:14317,Performance,concurren,concurrent,14317,; 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-74bf1eb; Error summary: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:14402,Performance,concurren,concurrent,14402,; 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332); 	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-74bf1eb; Error summary: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:1243,Safety,abort,aborted,1243," ### What you did:. copied from gitter:. I'm using hail 0.1 on dataproc and trying to import a vcf from the local filesystem; run(""wget ftp://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh{0}/clinvar.vcf.gz -O /tmp/clinvar.vcf.gz"".format(args.genome_version)); run(""ls -l /tmp/clinvar.vcf.gz""); vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); The output is; ls -l /tmp/clinvar.vcf.gz; -rw-r--r-- 1 root root 16218805 Jun 14 20:21 /tmp/clinvar.vcf.gz; Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputF",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:5078,Safety,abort,aborted,5078,".RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283). Konrad Karczewski @konradjk 16:24; this should work, so i think it's a bug. but in the short run, you could hdfs dfs -cp file:///tmp/clinvar.vcf.gz / and then just load /clinvar.vcf.gz; copy to hdfs; (you shouldn't have to, but \_()_/). bw2 @bw2 16:27; that worked. thanks!. ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/load_clinvar_to_es_pipeline.py"", line 31, in <module>; vds = hc.import_vcf(""file:///tmp/clinvar.vcf.gz"", force=True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:8690,Safety,abort,abortStage,8690,k.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:8788,Safety,abort,abortStage,8788,Compute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:9033,Safety,abort,abortStage,9033,(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:1888,Security,Checksum,ChecksumFileSystem,1888,"True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrRead",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:1907,Security,Checksum,ChecksumFSInputChecker,1907,"True); File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrRead",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:1937,Security,Checksum,ChecksumFileSystem,1937," line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:1991,Security,Checksum,ChecksumFileSystem,1991,"b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apach",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:2015,Security,Checksum,ChecksumFileSystem,2015,"-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartit",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:5728,Security,Checksum,ChecksumFileSystem,5728,"; File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); 	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:5747,Security,Checksum,ChecksumFSInputChecker,5747,"; File ""<decorator-gen-502>"", line 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); 	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:5777,Security,Checksum,ChecksumFileSystem,5777," 2, in import_vcf; File ""/tmp/7417fcfbbeee44d0b3f4c0b3750121a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); 	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:5832,Security,Checksum,ChecksumFileSystem,5832,"21a7/hail-0.1-es-6.2.4-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); 	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:5856,Security,Checksum,ChecksumFileSystem,5856,"-with-strip-chr-prefix.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, without-vep-520334-sw-rmwj.c.seqr-project.internal): java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); 	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:11766,Security,Checksum,ChecksumFileSystem,11766,legatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); 	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:11785,Security,Checksum,ChecksumFSInputChecker,11785,legatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); 	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:11815,Security,Checksum,ChecksumFileSystem,11815,); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); 	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:11870,Security,Checksum,ChecksumFileSystem,11870,at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); 	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/issues/3760:11894,Security,Checksum,ChecksumFileSystem,11894,hodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.io.FileNotFoundException: File file:/tmp/clinvar.vcf.gz does not exist; 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824); 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601); 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428); 	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142); 	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346); 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769); 	at org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109); 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67); 	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:245); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208); 	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3760
https://github.com/hail-is/hail/pull/3762:293,Availability,error,error,293,"Joins were not being tested and fail with source mismatch if joins are present in both key and agg expressions. This fix is analogous to that on Table.group_by.aggregate in #3730, processing all joins at once, and not reprocessing later. I don't address here a deeper bug that throws a source error when processing more than one entry-indexed. I've made an issue #3763",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3762
https://github.com/hail-is/hail/pull/3762:21,Testability,test,tested,21,"Joins were not being tested and fail with source mismatch if joins are present in both key and agg expressions. This fix is analogous to that on Table.group_by.aggregate in #3730, processing all joins at once, and not reprocessing later. I don't address here a deeper bug that throws a source error when processing more than one entry-indexed. I've made an issue #3763",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3762
https://github.com/hail-is/hail/issues/3763:167,Availability,error,error,167,"This impacts all the methods that use _select_entries, e.g. within process_joins, like group_by.aggregate_rows/cols, annotate_rows/cols, transmute_rows/cols, etc. The error is expression source mismatch, which may be due to `annotate_entries` being done separately from `_annotate_all` in the joiner for `index_entries` AST:; ```; def joiner(left: MatrixTable):; localized = Table(self._jvds.localizeEntries(row_uid)); src_cols_indexed = self.cols().add_index(col_uid); src_cols_indexed = src_cols_indexed.annotate(**{col_uid: hl.int32(src_cols_indexed[col_uid])}); left = left._annotate_all(row_exprs = {row_uid: localized.index(*row_exprs)[row_uid]},; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); return left.annotate_entries(**{uid: left[row_uid][left[col_uid]]}); ```. ### Hail version:; master; b1ac051d34bcc4c26fe9dea58aeac53038f2963e. ### What you did:. ```; mt = hl.utils.range_matrix_table(4, 4); mt2 = hl.utils.range_matrix_table(4, 4); mt2 = mt2.annotate_entries(x=mt2.row_idx + mt2.col_idx); mt.select_entries(a=mt2[mt.row_idx, mt.col_idx].x,; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = se",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3763
https://github.com/hail-is/hail/issues/3763:1164,Availability,error,error,1164,"ession source mismatch, which may be due to `annotate_entries` being done separately from `_annotate_all` in the joiner for `index_entries` AST:; ```; def joiner(left: MatrixTable):; localized = Table(self._jvds.localizeEntries(row_uid)); src_cols_indexed = self.cols().add_index(col_uid); src_cols_indexed = src_cols_indexed.annotate(**{col_uid: hl.int32(src_cols_indexed[col_uid])}); left = left._annotate_all(row_exprs = {row_uid: localized.index(*row_exprs)[row_uid]},; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); return left.annotate_entries(**{uid: left[row_uid][left[col_uid]]}); ```. ### Hail version:; master; b1ac051d34bcc4c26fe9dea58aeac53038f2963e. ### What you did:. ```; mt = hl.utils.range_matrix_table(4, 4); mt2 = hl.utils.range_matrix_table(4, 4); mt2 = mt2.annotate_entries(x=mt2.row_idx + mt2.col_idx); mt.select_entries(a=mt2[mt.row_idx, mt.col_idx].x,; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = self._process_joins(s); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2503, in _process_joins; return process_joins(self, exprs, broadcast_f); File ""/Users/jbloom/hail/p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3763
https://github.com/hail-is/hail/issues/3763:1229,Availability,Error,Error,1229,"nnotate_all` in the joiner for `index_entries` AST:; ```; def joiner(left: MatrixTable):; localized = Table(self._jvds.localizeEntries(row_uid)); src_cols_indexed = self.cols().add_index(col_uid); src_cols_indexed = src_cols_indexed.annotate(**{col_uid: hl.int32(src_cols_indexed[col_uid])}); left = left._annotate_all(row_exprs = {row_uid: localized.index(*row_exprs)[row_uid]},; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); return left.annotate_entries(**{uid: left[row_uid][left[col_uid]]}); ```. ### Hail version:; master; b1ac051d34bcc4c26fe9dea58aeac53038f2963e. ### What you did:. ```; mt = hl.utils.range_matrix_table(4, 4); mt2 = hl.utils.range_matrix_table(4, 4); mt2 = mt2.annotate_entries(x=mt2.row_idx + mt2.col_idx); mt.select_entries(a=mt2[mt.row_idx, mt.col_idx].x,; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = self._process_joins(s); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2503, in _process_joins; return process_joins(self, exprs, broadcast_f); File ""/Users/jbloom/hail/python/hail/utils/misc.py"", line 356, in process_joins; left = j.join_func(left); File ""/User",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3763
https://github.com/hail-is/hail/issues/3763:2881,Availability,error,errors,2881," wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = self._process_joins(s); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2503, in _process_joins; return process_joins(self, exprs, broadcast_f); File ""/Users/jbloom/hail/python/hail/utils/misc.py"", line 356, in process_joins; left = j.join_func(left); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2414, in joiner; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2443, in _annotate_all; analyze(""MatrixTable.annotate_rows"", row_struct, self._row_indices); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/expr/expressions/expression_utils.py"", line 105, in analyze; raise errors[0]; hail.expr.expressions.base_expression.ExpressionException: MatrixTable.annotate_rows expects an expression from source <hail.matrixtable.MatrixTable object at 0x10889c908>, found expression derived from <hail.matrixtable.MatrixTable object at 0x108acbf60>; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3763
https://github.com/hail-is/hail/issues/3763:1170,Integrability,message,messages,1170,"ession source mismatch, which may be due to `annotate_entries` being done separately from `_annotate_all` in the joiner for `index_entries` AST:; ```; def joiner(left: MatrixTable):; localized = Table(self._jvds.localizeEntries(row_uid)); src_cols_indexed = self.cols().add_index(col_uid); src_cols_indexed = src_cols_indexed.annotate(**{col_uid: hl.int32(src_cols_indexed[col_uid])}); left = left._annotate_all(row_exprs = {row_uid: localized.index(*row_exprs)[row_uid]},; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); return left.annotate_entries(**{uid: left[row_uid][left[col_uid]]}); ```. ### Hail version:; master; b1ac051d34bcc4c26fe9dea58aeac53038f2963e. ### What you did:. ```; mt = hl.utils.range_matrix_table(4, 4); mt2 = hl.utils.range_matrix_table(4, 4); mt2 = mt2.annotate_entries(x=mt2.row_idx + mt2.col_idx); mt.select_entries(a=mt2[mt.row_idx, mt.col_idx].x,; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = self._process_joins(s); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2503, in _process_joins; return process_joins(self, exprs, broadcast_f); File ""/Users/jbloom/hail/p",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3763
https://github.com/hail-is/hail/issues/3763:1860,Integrability,wrap,wrapper,1860," hl.utils.range_matrix_table(4, 4); mt2 = hl.utils.range_matrix_table(4, 4); mt2 = mt2.annotate_entries(x=mt2.row_idx + mt2.col_idx); mt.select_entries(a=mt2[mt.row_idx, mt.col_idx].x,; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = self._process_joins(s); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2503, in _process_joins; return process_joins(self, exprs, broadcast_f); File ""/Users/jbloom/hail/python/hail/utils/misc.py"", line 356, in process_joins; left = j.join_func(left); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2414, in joiner; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2443, in _annotate_all; analyze(""MatrixTable.annotate_rows"", row_struct, self._row_indices); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/expr/expressions/expression_utils.py"", line 105, in analyze; raise errors[0]; hai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3763
https://github.com/hail-is/hail/issues/3763:2478,Integrability,wrap,wrapper,2478," wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = self._process_joins(s); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2503, in _process_joins; return process_joins(self, exprs, broadcast_f); File ""/Users/jbloom/hail/python/hail/utils/misc.py"", line 356, in process_joins; left = j.join_func(left); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2414, in joiner; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2443, in _annotate_all; analyze(""MatrixTable.annotate_rows"", row_struct, self._row_indices); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/expr/expressions/expression_utils.py"", line 105, in analyze; raise errors[0]; hail.expr.expressions.base_expression.ExpressionException: MatrixTable.annotate_rows expects an expression from source <hail.matrixtable.MatrixTable object at 0x10889c908>, found expression derived from <hail.matrixtable.MatrixTable object at 0x108acbf60>; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3763
https://github.com/hail-is/hail/issues/3763:2739,Integrability,wrap,wrapper,2739," wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = self._process_joins(s); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2503, in _process_joins; return process_joins(self, exprs, broadcast_f); File ""/Users/jbloom/hail/python/hail/utils/misc.py"", line 356, in process_joins; left = j.join_func(left); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2414, in joiner; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2443, in _annotate_all; analyze(""MatrixTable.annotate_rows"", row_struct, self._row_indices); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/expr/expressions/expression_utils.py"", line 105, in analyze; raise errors[0]; hail.expr.expressions.base_expression.ExpressionException: MatrixTable.annotate_rows expects an expression from source <hail.matrixtable.MatrixTable object at 0x10889c908>, found expression derived from <hail.matrixtable.MatrixTable object at 0x108acbf60>; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3763
https://github.com/hail-is/hail/issues/3763:1356,Testability,test,testPartExecutor,1356,"._jvds.localizeEntries(row_uid)); src_cols_indexed = self.cols().add_index(col_uid); src_cols_indexed = src_cols_indexed.annotate(**{col_uid: hl.int32(src_cols_indexed[col_uid])}); left = left._annotate_all(row_exprs = {row_uid: localized.index(*row_exprs)[row_uid]},; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); return left.annotate_entries(**{uid: left[row_uid][left[col_uid]]}); ```. ### Hail version:; master; b1ac051d34bcc4c26fe9dea58aeac53038f2963e. ### What you did:. ```; mt = hl.utils.range_matrix_table(4, 4); mt2 = hl.utils.range_matrix_table(4, 4); mt2 = mt2.annotate_entries(x=mt2.row_idx + mt2.col_idx); mt.select_entries(a=mt2[mt.row_idx, mt.col_idx].x,; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = self._process_joins(s); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2503, in _process_joins; return process_joins(self, exprs, broadcast_f); File ""/Users/jbloom/hail/python/hail/utils/misc.py"", line 356, in process_joins; left = j.join_func(left); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2414, in joiner; col_exprs = {col_uid: src_cols_indexed.index(*co",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3763
https://github.com/hail-is/hail/issues/3763:1471,Testability,test,testMethod,1471,"_cols_indexed.annotate(**{col_uid: hl.int32(src_cols_indexed[col_uid])}); left = left._annotate_all(row_exprs = {row_uid: localized.index(*row_exprs)[row_uid]},; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); return left.annotate_entries(**{uid: left[row_uid][left[col_uid]]}); ```. ### Hail version:; master; b1ac051d34bcc4c26fe9dea58aeac53038f2963e. ### What you did:. ```; mt = hl.utils.range_matrix_table(4, 4); mt2 = hl.utils.range_matrix_table(4, 4); mt2 = mt2.annotate_entries(x=mt2.row_idx + mt2.col_idx); mt.select_entries(a=mt2[mt.row_idx, mt.col_idx].x,; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = self._process_joins(s); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2503, in _process_joins; return process_joins(self, exprs, broadcast_f); File ""/Users/jbloom/hail/python/hail/utils/misc.py"", line 356, in process_joins; left = j.join_func(left); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2414, in joiner; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3763
https://github.com/hail-is/hail/issues/3763:1522,Testability,test,tests,1522,"_cols_indexed.annotate(**{col_uid: hl.int32(src_cols_indexed[col_uid])}); left = left._annotate_all(row_exprs = {row_uid: localized.index(*row_exprs)[row_uid]},; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); return left.annotate_entries(**{uid: left[row_uid][left[col_uid]]}); ```. ### Hail version:; master; b1ac051d34bcc4c26fe9dea58aeac53038f2963e. ### What you did:. ```; mt = hl.utils.range_matrix_table(4, 4); mt2 = hl.utils.range_matrix_table(4, 4); mt2 = mt2.annotate_entries(x=mt2.row_idx + mt2.col_idx); mt.select_entries(a=mt2[mt.row_idx, mt.col_idx].x,; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Error; Traceback (most recent call last):; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 59, in testPartExecutor; yield; File ""/Users/jbloom/anaconda/envs/py36/lib/python3.6/unittest/case.py"", line 605, in run; testMethod(); File ""/Users/jbloom/hail/python/hail/tests/test_api.py"", line 1557, in test_force_bug; b=mt2[mt.row_idx, mt.col_idx].x)._force_count_rows(); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 1171, in select_entries; return self._select_entries(""MatrixTable.select_entries"", hl.struct(**entry)); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2844, in _select_entries; base, cleanup = self._process_joins(s); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2503, in _process_joins; return process_joins(self, exprs, broadcast_f); File ""/Users/jbloom/hail/python/hail/utils/misc.py"", line 356, in process_joins; left = j.join_func(left); File ""/Users/jbloom/hail/python/hail/matrixtable.py"", line 2414, in joiner; col_exprs = {col_uid: src_cols_indexed.index(*col_exprs)[col_uid]}); File ""/Users/jbloom/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3763
https://github.com/hail-is/hail/pull/3768:322,Security,expose,expose,322,"@cseed This is the IR infrastructure needed for the `groupBy` aggregator. I wrote tests by adding a `KeyedAggregator` so we can use the interpreter. I don't think it is trivial to incorporate `groupBy` into the AST / Parser. Since we're going to rip out the AST soon anyways, I decided to leave the code where it's at and expose it in Python once we can build IRs in Python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3768
https://github.com/hail-is/hail/pull/3768:82,Testability,test,tests,82,"@cseed This is the IR infrastructure needed for the `groupBy` aggregator. I wrote tests by adding a `KeyedAggregator` so we can use the interpreter. I don't think it is trivial to incorporate `groupBy` into the AST / Parser. Since we're going to rip out the AST soon anyways, I decided to leave the code where it's at and expose it in Python once we can build IRs in Python.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3768
https://github.com/hail-is/hail/pull/3771:49,Testability,assert,assert,49,from https://github.com/hail-is/hail/pull/3727. `assert`s don't protect the allocation of their strings. `If`'s do.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3771
https://github.com/hail-is/hail/pull/3772:13,Testability,test,test,13,"I wrote this test when moving groupColsBy to the IR, to make sure the AST path was still working, but it's not adding much value, so I think it may as well be deleted now, rather than later.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3772
https://github.com/hail-is/hail/pull/3773:24,Performance,perform,performance,24,"This provides necessary performance improvements when creating; an array of empty structs, which should have o(1) cost, not; O(n) cost.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3773
https://github.com/hail-is/hail/pull/3774:37,Integrability,rout,route,37,The old `Table.index` used the `RDD` route which forced a shuffle because all partitioning information was lost. I exposed `zipWithIndex` on `RVD` and used it in both MatrixTable and Table.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3774
https://github.com/hail-is/hail/pull/3774:115,Security,expose,exposed,115,The old `Table.index` used the `RDD` route which forced a shuffle because all partitioning information was lost. I exposed `zipWithIndex` on `RVD` and used it in both MatrixTable and Table.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3774
https://github.com/hail-is/hail/pull/3776:162,Testability,test,test,162,"I forget why I added this, but I came across something in; Uniroot that either was not or made me think that Uniroot; was not strict in max and min, so I added a test.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3776
https://github.com/hail-is/hail/pull/3777:13,Testability,log,logging,13,"I found this logging helpful but not noisy. The blockSize change; is inspired by @rcownie. It means that things that take half or more of the block will get a custom block, rather than possibly having a block half-occupied with something big and not have a lot of room for new stuff. (At least, that's my understanding of the rational)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3777
https://github.com/hail-is/hail/pull/3779:33,Performance,load,loading,33,"This option controls parsing and loading of the `rsid` and `varid`; BGEN row fields. When (in a future PR) the reader does not decompress; the genotypes (nor decode them), these row field imports become; a substantial portion of the time necessary to load just the row; keys. broken out from: https://github.com/hail-is/hail/pull/3727",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3779
https://github.com/hail-is/hail/pull/3779:251,Performance,load,load,251,"This option controls parsing and loading of the `rsid` and `varid`; BGEN row fields. When (in a future PR) the reader does not decompress; the genotypes (nor decode them), these row field imports become; a substantial portion of the time necessary to load just the row; keys. broken out from: https://github.com/hail-is/hail/pull/3727",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3779
https://github.com/hail-is/hail/pull/3783:172,Integrability,contract,contract,172,@tpoterba you were in here recently for performance so your eyes are appreciated. I simplified things a bit and localized almost all the parsing logic to `BgenRecord`. The contract for `advance` is that it is always called when `bfis` is pointing at the start of a record _or_ at or past the `end`. Advance will return the position to the start of a record or at or past the `end`. It returns true if there was a new record found. False otherwise. I avoided a couple allocating patterns. The rest of the diffs are copy pastes and some indentation changes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3783
https://github.com/hail-is/hail/pull/3783:40,Performance,perform,performance,40,@tpoterba you were in here recently for performance so your eyes are appreciated. I simplified things a bit and localized almost all the parsing logic to `BgenRecord`. The contract for `advance` is that it is always called when `bfis` is pointing at the start of a record _or_ at or past the `end`. Advance will return the position to the start of a record or at or past the `end`. It returns true if there was a new record found. False otherwise. I avoided a couple allocating patterns. The rest of the diffs are copy pastes and some indentation changes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3783
https://github.com/hail-is/hail/pull/3783:450,Safety,avoid,avoided,450,@tpoterba you were in here recently for performance so your eyes are appreciated. I simplified things a bit and localized almost all the parsing logic to `BgenRecord`. The contract for `advance` is that it is always called when `bfis` is pointing at the start of a record _or_ at or past the `end`. Advance will return the position to the start of a record or at or past the `end`. It returns true if there was a new record found. False otherwise. I avoided a couple allocating patterns. The rest of the diffs are copy pastes and some indentation changes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3783
https://github.com/hail-is/hail/pull/3783:145,Testability,log,logic,145,@tpoterba you were in here recently for performance so your eyes are appreciated. I simplified things a bit and localized almost all the parsing logic to `BgenRecord`. The contract for `advance` is that it is always called when `bfis` is pointing at the start of a record _or_ at or past the `end`. Advance will return the position to the start of a record or at or past the `end`. It returns true if there was a new record found. False otherwise. I avoided a couple allocating patterns. The rest of the diffs are copy pastes and some indentation changes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3783
https://github.com/hail-is/hail/pull/3783:84,Usability,simpl,simplified,84,@tpoterba you were in here recently for performance so your eyes are appreciated. I simplified things a bit and localized almost all the parsing logic to `BgenRecord`. The contract for `advance` is that it is always called when `bfis` is pointing at the start of a record _or_ at or past the `end`. Advance will return the position to the start of a record or at or past the `end`. It returns true if there was a new record found. False otherwise. I avoided a couple allocating patterns. The rest of the diffs are copy pastes and some indentation changes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3783
https://github.com/hail-is/hail/issues/3785:177,Availability,Error,Error,177,"`hl.literal(float('nan')).value` works as expected, the trouble arised when parsing non-numeric values inside an array, e.g. `[float('nan'), float('inf'), float('-inf')]`. ```; Error summary: JsonParseException: Non-standard token 'NaN': enable JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS to allow; at [Source: {""__uid_3"": [NaN, Infinity, -Infinity]}; line: 1, column: 17]; ```. I'm not sure how to enable this feature:; https://github.com/FasterXML/jackson-core/wiki/JsonParser-Features. ### Hail version:; master; c908fd74abd819f4fcfbcdad88c5db6bf77083b2. ### What you did:. `hl.literal([float('nan')]).value`. ### What went wrong (all error messages here, including the full java stack trace):. hl.literal([float('nan'), float('inf'), float('-inf')]).value; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-3-ea2ec4f5df06> in <module>(); ----> 1 hl.literal([float('nan'), float('inf'), float('-inf')]).value. ~/hail/python/hail/expr/expressions/base_expression.py in value(self); 773 ; 774 """"""; --> 775 return hl.eval_expr(self); 776 ; 777 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.sou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:639,Availability,error,error,639,"`hl.literal(float('nan')).value` works as expected, the trouble arised when parsing non-numeric values inside an array, e.g. `[float('nan'), float('inf'), float('-inf')]`. ```; Error summary: JsonParseException: Non-standard token 'NaN': enable JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS to allow; at [Source: {""__uid_3"": [NaN, Infinity, -Infinity]}; line: 1, column: 17]; ```. I'm not sure how to enable this feature:; https://github.com/FasterXML/jackson-core/wiki/JsonParser-Features. ### Hail version:; master; c908fd74abd819f4fcfbcdad88c5db6bf77083b2. ### What you did:. `hl.literal([float('nan')]).value`. ### What went wrong (all error messages here, including the full java stack trace):. hl.literal([float('nan'), float('inf'), float('-inf')]).value; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-3-ea2ec4f5df06> in <module>(); ----> 1 hl.literal([float('nan'), float('inf'), float('-inf')]).value. ~/hail/python/hail/expr/expressions/base_expression.py in value(self); 773 ; 774 """"""; --> 775 return hl.eval_expr(self); 776 ; 777 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.sou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:4787,Availability,Error,Error,4787,"elf, exprs, broadcast_f); 1487 ; 1488 def cache(self):. ~/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 364 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); 365 data_json = t._to_json(data); --> 366 left = broadcast_f(left, data_json, t._jtype); 367 ; 368 def cleanup(table):. ~/hail/python/hail/table.py in broadcast_f(left, data, jt); 1483 def _process_joins(self, *exprs):; 1484 def broadcast_f(left, data, jt):; -> 1485 return Table(left._jt.annotateGlobalJSON(data, jt)); 1486 return process_joins(self, exprs, broadcast_f); 1487 . ~/apache/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. ~/hail/python/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: JsonParseException: Non-standard token 'NaN': enable JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS to allow; at [Source: {""__uid_3"": [NaN, Infinity, -Infinity]}; line: 1, column: 17]. Java stack trace:; com.fasterxml.jackson.core.JsonParseException: Non-standard token 'NaN': enable JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS to allow; at [Source: {""__uid_3"": [NaN, Infinity, -Infinity]}; line: 1, column: 17]; 	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581); 	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533); 	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._handleOddValue(ReaderBasedJsonParser.java:1602); 	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(R",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:7147,Availability,Error,Error,7147," [NaN, Infinity, -Infinity]}; line: 1, column: 17]; 	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581); 	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533); 	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._handleOddValue(ReaderBasedJsonParser.java:1602); 	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:689); 	at org.json4s.jackson.JValueDeserializer.deserialize(JValueDeserializer.scala:26); 	at org.json4s.jackson.JValueDeserializer.deserialize(JValueDeserializer.scala:42); 	at org.json4s.jackson.JValueDeserializer.deserialize(JValueDeserializer.scala:35); 	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3736); 	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2726); 	at org.json4s.jackson.JsonMethods$class.parse(JsonMethods.scala:20); 	at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:50); 	at is.hail.table.Table.annotateGlobalJSON(Table.scala:476); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-2d80e80610df; Error summary: JsonParseException: Non-standard token 'NaN': enable JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS to allow; at [Source: {""__uid_3"": [NaN, Infinity, -Infinity]}; line: 1, column: 17]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:645,Integrability,message,messages,645,"`hl.literal(float('nan')).value` works as expected, the trouble arised when parsing non-numeric values inside an array, e.g. `[float('nan'), float('inf'), float('-inf')]`. ```; Error summary: JsonParseException: Non-standard token 'NaN': enable JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS to allow; at [Source: {""__uid_3"": [NaN, Infinity, -Infinity]}; line: 1, column: 17]; ```. I'm not sure how to enable this feature:; https://github.com/FasterXML/jackson-core/wiki/JsonParser-Features. ### Hail version:; master; c908fd74abd819f4fcfbcdad88c5db6bf77083b2. ### What you did:. `hl.literal([float('nan')]).value`. ### What went wrong (all error messages here, including the full java stack trace):. hl.literal([float('nan'), float('inf'), float('-inf')]).value; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-3-ea2ec4f5df06> in <module>(); ----> 1 hl.literal([float('nan'), float('inf'), float('-inf')]).value. ~/hail/python/hail/expr/expressions/base_expression.py in value(self); 773 ; 774 """"""; --> 775 return hl.eval_expr(self); 776 ; 777 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.sou",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:1176,Integrability,wrap,wrapper,1176,"C_NUMBERS to allow; at [Source: {""__uid_3"": [NaN, Infinity, -Infinity]}; line: 1, column: 17]; ```. I'm not sure how to enable this feature:; https://github.com/FasterXML/jackson-core/wiki/JsonParser-Features. ### Hail version:; master; c908fd74abd819f4fcfbcdad88c5db6bf77083b2. ### What you did:. `hl.literal([float('nan')]).value`. ### What went wrong (all error messages here, including the full java stack trace):. hl.literal([float('nan'), float('inf'), float('-inf')]).value; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-3-ea2ec4f5df06> in <module>(); ----> 1 hl.literal([float('nan'), float('inf'), float('-inf')]).value. ~/hail/python/hail/expr/expressions/base_expression.py in value(self); 773 ; 774 """"""; --> 775 return hl.eval_expr(self); 776 ; 777 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . ~/hail/python/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:1210,Integrability,wrap,wrapper,1210,"C_NUMBERS to allow; at [Source: {""__uid_3"": [NaN, Infinity, -Infinity]}; line: 1, column: 17]; ```. I'm not sure how to enable this feature:; https://github.com/FasterXML/jackson-core/wiki/JsonParser-Features. ### Hail version:; master; c908fd74abd819f4fcfbcdad88c5db6bf77083b2. ### What you did:. `hl.literal([float('nan')]).value`. ### What went wrong (all error messages here, including the full java stack trace):. hl.literal([float('nan'), float('inf'), float('-inf')]).value; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-3-ea2ec4f5df06> in <module>(); ----> 1 hl.literal([float('nan'), float('inf'), float('-inf')]).value. ~/hail/python/hail/expr/expressions/base_expression.py in value(self); 773 ; 774 """"""; --> 775 return hl.eval_expr(self); 776 ; 777 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . ~/hail/python/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:1379,Integrability,wrap,wrapper,1379,"C_NUMBERS to allow; at [Source: {""__uid_3"": [NaN, Infinity, -Infinity]}; line: 1, column: 17]; ```. I'm not sure how to enable this feature:; https://github.com/FasterXML/jackson-core/wiki/JsonParser-Features. ### Hail version:; master; c908fd74abd819f4fcfbcdad88c5db6bf77083b2. ### What you did:. `hl.literal([float('nan')]).value`. ### What went wrong (all error messages here, including the full java stack trace):. hl.literal([float('nan'), float('inf'), float('-inf')]).value; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-3-ea2ec4f5df06> in <module>(); ----> 1 hl.literal([float('nan'), float('inf'), float('-inf')]).value. ~/hail/python/hail/expr/expressions/base_expression.py in value(self); 773 ; 774 """"""; --> 775 return hl.eval_expr(self); 776 ; 777 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . ~/hail/python/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:1623,Integrability,wrap,wrapper,1623,"oat('inf'), float('-inf')]).value; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-3-ea2ec4f5df06> in <module>(); ----> 1 hl.literal([float('nan'), float('inf'), float('-inf')]).value. ~/hail/python/hail/expr/expressions/base_expression.py in value(self); 773 ; 774 """"""; --> 775 return hl.eval_expr(self); 776 ; 777 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . ~/hail/python/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . ~/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 578 # scalar expression; 579 df = Env.dummy_table(); --> 580 df = df.select(**{name: self}); 581 return df; 582 elif len(axes) == 0:. ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 54",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:1657,Integrability,wrap,wrapper,1657,"oat('inf'), float('-inf')]).value; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-3-ea2ec4f5df06> in <module>(); ----> 1 hl.literal([float('nan'), float('inf'), float('-inf')]).value. ~/hail/python/hail/expr/expressions/base_expression.py in value(self); 773 ; 774 """"""; --> 775 return hl.eval_expr(self); 776 ; 777 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . ~/hail/python/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . ~/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 578 # scalar expression; 579 df = Env.dummy_table(); --> 580 df = df.select(**{name: self}); 581 return df; 582 elif len(axes) == 0:. ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 54",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:1826,Integrability,wrap,wrapper,1826,"oat('inf'), float('-inf')]).value; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-3-ea2ec4f5df06> in <module>(); ----> 1 hl.literal([float('nan'), float('inf'), float('-inf')]).value. ~/hail/python/hail/expr/expressions/base_expression.py in value(self); 773 ; 774 """"""; --> 775 return hl.eval_expr(self); 776 ; 777 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr(expression); 135 Result of evaluating `expression`.; 136 """"""; --> 137 return eval_expr_typed(expression)[0]; 138 ; 139 . ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . ~/hail/python/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . ~/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 578 # scalar expression; 579 df = Env.dummy_table(); --> 580 df = df.select(**{name: self}); 581 return df; 582 elif len(axes) == 0:. ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 54",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:2581,Integrability,wrap,wrapper,2581,"46 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . ~/hail/python/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . ~/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 578 # scalar expression; 579 df = Env.dummy_table(); --> 580 df = df.select(**{name: self}); 581 return df; 582 elif len(axes) == 0:. ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 890 exprs, named_exprs, self._row_indices,; 891 protect_keys=True); --> 892 return self._select('Table.select', value_struct=hl.struct(**row)); 893 ; 894 @typecheck_method(exprs=oneof(str, Expression)). ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct, do_process_joins); 433 ; 434 if do_process_joins:; --> 435 base, cleanup = self._process_joins(row); 436 analyze(caller, row, self._row_indices); 437 else:. ~/hail/python/hail/table.py in _process_joins(self, *exprs); 1484 def broadcast_f(left, data, jt):; 1485 return Table(left._j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:2615,Integrability,wrap,wrapper,2615,"46 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . ~/hail/python/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . ~/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 578 # scalar expression; 579 df = Env.dummy_table(); --> 580 df = df.select(**{name: self}); 581 return df; 582 elif len(axes) == 0:. ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 890 exprs, named_exprs, self._row_indices,; 891 protect_keys=True); --> 892 return self._select('Table.select', value_struct=hl.struct(**row)); 893 ; 894 @typecheck_method(exprs=oneof(str, Expression)). ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct, do_process_joins); 433 ; 434 if do_process_joins:; --> 435 base, cleanup = self._process_joins(row); 436 analyze(caller, row, self._row_indices); 437 else:. ~/hail/python/hail/table.py in _process_joins(self, *exprs); 1484 def broadcast_f(left, data, jt):; 1485 return Table(left._j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:2784,Integrability,wrap,wrapper,2784,"46 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/expr/expressions/expression_utils.py in eval_expr_typed(expression); 169 analyze('eval_expr_typed', expression, Indices(expression._indices.source)); 170 ; --> 171 return expression.collect()[0], expression.dtype; 172 ; 173 . ~/hail/python/hail/expr/expressions/base_expression.py in collect(self); 755 """"""; 756 uid = Env.get_uid(); --> 757 t = self._to_table(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . ~/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 578 # scalar expression; 579 df = Env.dummy_table(); --> 580 df = df.select(**{name: self}); 581 return df; 582 elif len(axes) == 0:. ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 890 exprs, named_exprs, self._row_indices,; 891 protect_keys=True); --> 892 return self._select('Table.select', value_struct=hl.struct(**row)); 893 ; 894 @typecheck_method(exprs=oneof(str, Expression)). ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct, do_process_joins); 433 ; 434 if do_process_joins:; --> 435 base, cleanup = self._process_joins(row); 436 analyze(caller, row, self._row_indices); 437 else:. ~/hail/python/hail/table.py in _process_joins(self, *exprs); 1484 def broadcast_f(left, data, jt):; 1485 return Table(left._j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:3109,Integrability,wrap,wrapper,3109,"able(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . ~/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 578 # scalar expression; 579 df = Env.dummy_table(); --> 580 df = df.select(**{name: self}); 581 return df; 582 elif len(axes) == 0:. ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 890 exprs, named_exprs, self._row_indices,; 891 protect_keys=True); --> 892 return self._select('Table.select', value_struct=hl.struct(**row)); 893 ; 894 @typecheck_method(exprs=oneof(str, Expression)). ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct, do_process_joins); 433 ; 434 if do_process_joins:; --> 435 base, cleanup = self._process_joins(row); 436 analyze(caller, row, self._row_indices); 437 else:. ~/hail/python/hail/table.py in _process_joins(self, *exprs); 1484 def broadcast_f(left, data, jt):; 1485 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1486 return process_joins(self, exprs, broadcast_f); 1487 ; 1488 def cache(self):. ~/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 364 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); 365 data_json = t._to_json(data); --> 366 left = broadcast_f(left, data_json, t._jtype); 367 ; 368 def cleanup(table):. ~/hail/python/hail/table.py in broadcast_f(left, data, jt); 1483 def _process_joins(self, *exprs):; 1484 def broadcast_f(left, data, jt):; -> 1485 re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:3143,Integrability,wrap,wrapper,3143,"able(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . ~/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 578 # scalar expression; 579 df = Env.dummy_table(); --> 580 df = df.select(**{name: self}); 581 return df; 582 elif len(axes) == 0:. ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 890 exprs, named_exprs, self._row_indices,; 891 protect_keys=True); --> 892 return self._select('Table.select', value_struct=hl.struct(**row)); 893 ; 894 @typecheck_method(exprs=oneof(str, Expression)). ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct, do_process_joins); 433 ; 434 if do_process_joins:; --> 435 base, cleanup = self._process_joins(row); 436 analyze(caller, row, self._row_indices); 437 else:. ~/hail/python/hail/table.py in _process_joins(self, *exprs); 1484 def broadcast_f(left, data, jt):; 1485 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1486 return process_joins(self, exprs, broadcast_f); 1487 ; 1488 def cache(self):. ~/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 364 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); 365 data_json = t._to_json(data); --> 366 left = broadcast_f(left, data_json, t._jtype); 367 ; 368 def cleanup(table):. ~/hail/python/hail/table.py in broadcast_f(left, data, jt); 1483 def _process_joins(self, *exprs):; 1484 def broadcast_f(left, data, jt):; -> 1485 re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:3312,Integrability,wrap,wrapper,3312,"able(uid); 758 return [r[uid] for r in t._select(""collect"", None, hl.struct(**{uid: t[uid]})).collect()]; 759 . ~/hail/python/hail/expr/expressions/base_expression.py in _to_table(self, name); 578 # scalar expression; 579 df = Env.dummy_table(); --> 580 df = df.select(**{name: self}); 581 return df; 582 elif len(axes) == 0:. ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 890 exprs, named_exprs, self._row_indices,; 891 protect_keys=True); --> 892 return self._select('Table.select', value_struct=hl.struct(**row)); 893 ; 894 @typecheck_method(exprs=oneof(str, Expression)). ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct, do_process_joins); 433 ; 434 if do_process_joins:; --> 435 base, cleanup = self._process_joins(row); 436 analyze(caller, row, self._row_indices); 437 else:. ~/hail/python/hail/table.py in _process_joins(self, *exprs); 1484 def broadcast_f(left, data, jt):; 1485 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1486 return process_joins(self, exprs, broadcast_f); 1487 ; 1488 def cache(self):. ~/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 364 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); 365 data_json = t._to_json(data); --> 366 left = broadcast_f(left, data_json, t._jtype); 367 ; 368 def cleanup(table):. ~/hail/python/hail/table.py in broadcast_f(left, data, jt); 1483 def _process_joins(self, *exprs):; 1484 def broadcast_f(left, data, jt):; -> 1485 re",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3785:3791,Performance,cache,cache,3791," **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in select(self, *exprs, **named_exprs); 890 exprs, named_exprs, self._row_indices,; 891 protect_keys=True); --> 892 return self._select('Table.select', value_struct=hl.struct(**row)); 893 ; 894 @typecheck_method(exprs=oneof(str, Expression)). ~/hail/python/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). ~/hail/python/hail/table.py in _select(self, caller, key_struct, value_struct, do_process_joins); 433 ; 434 if do_process_joins:; --> 435 base, cleanup = self._process_joins(row); 436 analyze(caller, row, self._row_indices); 437 else:. ~/hail/python/hail/table.py in _process_joins(self, *exprs); 1484 def broadcast_f(left, data, jt):; 1485 return Table(left._jt.annotateGlobalJSON(data, jt)); -> 1486 return process_joins(self, exprs, broadcast_f); 1487 ; 1488 def cache(self):. ~/hail/python/hail/utils/misc.py in process_joins(obj, exprs, broadcast_f); 364 data = hail.Struct(**{b.uid: b.value for b in broadcasts}); 365 data_json = t._to_json(data); --> 366 left = broadcast_f(left, data_json, t._jtype); 367 ; 368 def cleanup(table):. ~/hail/python/hail/table.py in broadcast_f(left, data, jt); 1483 def _process_joins(self, *exprs):; 1484 def broadcast_f(left, data, jt):; -> 1485 return Table(left._jt.annotateGlobalJSON(data, jt)); 1486 return process_joins(self, exprs, broadcast_f); 1487 . ~/apache/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. ~/hail/python/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3785
https://github.com/hail-is/hail/issues/3790:405,Availability,error,error,405,"### Hail version:; Latest build of `devel` from https://storage.googleapis.com/hail-common/distributions/devel/Hail-devel-5d0f74cef4f2-Spark-2.2.0.zip. ### What you did:; 1. Import VCF file into MatrixTable.; 2. Annotate VCF file with `output = hl.vep(input, 'vep.properties', csq=True)`.; 3. Attempt to view output with `output.rows().show()`. With `csq=False`, step 3 succeeds. ### What went wrong (all error messages here, including the full java stack trace):; ```; 2018-06-19 17:15:41 Hail: INFO: vep: annotated 2 variants; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/opt/hail/python/hail/table.py"", line 1195, in show; print(self._show(n,width, truncate, types)); File ""/opt/hail/python/hail/table.py"", line 1198, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/opt/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/opt/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;). Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:489); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:350); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:345); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(Or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:1109,Availability,Error,Error,1109,"evel-5d0f74cef4f2-Spark-2.2.0.zip. ### What you did:; 1. Import VCF file into MatrixTable.; 2. Annotate VCF file with `output = hl.vep(input, 'vep.properties', csq=True)`.; 3. Attempt to view output with `output.rows().show()`. With `csq=False`, step 3 succeeds. ### What went wrong (all error messages here, including the full java stack trace):; ```; 2018-06-19 17:15:41 Hail: INFO: vep: annotated 2 variants; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/opt/hail/python/hail/table.py"", line 1195, in show; print(self._show(n,width, truncate, types)); File ""/opt/hail/python/hail/table.py"", line 1198, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/opt/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/opt/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;). Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:489); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:350); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:345); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:920); 	at scala.colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:1363,Availability,failure,failure,1363,"alse`, step 3 succeeds. ### What went wrong (all error messages here, including the full java stack trace):; ```; 2018-06-19 17:15:41 Hail: INFO: vep: annotated 2 variants; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/opt/hail/python/hail/table.py"", line 1195, in show; print(self._show(n,width, truncate, types)); File ""/opt/hail/python/hail/table.py"", line 1198, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/opt/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/opt/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;). Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:489); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:350); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:345); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:920); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:1421,Availability,failure,failure,1421,"ges here, including the full java stack trace):; ```; 2018-06-19 17:15:41 Hail: INFO: vep: annotated 2 variants; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/opt/hail/python/hail/table.py"", line 1195, in show; print(self._show(n,width, truncate, types)); File ""/opt/hail/python/hail/table.py"", line 1198, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/opt/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/opt/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;). Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:489); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:350); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:345); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:920); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.RVD$$anonfun$3$$anon$1.hasNext(R",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:14031,Availability,Error,Error,14031,$apply$23.apply(ContextRDD.scala:308); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-5d0f74cef4f2; Error summary: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:5933,Energy Efficiency,schedul,scheduler,5933,edRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:6005,Energy Efficiency,schedul,scheduler,6005,.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:6370,Energy Efficiency,schedul,scheduler,6370,package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:6410,Energy Efficiency,schedul,scheduler,6410,.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:6509,Energy Efficiency,schedul,scheduler,6509,l.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:6607,Energy Efficiency,schedul,scheduler,6607,DD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:6861,Energy Efficiency,schedul,scheduler,6861,nfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:6942,Energy Efficiency,schedul,scheduler,6942,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at is.hail.sparkextras.ContextRDD.runJob(Con,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:7048,Energy Efficiency,schedul,scheduler,7048,he.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:465); 	at is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:442); 	at is.hail.rvd.Ordered,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:7198,Energy Efficiency,schedul,scheduler,7198,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:465); 	at is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:442); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:286); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:22); 	at is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:247); 	at is.hai,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:7287,Energy Efficiency,schedul,scheduler,7287,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:465); 	at is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:442); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:286); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:22); 	at is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:247); 	at is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:22); 	at is.hail.rvd.RVD$class.take(RVD.sca,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:7385,Energy Efficiency,schedul,scheduler,7385,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:465); 	at is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:442); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:286); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:22); 	at is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:247); 	at is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:22); 	at is.hail.rvd.RVD$class.take(RVD.scala:251); 	at is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:22); 	at is.hail.table.Table.take(Table.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:7481,Energy Efficiency,schedul,scheduler,7481,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:465); 	at is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:442); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:286); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:22); 	at is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:247); 	at is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:22); 	at is.hail.rvd.RVD$class.take(RVD.scala:251); 	at is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:22); 	at is.hail.table.Table.take(Table.scala:958); 	at is.hail.table.Table.showString(Table.scala:999); 	at sun.reflect.NativeMethodAcc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:7646,Energy Efficiency,schedul,scheduler,7646,.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at is.hail.sparkextras.ContextRDD.runJob(ContextRDD.scala:465); 	at is.hail.sparkextras.ContextRDD.head(ContextRDD.scala:442); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:286); 	at is.hail.rvd.OrderedRVD.head(OrderedRVD.scala:22); 	at is.hail.rvd.RVD$class.takeAsBytes(RVD.scala:247); 	at is.hail.rvd.OrderedRVD.takeAsBytes(OrderedRVD.scala:22); 	at is.hail.rvd.RVD$class.take(RVD.scala:251); 	at is.hail.rvd.OrderedRVD.take(OrderedRVD.scala:22); 	at is.hail.table.Table.take(Table.scala:958); 	at is.hail.table.Table.showString(Table.scala:999); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.inv,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:13601,Energy Efficiency,schedul,scheduler,13601,$apply$23.apply(ContextRDD.scala:308); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-5d0f74cef4f2; Error summary: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:13673,Energy Efficiency,schedul,scheduler,13673,$apply$23.apply(ContextRDD.scala:308); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-5d0f74cef4f2; Error summary: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:411,Integrability,message,messages,411,"### Hail version:; Latest build of `devel` from https://storage.googleapis.com/hail-common/distributions/devel/Hail-devel-5d0f74cef4f2-Spark-2.2.0.zip. ### What you did:; 1. Import VCF file into MatrixTable.; 2. Annotate VCF file with `output = hl.vep(input, 'vep.properties', csq=True)`.; 3. Attempt to view output with `output.rows().show()`. With `csq=False`, step 3 succeeds. ### What went wrong (all error messages here, including the full java stack trace):; ```; 2018-06-19 17:15:41 Hail: INFO: vep: annotated 2 variants; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/opt/hail/python/hail/table.py"", line 1195, in show; print(self._show(n,width, truncate, types)); File ""/opt/hail/python/hail/table.py"", line 1198, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/opt/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/opt/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;). Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:489); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:350); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:345); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(Or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:664,Integrability,wrap,wrapper,664,"### Hail version:; Latest build of `devel` from https://storage.googleapis.com/hail-common/distributions/devel/Hail-devel-5d0f74cef4f2-Spark-2.2.0.zip. ### What you did:; 1. Import VCF file into MatrixTable.; 2. Annotate VCF file with `output = hl.vep(input, 'vep.properties', csq=True)`.; 3. Attempt to view output with `output.rows().show()`. With `csq=False`, step 3 succeeds. ### What went wrong (all error messages here, including the full java stack trace):; ```; 2018-06-19 17:15:41 Hail: INFO: vep: annotated 2 variants; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/opt/hail/python/hail/table.py"", line 1195, in show; print(self._show(n,width, truncate, types)); File ""/opt/hail/python/hail/table.py"", line 1198, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/opt/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/opt/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;). Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:489); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:350); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:345); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(Or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:6130,Performance,concurren,concurrent,6130,; 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:6215,Performance,concurren,concurrent,6215,.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:13798,Performance,concurren,concurrent,13798,$apply$23.apply(ContextRDD.scala:308); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-5d0f74cef4f2; Error summary: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:13883,Performance,concurren,concurrent,13883,$apply$23.apply(ContextRDD.scala:308); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.hasNext(OrderedRVD.scala:923); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.utils.package$.getIteratorSizeWithMaxN(package.scala:347); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-5d0f74cef4f2; Error summary: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:1342,Safety,abort,aborted,1342,"alse`, step 3 succeeds. ### What went wrong (all error messages here, including the full java stack trace):; ```; 2018-06-19 17:15:41 Hail: INFO: vep: annotated 2 variants; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/hail/python/hail/typecheck/check.py"", line 547, in wrapper; return f(*args_, **kwargs_); File ""/opt/hail/python/hail/table.py"", line 1195, in show; print(self._show(n,width, truncate, types)); File ""/opt/hail/python/hail/table.py"", line 1198, in _show; return self._jt.showString(n, joption(truncate), types, width); File ""/opt/spark-2.2.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/opt/hail/python/hail/utils/java.py"", line 196, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;). Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 11, localhost, executor driver): scala.MatchError: [Ljava.lang.String;@7cd5fe91 (of class [Ljava.lang.String;); 	at is.hail.annotations.RegionValueBuilder.addAnnotation(RegionValueBuilder.scala:489); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:350); 	at is.hail.methods.VEP$$anonfun$9$$anonfun$apply$4.apply(VEP.scala:345); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:926); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:920); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:6541,Safety,abort,abortStage,6541,DD$$anonfun$12.apply(ContextRDD.scala:442); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:6639,Safety,abort,abortStage,6639,pply(ContextRDD.scala:469); 	at is.hail.sparkextras.ContextRDD$$anonfun$runJob$1.apply(ContextRDD.scala:467); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/issues/3790:6884,Safety,abort,abortStage,6884,xt.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3790
https://github.com/hail-is/hail/pull/3793:117,Security,expose,exposed,117,- added `entropy` to IR and wrote missing 0.2 documentation; - fixed entropy to handle the empty string correctly; - exposed the `sign` documentation; - made `NaN` consistently Python's `nan` in docs,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3793
https://github.com/hail-is/hail/pull/3794:712,Modifiability,rewrite,rewrite,712,"`OnDiskBTreeIndexToValue` can index the elements at the base of a BTree. The BGen BTree is a BTree on the byte-offsets of variants in the BGen file. In a following PR, I will use this class to filter the BGen file to a subset of variants, specified by their index. This kind of filtering happens at the level of bytes, it permits me to avoid decoding/decompressing any variants I don't need. Ideally, the index would also include the variant keys themselves. That would be a really nice extension of this work and would enable a more natural file-level filtering user experience. Aside: `IndexBTree` could use some TLC. I'm sort of making the minimal changes to get Caitlin cooking. Last night, I slipped into a rewrite and it's just too big a task to get right before Friday because I don't have any documentation of precisely what our btree file format is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3794
https://github.com/hail-is/hail/pull/3794:336,Safety,avoid,avoid,336,"`OnDiskBTreeIndexToValue` can index the elements at the base of a BTree. The BGen BTree is a BTree on the byte-offsets of variants in the BGen file. In a following PR, I will use this class to filter the BGen file to a subset of variants, specified by their index. This kind of filtering happens at the level of bytes, it permits me to avoid decoding/decompressing any variants I don't need. Ideally, the index would also include the variant keys themselves. That would be a really nice extension of this work and would enable a more natural file-level filtering user experience. Aside: `IndexBTree` could use some TLC. I'm sort of making the minimal changes to get Caitlin cooking. Last night, I slipped into a rewrite and it's just too big a task to get right before Friday because I don't have any documentation of precisely what our btree file format is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3794
https://github.com/hail-is/hail/pull/3794:563,Usability,user experience,user experience,563,"`OnDiskBTreeIndexToValue` can index the elements at the base of a BTree. The BGen BTree is a BTree on the byte-offsets of variants in the BGen file. In a following PR, I will use this class to filter the BGen file to a subset of variants, specified by their index. This kind of filtering happens at the level of bytes, it permits me to avoid decoding/decompressing any variants I don't need. Ideally, the index would also include the variant keys themselves. That would be a really nice extension of this work and would enable a more natural file-level filtering user experience. Aside: `IndexBTree` could use some TLC. I'm sort of making the minimal changes to get Caitlin cooking. Last night, I slipped into a rewrite and it's just too big a task to get right before Friday because I don't have any documentation of precisely what our btree file format is.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3794
https://github.com/hail-is/hail/pull/3796:14,Availability,error,error,14,I ran into an error on another branch where ArrayFor doesn't work if the object being aggregated is a Set. I'm assuming here the ToArray is free for all container objects.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3796
https://github.com/hail-is/hail/pull/3802:50,Testability,test,tests,50,This was preventing some of the python import_vcf tests from going through IR.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3802
https://github.com/hail-is/hail/pull/3805:118,Testability,test,test,118,"I figured I'd make this PR while I try and figure out what is causing #2436. This PR fixes the problem. There's a new test for it. The code for the new aggregator is based on `TakeByAggregator`. The reason for having 36! versions of TakeBy is the seqOp types need to match up with the primitive types. Both annotation and Long have the second argument is a Long, so we can't add the seqOps to one class.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3805
https://github.com/hail-is/hail/pull/3806:33,Testability,test,tested,33,to ensure that combOps are being tested somewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3806
https://github.com/hail-is/hail/pull/3807:47,Modifiability,layers,layers,47,"I knew I was missing tests of greater than two layers. Sure enough there were more bugs lurking. I think >3 layers won't find any new bugs given that there are basically three kinds of b-trees:. - 1 layer: all leaf nodes, <=1024 elements; - 2 layers: one internal/key layer, one leaf layer [1025, 1024^2] elements; - n layers: n-1 internal/key layers, one leaf layer [1024^(n-1)+1, 1024^n] elements. The last case is the first case where we have to do two levels of internal layers. This traversal is defined inductively, so I suspect succeeding on 3 layers tests all the functionality of >3 layers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3807
https://github.com/hail-is/hail/pull/3807:108,Modifiability,layers,layers,108,"I knew I was missing tests of greater than two layers. Sure enough there were more bugs lurking. I think >3 layers won't find any new bugs given that there are basically three kinds of b-trees:. - 1 layer: all leaf nodes, <=1024 elements; - 2 layers: one internal/key layer, one leaf layer [1025, 1024^2] elements; - n layers: n-1 internal/key layers, one leaf layer [1024^(n-1)+1, 1024^n] elements. The last case is the first case where we have to do two levels of internal layers. This traversal is defined inductively, so I suspect succeeding on 3 layers tests all the functionality of >3 layers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3807
https://github.com/hail-is/hail/pull/3807:243,Modifiability,layers,layers,243,"I knew I was missing tests of greater than two layers. Sure enough there were more bugs lurking. I think >3 layers won't find any new bugs given that there are basically three kinds of b-trees:. - 1 layer: all leaf nodes, <=1024 elements; - 2 layers: one internal/key layer, one leaf layer [1025, 1024^2] elements; - n layers: n-1 internal/key layers, one leaf layer [1024^(n-1)+1, 1024^n] elements. The last case is the first case where we have to do two levels of internal layers. This traversal is defined inductively, so I suspect succeeding on 3 layers tests all the functionality of >3 layers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3807
https://github.com/hail-is/hail/pull/3807:319,Modifiability,layers,layers,319,"I knew I was missing tests of greater than two layers. Sure enough there were more bugs lurking. I think >3 layers won't find any new bugs given that there are basically three kinds of b-trees:. - 1 layer: all leaf nodes, <=1024 elements; - 2 layers: one internal/key layer, one leaf layer [1025, 1024^2] elements; - n layers: n-1 internal/key layers, one leaf layer [1024^(n-1)+1, 1024^n] elements. The last case is the first case where we have to do two levels of internal layers. This traversal is defined inductively, so I suspect succeeding on 3 layers tests all the functionality of >3 layers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3807
https://github.com/hail-is/hail/pull/3807:344,Modifiability,layers,layers,344,"I knew I was missing tests of greater than two layers. Sure enough there were more bugs lurking. I think >3 layers won't find any new bugs given that there are basically three kinds of b-trees:. - 1 layer: all leaf nodes, <=1024 elements; - 2 layers: one internal/key layer, one leaf layer [1025, 1024^2] elements; - n layers: n-1 internal/key layers, one leaf layer [1024^(n-1)+1, 1024^n] elements. The last case is the first case where we have to do two levels of internal layers. This traversal is defined inductively, so I suspect succeeding on 3 layers tests all the functionality of >3 layers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3807
https://github.com/hail-is/hail/pull/3807:475,Modifiability,layers,layers,475,"I knew I was missing tests of greater than two layers. Sure enough there were more bugs lurking. I think >3 layers won't find any new bugs given that there are basically three kinds of b-trees:. - 1 layer: all leaf nodes, <=1024 elements; - 2 layers: one internal/key layer, one leaf layer [1025, 1024^2] elements; - n layers: n-1 internal/key layers, one leaf layer [1024^(n-1)+1, 1024^n] elements. The last case is the first case where we have to do two levels of internal layers. This traversal is defined inductively, so I suspect succeeding on 3 layers tests all the functionality of >3 layers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3807
https://github.com/hail-is/hail/pull/3807:551,Modifiability,layers,layers,551,"I knew I was missing tests of greater than two layers. Sure enough there were more bugs lurking. I think >3 layers won't find any new bugs given that there are basically three kinds of b-trees:. - 1 layer: all leaf nodes, <=1024 elements; - 2 layers: one internal/key layer, one leaf layer [1025, 1024^2] elements; - n layers: n-1 internal/key layers, one leaf layer [1024^(n-1)+1, 1024^n] elements. The last case is the first case where we have to do two levels of internal layers. This traversal is defined inductively, so I suspect succeeding on 3 layers tests all the functionality of >3 layers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3807
https://github.com/hail-is/hail/pull/3807:592,Modifiability,layers,layers,592,"I knew I was missing tests of greater than two layers. Sure enough there were more bugs lurking. I think >3 layers won't find any new bugs given that there are basically three kinds of b-trees:. - 1 layer: all leaf nodes, <=1024 elements; - 2 layers: one internal/key layer, one leaf layer [1025, 1024^2] elements; - n layers: n-1 internal/key layers, one leaf layer [1024^(n-1)+1, 1024^n] elements. The last case is the first case where we have to do two levels of internal layers. This traversal is defined inductively, so I suspect succeeding on 3 layers tests all the functionality of >3 layers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3807
https://github.com/hail-is/hail/pull/3807:21,Testability,test,tests,21,"I knew I was missing tests of greater than two layers. Sure enough there were more bugs lurking. I think >3 layers won't find any new bugs given that there are basically three kinds of b-trees:. - 1 layer: all leaf nodes, <=1024 elements; - 2 layers: one internal/key layer, one leaf layer [1025, 1024^2] elements; - n layers: n-1 internal/key layers, one leaf layer [1024^(n-1)+1, 1024^n] elements. The last case is the first case where we have to do two levels of internal layers. This traversal is defined inductively, so I suspect succeeding on 3 layers tests all the functionality of >3 layers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3807
https://github.com/hail-is/hail/pull/3807:558,Testability,test,tests,558,"I knew I was missing tests of greater than two layers. Sure enough there were more bugs lurking. I think >3 layers won't find any new bugs given that there are basically three kinds of b-trees:. - 1 layer: all leaf nodes, <=1024 elements; - 2 layers: one internal/key layer, one leaf layer [1025, 1024^2] elements; - n layers: n-1 internal/key layers, one leaf layer [1024^(n-1)+1, 1024^n] elements. The last case is the first case where we have to do two levels of internal layers. This traversal is defined inductively, so I suspect succeeding on 3 layers tests all the functionality of >3 layers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3807
https://github.com/hail-is/hail/pull/3808:263,Testability,test,test,263,"This builds on top of #3779 and will merge clean once that it's in. Please review after #3779 merges. This adds a row field that is the index of the variant _in the original file_. This might not be the same as the locus ordered position. In particular, our 8bit test file has variants ordered by RSID, which is not the same ordering as the file ordering. The next file will add a filter on file row indices. Then, finally, we can use all this in combination to generate a list of file row indices to keep and dramatically increase filtering speed. ```python; In [1]: import hail as hl; ...: ; ...: mt = hl.import_bgen('/Users/dking/projects/hail/src/test/resources/example.8bits.bgen',; ...: entry_fields=['dosage'],; ...: contig_recoding={'01': '1'},; ...: reference_genome='GRCh37',; ...: _row_fields=['varid', 'rsid', 'file_row_idx']); ...: ; ...: mt.rows().show(); ```; ```; +---------------+------------+----------+-----------+--------------+; | locus | alleles | rsid | varid | file_row_idx |; +---------------+------------+----------+-----------+--------------+; | locus<GRCh37> | array<str> | str | str | int64 |; +---------------+------------+----------+-----------+--------------+; | 1:1001 | [""A"",""G""] | RSID_101 | SNPID_101 | 99 |; | 1:2000 | [""A"",""G""] | RSID_2 | SNPID_2 | 0 |; | 1:2001 | [""A"",""G""] | RSID_102 | SNPID_102 | 100 |; | 1:3000 | [""A"",""G""] | RSID_3 | SNPID_3 | 1 |; | 1:3001 | [""A"",""G""] | RSID_103 | SNPID_103 | 101 |; | 1:4000 | [""A"",""G""] | RSID_4 | SNPID_4 | 2 |; | 1:4001 | [""A"",""G""] | RSID_104 | SNPID_104 | 102 |; | 1:5000 | [""A"",""G""] | RSID_5 | SNPID_5 | 3 |; | 1:5001 | [""A"",""G""] | RSID_105 | SNPID_105 | 103 |; | 1:6000 | [""A"",""G""] | RSID_6 | SNPID_6 | 4 |; +---------------+------------+----------+-----------+--------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3808
https://github.com/hail-is/hail/pull/3808:651,Testability,test,test,651,"This builds on top of #3779 and will merge clean once that it's in. Please review after #3779 merges. This adds a row field that is the index of the variant _in the original file_. This might not be the same as the locus ordered position. In particular, our 8bit test file has variants ordered by RSID, which is not the same ordering as the file ordering. The next file will add a filter on file row indices. Then, finally, we can use all this in combination to generate a list of file row indices to keep and dramatically increase filtering speed. ```python; In [1]: import hail as hl; ...: ; ...: mt = hl.import_bgen('/Users/dking/projects/hail/src/test/resources/example.8bits.bgen',; ...: entry_fields=['dosage'],; ...: contig_recoding={'01': '1'},; ...: reference_genome='GRCh37',; ...: _row_fields=['varid', 'rsid', 'file_row_idx']); ...: ; ...: mt.rows().show(); ```; ```; +---------------+------------+----------+-----------+--------------+; | locus | alleles | rsid | varid | file_row_idx |; +---------------+------------+----------+-----------+--------------+; | locus<GRCh37> | array<str> | str | str | int64 |; +---------------+------------+----------+-----------+--------------+; | 1:1001 | [""A"",""G""] | RSID_101 | SNPID_101 | 99 |; | 1:2000 | [""A"",""G""] | RSID_2 | SNPID_2 | 0 |; | 1:2001 | [""A"",""G""] | RSID_102 | SNPID_102 | 100 |; | 1:3000 | [""A"",""G""] | RSID_3 | SNPID_3 | 1 |; | 1:3001 | [""A"",""G""] | RSID_103 | SNPID_103 | 101 |; | 1:4000 | [""A"",""G""] | RSID_4 | SNPID_4 | 2 |; | 1:4001 | [""A"",""G""] | RSID_104 | SNPID_104 | 102 |; | 1:5000 | [""A"",""G""] | RSID_5 | SNPID_5 | 3 |; | 1:5001 | [""A"",""G""] | RSID_105 | SNPID_105 | 103 |; | 1:6000 | [""A"",""G""] | RSID_6 | SNPID_6 | 4 |; +---------------+------------+----------+-----------+--------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3808
https://github.com/hail-is/hail/pull/3809:132,Testability,test,tests,132,"forgot to add it to this, so it wasn't actually being parsed here and was still going through AST. I double checked that the python tests that call this are now going through IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3809
https://github.com/hail-is/hail/pull/3812:111,Security,expose,expose,111,There is a function in scala to compute the global position of a locus along the reference genome. I'd like to expose it in python as a method on a LocusExpression so I can use it to make Manhattan plots.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3812
https://github.com/hail-is/hail/pull/3813:2617,Deployability,configurat,configuration,2617,"gen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths to lists of variants (identified by their in-file index) to keep; - a test for `_variants_per_file`; - a fixed `calcDepth` which is now used by both index classes; - a set of tests for `calcDepth`; - some clean up in `BgenBlockReader`: use `private[this]` for things that are truly private fields (otherwise they're accessed through `invokevirtual`) and use `using` to manage resources; - teach `BgenInputFormat` to produce splits that describe which variants to keep; - modify `BgenRecord` to use variant filtering information in file splits; - add some groady code to ship variant filter lists through the hadoop configuration",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3813
https://github.com/hail-is/hail/pull/3813:195,Integrability,interface,interface,195,"Finally add BGEN filtering at the level of bytes. We add a non-public, non-documented `_variants_per_file` argument that lists the variants to keep by their on-disk indices. It's a pretty groady interface, but it works with the current infrastructure and provides a _massive_ improvement. We go from decoding all the variants in the BGEN to decoding only those relevant to the computation at hand. In the case of Caitlin's PRS scoring method, that's about 1% or less of the original variant set. It's a bit janky. The file paths need to be the fully qualified ones that are seen by the hadoop reader. So `file:/full/path/to/file.bgen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3813
https://github.com/hail-is/hail/pull/3813:982,Modifiability,layers,layers,982,"Finally add BGEN filtering at the level of bytes. We add a non-public, non-documented `_variants_per_file` argument that lists the variants to keep by their on-disk indices. It's a pretty groady interface, but it works with the current infrastructure and provides a _massive_ improvement. We go from decoding all the variants in the BGEN to decoding only those relevant to the computation at hand. In the case of Caitlin's PRS scoring method, that's about 1% or less of the original variant set. It's a bit janky. The file paths need to be the fully qualified ones that are seen by the hadoop reader. So `file:/full/path/to/file.bgen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3813
https://github.com/hail-is/hail/pull/3813:1421,Modifiability,layers,layers,1421,"d. In the case of Caitlin's PRS scoring method, that's about 1% or less of the original variant set. It's a bit janky. The file paths need to be the fully qualified ones that are seen by the hadoop reader. So `file:/full/path/to/file.bgen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths to lists of variants (identified by their in-file index) to keep; - a test for `_variants_per_file`; - a fixed `calcDepth` which is now used by both index classes; - a set of tests for `calcDepth`; - some clean up in `BgenBlockReader`: use `private[this]` for things that are truly private fields (otherwise they're accessed through `invokevirtual`) and use `using` to manage resources; - teach",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3813
https://github.com/hail-is/hail/pull/3813:2617,Modifiability,config,configuration,2617,"gen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths to lists of variants (identified by their in-file index) to keep; - a test for `_variants_per_file`; - a fixed `calcDepth` which is now used by both index classes; - a set of tests for `calcDepth`; - some clean up in `BgenBlockReader`: use `private[this]` for things that are truly private fields (otherwise they're accessed through `invokevirtual`) and use `using` to manage resources; - teach `BgenInputFormat` to produce splits that describe which variants to keep; - modify `BgenRecord` to use variant filtering information in file splits; - add some groady code to ship variant filter lists through the hadoop configuration",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3813
https://github.com/hail-is/hail/pull/3813:2318,Security,access,accessed,2318,"gen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths to lists of variants (identified by their in-file index) to keep; - a test for `_variants_per_file`; - a fixed `calcDepth` which is now used by both index classes; - a set of tests for `calcDepth`; - some clean up in `BgenBlockReader`: use `private[this]` for things that are truly private fields (otherwise they're accessed through `invokevirtual`) and use `using` to manage resources; - teach `BgenInputFormat` to produce splits that describe which variants to keep; - modify `BgenRecord` to use variant filtering information in file splits; - add some groady code to ship variant filter lists through the hadoop configuration",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3813
https://github.com/hail-is/hail/pull/3813:1860,Testability,test,tests,1860,"gen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths to lists of variants (identified by their in-file index) to keep; - a test for `_variants_per_file`; - a fixed `calcDepth` which is now used by both index classes; - a set of tests for `calcDepth`; - some clean up in `BgenBlockReader`: use `private[this]` for things that are truly private fields (otherwise they're accessed through `invokevirtual`) and use `using` to manage resources; - teach `BgenInputFormat` to produce splits that describe which variants to keep; - modify `BgenRecord` to use variant filtering information in file splits; - add some groady code to ship variant filter lists through the hadoop configuration",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3813
https://github.com/hail-is/hail/pull/3813:2072,Testability,test,test,2072,"gen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths to lists of variants (identified by their in-file index) to keep; - a test for `_variants_per_file`; - a fixed `calcDepth` which is now used by both index classes; - a set of tests for `calcDepth`; - some clean up in `BgenBlockReader`: use `private[this]` for things that are truly private fields (otherwise they're accessed through `invokevirtual`) and use `using` to manage resources; - teach `BgenInputFormat` to produce splits that describe which variants to keep; - modify `BgenRecord` to use variant filtering information in file splits; - add some groady code to ship variant filter lists through the hadoop configuration",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3813
https://github.com/hail-is/hail/pull/3813:2177,Testability,test,tests,2177,"gen` or `gs://full/path/to/bgen.file`, which is annoying. I don't have any better way to generically uniquely identify files though. ---; ### Calc Depth Bug. I also had to fix a bug in the indices. Neither my `OnDiskBTreeIndexToValue` nor the existing `IndexBTree` correctly calculated the sizes of the given trees. Recall that a b-tree is a series of layers. Layer 0 is at most `branchingFactor` in size. Layer i is at most `branchingFactor ^ (i+1)` in size. The total size of the b-tree is the sum of the layer sizes. Here's a few max sizes for a branchingFactor of 1024:. - 1 layer tree: 1024; - 2 layer tree: 1024^2 + 1024; - 3 layer tree: 1024^3 + 1024^2 + 1024. If you look carefully at the old `calcDepth` method, it incorrectly concludes that fully populated 3 layer trees have four layers because they have more than 1024^3 total (internal+leaf) elements. This issue rears it's head on an exponentially small number of trees (at depth `i`, the number of leaf elements must lie in `[1024^i-1024^(i-1), 1024^i]`. This discrepancy is what lead to my confusion for the last few days. It shows up quite quickly with very small branching factors (e.g. 3) but with a large branching factor (the default of 1024 and what all the tests were written against) it's fairly rare. ---; ### Summary of Changes. - add `_variants_per_file` which is a map from absolute file paths to lists of variants (identified by their in-file index) to keep; - a test for `_variants_per_file`; - a fixed `calcDepth` which is now used by both index classes; - a set of tests for `calcDepth`; - some clean up in `BgenBlockReader`: use `private[this]` for things that are truly private fields (otherwise they're accessed through `invokevirtual`) and use `using` to manage resources; - teach `BgenInputFormat` to produce splits that describe which variants to keep; - modify `BgenRecord` to use variant filtering information in file splits; - add some groady code to ship variant filter lists through the hadoop configuration",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3813
https://github.com/hail-is/hail/issues/3814:1220,Availability,error,error,1220,"d | probands | n_probands | r_prob | idx | n_cum_probands |; +----------------+----------------+------------------------------------------+------------+-------------+-------+----------------+; | str | str | array<str> | int64 | float64 | int64 | int64 |; +----------------+----------------+------------------------------------------+------------+-------------+-------+----------------+; | RG1692 | RG1695 | [""RG1655"",""RG1681"",""RG1691"",""RG1706"",... | 8 | 1.71331e-01 | 0 | 8 |; | G01-GEA-28-PA | G01-GEA-28-MA | [""G01-GEA-28-HI""] | 1 | 4.93356e-02 | 867 | 717 |; | DEASD_0210_500 | DEASD_0210_600 | [""DEASD_0210_001""] | 1 | 4.93218e-02 | 866 | 716 |. ```; Then I do:; ```; keep_fams_ht = keep_fams_ht.annotate(relateds=keep_fams_ht.probands.append(keep_fams_ht.mother_id).append(keep_fams_ht.father_id)); relateds = keep_fams_ht.explode(keep_fams_ht.relateds).key_by('relateds').distinct(). ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; Py4JJavaError Traceback (most recent call last); <ipython-input-99-84209896ffb7> in <module>(); 1 keep_fams_ht = keep_fams_ht.annotate(relateds=keep_fams_ht.probands.append(keep_fams_ht.mother_id).append(keep_fams_ht.father_id)); ----> 2 relateds = keep_fams_ht.explode(keep_fams_ht.relateds).key_by('relateds').distinct(). /home/hail/hail.zip/hail/table.py in distinct(self); 2607 """"""; 2608 hail.methods.misc.require_key(self, ""distinct""); -> 2609 return Table(self._jt.distinctByKey()); 2610 ; 2611 table_type.set(Table). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 182 import pyspark; 183 try:; --> 184 return f(*args, **kwargs); 185 except py4j.protocol.Py4JJavaError as e:; 186 s = e.java_except",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3814
https://github.com/hail-is/hail/issues/3814:2475,Availability,error,error,2475,"bands.append(keep_fams_ht.mother_id).append(keep_fams_ht.father_id)); ----> 2 relateds = keep_fams_ht.explode(keep_fams_ht.relateds).key_by('relateds').distinct(). /home/hail/hail.zip/hail/table.py in distinct(self); 2607 """"""; 2608 hail.methods.misc.require_key(self, ""distinct""); -> 2609 return Table(self._jt.distinctByKey()); 2610 ; 2611 table_type.set(Table). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 182 import pyspark; 183 try:; --> 184 return f(*args, **kwargs); 185 except py4j.protocol.Py4JJavaError as e:; 186 s = e.java_exception.toString(). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 317 raise Py4JJavaError(; 318 ""An error occurred while calling {0}{1}{2}.\n"".; --> 319 format(target_id, ""."", name), value); 320 else:; 321 raise Py4JError(. Py4JJavaError: An error occurred while calling o1507.distinctByKey.; : java.util.NoSuchElementException: None.get; 	at scala.None$.get(Option.scala:347); 	at scala.None$.get(Option.scala:345); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2089); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2064); 	at is.hail.expr.TableExplode.execute(Relational.scala:2166); 	at is.hail.expr.TableUnkey.execute(Relational.scala:1857); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2064); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1820); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1820); 	at is.hail.table.Table.value$lzycompute(Table.scala:237); 	at is.hail.table.Table.value(Table.scala:232); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:240); 	at is.hail.table.Table.x$5(Table.scala:240); 	at is.hail.tab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3814
https://github.com/hail-is/hail/issues/3814:2617,Availability,error,error,2617,"zip/hail/table.py in distinct(self); 2607 """"""; 2608 hail.methods.misc.require_key(self, ""distinct""); -> 2609 return Table(self._jt.distinctByKey()); 2610 ; 2611 table_type.set(Table). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 182 import pyspark; 183 try:; --> 184 return f(*args, **kwargs); 185 except py4j.protocol.Py4JJavaError as e:; 186 s = e.java_exception.toString(). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 317 raise Py4JJavaError(; 318 ""An error occurred while calling {0}{1}{2}.\n"".; --> 319 format(target_id, ""."", name), value); 320 else:; 321 raise Py4JError(. Py4JJavaError: An error occurred while calling o1507.distinctByKey.; : java.util.NoSuchElementException: None.get; 	at scala.None$.get(Option.scala:347); 	at scala.None$.get(Option.scala:345); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2089); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2064); 	at is.hail.expr.TableExplode.execute(Relational.scala:2166); 	at is.hail.expr.TableUnkey.execute(Relational.scala:1857); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2064); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1820); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1820); 	at is.hail.table.Table.value$lzycompute(Table.scala:237); 	at is.hail.table.Table.value(Table.scala:232); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:240); 	at is.hail.table.Table.x$5(Table.scala:240); 	at is.hail.table.Table.rvd$lzycompute(Table.scala:240); 	at is.hail.table.Table.rvd(Table.scala:240); 	at is.hail.table.Table.distinctByKey(Table.scala:593); 	at sun.reflect.NativeMethodAccessor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3814
https://github.com/hail-is/hail/issues/3814:1226,Integrability,message,messages,1226,"d | probands | n_probands | r_prob | idx | n_cum_probands |; +----------------+----------------+------------------------------------------+------------+-------------+-------+----------------+; | str | str | array<str> | int64 | float64 | int64 | int64 |; +----------------+----------------+------------------------------------------+------------+-------------+-------+----------------+; | RG1692 | RG1695 | [""RG1655"",""RG1681"",""RG1691"",""RG1706"",... | 8 | 1.71331e-01 | 0 | 8 |; | G01-GEA-28-PA | G01-GEA-28-MA | [""G01-GEA-28-HI""] | 1 | 4.93356e-02 | 867 | 717 |; | DEASD_0210_500 | DEASD_0210_600 | [""DEASD_0210_001""] | 1 | 4.93218e-02 | 866 | 716 |. ```; Then I do:; ```; keep_fams_ht = keep_fams_ht.annotate(relateds=keep_fams_ht.probands.append(keep_fams_ht.mother_id).append(keep_fams_ht.father_id)); relateds = keep_fams_ht.explode(keep_fams_ht.relateds).key_by('relateds').distinct(). ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; Py4JJavaError Traceback (most recent call last); <ipython-input-99-84209896ffb7> in <module>(); 1 keep_fams_ht = keep_fams_ht.annotate(relateds=keep_fams_ht.probands.append(keep_fams_ht.mother_id).append(keep_fams_ht.father_id)); ----> 2 relateds = keep_fams_ht.explode(keep_fams_ht.relateds).key_by('relateds').distinct(). /home/hail/hail.zip/hail/table.py in distinct(self); 2607 """"""; 2608 hail.methods.misc.require_key(self, ""distinct""); -> 2609 return Table(self._jt.distinctByKey()); 2610 ; 2611 table_type.set(Table). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 182 import pyspark; 183 try:; --> 184 return f(*args, **kwargs); 185 except py4j.protocol.Py4JJavaError as e:; 186 s = e.java_except",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3814
https://github.com/hail-is/hail/issues/3814:2249,Integrability,protocol,protocol,2249,"e full java stack trace):; ```; Py4JJavaError Traceback (most recent call last); <ipython-input-99-84209896ffb7> in <module>(); 1 keep_fams_ht = keep_fams_ht.annotate(relateds=keep_fams_ht.probands.append(keep_fams_ht.mother_id).append(keep_fams_ht.father_id)); ----> 2 relateds = keep_fams_ht.explode(keep_fams_ht.relateds).key_by('relateds').distinct(). /home/hail/hail.zip/hail/table.py in distinct(self); 2607 """"""; 2608 hail.methods.misc.require_key(self, ""distinct""); -> 2609 return Table(self._jt.distinctByKey()); 2610 ; 2611 table_type.set(Table). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 182 import pyspark; 183 try:; --> 184 return f(*args, **kwargs); 185 except py4j.protocol.Py4JJavaError as e:; 186 s = e.java_exception.toString(). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 317 raise Py4JJavaError(; 318 ""An error occurred while calling {0}{1}{2}.\n"".; --> 319 format(target_id, ""."", name), value); 320 else:; 321 raise Py4JError(. Py4JJavaError: An error occurred while calling o1507.distinctByKey.; : java.util.NoSuchElementException: None.get; 	at scala.None$.get(Option.scala:347); 	at scala.None$.get(Option.scala:345); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2089); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2064); 	at is.hail.expr.TableExplode.execute(Relational.scala:2166); 	at is.hail.expr.TableUnkey.execute(Relational.scala:1857); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2064); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1820); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1820); 	at is.hail.table.Table.value$lzyc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3814
https://github.com/hail-is/hail/issues/3814:2367,Integrability,protocol,protocol,2367,"n <module>(); 1 keep_fams_ht = keep_fams_ht.annotate(relateds=keep_fams_ht.probands.append(keep_fams_ht.mother_id).append(keep_fams_ht.father_id)); ----> 2 relateds = keep_fams_ht.explode(keep_fams_ht.relateds).key_by('relateds').distinct(). /home/hail/hail.zip/hail/table.py in distinct(self); 2607 """"""; 2608 hail.methods.misc.require_key(self, ""distinct""); -> 2609 return Table(self._jt.distinctByKey()); 2610 ; 2611 table_type.set(Table). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 182 import pyspark; 183 try:; --> 184 return f(*args, **kwargs); 185 except py4j.protocol.Py4JJavaError as e:; 186 s = e.java_exception.toString(). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 317 raise Py4JJavaError(; 318 ""An error occurred while calling {0}{1}{2}.\n"".; --> 319 format(target_id, ""."", name), value); 320 else:; 321 raise Py4JError(. Py4JJavaError: An error occurred while calling o1507.distinctByKey.; : java.util.NoSuchElementException: None.get; 	at scala.None$.get(Option.scala:347); 	at scala.None$.get(Option.scala:345); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2089); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2064); 	at is.hail.expr.TableExplode.execute(Relational.scala:2166); 	at is.hail.expr.TableUnkey.execute(Relational.scala:1857); 	at is.hail.expr.TableMapRows.execute(Relational.scala:2064); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1820); 	at is.hail.expr.TableKeyBy.execute(Relational.scala:1820); 	at is.hail.table.Table.value$lzycompute(Table.scala:237); 	at is.hail.table.Table.value(Table.scala:232); 	at is.hail.table.Table.x$5$lzycompute(T",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3814
https://github.com/hail-is/hail/pull/3816:33,Testability,test,test,33,"I'm still not really sure how to test this properly since it's not in the IR function registry, but I checked that the tests in is.hail.methods.AggregatorSuite that rely on this are going through IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3816
https://github.com/hail-is/hail/pull/3816:119,Testability,test,tests,119,"I'm still not really sure how to test this properly since it's not in the IR function registry, but I checked that the tests in is.hail.methods.AggregatorSuite that rely on this are going through IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3816
https://github.com/hail-is/hail/pull/3819:162,Availability,down,down,162,"I needed to make the read function in MatrixRead into a class so I can print/parse as part of the work to replace AST with IR in the Python interface. Also, push down required type into MT decoder. I also use the compiler to build the function to add the entries to the row values (no RegionValueBuilder).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3819
https://github.com/hail-is/hail/pull/3819:140,Integrability,interface,interface,140,"I needed to make the read function in MatrixRead into a class so I can print/parse as part of the work to replace AST with IR in the Python interface. Also, push down required type into MT decoder. I also use the compiler to build the function to add the entries to the row values (no RegionValueBuilder).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3819
https://github.com/hail-is/hail/pull/3821:18,Usability,feedback,feedback,18,This was based on feedback Cotton gave me in the Counter PR. I couldn't figure out how to get a nicer count of the number of elements for `result`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3821
https://github.com/hail-is/hail/pull/3822:381,Testability,test,test,381,"I will need deep copy for KeyedRegionValueAggregators. I only need deepCopy for aggregators with defined `initOp` methods (CallStats), but figured it would be better to add deepCopy to all aggregators so we have both types of copy. Feel free to push back on adding deep copy to all aggregators. I could change `Code.lookupMethod` to return an `Option[Invokeable[T, S]]` instead to test if `deepCopy` exists on the aggregator.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3822
https://github.com/hail-is/hail/pull/3824:121,Modifiability,refactor,refactoring,121,"Based on #3822. Make sure you start looking at commits at `Keyed RV Aggregator`. Same PR as #3768, but rebased with some refactoring.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3824
https://github.com/hail-is/hail/pull/3825:555,Testability,test,test,555,"So I thought we had a bug in `r_expected_het_freq` because it's documented as the ratio of observed to expected heterozygote frequency, the ratio of observed to expected heterozygote count, and the heterozygosity ratio (which I assumed must have meant the same as the first two):; https://hail.is/docs/devel/aggregators.html#hail.expr.aggregators.hardy_weinberg; https://hail.is/docs/devel/methods/genetics.html#hail.methods.variant_qc; https://hail.is/docs/devel/functions/stats.html#hail.expr.functions.hardy_weinberg_p. Also, the null dist for the HWE test is on the number of hets, so it seemed natural to report some notion of deviation between observed and expected count (or equivalently, frequency), e.g. ratio of 2.0 means there are twice as many hets as expected under HWE. So while the name `r_expected_het_freq` didn't seem ideal, I read it as ratio of observed to expected het count/freq, so changed the name to `r_obs_exp_het` and the code and tests to match the documentation. I feel pretty dense that it only now just occurred to me that it was probably supposed to mean what was in the code, that is, the expected frequency of heterozygous samples under HWE, and its the documentation that was wrong. I propose to change it back, fix the docs, and name it `het_freq_hwe`, that is, the expected frequency of het under HWE, to go with `p_value_hwe`. Does that sound reasonable? The rest of the PR is ready for review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3825
https://github.com/hail-is/hail/pull/3825:958,Testability,test,tests,958,"So I thought we had a bug in `r_expected_het_freq` because it's documented as the ratio of observed to expected heterozygote frequency, the ratio of observed to expected heterozygote count, and the heterozygosity ratio (which I assumed must have meant the same as the first two):; https://hail.is/docs/devel/aggregators.html#hail.expr.aggregators.hardy_weinberg; https://hail.is/docs/devel/methods/genetics.html#hail.methods.variant_qc; https://hail.is/docs/devel/functions/stats.html#hail.expr.functions.hardy_weinberg_p. Also, the null dist for the HWE test is on the number of hets, so it seemed natural to report some notion of deviation between observed and expected count (or equivalently, frequency), e.g. ratio of 2.0 means there are twice as many hets as expected under HWE. So while the name `r_expected_het_freq` didn't seem ideal, I read it as ratio of observed to expected het count/freq, so changed the name to `r_obs_exp_het` and the code and tests to match the documentation. I feel pretty dense that it only now just occurred to me that it was probably supposed to mean what was in the code, that is, the expected frequency of heterozygous samples under HWE, and its the documentation that was wrong. I propose to change it back, fix the docs, and name it `het_freq_hwe`, that is, the expected frequency of het under HWE, to go with `p_value_hwe`. Does that sound reasonable? The rest of the PR is ready for review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3825
https://github.com/hail-is/hail/pull/3826:59,Testability,Test,Test,59,"This one just handles value IRs, needed to get rid of AST. Test by verifying x == parse(pretty(x)) on an example of each value IR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3826
https://github.com/hail-is/hail/pull/3830:14,Integrability,message,message,14,Prints a nice message if the user forgets default/or_missing. Fixes #3696,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3830
https://github.com/hail-is/hail/pull/3832:32,Availability,failure,failure,32,"Since Amanda is out, fixed test failure, replacing: https://github.com/hail-is/hail/pull/3817. @catoverdrive wrote it and I already reviewed it. Last commit is my fix, mostly unrelated to this change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3832
https://github.com/hail-is/hail/pull/3832:27,Testability,test,test,27,"Since Amanda is out, fixed test failure, replacing: https://github.com/hail-is/hail/pull/3817. @catoverdrive wrote it and I already reviewed it. Last commit is my fix, mostly unrelated to this change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3832
https://github.com/hail-is/hail/pull/3833:161,Integrability,interface,interface,161,"- ir for chisq, fet, ctt, hwe; - using nan instead of missing; - direct implementation of chisq. This is first half of closed #3825, leaving all python docs and interface as is. To cross-check, I used R and https://www.cog-genomics.org/software/stats",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3833
https://github.com/hail-is/hail/issues/3835:334,Availability,error,error,334,"### Hail version:. d5007d8878f2dc330dd2f01c1c0251785f259dac. ### What you did:. ```; ds = hl.import_vcf(resource(""sample.vcf"")); ds_duplicate = ds.annotate_rows(duplicate = [1,2]); ds_duplicate = ds_duplicate.explode_rows(ds_duplicate['duplicate']); result = hl.ld_prune(ds_duplicate.GT); result.show(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Java stack trace:; java.lang.AssertionError: assertion failed: absoluteUpperIndexBounds length 197 did not match GridPartition nRows 394; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.methods.UpperIndexBounds$.computeCoverByUpperTriangularBlocks(UpperIndexBounds.scala:69); 	at is.hail.linalg.BlockMatrix.filteredEntriesTable(BlockMatrix.scala:1198); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3835
https://github.com/hail-is/hail/issues/3835:340,Integrability,message,messages,340,"### Hail version:. d5007d8878f2dc330dd2f01c1c0251785f259dac. ### What you did:. ```; ds = hl.import_vcf(resource(""sample.vcf"")); ds_duplicate = ds.annotate_rows(duplicate = [1,2]); ds_duplicate = ds_duplicate.explode_rows(ds_duplicate['duplicate']); result = hl.ld_prune(ds_duplicate.GT); result.show(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Java stack trace:; java.lang.AssertionError: assertion failed: absoluteUpperIndexBounds length 197 did not match GridPartition nRows 394; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.methods.UpperIndexBounds$.computeCoverByUpperTriangularBlocks(UpperIndexBounds.scala:69); 	at is.hail.linalg.BlockMatrix.filteredEntriesTable(BlockMatrix.scala:1198); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3835
https://github.com/hail-is/hail/issues/3835:428,Testability,Assert,AssertionError,428,"### Hail version:. d5007d8878f2dc330dd2f01c1c0251785f259dac. ### What you did:. ```; ds = hl.import_vcf(resource(""sample.vcf"")); ds_duplicate = ds.annotate_rows(duplicate = [1,2]); ds_duplicate = ds_duplicate.explode_rows(ds_duplicate['duplicate']); result = hl.ld_prune(ds_duplicate.GT); result.show(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Java stack trace:; java.lang.AssertionError: assertion failed: absoluteUpperIndexBounds length 197 did not match GridPartition nRows 394; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.methods.UpperIndexBounds$.computeCoverByUpperTriangularBlocks(UpperIndexBounds.scala:69); 	at is.hail.linalg.BlockMatrix.filteredEntriesTable(BlockMatrix.scala:1198); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3835
https://github.com/hail-is/hail/issues/3835:444,Testability,assert,assertion,444,"### Hail version:. d5007d8878f2dc330dd2f01c1c0251785f259dac. ### What you did:. ```; ds = hl.import_vcf(resource(""sample.vcf"")); ds_duplicate = ds.annotate_rows(duplicate = [1,2]); ds_duplicate = ds_duplicate.explode_rows(ds_duplicate['duplicate']); result = hl.ld_prune(ds_duplicate.GT); result.show(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Java stack trace:; java.lang.AssertionError: assertion failed: absoluteUpperIndexBounds length 197 did not match GridPartition nRows 394; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.methods.UpperIndexBounds$.computeCoverByUpperTriangularBlocks(UpperIndexBounds.scala:69); 	at is.hail.linalg.BlockMatrix.filteredEntriesTable(BlockMatrix.scala:1198); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3835
https://github.com/hail-is/hail/issues/3835:555,Testability,assert,assert,555,"### Hail version:. d5007d8878f2dc330dd2f01c1c0251785f259dac. ### What you did:. ```; ds = hl.import_vcf(resource(""sample.vcf"")); ds_duplicate = ds.annotate_rows(duplicate = [1,2]); ds_duplicate = ds_duplicate.explode_rows(ds_duplicate['duplicate']); result = hl.ld_prune(ds_duplicate.GT); result.show(); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; Java stack trace:; java.lang.AssertionError: assertion failed: absoluteUpperIndexBounds length 197 did not match GridPartition nRows 394; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.methods.UpperIndexBounds$.computeCoverByUpperTriangularBlocks(UpperIndexBounds.scala:69); 	at is.hail.linalg.BlockMatrix.filteredEntriesTable(BlockMatrix.scala:1198); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3835
https://github.com/hail-is/hail/pull/3837:45,Testability,test,testConstructor,45,"@cseed this fixes the bug that was caught by testConstructor in your branch, but it was distinct enough that I broke it out as a separate PR, if that's ok.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3837
https://github.com/hail-is/hail/pull/3853:55,Safety,avoid,avoid,55,"Builds on https://github.com/hail-is/hail/pull/3852 to avoid a conflict nightmare. You probably don't want to reivew until that goes in. This adds TableIR parser. TableImport is missing because handling the import options requires a bit more work. Tested by pretty printing/parsing example TableIR and verifying the result is the same. Also added (most of the) MatrixIR parser, but it is untested, that will come in part (3). Had to change some Array => IndexedSeq in various places to get proper equality. Rewrote Parser.quotedLiteral to avoid JVM stack depth limits when parsing non-small string literals.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3853
https://github.com/hail-is/hail/pull/3853:539,Safety,avoid,avoid,539,"Builds on https://github.com/hail-is/hail/pull/3852 to avoid a conflict nightmare. You probably don't want to reivew until that goes in. This adds TableIR parser. TableImport is missing because handling the import options requires a bit more work. Tested by pretty printing/parsing example TableIR and verifying the result is the same. Also added (most of the) MatrixIR parser, but it is untested, that will come in part (3). Had to change some Array => IndexedSeq in various places to get proper equality. Rewrote Parser.quotedLiteral to avoid JVM stack depth limits when parsing non-small string literals.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3853
https://github.com/hail-is/hail/pull/3853:248,Testability,Test,Tested,248,"Builds on https://github.com/hail-is/hail/pull/3852 to avoid a conflict nightmare. You probably don't want to reivew until that goes in. This adds TableIR parser. TableImport is missing because handling the import options requires a bit more work. Tested by pretty printing/parsing example TableIR and verifying the result is the same. Also added (most of the) MatrixIR parser, but it is untested, that will come in part (3). Had to change some Array => IndexedSeq in various places to get proper equality. Rewrote Parser.quotedLiteral to avoid JVM stack depth limits when parsing non-small string literals.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3853
https://github.com/hail-is/hail/issues/3856:8,Availability,error,error,8,current error message is a FileNotFoundException,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3856
https://github.com/hail-is/hail/issues/3856:14,Integrability,message,message,14,current error message is a FileNotFoundException,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3856
https://github.com/hail-is/hail/pull/3859:295,Integrability,rout,route,295,"Bundling up the LD score calculation work @jbloom22 and @ktashman did during the Hail hackathon into a Hail method. Not sure if this should be a part of core Hail methods, or sequestered as a user-contributed method, of which there will hopefully be more in 0.2. I've currently taken the latter route, but we can discuss at office hours tomorrow @tpoterba.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3859
https://github.com/hail-is/hail/issues/3860:23,Availability,error,error,23,```; Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; : java.util.NoSuchElementException: spark.serializer; at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:245); at org.apache.spark.SparkConf$$anonfun$get$1.apply(SparkConf.scala:245); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.SparkConf.get(SparkConf.scala:245); at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:108); at is.hail.HailContext$.apply(HailContext.scala:180); at is.hail.HailContext.apply(HailContext.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3860
https://github.com/hail-is/hail/issues/3861:45,Deployability,configurat,configuration,45,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3861
https://github.com/hail-is/hail/issues/3861:658,Deployability,configurat,configuration,658,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3861
https://github.com/hail-is/hail/issues/3861:45,Modifiability,config,configuration,45,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3861
https://github.com/hail-is/hail/issues/3861:658,Modifiability,config,configuration,658,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3861
https://github.com/hail-is/hail/issues/3861:755,Performance,perform,performance,755,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3861
https://github.com/hail-is/hail/issues/3861:885,Performance,load,loaders,885,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3861
https://github.com/hail-is/hail/issues/3861:676,Safety,avoid,avoid,676,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3861
https://github.com/hail-is/hail/issues/3861:190,Testability,test,test,190,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3861
https://github.com/hail-is/hail/issues/3861:287,Testability,test,test-bgz,287,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3861
https://github.com/hail-is/hail/issues/3861:374,Testability,test,test-bgz,374,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3861
https://github.com/hail-is/hail/issues/3861:791,Testability,test,tested,791,"There are many things wrong here. The Hadoop configuration is not copied per HadoopRDD operation. Proof:. ```; >>> import hail as hl; >>> hl.init(min_block_size=0); >>> t = hl.import_table('test.tsv.bgz', impute=True, min_partitions=8); >>> t.n_partitions(); 8; >>> t = hl.import_table('test-bgz.tsv.gz', impute=True, min_partitions=8); >>> t.n_partitions(); 1; ```. where `test-bgz.tsv.gz` is a bgz in gz's clothing. This is compounded by the fact that SparkContext.hadoopFile is not invoked until TableIR.execute is run making HailContext.forceBGZ() completely ineffective. One option is turning on spark.hadoop.cloneConf, that appears to clone the Hadoop configuration (to avoid some multithreading issues) although the docs don't recommend it due to ""performance regressions"". I haven't tested it. The other option is stop using the Hadoop stuff so we can pass state into the file loaders. Doing that for text files/line splitting is a bit nasty, but it would mean we could properly fix this gz/bgz business once and for all (look at the GZ header to see if it is block gzip'ed).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3861
https://github.com/hail-is/hail/issues/3862:10,Availability,down,down,10,"I tracked down why this is happening. The old code stored the (compressed) genotype data per variant in a buffer and decoded it in BgenRecord.getValue. The new code decodes eagerly, but only if the entries are needed. I assume the intention was to mark the entries as unneeded during the scan, but not when decoding the actual values, but this wasn't done. It isn't done easily, either, since we can't set a per-Hadoop import configuration, see: https://github.com/hail-is/hail/issues/3861. Options:. - go back to the old code that stashes the compressed value and evaluates lazily,; - have separate InputFormat/RecordReader for scan and decode,; - stop using Hadoop InputFormat to load BGEN and just code it in directly in Spark, where it is trivial to pass different parameters to scan and decode. I personally vote for the latter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3862
https://github.com/hail-is/hail/issues/3862:426,Deployability,configurat,configuration,426,"I tracked down why this is happening. The old code stored the (compressed) genotype data per variant in a buffer and decoded it in BgenRecord.getValue. The new code decodes eagerly, but only if the entries are needed. I assume the intention was to mark the entries as unneeded during the scan, but not when decoding the actual values, but this wasn't done. It isn't done easily, either, since we can't set a per-Hadoop import configuration, see: https://github.com/hail-is/hail/issues/3861. Options:. - go back to the old code that stashes the compressed value and evaluates lazily,; - have separate InputFormat/RecordReader for scan and decode,; - stop using Hadoop InputFormat to load BGEN and just code it in directly in Spark, where it is trivial to pass different parameters to scan and decode. I personally vote for the latter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3862
https://github.com/hail-is/hail/issues/3862:426,Modifiability,config,configuration,426,"I tracked down why this is happening. The old code stored the (compressed) genotype data per variant in a buffer and decoded it in BgenRecord.getValue. The new code decodes eagerly, but only if the entries are needed. I assume the intention was to mark the entries as unneeded during the scan, but not when decoding the actual values, but this wasn't done. It isn't done easily, either, since we can't set a per-Hadoop import configuration, see: https://github.com/hail-is/hail/issues/3861. Options:. - go back to the old code that stashes the compressed value and evaluates lazily,; - have separate InputFormat/RecordReader for scan and decode,; - stop using Hadoop InputFormat to load BGEN and just code it in directly in Spark, where it is trivial to pass different parameters to scan and decode. I personally vote for the latter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3862
https://github.com/hail-is/hail/issues/3862:682,Performance,load,load,682,"I tracked down why this is happening. The old code stored the (compressed) genotype data per variant in a buffer and decoded it in BgenRecord.getValue. The new code decodes eagerly, but only if the entries are needed. I assume the intention was to mark the entries as unneeded during the scan, but not when decoding the actual values, but this wasn't done. It isn't done easily, either, since we can't set a per-Hadoop import configuration, see: https://github.com/hail-is/hail/issues/3861. Options:. - go back to the old code that stashes the compressed value and evaluates lazily,; - have separate InputFormat/RecordReader for scan and decode,; - stop using Hadoop InputFormat to load BGEN and just code it in directly in Spark, where it is trivial to pass different parameters to scan and decode. I personally vote for the latter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3862
https://github.com/hail-is/hail/pull/3872:18,Deployability,configurat,configuration,18,"- convert the VEP configuration file to JSON; - let the conf file fully specify the command line, environment and output schema. This is breaking change. I updated the VEP config files on the cloud:. ```; $ gsutil ls gs://hail-common/vep/vep/*.json; gs://hail-common/vep/vep/vep81-gcloud.json; gs://hail-common/vep/vep/vep85-gcloud.json; gs://hail-common/vep/vep/vep92-GRCm38-gcloud.json; ```. But pipelines will have to change to use them. I tested GRCm38, but not the other ones. @konradjk, would you be up for testing the standard usage? (We really need to add automated VEP tests.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3872
https://github.com/hail-is/hail/pull/3872:156,Deployability,update,updated,156,"- convert the VEP configuration file to JSON; - let the conf file fully specify the command line, environment and output schema. This is breaking change. I updated the VEP config files on the cloud:. ```; $ gsutil ls gs://hail-common/vep/vep/*.json; gs://hail-common/vep/vep/vep81-gcloud.json; gs://hail-common/vep/vep/vep85-gcloud.json; gs://hail-common/vep/vep/vep92-GRCm38-gcloud.json; ```. But pipelines will have to change to use them. I tested GRCm38, but not the other ones. @konradjk, would you be up for testing the standard usage? (We really need to add automated VEP tests.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3872
https://github.com/hail-is/hail/pull/3872:398,Deployability,pipeline,pipelines,398,"- convert the VEP configuration file to JSON; - let the conf file fully specify the command line, environment and output schema. This is breaking change. I updated the VEP config files on the cloud:. ```; $ gsutil ls gs://hail-common/vep/vep/*.json; gs://hail-common/vep/vep/vep81-gcloud.json; gs://hail-common/vep/vep/vep85-gcloud.json; gs://hail-common/vep/vep/vep92-GRCm38-gcloud.json; ```. But pipelines will have to change to use them. I tested GRCm38, but not the other ones. @konradjk, would you be up for testing the standard usage? (We really need to add automated VEP tests.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3872
https://github.com/hail-is/hail/pull/3872:18,Modifiability,config,configuration,18,"- convert the VEP configuration file to JSON; - let the conf file fully specify the command line, environment and output schema. This is breaking change. I updated the VEP config files on the cloud:. ```; $ gsutil ls gs://hail-common/vep/vep/*.json; gs://hail-common/vep/vep/vep81-gcloud.json; gs://hail-common/vep/vep/vep85-gcloud.json; gs://hail-common/vep/vep/vep92-GRCm38-gcloud.json; ```. But pipelines will have to change to use them. I tested GRCm38, but not the other ones. @konradjk, would you be up for testing the standard usage? (We really need to add automated VEP tests.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3872
https://github.com/hail-is/hail/pull/3872:172,Modifiability,config,config,172,"- convert the VEP configuration file to JSON; - let the conf file fully specify the command line, environment and output schema. This is breaking change. I updated the VEP config files on the cloud:. ```; $ gsutil ls gs://hail-common/vep/vep/*.json; gs://hail-common/vep/vep/vep81-gcloud.json; gs://hail-common/vep/vep/vep85-gcloud.json; gs://hail-common/vep/vep/vep92-GRCm38-gcloud.json; ```. But pipelines will have to change to use them. I tested GRCm38, but not the other ones. @konradjk, would you be up for testing the standard usage? (We really need to add automated VEP tests.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3872
https://github.com/hail-is/hail/pull/3872:443,Testability,test,tested,443,"- convert the VEP configuration file to JSON; - let the conf file fully specify the command line, environment and output schema. This is breaking change. I updated the VEP config files on the cloud:. ```; $ gsutil ls gs://hail-common/vep/vep/*.json; gs://hail-common/vep/vep/vep81-gcloud.json; gs://hail-common/vep/vep/vep85-gcloud.json; gs://hail-common/vep/vep/vep92-GRCm38-gcloud.json; ```. But pipelines will have to change to use them. I tested GRCm38, but not the other ones. @konradjk, would you be up for testing the standard usage? (We really need to add automated VEP tests.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3872
https://github.com/hail-is/hail/pull/3872:513,Testability,test,testing,513,"- convert the VEP configuration file to JSON; - let the conf file fully specify the command line, environment and output schema. This is breaking change. I updated the VEP config files on the cloud:. ```; $ gsutil ls gs://hail-common/vep/vep/*.json; gs://hail-common/vep/vep/vep81-gcloud.json; gs://hail-common/vep/vep/vep85-gcloud.json; gs://hail-common/vep/vep/vep92-GRCm38-gcloud.json; ```. But pipelines will have to change to use them. I tested GRCm38, but not the other ones. @konradjk, would you be up for testing the standard usage? (We really need to add automated VEP tests.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3872
https://github.com/hail-is/hail/pull/3872:578,Testability,test,tests,578,"- convert the VEP configuration file to JSON; - let the conf file fully specify the command line, environment and output schema. This is breaking change. I updated the VEP config files on the cloud:. ```; $ gsutil ls gs://hail-common/vep/vep/*.json; gs://hail-common/vep/vep/vep81-gcloud.json; gs://hail-common/vep/vep/vep85-gcloud.json; gs://hail-common/vep/vep/vep92-GRCm38-gcloud.json; ```. But pipelines will have to change to use them. I tested GRCm38, but not the other ones. @konradjk, would you be up for testing the standard usage? (We really need to add automated VEP tests.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3872
https://github.com/hail-is/hail/pull/3873:18,Deployability,update,update,18,"This is a rebased update to the closed PR #3715. Updates:. - `locus_table` is now `locus_expr`, an expression on a table or matrix table. This is more flexible, and avoids unnatural requirements on key (rather than order). - uses `global_position` and requires/checks ascending order, rather than reordering within the method. Re-ordering, if non-trivial, would silently invalidate the results with respect to the row-order of the source. This also avoids a potentially unnecessary shuffle, and addresses the issue that contig order may not be alphabetical in the reference. - keeps the size of data collected close to the minimal data necessary (in particular, no collection of contigs). When `coord_expr` is not set, its an array of int. When `coord_expr` is set, its an array of (int, float). A tuple of arrays would be better, but directly collecting two arrays in order would require two actions with the current infrastructure since agg.collect() does not preserve order.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3873
https://github.com/hail-is/hail/pull/3873:49,Deployability,Update,Updates,49,"This is a rebased update to the closed PR #3715. Updates:. - `locus_table` is now `locus_expr`, an expression on a table or matrix table. This is more flexible, and avoids unnatural requirements on key (rather than order). - uses `global_position` and requires/checks ascending order, rather than reordering within the method. Re-ordering, if non-trivial, would silently invalidate the results with respect to the row-order of the source. This also avoids a potentially unnecessary shuffle, and addresses the issue that contig order may not be alphabetical in the reference. - keeps the size of data collected close to the minimal data necessary (in particular, no collection of contigs). When `coord_expr` is not set, its an array of int. When `coord_expr` is set, its an array of (int, float). A tuple of arrays would be better, but directly collecting two arrays in order would require two actions with the current infrastructure since agg.collect() does not preserve order.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3873
https://github.com/hail-is/hail/pull/3873:151,Modifiability,flexible,flexible,151,"This is a rebased update to the closed PR #3715. Updates:. - `locus_table` is now `locus_expr`, an expression on a table or matrix table. This is more flexible, and avoids unnatural requirements on key (rather than order). - uses `global_position` and requires/checks ascending order, rather than reordering within the method. Re-ordering, if non-trivial, would silently invalidate the results with respect to the row-order of the source. This also avoids a potentially unnecessary shuffle, and addresses the issue that contig order may not be alphabetical in the reference. - keeps the size of data collected close to the minimal data necessary (in particular, no collection of contigs). When `coord_expr` is not set, its an array of int. When `coord_expr` is set, its an array of (int, float). A tuple of arrays would be better, but directly collecting two arrays in order would require two actions with the current infrastructure since agg.collect() does not preserve order.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3873
https://github.com/hail-is/hail/pull/3873:165,Safety,avoid,avoids,165,"This is a rebased update to the closed PR #3715. Updates:. - `locus_table` is now `locus_expr`, an expression on a table or matrix table. This is more flexible, and avoids unnatural requirements on key (rather than order). - uses `global_position` and requires/checks ascending order, rather than reordering within the method. Re-ordering, if non-trivial, would silently invalidate the results with respect to the row-order of the source. This also avoids a potentially unnecessary shuffle, and addresses the issue that contig order may not be alphabetical in the reference. - keeps the size of data collected close to the minimal data necessary (in particular, no collection of contigs). When `coord_expr` is not set, its an array of int. When `coord_expr` is set, its an array of (int, float). A tuple of arrays would be better, but directly collecting two arrays in order would require two actions with the current infrastructure since agg.collect() does not preserve order.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3873
https://github.com/hail-is/hail/pull/3873:449,Safety,avoid,avoids,449,"This is a rebased update to the closed PR #3715. Updates:. - `locus_table` is now `locus_expr`, an expression on a table or matrix table. This is more flexible, and avoids unnatural requirements on key (rather than order). - uses `global_position` and requires/checks ascending order, rather than reordering within the method. Re-ordering, if non-trivial, would silently invalidate the results with respect to the row-order of the source. This also avoids a potentially unnecessary shuffle, and addresses the issue that contig order may not be alphabetical in the reference. - keeps the size of data collected close to the minimal data necessary (in particular, no collection of contigs). When `coord_expr` is not set, its an array of int. When `coord_expr` is set, its an array of (int, float). A tuple of arrays would be better, but directly collecting two arrays in order would require two actions with the current infrastructure since agg.collect() does not preserve order.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3873
https://github.com/hail-is/hail/pull/3874:159,Integrability,interface,interface,159,"so the scan operations are just built in to MatrixMapRows, MatrixMapCols, TableMapRows, as an alternative to #3855. I had to modify the CompileWithAggregators interface slightly so that aggregations could be extracted in multiple steps. This lets us have aggregations within a scan expression pretty easily. I've implemented scans in MatrixMapRows for testing but plan to save the MatrixMapCols and TableMapRows implementation for a separate PR since this one is getting large.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3874
https://github.com/hail-is/hail/pull/3874:352,Testability,test,testing,352,"so the scan operations are just built in to MatrixMapRows, MatrixMapCols, TableMapRows, as an alternative to #3855. I had to modify the CompileWithAggregators interface slightly so that aggregations could be extracted in multiple steps. This lets us have aggregations within a scan expression pretty easily. I've implemented scans in MatrixMapRows for testing but plan to save the MatrixMapCols and TableMapRows implementation for a separate PR since this one is getting large.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3874
https://github.com/hail-is/hail/pull/3875:131,Integrability,interface,interface,131,"These are doc corrections/improvements I'd made in #3825 but then removed from #3833, except for the name changes that would break interface. I'd like to get feedback on those at check-in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3875
https://github.com/hail-is/hail/pull/3875:158,Usability,feedback,feedback,158,"These are doc corrections/improvements I'd made in #3825 but then removed from #3833, except for the name changes that would break interface. I'd like to get feedback on those at check-in.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3875
https://github.com/hail-is/hail/pull/3887:67,Testability,test,testing,67,"cc @danking @patrick-schultz @jigold . Was anyone else part of the testing discussion? Would like input on the new structure:. <img width=""396"" alt=""image"" src=""https://user-images.githubusercontent.com/10562794/42187031-af030a1e-7e1c-11e8-9ed4-cfa826743276.png"">",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3887
https://github.com/hail-is/hail/pull/3891:410,Availability,avail,available,410,"@tpoterba this builds on your recent PR https://github.com/hail-is/hail/pull/3882. partition counts are computed through Interpret on demand and memoized in the IR. fastPartitionCounts means get the partition counts if you have them (either because the IR know their partition counts, or they were previously computed by counting the RVD partitions). partitionCounts means compute (and memoize) if they aren't available the fast way. I think this is now optimal except that MatrixTable.count potentially runs things twice. We might be able to fix this with a MatrixLet in the case you're calling MatrixTable.count(). Next Table.index/MatrixTable.indexRows should use partitionCounts instead of zipWithIndex because computing the partition counts via the optimizer will potentially be much faster.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3891
https://github.com/hail-is/hail/pull/3891:754,Performance,optimiz,optimizer,754,"@tpoterba this builds on your recent PR https://github.com/hail-is/hail/pull/3882. partition counts are computed through Interpret on demand and memoized in the IR. fastPartitionCounts means get the partition counts if you have them (either because the IR know their partition counts, or they were previously computed by counting the RVD partitions). partitionCounts means compute (and memoize) if they aren't available the fast way. I think this is now optimal except that MatrixTable.count potentially runs things twice. We might be able to fix this with a MatrixLet in the case you're calling MatrixTable.count(). Next Table.index/MatrixTable.indexRows should use partitionCounts instead of zipWithIndex because computing the partition counts via the optimizer will potentially be much faster.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3891
https://github.com/hail-is/hail/pull/3893:15,Performance,perform,performance,15,"I introduced a performance bug when I made the BgenRecord eagerly decode the genotypes. The genotypes are not needed for fast keys. This change throws out Hadoop in favor of a Spark-based approach. I also did an instrumented profiling of `hl.import_bgen(...)._force_count_rows()` and found that almost all our time is spent in region value builder state manipulation. I included a small fix that makes `ArrayStack` actually use field references instead of method calls (this shaved about 1/10 off of force count rows time) (cc: @cseed). The real solution is to use a staged region value builder. I will follow this PR with a SRVB pull request. Finally, I will also hook it up to `MatrixRead` so that the dead fields pruner can prune bgen fields too (cc: @tpoterba)!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3893
https://github.com/hail-is/hail/pull/3896:24,Testability,test,tested,24,"They have `__str__` and tested by verifying, value IR, TableIR can be parsed by the Scala parser. (MatrixIR parser is still pending.). Made some changes to Table/MatrixRead. Instead of rendering them exactly (which we can't construct from Python), assume they will read on parser and only require the minimal information necessary to construct read IRs. MatrixRead doesn't contain MatrixReader in the parser, matrix readers are treated as full ""virtual"" MatrixIR nodes in the parser. When MatrixIR is done, we can replace expr.AST with IR and we're golden. @tpoterba commentary on the Python organization welcome.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3896
https://github.com/hail-is/hail/pull/3897:26,Availability,error,error,26,Hail should not signal an error if the PAR is empty. This change provides an explicit type to the PAR before handing it to filter_intervals. . @tpoterba assigning you because I want to make sure I'm doing this right.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3897
https://github.com/hail-is/hail/pull/3898:91,Testability,test,tests,91,- Keyed AggOp; - KeyedRegionValueAggregator; - KeyedCodeAggregator; - IR Parser support; - tests,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3898
https://github.com/hail-is/hail/issues/3901:339,Availability,error,error,339,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; Running on Apache Spark version 2.2.0 version devel-cfdbb87; ### What you did:; This error message is related to this filter:; passed=passed.filter_rows((passed.variant_qc.AC>= 10)); Without this filter it runs OK. This file is a merged vcf file from Lumpy. Some sites may have no alternate alleles called (all 0/0 or ./.). ### What went wrong (all error messages here, including the full java stack trace):; [Stage 2:> (0 + 72) / 125]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/hail/lumpy/models.all.py"", line 80, in <module>; print(""Filtered Passed Rows:"",passed.count_rows()); File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2072, in count_rows; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 4 times, most recent failure: Lost task 30.3 in stage 2.0 (TID 91, scc-q05.scc.bu.edu, executor 9): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailCon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:603,Availability,error,error,603,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; Running on Apache Spark version 2.2.0 version devel-cfdbb87; ### What you did:; This error message is related to this filter:; passed=passed.filter_rows((passed.variant_qc.AC>= 10)); Without this filter it runs OK. This file is a merged vcf file from Lumpy. Some sites may have no alternate alleles called (all 0/0 or ./.). ### What went wrong (all error messages here, including the full java stack trace):; [Stage 2:> (0 + 72) / 125]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/hail/lumpy/models.all.py"", line 80, in <module>; print(""Filtered Passed Rows:"",passed.count_rows()); File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2072, in count_rows; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 4 times, most recent failure: Lost task 30.3 in stage 2.0 (TID 91, scc-q05.scc.bu.edu, executor 9): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailCon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:1376,Availability,failure,failure,1376,"is filter:; passed=passed.filter_rows((passed.variant_qc.AC>= 10)); Without this filter it runs OK. This file is a merged vcf file from Lumpy. Some sites may have no alternate alleles called (all 0/0 or ./.). ### What went wrong (all error messages here, including the full java stack trace):; [Stage 2:> (0 + 72) / 125]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/hail/lumpy/models.all.py"", line 80, in <module>; print(""Filtered Passed Rows:"",passed.count_rows()); File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2072, in count_rows; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 4 times, most recent failure: Lost task 30.3 in stage 2.0 (TID 91, scc-q05.scc.bu.edu, executor 9): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:246); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:219); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:322); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:1434,Availability,failure,failure,1434," 10)); Without this filter it runs OK. This file is a merged vcf file from Lumpy. Some sites may have no alternate alleles called (all 0/0 or ./.). ### What went wrong (all error messages here, including the full java stack trace):; [Stage 2:> (0 + 72) / 125]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/hail/lumpy/models.all.py"", line 80, in <module>; print(""Filtered Passed Rows:"",passed.count_rows()); File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2072, in count_rows; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 4 times, most recent failure: Lost task 30.3 in stage 2.0 (TID 91, scc-q05.scc.bu.edu, executor 9): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:246); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:219); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:322); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:912); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:1028,Deployability,install,install,1028," below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; Running on Apache Spark version 2.2.0 version devel-cfdbb87; ### What you did:; This error message is related to this filter:; passed=passed.filter_rows((passed.variant_qc.AC>= 10)); Without this filter it runs OK. This file is a merged vcf file from Lumpy. Some sites may have no alternate alleles called (all 0/0 or ./.). ### What went wrong (all error messages here, including the full java stack trace):; [Stage 2:> (0 + 72) / 125]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/hail/lumpy/models.all.py"", line 80, in <module>; print(""Filtered Passed Rows:"",passed.count_rows()); File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2072, in count_rows; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 4 times, most recent failure: Lost task 30.3 in stage 2.0 (TID 91, scc-q05.scc.bu.edu, executor 9): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:246); 	at is.hail.HailContex",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:1686,Energy Efficiency,allocate,allocate,1686,"back (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/hail/lumpy/models.all.py"", line 80, in <module>; print(""Filtered Passed Rows:"",passed.count_rows()); File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2072, in count_rows; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 4 times, most recent failure: Lost task 30.3 in stage 2.0 (TID 91, scc-q05.scc.bu.edu, executor 9): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:246); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:219); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:322); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:912); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:906); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:1745,Energy Efficiency,allocate,allocate,1745,"casa/wgs.hg38/hail/lumpy/models.all.py"", line 80, in <module>; print(""Filtered Passed Rows:"",passed.count_rows()); File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2072, in count_rows; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 4 times, most recent failure: Lost task 30.3 in stage 2.0 (TID 91, scc-q05.scc.bu.edu, executor 9): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:246); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:219); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:322); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:912); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:906); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:912); 	at is.hail.rvd.Ord",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:6498,Energy Efficiency,schedul,scheduler,6498,ic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:6570,Energy Efficiency,schedul,scheduler,6570,.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:6935,Energy Efficiency,schedul,scheduler,6935,ollection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:6975,Energy Efficiency,schedul,scheduler,6975,at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:7074,Energy Efficiency,schedul,scheduler,7074,t scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:7172,Energy Efficiency,schedul,scheduler,7172,onfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:7426,Energy Efficiency,schedul,scheduler,7426,nfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:7507,Energy Efficiency,schedul,scheduler,7507,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:7613,Energy Efficiency,schedul,scheduler,7613,he.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:7763,Energy Efficiency,schedul,scheduler,7763,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:7852,Energy Efficiency,schedul,scheduler,7852,va.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:7950,Energy Efficiency,schedul,scheduler,7950,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:8046,Energy Efficiency,schedul,scheduler,8046,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.hail.rvd.RVD$class.countPerPartition(RVD.scala:359); 	at is.hail.rvd.OrderedRVD.countPerPartition(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:8211,Energy Efficiency,schedul,scheduler,8211,.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:132); 	at is.hail.rvd.RVD$class.countPerPartition(RVD.scala:359); 	at is.hail.rvd.OrderedRVD.countPerPartition(OrderedRVD.scala:21); 	at is.hail.variant.MatrixTable.partitionCounts(MatrixTable.scala:697); 	at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:1284); 	at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:10100,Energy Efficiency,allocate,allocate,10100,titionCounts(MatrixTable.scala:697); 	at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:1284); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.NegativeArraySizeException: null; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:246); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:219); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:322); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:912); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:906); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:10159,Energy Efficiency,allocate,allocate,10159,trixTable.countRows(MatrixTable.scala:1284); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.NegativeArraySizeException: null; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:246); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:219); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:322); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:912); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:906); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:912); 	at is.hail.rvd.Ord,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:14912,Energy Efficiency,schedul,scheduler,14912,xtras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:14984,Energy Efficiency,schedul,scheduler,14984,xtras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:345,Integrability,message,message,345,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; Running on Apache Spark version 2.2.0 version devel-cfdbb87; ### What you did:; This error message is related to this filter:; passed=passed.filter_rows((passed.variant_qc.AC>= 10)); Without this filter it runs OK. This file is a merged vcf file from Lumpy. Some sites may have no alternate alleles called (all 0/0 or ./.). ### What went wrong (all error messages here, including the full java stack trace):; [Stage 2:> (0 + 72) / 125]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/hail/lumpy/models.all.py"", line 80, in <module>; print(""Filtered Passed Rows:"",passed.count_rows()); File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2072, in count_rows; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 4 times, most recent failure: Lost task 30.3 in stage 2.0 (TID 91, scc-q05.scc.bu.edu, executor 9): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailCon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:609,Integrability,message,messages,609,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; Running on Apache Spark version 2.2.0 version devel-cfdbb87; ### What you did:; This error message is related to this filter:; passed=passed.filter_rows((passed.variant_qc.AC>= 10)); Without this filter it runs OK. This file is a merged vcf file from Lumpy. Some sites may have no alternate alleles called (all 0/0 or ./.). ### What went wrong (all error messages here, including the full java stack trace):; [Stage 2:> (0 + 72) / 125]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/hail/lumpy/models.all.py"", line 80, in <module>; print(""Filtered Passed Rows:"",passed.count_rows()); File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2072, in count_rows; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 4 times, most recent failure: Lost task 30.3 in stage 2.0 (TID 91, scc-q05.scc.bu.edu, executor 9): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailCon",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:6695,Performance,concurren,concurrent,6695,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:6780,Performance,concurren,concurrent,6780, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:15109,Performance,concurren,concurrent,15109,xtras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:15194,Performance,concurren,concurrent,15194,xtras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5$$anonfun$apply$6.apply(ContextRDD.scala:129); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:1355,Safety,abort,aborted,1355,"is filter:; passed=passed.filter_rows((passed.variant_qc.AC>= 10)); Without this filter it runs OK. This file is a merged vcf file from Lumpy. Some sites may have no alternate alleles called (all 0/0 or ./.). ### What went wrong (all error messages here, including the full java stack trace):; [Stage 2:> (0 + 72) / 125]Traceback (most recent call last):; File ""/restricted/projectnb/casa/wgs.hg38/hail/lumpy/models.all.py"", line 80, in <module>; print(""Filtered Passed Rows:"",passed.count_rows()); File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2072, in count_rows; File ""/share/pkg/spark/2.2.0/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: NegativeArraySizeException: null. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 4 times, most recent failure: Lost task 30.3 in stage 2.0 (TID 91, scc-q05.scc.bu.edu, executor 9): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.codegen.generated.C4.apply(Unknown Source); 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:649); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:246); 	at is.hail.HailContext$$anon$2.next(HailContext.scala:219); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.sparkextras.ContextRDD$$anonfun$czip$1$$anon$1.next(ContextRDD.scala:322); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:7106,Safety,abort,abortStage,7106,tractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:7204,Safety,abort,abortStage,7204,un$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/issues/3901:7449,Safety,abort,abortStage,7449,xt.scala:2062); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3901
https://github.com/hail-is/hail/pull/3913:49,Testability,test,test,49,assigning Cotton because we need to make sure we test this and the other VEP PR.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3913
https://github.com/hail-is/hail/pull/3916:124,Availability,error,errored,124,"This is the compiled BGEN decoder. there's a fix to the partitioning logic as well. If there's no partitions, it previously errored. I added a `math.max` in case the maxRecordPerPartition is huge and the records is small and their floating point ratio ends up as zero. You can more easily see these changes when ignoring whitespace: https://github.com/hail-is/hail/pull/3916/files?w=1. ---. I reverted back to the `Iterator[Option[RegionValue]]` because per-row allocation isn't that bad and it just seems kind of hard an unnecessary  . cc: @patrick-schultz",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3916
https://github.com/hail-is/hail/pull/3916:69,Testability,log,logic,69,"This is the compiled BGEN decoder. there's a fix to the partitioning logic as well. If there's no partitions, it previously errored. I added a `math.max` in case the maxRecordPerPartition is huge and the records is small and their floating point ratio ends up as zero. You can more easily see these changes when ignoring whitespace: https://github.com/hail-is/hail/pull/3916/files?w=1. ---. I reverted back to the `Iterator[Option[RegionValue]]` because per-row allocation isn't that bad and it just seems kind of hard an unnecessary  . cc: @patrick-schultz",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3916
https://github.com/hail-is/hail/issues/3920:767,Availability,error,error,767,"### Hail version:; 18d0195e6; ### What you did:; ```; import hail as hl; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; mt = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr22_v3.bgen',; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; min_partitions=100); sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'); og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])); og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2); og_sample._force_count_rows(); ```; The important part is that I used `annotate_rows` on a sufficiently large dataset.; ### What went wrong (all error messages here, including the full java stack trace):; Container failures; ```; Job aborted due to stage failure: Task 5 in stage 9.0 failed 20 times, most recent failure: Lost task 5.19 in stage 9.0 (TID 603, dk-w-0.c.broad-ctsa.internal, executor 63): ExecutorLostFailure (executor 63 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 15.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; ```; This is due to `collectPerPartition` allowing regions to grow without bound.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3920
https://github.com/hail-is/hail/issues/3920:837,Availability,failure,failures,837,"### Hail version:; 18d0195e6; ### What you did:; ```; import hail as hl; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; mt = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr22_v3.bgen',; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; min_partitions=100); sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'); og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])); og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2); og_sample._force_count_rows(); ```; The important part is that I used `annotate_rows` on a sufficiently large dataset.; ### What went wrong (all error messages here, including the full java stack trace):; Container failures; ```; Job aborted due to stage failure: Task 5 in stage 9.0 failed 20 times, most recent failure: Lost task 5.19 in stage 9.0 (TID 603, dk-w-0.c.broad-ctsa.internal, executor 63): ExecutorLostFailure (executor 63 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 15.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; ```; This is due to `collectPerPartition` allowing regions to grow without bound.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3920
https://github.com/hail-is/hail/issues/3920:877,Availability,failure,failure,877,"### Hail version:; 18d0195e6; ### What you did:; ```; import hail as hl; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; mt = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr22_v3.bgen',; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; min_partitions=100); sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'); og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])); og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2); og_sample._force_count_rows(); ```; The important part is that I used `annotate_rows` on a sufficiently large dataset.; ### What went wrong (all error messages here, including the full java stack trace):; Container failures; ```; Job aborted due to stage failure: Task 5 in stage 9.0 failed 20 times, most recent failure: Lost task 5.19 in stage 9.0 (TID 603, dk-w-0.c.broad-ctsa.internal, executor 63): ExecutorLostFailure (executor 63 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 15.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; ```; This is due to `collectPerPartition` allowing regions to grow without bound.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3920
https://github.com/hail-is/hail/issues/3920:935,Availability,failure,failure,935,"### Hail version:; 18d0195e6; ### What you did:; ```; import hail as hl; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; mt = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr22_v3.bgen',; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; min_partitions=100); sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'); og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])); og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2); og_sample._force_count_rows(); ```; The important part is that I used `annotate_rows` on a sufficiently large dataset.; ### What went wrong (all error messages here, including the full java stack trace):; Container failures; ```; Job aborted due to stage failure: Task 5 in stage 9.0 failed 20 times, most recent failure: Lost task 5.19 in stage 9.0 (TID 603, dk-w-0.c.broad-ctsa.internal, executor 63): ExecutorLostFailure (executor 63 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 15.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; ```; This is due to `collectPerPartition` allowing regions to grow without bound.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3920
https://github.com/hail-is/hail/issues/3920:773,Integrability,message,messages,773,"### Hail version:; 18d0195e6; ### What you did:; ```; import hail as hl; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; mt = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr22_v3.bgen',; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; min_partitions=100); sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'); og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])); og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2); og_sample._force_count_rows(); ```; The important part is that I used `annotate_rows` on a sufficiently large dataset.; ### What went wrong (all error messages here, including the full java stack trace):; Container failures; ```; Job aborted due to stage failure: Task 5 in stage 9.0 failed 20 times, most recent failure: Lost task 5.19 in stage 9.0 (TID 603, dk-w-0.c.broad-ctsa.internal, executor 63): ExecutorLostFailure (executor 63 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 15.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; ```; This is due to `collectPerPartition` allowing regions to grow without bound.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3920
https://github.com/hail-is/hail/issues/3920:856,Safety,abort,aborted,856,"### Hail version:; 18d0195e6; ### What you did:; ```; import hail as hl; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; mt = hl.methods.import_bgen('gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr22_v3.bgen',; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; min_partitions=100); sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'); og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])); og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2); og_sample._force_count_rows(); ```; The important part is that I used `annotate_rows` on a sufficiently large dataset.; ### What went wrong (all error messages here, including the full java stack trace):; Container failures; ```; Job aborted due to stage failure: Task 5 in stage 9.0 failed 20 times, most recent failure: Lost task 5.19 in stage 9.0 (TID 603, dk-w-0.c.broad-ctsa.internal, executor 63): ExecutorLostFailure (executor 63 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 15.9 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.; ```; This is due to `collectPerPartition` allowing regions to grow without bound.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3920
https://github.com/hail-is/hail/pull/3921:630,Availability,error,errors,630,"Fixes #3920. I'm a bit dubious on `collectPerPartitions` because it can only be safely used if there's a `ctx.region.clear` in the right spot. This should avoid some issues that caitlin was experiencing. The root issue is that `clearingRun` clears once per item, but, after the `cmapPartitions` there is only one item per partition. That item was produced by iterating through every element (with `it.foreach`) and accumulating some state. When `it.foreach` is finished, the entire partition will be in memory. On sufficiently large datasets, YARN will kill the executors for exceeding memory limits. We previously observed these errors coming from Java, but now that the regions are in native code, there are no JVM limits, the memory usage is instead noticed by YARN at the container-level. The fix is to clear after we are finished with each row, i.e. before the lambda passed to `foreach` returns.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3921
https://github.com/hail-is/hail/pull/3921:80,Safety,safe,safely,80,"Fixes #3920. I'm a bit dubious on `collectPerPartitions` because it can only be safely used if there's a `ctx.region.clear` in the right spot. This should avoid some issues that caitlin was experiencing. The root issue is that `clearingRun` clears once per item, but, after the `cmapPartitions` there is only one item per partition. That item was produced by iterating through every element (with `it.foreach`) and accumulating some state. When `it.foreach` is finished, the entire partition will be in memory. On sufficiently large datasets, YARN will kill the executors for exceeding memory limits. We previously observed these errors coming from Java, but now that the regions are in native code, there are no JVM limits, the memory usage is instead noticed by YARN at the container-level. The fix is to clear after we are finished with each row, i.e. before the lambda passed to `foreach` returns.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3921
https://github.com/hail-is/hail/pull/3921:155,Safety,avoid,avoid,155,"Fixes #3920. I'm a bit dubious on `collectPerPartitions` because it can only be safely used if there's a `ctx.region.clear` in the right spot. This should avoid some issues that caitlin was experiencing. The root issue is that `clearingRun` clears once per item, but, after the `cmapPartitions` there is only one item per partition. That item was produced by iterating through every element (with `it.foreach`) and accumulating some state. When `it.foreach` is finished, the entire partition will be in memory. On sufficiently large datasets, YARN will kill the executors for exceeding memory limits. We previously observed these errors coming from Java, but now that the regions are in native code, there are no JVM limits, the memory usage is instead noticed by YARN at the container-level. The fix is to clear after we are finished with each row, i.e. before the lambda passed to `foreach` returns.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3921
https://github.com/hail-is/hail/pull/3921:117,Usability,clear,clear,117,"Fixes #3920. I'm a bit dubious on `collectPerPartitions` because it can only be safely used if there's a `ctx.region.clear` in the right spot. This should avoid some issues that caitlin was experiencing. The root issue is that `clearingRun` clears once per item, but, after the `cmapPartitions` there is only one item per partition. That item was produced by iterating through every element (with `it.foreach`) and accumulating some state. When `it.foreach` is finished, the entire partition will be in memory. On sufficiently large datasets, YARN will kill the executors for exceeding memory limits. We previously observed these errors coming from Java, but now that the regions are in native code, there are no JVM limits, the memory usage is instead noticed by YARN at the container-level. The fix is to clear after we are finished with each row, i.e. before the lambda passed to `foreach` returns.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3921
https://github.com/hail-is/hail/pull/3921:228,Usability,clear,clearingRun,228,"Fixes #3920. I'm a bit dubious on `collectPerPartitions` because it can only be safely used if there's a `ctx.region.clear` in the right spot. This should avoid some issues that caitlin was experiencing. The root issue is that `clearingRun` clears once per item, but, after the `cmapPartitions` there is only one item per partition. That item was produced by iterating through every element (with `it.foreach`) and accumulating some state. When `it.foreach` is finished, the entire partition will be in memory. On sufficiently large datasets, YARN will kill the executors for exceeding memory limits. We previously observed these errors coming from Java, but now that the regions are in native code, there are no JVM limits, the memory usage is instead noticed by YARN at the container-level. The fix is to clear after we are finished with each row, i.e. before the lambda passed to `foreach` returns.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3921
https://github.com/hail-is/hail/pull/3921:241,Usability,clear,clears,241,"Fixes #3920. I'm a bit dubious on `collectPerPartitions` because it can only be safely used if there's a `ctx.region.clear` in the right spot. This should avoid some issues that caitlin was experiencing. The root issue is that `clearingRun` clears once per item, but, after the `cmapPartitions` there is only one item per partition. That item was produced by iterating through every element (with `it.foreach`) and accumulating some state. When `it.foreach` is finished, the entire partition will be in memory. On sufficiently large datasets, YARN will kill the executors for exceeding memory limits. We previously observed these errors coming from Java, but now that the regions are in native code, there are no JVM limits, the memory usage is instead noticed by YARN at the container-level. The fix is to clear after we are finished with each row, i.e. before the lambda passed to `foreach` returns.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3921
https://github.com/hail-is/hail/pull/3921:807,Usability,clear,clear,807,"Fixes #3920. I'm a bit dubious on `collectPerPartitions` because it can only be safely used if there's a `ctx.region.clear` in the right spot. This should avoid some issues that caitlin was experiencing. The root issue is that `clearingRun` clears once per item, but, after the `cmapPartitions` there is only one item per partition. That item was produced by iterating through every element (with `it.foreach`) and accumulating some state. When `it.foreach` is finished, the entire partition will be in memory. On sufficiently large datasets, YARN will kill the executors for exceeding memory limits. We previously observed these errors coming from Java, but now that the regions are in native code, there are no JVM limits, the memory usage is instead noticed by YARN at the container-level. The fix is to clear after we are finished with each row, i.e. before the lambda passed to `foreach` returns.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3921
https://github.com/hail-is/hail/issues/3922:393,Availability,error,error,393,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2. ### What you did: read in a Hail Table with 96 rows and 5960 Columns; `table1.row.show(); table2.row.show()`. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-80-bf4d6c719c23> in <module>(); 1 table1.row.show(); ----> 2 table2.row.show(). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/base_expression.py in show(self, n, width, truncate, types); 677 Print an extra header line with the type of each field.; 678 """"""; --> 679 print(self._show(n, width, truncate, types)); 680 ; 681 def _show(self, n=10, width=90, truncate=None, types=True):. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _show(self, n, width, truncate, types); 684 if isinstance(source, hl.Table):; 685 if self is source.row:; --> 686 return source._show(n, width, truncate, types); 687 elif self is source.key:; 688 return source.select()._show(n, width, truncate, types). /home/hail/hail.zip/hail/table.py in _show(self, n, width, truncate, types); 1201 ; 1202 def _show(self, n=10, width=90, truncate=None, types=True):; -> 1203 return self._jt.showString(n, joption(truncate), types, width); 1204 ; 1205 def index(self, *exprs):. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3922
https://github.com/hail-is/hail/issues/3922:2262,Availability,Error,Error,2262,"ypes=True):. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _show(self, n, width, truncate, types); 684 if isinstance(source, hl.Table):; 685 if self is source.row:; --> 686 return source._show(n, width, truncate, types); 687 elif self is source.key:; 688 return source.select()._show(n, width, truncate, types). /home/hail/hail.zip/hail/table.py in _show(self, n, width, truncate, types); 1201 ; 1202 def _show(self, n=10, width=90, truncate=None, types=True):; -> 1203 return self._jt.showString(n, joption(truncate), types, width); 1204 ; 1205 def index(self, *exprs):. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134 ; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 194 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 195 'Hail version: %s\n'; --> 196 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 197 except pyspark.sql.utils.CapturedException as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: RuntimeException: Method code too large!. Java stack trace:; java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:306); 	at is.hail.expr.ir.EmitFunctionBuilder.result(EmitFunctionBuilder.scala:284); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:50); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:31); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:72); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:413); 	at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:161); 	at is.hail.table.Table.value$lzycompute(Table.scala:237); 	at i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3922
https://github.com/hail-is/hail/issues/3922:4237,Availability,Error,Error,4237,xception as e:; 198 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: RuntimeException: Method code too large!. Java stack trace:; java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:306); 	at is.hail.expr.ir.EmitFunctionBuilder.result(EmitFunctionBuilder.scala:284); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:50); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:31); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:72); 	at is.hail.expr.ir.TableMapRows.execute(TableIR.scala:413); 	at is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:161); 	at is.hail.table.Table.value$lzycompute(Table.scala:237); 	at is.hail.table.Table.value(Table.scala:232); 	at is.hail.table.Table.x$5$lzycompute(Table.scala:240); 	at is.hail.table.Table.x$5(Table.scala:240); 	at is.hail.table.Table.rvd$lzycompute(Table.scala:240); 	at is.hail.table.Table.rvd(Table.scala:240); 	at is.hail.table.Table.take(Table.scala:875); 	at is.hail.table.Table.showString(Table.scala:911); 	at sun.reflect.GeneratedMethodAccessor108.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-eb1e04205793; Error summary: RuntimeException: Method code too large!; ```. [; [hail.zip](https://github.com/hail-is/hail/files/2189721/hail.zip). ](url),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3922
https://github.com/hail-is/hail/issues/3922:399,Integrability,message,messages,399,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2. ### What you did: read in a Hail Table with 96 rows and 5960 Columns; `table1.row.show(); table2.row.show()`. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-80-bf4d6c719c23> in <module>(); 1 table1.row.show(); ----> 2 table2.row.show(). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/base_expression.py in show(self, n, width, truncate, types); 677 Print an extra header line with the type of each field.; 678 """"""; --> 679 print(self._show(n, width, truncate, types)); 680 ; 681 def _show(self, n=10, width=90, truncate=None, types=True):. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _show(self, n, width, truncate, types); 684 if isinstance(source, hl.Table):; 685 if self is source.row:; --> 686 return source._show(n, width, truncate, types); 687 elif self is source.key:; 688 return source.select()._show(n, width, truncate, types). /home/hail/hail.zip/hail/table.py in _show(self, n, width, truncate, types); 1201 ; 1202 def _show(self, n=10, width=90, truncate=None, types=True):; -> 1203 return self._jt.showString(n, joption(truncate), types, width); 1204 ; 1205 def index(self, *exprs):. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3922
https://github.com/hail-is/hail/issues/3922:723,Integrability,wrap,wrapper,723,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2. ### What you did: read in a Hail Table with 96 rows and 5960 Columns; `table1.row.show(); table2.row.show()`. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-80-bf4d6c719c23> in <module>(); 1 table1.row.show(); ----> 2 table2.row.show(). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/base_expression.py in show(self, n, width, truncate, types); 677 Print an extra header line with the type of each field.; 678 """"""; --> 679 print(self._show(n, width, truncate, types)); 680 ; 681 def _show(self, n=10, width=90, truncate=None, types=True):. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _show(self, n, width, truncate, types); 684 if isinstance(source, hl.Table):; 685 if self is source.row:; --> 686 return source._show(n, width, truncate, types); 687 elif self is source.key:; 688 return source.select()._show(n, width, truncate, types). /home/hail/hail.zip/hail/table.py in _show(self, n, width, truncate, types); 1201 ; 1202 def _show(self, n=10, width=90, truncate=None, types=True):; -> 1203 return self._jt.showString(n, joption(truncate), types, width); 1204 ; 1205 def index(self, *exprs):. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3922
https://github.com/hail-is/hail/issues/3922:757,Integrability,wrap,wrapper,757,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2. ### What you did: read in a Hail Table with 96 rows and 5960 Columns; `table1.row.show(); table2.row.show()`. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-80-bf4d6c719c23> in <module>(); 1 table1.row.show(); ----> 2 table2.row.show(). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/base_expression.py in show(self, n, width, truncate, types); 677 Print an extra header line with the type of each field.; 678 """"""; --> 679 print(self._show(n, width, truncate, types)); 680 ; 681 def _show(self, n=10, width=90, truncate=None, types=True):. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _show(self, n, width, truncate, types); 684 if isinstance(source, hl.Table):; 685 if self is source.row:; --> 686 return source._show(n, width, truncate, types); 687 elif self is source.key:; 688 return source.select()._show(n, width, truncate, types). /home/hail/hail.zip/hail/table.py in _show(self, n, width, truncate, types); 1201 ; 1202 def _show(self, n=10, width=90, truncate=None, types=True):; -> 1203 return self._jt.showString(n, joption(truncate), types, width); 1204 ; 1205 def index(self, *exprs):. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3922
https://github.com/hail-is/hail/issues/3922:926,Integrability,wrap,wrapper,926,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2. ### What you did: read in a Hail Table with 96 rows and 5960 Columns; `table1.row.show(); table2.row.show()`. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-80-bf4d6c719c23> in <module>(); 1 table1.row.show(); ----> 2 table2.row.show(). /home/hail/hail.zip/hail/typecheck/check.py in wrapper(*args, **kwargs); 545 def wrapper(*args, **kwargs):; 546 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=is_method); --> 547 return f(*args_, **kwargs_); 548 ; 549 update_wrapper(wrapper, f). /home/hail/hail.zip/hail/expr/expressions/base_expression.py in show(self, n, width, truncate, types); 677 Print an extra header line with the type of each field.; 678 """"""; --> 679 print(self._show(n, width, truncate, types)); 680 ; 681 def _show(self, n=10, width=90, truncate=None, types=True):. /home/hail/hail.zip/hail/expr/expressions/base_expression.py in _show(self, n, width, truncate, types); 684 if isinstance(source, hl.Table):; 685 if self is source.row:; --> 686 return source._show(n, width, truncate, types); 687 elif self is source.key:; 688 return source.select()._show(n, width, truncate, types). /home/hail/hail.zip/hail/table.py in _show(self, n, width, truncate, types); 1201 ; 1202 def _show(self, n=10, width=90, truncate=None, types=True):; -> 1203 return self._jt.showString(n, joption(truncate), types, width); 1204 ; 1205 def index(self, *exprs):. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3922
https://github.com/hail-is/hail/issues/3923:279,Availability,error,errors,279,"table.py L433; ```; row = hl.bind(lambda v: self.key.annotate(**v), value_struct) if self.key else value_struct; ```. table.py L1612; ```; row_key = '[' + ', '.join(""'{name}'"".format(name=f) for f in self.key) + ']' \; if self.key else None; ```. I'm surprised these don't raise errors about the implicit __nonzero__ call, so I assume that Python is using the `Mapping` StructExpression superclass for __nonzero__ after hitting a `NotImplementedError`. Anyway, these should be fixed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3923
https://github.com/hail-is/hail/issues/3925:25,Testability,test,tests,25,"The new LinearMixedModel tests require specific pandas version, for instance.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3925
https://github.com/hail-is/hail/pull/3931:10,Integrability,depend,dependencies,10,"- removed dependencies of lmm tests on linear_mixed_regression; - added test against fast_lmm; - added h_sq_standard_error feature from linear_mixed_regression. Toward goal of deleting linear_mixed_regression and its Scala / suites ASAP, as well as the KinshipMatrix class",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3931
https://github.com/hail-is/hail/pull/3931:30,Testability,test,tests,30,"- removed dependencies of lmm tests on linear_mixed_regression; - added test against fast_lmm; - added h_sq_standard_error feature from linear_mixed_regression. Toward goal of deleting linear_mixed_regression and its Scala / suites ASAP, as well as the KinshipMatrix class",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3931
https://github.com/hail-is/hail/pull/3931:72,Testability,test,test,72,"- removed dependencies of lmm tests on linear_mixed_regression; - added test against fast_lmm; - added h_sq_standard_error feature from linear_mixed_regression. Toward goal of deleting linear_mixed_regression and its Scala / suites ASAP, as well as the KinshipMatrix class",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3931
https://github.com/hail-is/hail/pull/3936:84,Security,expose,exposed,84,"`h_sq_normalized_lkhd` is the final feature of LinearMixedRegression that should be exposed directly on LinearMixedModel. I also fixed a bug in the relationship of `h2 = s2 / (s2 + t2)` and `gamma = s2/t2` in `_estimate_h_sq_standard_error`, which didn't noticeably change the answer because of the symmetry of the left and right points about the max (curvature of fit parabola doesn't change when reflected over a vertical axis).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3936
https://github.com/hail-is/hail/issues/3937:371,Availability,error,error,371,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: f7631a0c96cd. ### What you did:. ```; hl.empty_dict(hl.tint32, hl.tint32).get(0).value; ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Hail version: devel-f7631a0c96cd; Error summary: HailException: array index out of bounds: 0 / 0. IR: (ArrayRef; (ToArray; (ApplyIR dict; (ArrayFilter __uid_191; (MakeArray Array[Tuple[I ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3937
https://github.com/hail-is/hail/issues/3937:470,Availability,Error,Error,470,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: f7631a0c96cd. ### What you did:. ```; hl.empty_dict(hl.tint32, hl.tint32).get(0).value; ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Hail version: devel-f7631a0c96cd; Error summary: HailException: array index out of bounds: 0 / 0. IR: (ArrayRef; (ToArray; (ApplyIR dict; (ArrayFilter __uid_191; (MakeArray Array[Tuple[I ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3937
https://github.com/hail-is/hail/issues/3937:377,Integrability,message,messages,377,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: f7631a0c96cd. ### What you did:. ```; hl.empty_dict(hl.tint32, hl.tint32).get(0).value; ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Hail version: devel-f7631a0c96cd; Error summary: HailException: array index out of bounds: 0 / 0. IR: (ArrayRef; (ToArray; (ApplyIR dict; (ArrayFilter __uid_191; (MakeArray Array[Tuple[I ...; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3937
https://github.com/hail-is/hail/pull/3939:43,Testability,test,tests,43,"currently shouldn't do much, except in IBD tests and PCRelate, which now construct IR strings to parse instead of expr. Mostly just separating this out from the python-side changes (in a forthcoming PR), and making sure I didn't break all the scala tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3939
https://github.com/hail-is/hail/pull/3939:249,Testability,test,tests,249,"currently shouldn't do much, except in IBD tests and PCRelate, which now construct IR strings to parse instead of expr. Mostly just separating this out from the python-side changes (in a forthcoming PR), and making sure I didn't break all the scala tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3939
https://github.com/hail-is/hail/pull/3940:142,Integrability,interface,interfaced,142,"cc @cseed @jigold @tpoterba. This replaces the python AST with directly-generated IR nodes. I tried to make minimal changes to how the AST/IR interfaced with expressions and tables/matrixtables, since this PR is already super big. The main changes I had to make were to the aggregator interface---`_agg_func` and the current aggregable map/filter/flatmap stuff now mimics the scala `toIR` logic. The other functions that get lifted out specially in `toIR`--mostly lambda functions--also got rewritten directly in python. (I also implemented `agg.map`, `agg.flatmap`, and `functions.fold` to do this, but left them undocumented since I didn't want to put a bunch of documentation in this PR. I'll follow up with documentation separately.). I also added some tests to test_expr that were getting caught in the doctests but didn't have other corresponding python tests. On the Scala side, I rewrote the IR parser to allow `Ref`s to be typeless if the type could be looked up in a corresponding map from the Table or MatrixTable, and to allow ApplyComparisonOps to not pass the type. The other changes are mostly changing the Table/MatrixTable methods to parse IR strings directly. The old AST methods have been left and renamed if the tests still needed them; was planning on moving to TestUtils in a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3940
https://github.com/hail-is/hail/pull/3940:285,Integrability,interface,interface,285,"cc @cseed @jigold @tpoterba. This replaces the python AST with directly-generated IR nodes. I tried to make minimal changes to how the AST/IR interfaced with expressions and tables/matrixtables, since this PR is already super big. The main changes I had to make were to the aggregator interface---`_agg_func` and the current aggregable map/filter/flatmap stuff now mimics the scala `toIR` logic. The other functions that get lifted out specially in `toIR`--mostly lambda functions--also got rewritten directly in python. (I also implemented `agg.map`, `agg.flatmap`, and `functions.fold` to do this, but left them undocumented since I didn't want to put a bunch of documentation in this PR. I'll follow up with documentation separately.). I also added some tests to test_expr that were getting caught in the doctests but didn't have other corresponding python tests. On the Scala side, I rewrote the IR parser to allow `Ref`s to be typeless if the type could be looked up in a corresponding map from the Table or MatrixTable, and to allow ApplyComparisonOps to not pass the type. The other changes are mostly changing the Table/MatrixTable methods to parse IR strings directly. The old AST methods have been left and renamed if the tests still needed them; was planning on moving to TestUtils in a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3940
https://github.com/hail-is/hail/pull/3940:389,Testability,log,logic,389,"cc @cseed @jigold @tpoterba. This replaces the python AST with directly-generated IR nodes. I tried to make minimal changes to how the AST/IR interfaced with expressions and tables/matrixtables, since this PR is already super big. The main changes I had to make were to the aggregator interface---`_agg_func` and the current aggregable map/filter/flatmap stuff now mimics the scala `toIR` logic. The other functions that get lifted out specially in `toIR`--mostly lambda functions--also got rewritten directly in python. (I also implemented `agg.map`, `agg.flatmap`, and `functions.fold` to do this, but left them undocumented since I didn't want to put a bunch of documentation in this PR. I'll follow up with documentation separately.). I also added some tests to test_expr that were getting caught in the doctests but didn't have other corresponding python tests. On the Scala side, I rewrote the IR parser to allow `Ref`s to be typeless if the type could be looked up in a corresponding map from the Table or MatrixTable, and to allow ApplyComparisonOps to not pass the type. The other changes are mostly changing the Table/MatrixTable methods to parse IR strings directly. The old AST methods have been left and renamed if the tests still needed them; was planning on moving to TestUtils in a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3940
https://github.com/hail-is/hail/pull/3940:757,Testability,test,tests,757,"cc @cseed @jigold @tpoterba. This replaces the python AST with directly-generated IR nodes. I tried to make minimal changes to how the AST/IR interfaced with expressions and tables/matrixtables, since this PR is already super big. The main changes I had to make were to the aggregator interface---`_agg_func` and the current aggregable map/filter/flatmap stuff now mimics the scala `toIR` logic. The other functions that get lifted out specially in `toIR`--mostly lambda functions--also got rewritten directly in python. (I also implemented `agg.map`, `agg.flatmap`, and `functions.fold` to do this, but left them undocumented since I didn't want to put a bunch of documentation in this PR. I'll follow up with documentation separately.). I also added some tests to test_expr that were getting caught in the doctests but didn't have other corresponding python tests. On the Scala side, I rewrote the IR parser to allow `Ref`s to be typeless if the type could be looked up in a corresponding map from the Table or MatrixTable, and to allow ApplyComparisonOps to not pass the type. The other changes are mostly changing the Table/MatrixTable methods to parse IR strings directly. The old AST methods have been left and renamed if the tests still needed them; was planning on moving to TestUtils in a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3940
https://github.com/hail-is/hail/pull/3940:860,Testability,test,tests,860,"cc @cseed @jigold @tpoterba. This replaces the python AST with directly-generated IR nodes. I tried to make minimal changes to how the AST/IR interfaced with expressions and tables/matrixtables, since this PR is already super big. The main changes I had to make were to the aggregator interface---`_agg_func` and the current aggregable map/filter/flatmap stuff now mimics the scala `toIR` logic. The other functions that get lifted out specially in `toIR`--mostly lambda functions--also got rewritten directly in python. (I also implemented `agg.map`, `agg.flatmap`, and `functions.fold` to do this, but left them undocumented since I didn't want to put a bunch of documentation in this PR. I'll follow up with documentation separately.). I also added some tests to test_expr that were getting caught in the doctests but didn't have other corresponding python tests. On the Scala side, I rewrote the IR parser to allow `Ref`s to be typeless if the type could be looked up in a corresponding map from the Table or MatrixTable, and to allow ApplyComparisonOps to not pass the type. The other changes are mostly changing the Table/MatrixTable methods to parse IR strings directly. The old AST methods have been left and renamed if the tests still needed them; was planning on moving to TestUtils in a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3940
https://github.com/hail-is/hail/pull/3940:1232,Testability,test,tests,1232,"cc @cseed @jigold @tpoterba. This replaces the python AST with directly-generated IR nodes. I tried to make minimal changes to how the AST/IR interfaced with expressions and tables/matrixtables, since this PR is already super big. The main changes I had to make were to the aggregator interface---`_agg_func` and the current aggregable map/filter/flatmap stuff now mimics the scala `toIR` logic. The other functions that get lifted out specially in `toIR`--mostly lambda functions--also got rewritten directly in python. (I also implemented `agg.map`, `agg.flatmap`, and `functions.fold` to do this, but left them undocumented since I didn't want to put a bunch of documentation in this PR. I'll follow up with documentation separately.). I also added some tests to test_expr that were getting caught in the doctests but didn't have other corresponding python tests. On the Scala side, I rewrote the IR parser to allow `Ref`s to be typeless if the type could be looked up in a corresponding map from the Table or MatrixTable, and to allow ApplyComparisonOps to not pass the type. The other changes are mostly changing the Table/MatrixTable methods to parse IR strings directly. The old AST methods have been left and renamed if the tests still needed them; was planning on moving to TestUtils in a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3940
https://github.com/hail-is/hail/pull/3940:1283,Testability,Test,TestUtils,1283,"cc @cseed @jigold @tpoterba. This replaces the python AST with directly-generated IR nodes. I tried to make minimal changes to how the AST/IR interfaced with expressions and tables/matrixtables, since this PR is already super big. The main changes I had to make were to the aggregator interface---`_agg_func` and the current aggregable map/filter/flatmap stuff now mimics the scala `toIR` logic. The other functions that get lifted out specially in `toIR`--mostly lambda functions--also got rewritten directly in python. (I also implemented `agg.map`, `agg.flatmap`, and `functions.fold` to do this, but left them undocumented since I didn't want to put a bunch of documentation in this PR. I'll follow up with documentation separately.). I also added some tests to test_expr that were getting caught in the doctests but didn't have other corresponding python tests. On the Scala side, I rewrote the IR parser to allow `Ref`s to be typeless if the type could be looked up in a corresponding map from the Table or MatrixTable, and to allow ApplyComparisonOps to not pass the type. The other changes are mostly changing the Table/MatrixTable methods to parse IR strings directly. The old AST methods have been left and renamed if the tests still needed them; was planning on moving to TestUtils in a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3940
https://github.com/hail-is/hail/pull/3940:614,Usability,undo,undocumented,614,"cc @cseed @jigold @tpoterba. This replaces the python AST with directly-generated IR nodes. I tried to make minimal changes to how the AST/IR interfaced with expressions and tables/matrixtables, since this PR is already super big. The main changes I had to make were to the aggregator interface---`_agg_func` and the current aggregable map/filter/flatmap stuff now mimics the scala `toIR` logic. The other functions that get lifted out specially in `toIR`--mostly lambda functions--also got rewritten directly in python. (I also implemented `agg.map`, `agg.flatmap`, and `functions.fold` to do this, but left them undocumented since I didn't want to put a bunch of documentation in this PR. I'll follow up with documentation separately.). I also added some tests to test_expr that were getting caught in the doctests but didn't have other corresponding python tests. On the Scala side, I rewrote the IR parser to allow `Ref`s to be typeless if the type could be looked up in a corresponding map from the Table or MatrixTable, and to allow ApplyComparisonOps to not pass the type. The other changes are mostly changing the Table/MatrixTable methods to parse IR strings directly. The old AST methods have been left and renamed if the tests still needed them; was planning on moving to TestUtils in a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3940
https://github.com/hail-is/hail/pull/3941:164,Testability,test,tests,164,"- deleted LinearMixedRegression, KinshipMatrix, ExportableMatrix; - deleted Suites; - converted rrm and grm functions to return BlockMatrix; - improved rrm and grm tests",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3941
https://github.com/hail-is/hail/issues/3942:467,Availability,error,error,467,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:. eb1e04205793. ### What you did:. https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Class.20file.20too.20large!. Ran filter_intervals with 200k intervals, got Java ""Class file too large!"" error.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3942
https://github.com/hail-is/hail/issues/3943:118,Testability,log,logic,118,### Hail version:. `b6d163983`. ### What you did:. `import_bgen` usually picks a bad number of partitions because the logic in `BgenRDDPartitions` is too naive. We should probably take a `block_size` parameter or put one on the hail context so that users can directly state a desired block size.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3943
https://github.com/hail-is/hail/pull/3944:382,Availability,error,error,382,"For rrm and grm, I've pushed overall rescaling to the faster block matrix side and thereby removed the action that counts the number of variants after filtering. The behavior is identical except that we no longer warn when constant variants are dropped, but I've clarified the behavior in the docs. In the unfathomable case that all variants are constant, the user will receive the error message `HailException: block matrix must have at least one row` when the BlockMatrix is being created. For rrm, I've also simplified the filtering, as well as the expression algebra for std_dev from; ```; hl.sqrt((mt.__ACsq + (n_samples - mt.__n_called) * mt.__mean_gt ** 2) / n_samples - mt.__mean_gt ** 2); ```; to; ```; hl.sqrt(mt.__ACsq - (mt.__AC ** 2) / mt.__n_called); ```; with the added benefit of combining two annotates.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3944
https://github.com/hail-is/hail/pull/3944:388,Integrability,message,message,388,"For rrm and grm, I've pushed overall rescaling to the faster block matrix side and thereby removed the action that counts the number of variants after filtering. The behavior is identical except that we no longer warn when constant variants are dropped, but I've clarified the behavior in the docs. In the unfathomable case that all variants are constant, the user will receive the error message `HailException: block matrix must have at least one row` when the BlockMatrix is being created. For rrm, I've also simplified the filtering, as well as the expression algebra for std_dev from; ```; hl.sqrt((mt.__ACsq + (n_samples - mt.__n_called) * mt.__mean_gt ** 2) / n_samples - mt.__mean_gt ** 2); ```; to; ```; hl.sqrt(mt.__ACsq - (mt.__AC ** 2) / mt.__n_called); ```; with the added benefit of combining two annotates.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3944
https://github.com/hail-is/hail/pull/3944:511,Usability,simpl,simplified,511,"For rrm and grm, I've pushed overall rescaling to the faster block matrix side and thereby removed the action that counts the number of variants after filtering. The behavior is identical except that we no longer warn when constant variants are dropped, but I've clarified the behavior in the docs. In the unfathomable case that all variants are constant, the user will receive the error message `HailException: block matrix must have at least one row` when the BlockMatrix is being created. For rrm, I've also simplified the filtering, as well as the expression algebra for std_dev from; ```; hl.sqrt((mt.__ACsq + (n_samples - mt.__n_called) * mt.__mean_gt ** 2) / n_samples - mt.__mean_gt ** 2); ```; to; ```; hl.sqrt(mt.__ACsq - (mt.__AC ** 2) / mt.__n_called); ```; with the added benefit of combining two annotates.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3944
https://github.com/hail-is/hail/issues/3946:112,Availability,error,error,112,"### Hail version: ; 0.1-74bf1eb. ### What you did: ; Export vcf to local file:// path. ### What went wrong (all error messages here, including the full java stack trace): ; When exporting vcf to path that begins with 'file://', I get the error: `ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found`. I am using Spark 2.2.1 (prebuilt with hadoop2.7) with AWS-Hadoop 2.7.4. I have the following settings in spark config and am using a custom directParquetOutputCommitter. Standard writes to 'file://' of Spark dataframes work without issue. Thanks for any help!. ```; spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:238,Availability,error,error,238,"### Hail version: ; 0.1-74bf1eb. ### What you did: ; Export vcf to local file:// path. ### What went wrong (all error messages here, including the full java stack trace): ; When exporting vcf to path that begins with 'file://', I get the error: `ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found`. I am using Spark 2.2.1 (prebuilt with hadoop2.7) with AWS-Hadoop 2.7.4. I have the following settings in spark config and am using a custom directParquetOutputCommitter. Standard writes to 'file://' of Spark dataframes work without issue. Thanks for any help!. ```; spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:1104,Availability,FAILURE,FAILURES,1104,"e full java stack trace): ; When exporting vcf to path that begins with 'file://', I get the error: `ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found`. I am using Spark 2.2.1 (prebuilt with hadoop2.7) with AWS-Hadoop 2.7.4. I have the following settings in spark config and am using a custom directParquetOutputCommitter. Standard writes to 'file://' of Spark dataframes work without issue. Thanks for any help!. ```; spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:2633,Availability,error,error,2633,"testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; <decorator-gen-398>:2: in export_vcf; ???; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. func = <function export_vcf at 0x7fa13c4d9938>, args = (<hail.dataset.VariantDataset object at 0x7fa13c3c9390>, 'file:///scratch/test_vcf_export.vcf.bgz', None, False, False), kwargs = {}; e = Py4JJavaError(u'An error occurred while calling o160.exportVCF.\n', JavaObject id=o162), tpl = JavaObject id=o210; deepest = 'ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found'; full = 'java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.map...mmand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). '. @decorator; def handle_py4j(func, *args, **kwargs):; try:; r = func(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); E FatalError: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E; E Java stack trace:; E java.lang.RuntimeException: java.lang.RuntimeEx",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:3374,Availability,Error,Error,3374,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. func = <function export_vcf at 0x7fa13c4d9938>, args = (<hail.dataset.VariantDataset object at 0x7fa13c3c9390>, 'file:///scratch/test_vcf_export.vcf.bgz', None, False, False), kwargs = {}; e = Py4JJavaError(u'An error occurred while calling o160.exportVCF.\n', JavaObject id=o162), tpl = JavaObject id=o210; deepest = 'ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found'; full = 'java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.map...mmand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). '. @decorator; def handle_py4j(func, *args, **kwargs):; try:; r = func(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); E FatalError: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E; E Java stack trace:; E java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2227); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:15416,Availability,Error,Error,15416,RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:971); E at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply$mcV$sp(RDD.scala:1507); E at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1495); E at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$2.apply(RDD.scala:1495); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1495); E at is.hail.utils.richUtils.RichRDD$.writeTable$extension1(RichRDD.scala:77); E at is.hail.utils.richUtils.RichRDD$.writeTable$extension0(RichRDD.scala:38); E at is.hail.io.vcf.ExportVCF$.apply(ExportVCF.scala:453); E at is.hail.variant.VariantDatasetFunctions$.exportVCF$extension(VariantDataset.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748); E; E; E; E Hail version: 0.1-74bf1eb; E Error summary: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:3758,Deployability,Configurat,Configuration,3758,"lass org.apache.hadoop.mapred.DirectFileOutputCommitter not found'; full = 'java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.map...mmand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). '. @decorator; def handle_py4j(func, *args, **kwargs):; try:; r = func(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); E FatalError: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E; E Java stack trace:; E java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2227); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:3781,Deployability,Configurat,Configuration,3781,"doop.mapred.DirectFileOutputCommitter not found'; full = 'java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.map...mmand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). '. @decorator; def handle_py4j(func, *args, **kwargs):; try:; r = func(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); E FatalError: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E; E Java stack trace:; E java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2227); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFun",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:7613,Deployability,Configurat,Configuration,7613,nsion(VariantDataset.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:7636,Deployability,Configurat,Configuration,7636,et.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:7690,Deployability,Configurat,Configuration,7690,.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:7713,Deployability,Configurat,Configuration,7713,Dataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFun,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:11517,Deployability,Configurat,Configuration,11517,setFunctions$.exportVCF$extension(VariantDataset.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$m,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:11546,Deployability,Configurat,Configuration,11546,CF$extension(VariantDataset.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:11600,Deployability,Configurat,Configuration,11600,ariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:11623,Deployability,Configurat,Configuration,11623,ions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:11677,Deployability,Configurat,Configuration,11677,NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:11700,Deployability,Configurat,Configuration,11700,orImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFun,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:118,Integrability,message,messages,118,"### Hail version: ; 0.1-74bf1eb. ### What you did: ; Export vcf to local file:// path. ### What went wrong (all error messages here, including the full java stack trace): ; When exporting vcf to path that begins with 'file://', I get the error: `ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found`. I am using Spark 2.2.1 (prebuilt with hadoop2.7) with AWS-Hadoop 2.7.4. I have the following settings in spark config and am using a custom directParquetOutputCommitter. Standard writes to 'file://' of Spark dataframes work without issue. Thanks for any help!. ```; spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:3179,Integrability,protocol,protocol,3179,"-398>:2: in export_vcf; ???; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. func = <function export_vcf at 0x7fa13c4d9938>, args = (<hail.dataset.VariantDataset object at 0x7fa13c3c9390>, 'file:///scratch/test_vcf_export.vcf.bgz', None, False, False), kwargs = {}; e = Py4JJavaError(u'An error occurred while calling o160.exportVCF.\n', JavaObject id=o162), tpl = JavaObject id=o210; deepest = 'ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found'; full = 'java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.map...mmand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). '. @decorator; def handle_py4j(func, *args, **kwargs):; try:; r = func(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); E FatalError: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E; E Java stack trace:; E java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2227); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(Pai",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:450,Modifiability,config,config,450,"### Hail version: ; 0.1-74bf1eb. ### What you did: ; Export vcf to local file:// path. ### What went wrong (all error messages here, including the full java stack trace): ; When exporting vcf to path that begins with 'file://', I get the error: `ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found`. I am using Spark 2.2.1 (prebuilt with hadoop2.7) with AWS-Hadoop 2.7.4. I have the following settings in spark config and am using a custom directParquetOutputCommitter. Standard writes to 'file://' of Spark dataframes work without issue. Thanks for any help!. ```; spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:3758,Modifiability,Config,Configuration,3758,"lass org.apache.hadoop.mapred.DirectFileOutputCommitter not found'; full = 'java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.map...mmand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). '. @decorator; def handle_py4j(func, *args, **kwargs):; try:; r = func(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); E FatalError: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E; E Java stack trace:; E java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2227); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:3781,Modifiability,Config,Configuration,3781,"doop.mapred.DirectFileOutputCommitter not found'; full = 'java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.map...mmand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748). '. @decorator; def handle_py4j(func, *args, **kwargs):; try:; r = func(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); E FatalError: ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E; E Java stack trace:; E java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2227); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFun",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:7613,Modifiability,Config,Configuration,7613,nsion(VariantDataset.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:7636,Modifiability,Config,Configuration,7636,et.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:7690,Modifiability,Config,Configuration,7690,.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:7713,Modifiability,Config,Configuration,7713,Dataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFun,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:11517,Modifiability,Config,Configuration,11517,setFunctions$.exportVCF$extension(VariantDataset.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$m,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:11546,Modifiability,Config,Configuration,11546,CF$extension(VariantDataset.scala:425); E at is.hail.variant.VariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:11600,Modifiability,Config,Configuration,11600,ariantDatasetFunctions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$ano,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:11623,Modifiability,Config,Configuration,11623,ions.exportVCF(VariantDataset.scala:425); E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFi,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:11677,Modifiability,Config,Configuration,11677,NativeMethodAccessorImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:11700,Modifiability,Config,Configuration,11700,orImpl.invoke0(Native Method); E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E at java.lang.reflect.Method.invoke(Method.java:498); E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E at py4j.Gateway.invoke(Gateway.java:280); E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E at py4j.commands.CallCommand.execute(CallCommand.java:79); E at py4j.GatewayConnection.run(GatewayConnection.java:214); E at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found; E at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193); E at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2219); E at org.apache.hadoop.mapred.JobConf.getOutputCommitter(JobConf.java:726); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1051); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); E at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); E at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply$mcV$sp(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$3.apply(PairRDDFunctions.scala:1016); E at org.apache.spark.rdd.PairRDDFun,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:1693,Performance,load,load,1693,"ommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; <decorator-gen-398>:2: in export_vcf; ???; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. func = <function export_vcf at 0x7fa13c4d9938>, args = (<hail.dataset.VariantDataset object at 0x7fa13c3c9390>, 'file:///scratch/test_vcf_export.vcf.bgz', None, False, False), kwargs = {}; e = Py4JJavaError(u'An error occurred while calling o160.exportVCF.\n', JavaObject id",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:1305,Testability,Test,TestHAIL,1305,"e full java stack trace): ; When exporting vcf to path that begins with 'file://', I get the error: `ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found`. I am using Spark 2.2.1 (prebuilt with hadoop2.7) with AWS-Hadoop 2.7.4. I have the following settings in spark config and am using a custom directParquetOutputCommitter. Standard writes to 'file://' of Spark dataframes work without issue. Thanks for any help!. ```; spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:1441,Testability,Test,TestHAIL,1441,"d am using a custom directParquetOutputCommitter. Standard writes to 'file://' of Spark dataframes work without issue. Thanks for any help!. ```; spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; <decorator-gen-398>:2: in export_vcf; ???; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. func = <function export_vcf at 0x7fa13c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:1450,Testability,test,testMethod,1450,"d am using a custom directParquetOutputCommitter. Standard writes to 'file://' of Spark dataframes work without issue. Thanks for any help!. ```; spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; <decorator-gen-398>:2: in export_vcf; ???; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. func = <function export_vcf at 0x7fa13c",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:1553,Testability,test,testdir,1553," without issue. Thanks for any help!. ```; spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; <decorator-gen-398>:2: in export_vcf; ???; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. func = <function export_vcf at 0x7fa13c4d9938>, args = (<hail.dataset.VariantDataset object at 0x7fa13c3c9390>, 'file:///scratch/test_vcf_exp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:1618,Testability,test,testdir,1618,"ut.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; spark.hadoop.mapred.output.committer.class org.apache.hadoop.mapred.DirectFileOutputCommitter; spark.hadoop.mapreduce.use.directfileoutputcommitter true; spark.hadoop.spark.sql.parquet.output.committer.class org.apache.spark.sql.parquet.DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; <decorator-gen-398>:2: in export_vcf; ???; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. func = <function export_vcf at 0x7fa13c4d9938>, args = (<hail.dataset.VariantDataset object at 0x7fa13c3c9390>, 'file:///scratch/test_vcf_export.vcf.bgz', None, False, False), kwargs = {}; e = Py4JJavaError",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3946:1931,Testability,test,tests,1931,".DirectParquetOutputCommitter; ```. Code and stack trace:; ```; ================================================================================================== FAILURES ===================================================================================================; __________________________________________________________________________________________ TestHAIL.test_export_vcf ___________________________________________________________________________________________. self = <test_hail.TestHAIL testMethod=test_export_vcf>. def test_export_vcf(self):; # define files; bgen_file = os.path.join(self.testdir, 'example.10bits.bgen'); sample_file = os.path.join(self.testdir, 'example.sample'); # make index; self.hc.index_bgen(bgen_file); # load to vds; bgen_vds = self.hc.import_bgen(bgen_file, sample_file=sample_file); # export vcf; out_path = 'file://' + os.path.join(self.tmpdir, 'test_vcf_export.vcf.bgz'); > bgen_vds.export_vcf(out_path, export_pp=False, parallel=False). tests/hail/test_hail.py:55:; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; <decorator-gen-398>:2: in export_vcf; ???; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. func = <function export_vcf at 0x7fa13c4d9938>, args = (<hail.dataset.VariantDataset object at 0x7fa13c3c9390>, 'file:///scratch/test_vcf_export.vcf.bgz', None, False, False), kwargs = {}; e = Py4JJavaError(u'An error occurred while calling o160.exportVCF.\n', JavaObject id=o162), tpl = JavaObject id=o210; deepest = 'ClassNotFoundException: Class org.apache.hadoop.mapred.DirectFileOutputCommitter not found'; full = 'java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3946
https://github.com/hail-is/hail/issues/3951:77,Testability,test,test-when-testngs-dataprovider-throws-an-exception,77,See discussion here: http://rolf-engelhard.de/2011/10/fail-instead-of-skip-a-test-when-testngs-dataprovider-throws-an-exception/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3951
https://github.com/hail-is/hail/issues/3953:4374,Availability,error,error,4374,"n, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all error messages here, including the full java stack trace):; No error messages, but my pipeline craps out at the following lines:; ```; pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(); ```. `.count()` works instantaneously up until that `pcloadings.annotate()` line. After that line, it gets stuck at 0 of 1 tasks for ~20 minutes before I give up and cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3953
https://github.com/hail-is/hail/issues/3953:4437,Availability,error,error,4437,"n, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all error messages here, including the full java stack trace):; No error messages, but my pipeline craps out at the following lines:; ```; pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(); ```. `.count()` works instantaneously up until that `pcloadings.annotate()` line. After that line, it gets stuck at 0 of 1 tasks for ~20 minutes before I give up and cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3953
https://github.com/hail-is/hail/issues/3953:497,Deployability,pipeline,pipeline,497,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2/devel hash eb1e04205793. ### What you did:; I'm trying to take PC loadings generated using Hail in one sample and project another sample into that PC space to generate scores for them. I've been using @danking's tricks developed for my PRS pipeline to speed things along and am now trying to feed the relevant inputs into gnomad Hail's [pc_project()](https://github.com/macarthur-lab/gnomad_hail/blob/master/utils/generic.py#L164) function. I've written out the function in script format for debugging purposes. Full code is below. . ```import hail as hl; import pickle; import time. generate_pcloadings_table = True; pcloadings_table_location = 'gs://ukbb_prs/sibdiff/keytables/ukb-pca-locus-allele-keyed.kt'; generate_contig_row_dict = True; contig_row_dict_location = 'gs://ukbb_prs/sibdiff/keytables/contig_row_dict-UKB'; output_location = 'gs://ukbb_prs/sibdiff/UKB_sibloadings.txt'; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; bgen_files = 'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}_v3.bgen'. # large block size because we read very little data (due to filtering & ignoring genotypes); hl.init(branching_factor=10, min_block_size=2000). ### set up the pcloadings table; if (generate_pcloadings_table):; pcloadings = hl.import_table('gs://phenotype_31063/ukb31063.gwas.pca_loadings.tsv.gz', impute=True); pcloadings = pcloadings.annotate(locus=hl.parse_locus(hl.str(pcloadings.chr) + "":"" + hl.str(pcloadings.pos)),; alleles=[pcloadings.ref,pcloadings.alt]).key_by('locus','alleles'). pcloadings.write(pcloadings_table_location, overwrite=True). pcloadings = hl.read_table(pcloadings_table_location). ### determine the file locations of the pca ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3953
https://github.com/hail-is/hail/issues/3953:4460,Deployability,pipeline,pipeline,4460,"n, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all error messages here, including the full java stack trace):; No error messages, but my pipeline craps out at the following lines:; ```; pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(); ```. `.count()` works instantaneously up until that `pcloadings.annotate()` line. After that line, it gets stuck at 0 of 1 tasks for ~20 minutes before I give up and cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3953
https://github.com/hail-is/hail/issues/3953:4380,Integrability,message,messages,4380,"n, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all error messages here, including the full java stack trace):; No error messages, but my pipeline craps out at the following lines:; ```; pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(); ```. `.count()` works instantaneously up until that `pcloadings.annotate()` line. After that line, it gets stuck at 0 of 1 tasks for ~20 minutes before I give up and cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3953
https://github.com/hail-is/hail/issues/3953:4443,Integrability,message,messages,4443,"n, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all error messages here, including the full java stack trace):; No error messages, but my pipeline craps out at the following lines:; ```; pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(); ```. `.count()` works instantaneously up until that `pcloadings.annotate()` line. After that line, it gets stuck at 0 of 1 tasks for ~20 minutes before I give up and cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3953
https://github.com/hail-is/hail/issues/3953:323,Performance,load,loadings,323,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2/devel hash eb1e04205793. ### What you did:; I'm trying to take PC loadings generated using Hail in one sample and project another sample into that PC space to generate scores for them. I've been using @danking's tricks developed for my PRS pipeline to speed things along and am now trying to feed the relevant inputs into gnomad Hail's [pc_project()](https://github.com/macarthur-lab/gnomad_hail/blob/master/utils/generic.py#L164) function. I've written out the function in script format for debugging purposes. Full code is below. . ```import hail as hl; import pickle; import time. generate_pcloadings_table = True; pcloadings_table_location = 'gs://ukbb_prs/sibdiff/keytables/ukb-pca-locus-allele-keyed.kt'; generate_contig_row_dict = True; contig_row_dict_location = 'gs://ukbb_prs/sibdiff/keytables/contig_row_dict-UKB'; output_location = 'gs://ukbb_prs/sibdiff/UKB_sibloadings.txt'; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; bgen_files = 'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}_v3.bgen'. # large block size because we read very little data (due to filtering & ignoring genotypes); hl.init(branching_factor=10, min_block_size=2000). ### set up the pcloadings table; if (generate_pcloadings_table):; pcloadings = hl.import_table('gs://phenotype_31063/ukb31063.gwas.pca_loadings.tsv.gz', impute=True); pcloadings = pcloadings.annotate(locus=hl.parse_locus(hl.str(pcloadings.chr) + "":"" + hl.str(pcloadings.pos)),; alleles=[pcloadings.ref,pcloadings.alt]).key_by('locus','alleles'). pcloadings.write(pcloadings_table_location, overwrite=True). pcloadings = hl.read_table(pcloadings_table_location). ### determine the file locations of the pca ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3953
https://github.com/hail-is/hail/issues/3953:2936,Performance,load,load,2936,"table_location). ### determine the file locations of the pca variants; if (generate_contig_row_dict):; mt = hl.methods.import_bgen(bgen_files,; [],; contig_recoding=contigs,; _row_fields=['file_row_idx']); pca_rows = mt.filter_rows(hl.is_defined(pcloadings[mt.row_key])).rows(); print('about to collect'); # remove all unnecessary data, dropping keys and other irrelevant fields; pca_rows = pca_rows.key_by(); pca_rows = pca_rows.select(pca_rows.locus.contig, pca_rows.file_row_idx); contig_row_list = pca_rows.collect(); print('finished collecting'); contig_reformed = [(x['contig'], x['file_row_idx']) for x in contig_row_list]; print('reformed'); from collections import defaultdict; contig_row_dict = defaultdict(list); for k, v in contig_reformed:; contig_row_dict[k].append(v); print('dictionary created'). with hl.hadoop_open(contig_row_dict_location, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3953
https://github.com/hail-is/hail/issues/3953:3351,Performance,load,loadings,3351,"ect(pca_rows.locus.contig, pca_rows.file_row_idx); contig_row_list = pca_rows.collect(); print('finished collecting'); contig_reformed = [(x['contig'], x['file_row_idx']) for x in contig_row_list]; print('reformed'); from collections import defaultdict; contig_row_dict = defaultdict(list); for k, v in contig_reformed:; contig_row_dict[k].append(v); print('dictionary created'). with hl.hadoop_open(contig_row_dict_location, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3953
https://github.com/hail-is/hail/issues/3953:3408,Performance,load,load,3408,"ntig_row_list = pca_rows.collect(); print('finished collecting'); contig_reformed = [(x['contig'], x['file_row_idx']) for x in contig_row_list]; print('reformed'); from collections import defaultdict; contig_row_dict = defaultdict(list); for k, v in contig_reformed:; contig_row_dict[k].append(v); print('dictionary created'). with hl.hadoop_open(contig_row_dict_location, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all error messages here, including the full java stack t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3953
https://github.com/hail-is/hail/issues/3953:3941,Performance,load,loadings,3941,"n, 'wb') as f:; pickle.dump(contig_row_dict, f); else:; with hl.hadoop_open(contig_row_dict_location, 'rb') as f:; contig_row_dict = pickle.load(f). ### Run the PCA; contig_row_dict2 = {'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{contig}_v3.bgen'.format(contig=k): v for k, v in contig_row_dict.items()}; mt = hl.methods.import_bgen(bgen_files,; ['GT'],; sample_file='gs://phenotype_31063/ukb31063.autosomes.sample',; contig_recoding=contigs,; _variants_per_file=contig_row_dict2,; _row_fields=[]). pcloadings = pcloadings.transmute(loadings=[pcloadings[f'PC{i+1}'] for i in range(20)]). # load OG scoring sample; sampleids = hl.import_table('gs://ukb31063-mega-gwas/qc/ukb31063.gwas_samples.txt', delimiter='\s+').key_by('s'). # filter bgen matrixtable to only include people in scoring sample; og_sample = mt.filter_cols(hl.is_defined(sampleids[mt.s])). og_sample = og_sample.annotate_rows(pca_af=hl.agg.mean(og_sample.GT.n_alt_alleles()) / 2). pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(). mt = sibs.annotate_rows(; pca_loadings=pcloadings[sibs.row_key][""loadings""],; pca_af=pcloadings[sibs.row_key][""pca_af""]; ). mt = mt.filter_rows(hl.is_defined(mt.pca_loadings) & hl.is_defined(mt.pca_af) &; (mt.pca_af > 0) & (mt.pca_af < 1)). gt_norm = (mt.GT.n_alt_alleles() - 2 * mt.pca_af) / hl.sqrt(n_variants * 2 * mt.pca_af * (1 - mt.pca_af)). mt = mt.annotate_cols(scores=hl.agg.array_sum(mt.pca_loadings * gt_norm)). related_scores = mt.cols().select('scores'); ```. ### What went wrong (all error messages here, including the full java stack trace):; No error messages, but my pipeline craps out at the following lines:; ```; pcloadings = pcloadings.annotate(pca_af=og_sample[pcloadings.key, :].pca_af). n_variants = pcloadings.count(); ```. `.count()` works instantaneously up until that `pcloadings.annotate()` line. After that line, it gets stuck at 0 of 1 tasks for ~20 minutes before I give up and cancel it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3953
https://github.com/hail-is/hail/issues/3953:263,Security,hash,hash,263,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2/devel hash eb1e04205793. ### What you did:; I'm trying to take PC loadings generated using Hail in one sample and project another sample into that PC space to generate scores for them. I've been using @danking's tricks developed for my PRS pipeline to speed things along and am now trying to feed the relevant inputs into gnomad Hail's [pc_project()](https://github.com/macarthur-lab/gnomad_hail/blob/master/utils/generic.py#L164) function. I've written out the function in script format for debugging purposes. Full code is below. . ```import hail as hl; import pickle; import time. generate_pcloadings_table = True; pcloadings_table_location = 'gs://ukbb_prs/sibdiff/keytables/ukb-pca-locus-allele-keyed.kt'; generate_contig_row_dict = True; contig_row_dict_location = 'gs://ukbb_prs/sibdiff/keytables/contig_row_dict-UKB'; output_location = 'gs://ukbb_prs/sibdiff/UKB_sibloadings.txt'; contigs = {'0{}'.format(x):str(x) for x in range(1, 10)}; bgen_files = 'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_imp_chr{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22}_v3.bgen'. # large block size because we read very little data (due to filtering & ignoring genotypes); hl.init(branching_factor=10, min_block_size=2000). ### set up the pcloadings table; if (generate_pcloadings_table):; pcloadings = hl.import_table('gs://phenotype_31063/ukb31063.gwas.pca_loadings.tsv.gz', impute=True); pcloadings = pcloadings.annotate(locus=hl.parse_locus(hl.str(pcloadings.chr) + "":"" + hl.str(pcloadings.pos)),; alleles=[pcloadings.ref,pcloadings.alt]).key_by('locus','alleles'). pcloadings.write(pcloadings_table_location, overwrite=True). pcloadings = hl.read_table(pcloadings_table_location). ### determine the file locations of the pca ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3953
https://github.com/hail-is/hail/pull/3954:13,Security,access,access,13,"I don't have access to Laurent's repo, so I can't push to address comments on his PR. Opening a new one instead",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3954
https://github.com/hail-is/hail/issues/3955:345,Availability,error,error,345,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 19213e50. ### What you did:; Tried compiling hail with GCC-8.1.0. ### What went wrong (all error messages here, including the full java stack trace):. Full output from `make` invocation:; ```; echo ""make debug""; make debug; echo ""JAVA_HOME is /usr/lib/jvm/java-8-openjdk-amd64""; JAVA_HOME is /usr/lib/jvm/java-8-openjdk-amd64; echo ""CXX is g++""; CXX is g++; g++ --version; g++ (GCC) 8.1.0; Copyright (C) 2018 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. tar -xzf libsimdpp-2.0-rc2.tar.gz; g++ -O3 -march=native -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -c -o ibs.o ibs.cpp; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:253:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:447,Availability,echo,echo,447,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 19213e50. ### What you did:; Tried compiling hail with GCC-8.1.0. ### What went wrong (all error messages here, including the full java stack trace):. Full output from `make` invocation:; ```; echo ""make debug""; make debug; echo ""JAVA_HOME is /usr/lib/jvm/java-8-openjdk-amd64""; JAVA_HOME is /usr/lib/jvm/java-8-openjdk-amd64; echo ""CXX is g++""; CXX is g++; g++ --version; g++ (GCC) 8.1.0; Copyright (C) 2018 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. tar -xzf libsimdpp-2.0-rc2.tar.gz; g++ -O3 -march=native -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -c -o ibs.o ibs.cpp; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:253:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:478,Availability,echo,echo,478,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 19213e50. ### What you did:; Tried compiling hail with GCC-8.1.0. ### What went wrong (all error messages here, including the full java stack trace):. Full output from `make` invocation:; ```; echo ""make debug""; make debug; echo ""JAVA_HOME is /usr/lib/jvm/java-8-openjdk-amd64""; JAVA_HOME is /usr/lib/jvm/java-8-openjdk-amd64; echo ""CXX is g++""; CXX is g++; g++ --version; g++ (GCC) 8.1.0; Copyright (C) 2018 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. tar -xzf libsimdpp-2.0-rc2.tar.gz; g++ -O3 -march=native -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -c -o ibs.o ibs.cpp; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:253:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:581,Availability,echo,echo,581,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 19213e50. ### What you did:; Tried compiling hail with GCC-8.1.0. ### What went wrong (all error messages here, including the full java stack trace):. Full output from `make` invocation:; ```; echo ""make debug""; make debug; echo ""JAVA_HOME is /usr/lib/jvm/java-8-openjdk-amd64""; JAVA_HOME is /usr/lib/jvm/java-8-openjdk-amd64; echo ""CXX is g++""; CXX is g++; g++ --version; g++ (GCC) 8.1.0; Copyright (C) 2018 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. tar -xzf libsimdpp-2.0-rc2.tar.gz; g++ -O3 -march=native -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -c -o ibs.o ibs.cpp; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:253:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:1563,Availability,Mask,MaskCastOverride,1563,"g++ --version; g++ (GCC) 8.1.0; Copyright (C) 2018 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. tar -xzf libsimdpp-2.0-rc2.tar.gz; g++ -O3 -march=native -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -c -o ibs.o ibs.cpp; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:253:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::float64<2> with private member simdpp::arch_avx2::float64<2>::d_ from an array of const class simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:32,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:1716,Availability,Mask,MaskCastOverride,1716,"g++ --version; g++ (GCC) 8.1.0; Copyright (C) 2018 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. tar -xzf libsimdpp-2.0-rc2.tar.gz; g++ -O3 -march=native -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/usr/lib/jvm/java-8-openjdk-amd64/include -I/usr/lib/jvm/java-8-openjdk-amd64/include/linux -c -o ibs.o ibs.cpp; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:253:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::float64<2> with private member simdpp::arch_avx2::float64<2>::d_ from an array of const class simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:32,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:2073,Availability,error,error,2073,"::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float64<2>; T = simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>]; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:253:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::float64<2> with private member simdpp::arch_avx2::float64<2>::d_ from an array of const class simdpp::arch_avx2::float32<4, simdpp::arch_avx2::expr_empty>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:32,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float64x2.h:32:7: note: class simdpp::arch_avx2::float64<2> declared here; class float64<2, void> : public any_float64<2, float64<2,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:3290,Availability,Mask,MaskCastOverride,3290,"se assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:32,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float64x2.h:32:7: note: class simdpp::arch_avx2::float64<2> declared here; class float64<2, void> : public any_float64<2, float64<2,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>]; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:255:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::float32<4> with private member simdpp::arch_avx2::float32<4>::d_ from an array of const class simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:29,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:3443,Availability,Mask,MaskCastOverride,3443,"se assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:32,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float64x2.h:32:7: note: class simdpp::arch_avx2::float64<2> declared here; class float64<2, void> : public any_float64<2, float64<2,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>]; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:255:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::float32<4> with private member simdpp::arch_avx2::float32<4>::d_ from an array of const class simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:29,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:3800,Availability,error,error,3800,"::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::float32<4>; T = simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>]; libsimdpp-2.0-rc2/simdpp/detail/insn/transpose.h:255:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::float32<4> with private member simdpp::arch_avx2::float32<4>::d_ from an array of const class simdpp::arch_avx2::float64<2, simdpp::arch_avx2::expr_empty>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:29,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float32x4.h:32:7: note: class simdpp::arch_avx2::float32<4> declared here; class float32<4, void> : public any_float32<4, float32<4,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(con",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:4984,Availability,Mask,MaskCastOverride,4984,"x2::float64<2, simdpp::arch_avx2::expr_empty>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:29,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float32x4.h:32:7: note: class simdpp::arch_avx2::float32<4> declared here; class float32<4, void> : public any_float32<4, float32<4,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>]; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint16<8>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:69:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<16> with private member simdpp::arch_avx2::uint8<16>::d_ from an array of const class simdpp::arch_avx2::uint16<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:5104,Availability,Mask,MaskCastOverride,5104,"x2::float64<2, simdpp::arch_avx2::expr_empty>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:29,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/float32x4.h:32:7: note: class simdpp::arch_avx2::float32<4> declared here; class float32<4, void> : public any_float32<4, float32<4,void>> {; ^~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>]; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint16<8>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:69:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<16> with private member simdpp::arch_avx2::uint8<16>::d_ from an array of const class simdpp::arch_avx2::uint16<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:5608,Availability,error,error,5608,"h_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint16<8>]; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint16<8>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:69:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<16> with private member simdpp::arch_avx2::uint8<16>::d_ from an array of const class simdpp::arch_avx2::uint16<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: class simdpp::arch_avx2::uint8<16> declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:6751,Availability,Mask,MaskCastOverride,6751,"n array of const class simdpp::arch_avx2::uint16<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: class simdpp::arch_avx2::uint8<16> declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:129:36: required from simdpp::arch_avx2::uint16<8>::uint16(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:69:58: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint16<8> with private member simdpp::arch_avx2::uint16<8>::d_ from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:6871,Availability,Mask,MaskCastOverride,6871,"n array of const class simdpp::arch_avx2::uint16<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: class simdpp::arch_avx2::uint8<16> declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:129:36: required from simdpp::arch_avx2::uint16<8>::uint16(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:69:58: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint16<8> with private member simdpp::arch_avx2::uint16<8>::d_ from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:7376,Availability,error,error,7376,"_avx2::uint16<8>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint16<8>; T = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:129:36: required from simdpp::arch_avx2::uint16<8>::uint16(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:69:58: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint16<8> with private member simdpp::arch_avx2::uint16<8>::d_ from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:104:7: note: class simdpp::arch_avx2::uint16<8> declared here; class uint16<8, void> : public any_int16<8, uint16<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint32<4>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:8519,Availability,Mask,MaskCastOverride,8519,"n array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:104:7: note: class simdpp::arch_avx2::uint16<8> declared here; class uint16<8, void> : public any_int16<8, uint16<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint32<4>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint32<4>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint32<4>]; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint32<4>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:101:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<16> with private member simdpp::arch_avx2::uint8<16>::d_ from an array of const class simdpp::arch_avx2::uint32<4>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:8639,Availability,Mask,MaskCastOverride,8639,"n array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:104:7: note: class simdpp::arch_avx2::uint16<8> declared here; class uint16<8, void> : public any_int16<8, uint16<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint32<4>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint32<4>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint32<4>]; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint32<4>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:101:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<16> with private member simdpp::arch_avx2::uint8<16>::d_ from an array of const class simdpp::arch_avx2::uint32<4>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:9144,Availability,error,error,9144,"_avx2::uint8<16>; T = simdpp::arch_avx2::uint32<4>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint32<4>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint32<4>]; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint32<4>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:101:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<16> with private member simdpp::arch_avx2::uint8<16>::d_ from an array of const class simdpp::arch_avx2::uint32<4>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: class simdpp::arch_avx2::uint8<16> declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:10287,Availability,Mask,MaskCastOverride,10287,"n array of const class simdpp::arch_avx2::uint32<4>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: class simdpp::arch_avx2::uint8<16> declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:129:36: required from simdpp::arch_avx2::uint32<4>::uint32(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:101:58: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint32<4> with private member simdpp::arch_avx2::uint32<4>::d_ from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:10407,Availability,Mask,MaskCastOverride,10407,"n array of const class simdpp::arch_avx2::uint32<4>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: class simdpp::arch_avx2::uint8<16> declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:129:36: required from simdpp::arch_avx2::uint32<4>::uint32(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:101:58: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint32<4> with private member simdpp::arch_avx2::uint32<4>::d_ from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:10913,Availability,error,error,10913,"avx2::uint32<4>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<4>; T = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:129:36: required from simdpp::arch_avx2::uint32<4>::uint32(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:101:58: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint32<4> with private member simdpp::arch_avx2::uint32<4>::d_ from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:104:7: note: class simdpp::arch_avx2::uint32<4> declared here; class uint32<4, void> : public any_int32<4, uint32<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint64<2>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:12056,Availability,Mask,MaskCastOverride,12056,"n array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:104:7: note: class simdpp::arch_avx2::uint32<4> declared here; class uint32<4, void> : public any_int32<4, uint32<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint64<2>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint64<2>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint64<2>]; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint64<2>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:150:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<16> with private member simdpp::arch_avx2::uint8<16>::d_ from an array of const class simdpp::arch_avx2::uint64<2>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:12176,Availability,Mask,MaskCastOverride,12176,"n array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:23,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x4.h:104:7: note: class simdpp::arch_avx2::uint32<4> declared here; class uint32<4, void> : public any_int32<4, uint32<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint64<2>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint64<2>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint64<2>]; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint64<2>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:150:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<16> with private member simdpp::arch_avx2::uint8<16>::d_ from an array of const class simdpp::arch_avx2::uint64<2>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:12681,Availability,error,error,12681,"_avx2::uint8<16>; T = simdpp::arch_avx2::uint64<2>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint64<2>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::uint64<2>]; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint64<2>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:150:42: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<16> with private member simdpp::arch_avx2::uint8<16>::d_ from an array of const class simdpp::arch_avx2::uint64<2>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: class simdpp::arch_avx2::uint8<16> declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:13824,Availability,Mask,MaskCastOverride,13824,"n array of const class simdpp::arch_avx2::uint64<2>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: class simdpp::arch_avx2::uint8<16> declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:125:36: required from simdpp::arch_avx2::uint64<2>::uint64(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:150:58: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint64<2> with private member simdpp::arch_avx2::uint64<2>::d_ from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:13944,Availability,Mask,MaskCastOverride,13944,"n array of const class simdpp::arch_avx2::uint64<2>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: class simdpp::arch_avx2::uint8<16> declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:125:36: required from simdpp::arch_avx2::uint64<2>::uint64(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:150:58: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint64<2> with private member simdpp::arch_avx2::uint64<2>::d_ from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:14450,Availability,error,error,14450,"avx2::uint64<2>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<2>; T = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:125:36: required from simdpp::arch_avx2::uint64<2>::uint64(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_or.h:150:58: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint64<2> with private member simdpp::arch_avx2::uint64<2>::d_ from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:102:7: note: class simdpp::arch_avx2::uint64<2> declared here; class uint64<2, void> : public any_int64<2, uint64<2,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:15592,Availability,Mask,MaskCastOverride,15592," an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:102:7: note: class simdpp::arch_avx2::uint64<2> declared here; class uint64<2, void> : public any_int64<2, uint64<2,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:62:35: required from simdpp::arch_avx2::int16<8>& simdpp::arch_avx2::int16<8>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:45:11: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::int16<8> with private member simdpp::arch_avx2::int16<8>::d_ from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:15711,Availability,Mask,MaskCastOverride,15711," an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:26,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x2.h:102:7: note: class simdpp::arch_avx2::uint64<2> declared here; class uint64<2, void> : public any_int64<2, uint64<2,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:62:35: required from simdpp::arch_avx2::int16<8>& simdpp::arch_avx2::int16<8>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:45:11: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::int16<8> with private member simdpp::arch_avx2::int16<8>::d_ from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file in",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:16254,Availability,error,error,16254,"vx2::uint8<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int16<8>; T = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:62:35: required from simdpp::arch_avx2::int16<8>& simdpp::arch_avx2::int16<8>::operator=(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::uint8<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:45:11: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::int16<8> with private member simdpp::arch_avx2::int16<8>::d_ from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:33:7: note: class simdpp::arch_avx2::int16<8> declared here; class int16<8, void> : public any_int16<8, int16<8,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:17389,Availability,Mask,MaskCastOverride,17389," from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:33:7: note: class simdpp::arch_avx2::int16<8> declared here; class int16<8, void> : public any_int16<8, int16<8,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>]; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int16<8>]; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:51:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<16> with private member simdpp::arch_avx2::uint8<16>::d_ from an array of const class simdpp::arch_avx2::int16<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:17508,Availability,Mask,MaskCastOverride,17508," from an array of const class simdpp::arch_avx2::uint8<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:21,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x8.h:33:7: note: class simdpp::arch_avx2::int16<8> declared here; class int16<8, void> : public any_int16<8, int16<8,void>> {; ^~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>]; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int16<8>]; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:51:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<16> with private member simdpp::arch_avx2::uint8<16>::d_ from an array of const class simdpp::arch_avx2::int16<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:18019,Availability,error,error,18019,"x2::uint8<16>; T = simdpp::arch_avx2::int16<8>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<16>; T = simdpp::arch_avx2::int16<8>]; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:129:36: required from simdpp::arch_avx2::uint8<16>::uint8(const simdpp::arch_avx2::any_vec<16, V>&) [with V = simdpp::arch_avx2::int16<8>]; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:51:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<16> with private member simdpp::arch_avx2::uint8<16>::d_ from an array of const class simdpp::arch_avx2::int16<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: class simdpp::arch_avx2::uint8<16> declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint8<32>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:19161,Availability,Mask,MaskCastOverride,19161,"an array of const class simdpp::arch_avx2::int16<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: class simdpp::arch_avx2::uint8<16> declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint8<32>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint8<32>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:56:36: required from simdpp::arch_avx2::int16<16>& simdpp::arch_avx2::int16<16>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:85:11: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::int16<16> with private member simdpp::arch_avx2::int16<16>::d_ from an array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:19281,Availability,Mask,MaskCastOverride,19281,"an array of const class simdpp::arch_avx2::int16<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:19,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x16.h:104:7: note: class simdpp::arch_avx2::uint8<16> declared here; class uint8<16, void> : public any_int8<16, uint8<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint8<32>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint8<32>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:56:36: required from simdpp::arch_avx2::int16<16>& simdpp::arch_avx2::int16<16>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:85:11: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::int16<16> with private member simdpp::arch_avx2::int16<16>::d_ from an array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:19828,Availability,error,error,19828,"int8<32>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint8<32>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::int16<16>; T = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:56:36: required from simdpp::arch_avx2::int16<16>& simdpp::arch_avx2::int16<16>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:85:11: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::int16<16> with private member simdpp::arch_avx2::int16<16>::d_ from an array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:22,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:33:7: note: class simdpp::arch_avx2::int16<16> declared here; class int16<16, void> : public any_int16<16, int16<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int16<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:20972,Availability,Mask,MaskCastOverride,20972," array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:22,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:33:7: note: class simdpp::arch_avx2::int16<16> declared here; class int16<16, void> : public any_int16<16, int16<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int16<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int16<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int16<16>]; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:110:36: required from simdpp::arch_avx2::uint8<32>::uint8(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int16<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:91:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<32> with private member simdpp::arch_avx2::uint8<32>::d_ from an array of const class simdpp::arch_avx2::int16<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:21092,Availability,Mask,MaskCastOverride,21092," array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:22,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int16x16.h:33:7: note: class simdpp::arch_avx2::int16<16> declared here; class int16<16, void> : public any_int16<16, int16<16,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int16<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int16<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int16<16>]; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:110:36: required from simdpp::arch_avx2::uint8<32>::uint8(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int16<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:91:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<32> with private member simdpp::arch_avx2::uint8<32>::d_ from an array of const class simdpp::arch_avx2::int16<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-r",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:21605,Availability,error,error,21605,"int8<32>; T = simdpp::arch_avx2::int16<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int16<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::int16<16>]; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:110:36: required from simdpp::arch_avx2::uint8<32>::uint8(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::int16<16>]; libsimdpp-2.0-rc2/simdpp/detail/insn/shuffle_bytes16.h:91:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<32> with private member simdpp::arch_avx2::uint8<32>::d_ from an array of const class simdpp::arch_avx2::int16<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: class simdpp::arch_avx2::uint8<32> declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint32<8>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:22747,Availability,Mask,MaskCastOverride,22747,"an array of const class simdpp::arch_avx2::int16<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: class simdpp::arch_avx2::uint8<32> declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint32<8>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint32<8>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint32<8>]; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:110:36: required from simdpp::arch_avx2::uint8<32>::uint8(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint32<8>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_not.h:64:43: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<32> with private member simdpp::arch_avx2::uint8<32>::d_ from an array of const class simdpp::arch_avx2::uint32<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:22867,Availability,Mask,MaskCastOverride,22867,"an array of const class simdpp::arch_avx2::int16<16>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: class simdpp::arch_avx2::uint8<32> declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint32<8>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint32<8>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint32<8>]; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:110:36: required from simdpp::arch_avx2::uint8<32>::uint8(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint32<8>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_not.h:64:43: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<32> with private member simdpp::arch_avx2::uint8<32>::d_ from an array of const class simdpp::arch_avx2::uint32<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:23372,Availability,error,error,23372,"_avx2::uint8<32>; T = simdpp::arch_avx2::uint32<8>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint32<8>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint32<8>]; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:110:36: required from simdpp::arch_avx2::uint8<32>::uint8(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint32<8>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_not.h:64:43: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<32> with private member simdpp::arch_avx2::uint8<32>::d_ from an array of const class simdpp::arch_avx2::uint32<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: class simdpp::arch_avx2::uint8<32> declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint8<32>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:24514,Availability,Mask,MaskCastOverride,24514,"an array of const class simdpp::arch_avx2::uint32<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: class simdpp::arch_avx2::uint8<32> declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint8<32>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint8<32>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:110:36: required from simdpp::arch_avx2::uint32<8>::uint32(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_not.h:64:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint32<8> with private member simdpp::arch_avx2::uint32<8>::d_ from an array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:24634,Availability,Mask,MaskCastOverride,24634,"an array of const class simdpp::arch_avx2::uint32<8>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: class simdpp::arch_avx2::uint8<32> declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint8<32>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint8<32>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:110:36: required from simdpp::arch_avx2::uint32<8>::uint32(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_not.h:64:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint32<8> with private member simdpp::arch_avx2::uint32<8>::d_ from an array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:25140,Availability,error,error,25140,"avx2::uint32<8>; T = simdpp::arch_avx2::uint8<32>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint8<32>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint32<8>; T = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:110:36: required from simdpp::arch_avx2::uint32<8>::uint32(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_not.h:64:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint32<8> with private member simdpp::arch_avx2::uint32<8>::d_ from an array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:24,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:91:7: note: class simdpp::arch_avx2::uint32<8> declared here; class uint32<8, void> : public any_int32<8, uint32<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint64<4>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:26282,Availability,Mask,MaskCastOverride,26282,"an array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:24,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:91:7: note: class simdpp::arch_avx2::uint32<8> declared here; class uint32<8, void> : public any_int32<8, uint32<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint64<4>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint64<4>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint64<4>]; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:110:36: required from simdpp::arch_avx2::uint8<32>::uint8(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint64<4>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_not.h:94:43: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<32> with private member simdpp::arch_avx2::uint8<32>::d_ from an array of const class simdpp::arch_avx2::uint64<4>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:26402,Availability,Mask,MaskCastOverride,26402,"an array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:24,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int32x8.h:91:7: note: class simdpp::arch_avx2::uint32<8> declared here; class uint32<8, void> : public any_int32<8, uint32<8,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint64<4>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint64<4>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint64<4>]; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:110:36: required from simdpp::arch_avx2::uint8<32>::uint8(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint64<4>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_not.h:94:43: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<32> with private member simdpp::arch_avx2::uint8<32>::d_ from an array of const class simdpp::arch_avx2::uint64<4>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:26907,Availability,error,error,26907,"_avx2::uint8<32>; T = simdpp::arch_avx2::uint64<4>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint64<4>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint64<4>]; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:110:36: required from simdpp::arch_avx2::uint8<32>::uint8(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint64<4>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_not.h:94:43: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<32> with private member simdpp::arch_avx2::uint8<32>::d_ from an array of const class simdpp::arch_avx2::uint64<4>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: class simdpp::arch_avx2::uint8<32> declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint8<32>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:28049,Availability,Mask,MaskCastOverride,28049,"an array of const class simdpp::arch_avx2::uint64<4>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: class simdpp::arch_avx2::uint8<32> declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint8<32>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint8<32>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:110:36: required from simdpp::arch_avx2::uint64<4>::uint64(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_not.h:94:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint64<4> with private member simdpp::arch_avx2::uint64<4>::d_ from an array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:28169,Availability,Mask,MaskCastOverride,28169,"an array of const class simdpp::arch_avx2::uint64<4>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:20,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:91:7: note: class simdpp::arch_avx2::uint8<32> declared here; class uint8<32, void> : public any_int8<32, uint8<32,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint8<32>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint8<32>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:110:36: required from simdpp::arch_avx2::uint64<4>::uint64(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_not.h:94:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint64<4> with private member simdpp::arch_avx2::uint64<4>::d_ from an array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:28675,Availability,error,error,28675,"avx2::uint64<4>; T = simdpp::arch_avx2::uint8<32>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint8<32>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint64<4>; T = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:110:36: required from simdpp::arch_avx2::uint64<4>::uint64(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint8<32>]; libsimdpp-2.0-rc2/simdpp/detail/insn/bit_not.h:94:45: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint64<4> with private member simdpp::arch_avx2::uint64<4>::d_ from an array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:27,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:91:7: note: class simdpp::arch_avx2::uint64<4> declared here; class uint64<4, void> : public any_int64<4, uint64<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint16<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:29818,Availability,Mask,MaskCastOverride,29818,"n array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:27,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:91:7: note: class simdpp::arch_avx2::uint64<4> declared here; class uint64<4, void> : public any_int64<4, uint64<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint16<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint16<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint16<16>]; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:114:36: required from simdpp::arch_avx2::uint8<32>& simdpp::arch_avx2::uint8<32>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::expr_bit_and<simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::uint8<32> >, simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::uint16<16> > > >]; libsimdpp-2.0-rc2/simdpp/detail/insn/unzip_lo.h:56:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<32> with private member simdpp::arch_avx2::uint8<32>::d_ from an array of const class simdpp::arc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
https://github.com/hail-is/hail/issues/3955:29939,Availability,Mask,MaskCastOverride,29939,"n array of const class simdpp::arch_avx2::uint8<32>; use assignment or copy-initialization instead [-Werror=class-memaccess]; ::memcpy(&r, &t, sizeof(R));; ~~~~~~~~^~~~~~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/types.h:27,; from libsimdpp-2.0-rc2/simdpp/core/align.h:15,; from libsimdpp-2.0-rc2/simdpp/simd.h:22,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/types/int64x4.h:91:7: note: class simdpp::arch_avx2::uint64<4> declared here; class uint64<4, void> : public any_int64<4, uint64<4,void>> {; ^~~~~~~~~~~~~~~; In file included from libsimdpp-2.0-rc2/simdpp/simd.h:132,; from ibs.h:17,; from ibs.cpp:5:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl: In instantiation of R simdpp::arch_avx2::detail::cast_memcpy(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint16<16>]:; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:120:30: required from static R simdpp::arch_avx2::detail::cast_wrapper<false, false, MaskCastOverride>::run(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint16<16>; unsigned int MaskCastOverride = 0]; libsimdpp-2.0-rc2/simdpp/core/cast.h:63:89: required from R simdpp::arch_avx2::bit_cast(const T&) [with R = simdpp::arch_avx2::uint8<32>; T = simdpp::arch_avx2::uint16<16>]; libsimdpp-2.0-rc2/simdpp/types/int8x32.h:114:36: required from simdpp::arch_avx2::uint8<32>& simdpp::arch_avx2::uint8<32>::operator=(const simdpp::arch_avx2::any_vec<32, V>&) [with V = simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::expr_bit_and<simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::uint8<32> >, simdpp::arch_avx2::uint16<16, simdpp::arch_avx2::uint16<16> > > >]; libsimdpp-2.0-rc2/simdpp/detail/insn/unzip_lo.h:56:24: required from here; libsimdpp-2.0-rc2/simdpp/detail/cast.inl:40:13: error: void* memcpy(void*, const void*, size_t) copying an object of type class simdpp::arch_avx2::uint8<32> with private member simdpp::arch_avx2::uint8<32>::d_ from an array of const class simdpp::arc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3955
