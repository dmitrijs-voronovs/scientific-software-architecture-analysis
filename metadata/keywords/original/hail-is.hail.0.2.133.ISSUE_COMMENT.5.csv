id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/pull/8008#issuecomment-581053876:355,Testability,assert,assert,355,"actually, here's the test I think I want -- it probably won't segfault as written, but will write bad data:. ```python; mt1 = hl.import_vcf(resource('sample.vcf'), array_elements_required=True); mt2 = hl.import_vcf(resource('sample.vcf'), array_elements_required=False). mt3 = mt1.entries().union(mt2.entries()); mt4 = mt1.entries().union(mt2.entries()); assert mt3._same(mt3); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581053876
https://github.com/hail-is/hail/pull/8008#issuecomment-581984059:57,Testability,test,test,57,"Still not seeing an issue with the latest version of the test:. <img width=""472"" alt=""Screenshot 2020-02-04 11 07 51"" src=""https://user-images.githubusercontent.com/5543229/73762867-ac26f480-473e-11ea-8d62-d2f5a0aa1851.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581984059
https://github.com/hail-is/hail/pull/8008#issuecomment-582140545:168,Availability,failure,failure,168,"this is now failing (previously passed, no issues on output) due to subsetTo assertion in master. edit: Strange the subsetTo commit was made 19 days ago. The assertion failure originates from subsetTo, but must have been caused by something else. Ah, my local master branch was old.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-582140545
https://github.com/hail-is/hail/pull/8008#issuecomment-582140545:77,Testability,assert,assertion,77,"this is now failing (previously passed, no issues on output) due to subsetTo assertion in master. edit: Strange the subsetTo commit was made 19 days ago. The assertion failure originates from subsetTo, but must have been caused by something else. Ah, my local master branch was old.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-582140545
https://github.com/hail-is/hail/pull/8008#issuecomment-582140545:158,Testability,assert,assertion,158,"this is now failing (previously passed, no issues on output) due to subsetTo assertion in master. edit: Strange the subsetTo commit was made 19 days ago. The assertion failure originates from subsetTo, but must have been caused by something else. Ah, my local master branch was old.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-582140545
https://github.com/hail-is/hail/pull/8008#issuecomment-582143903:94,Testability,test,test,94,"Yeah, so if I set deepOptional in subsetTo in the case that `virtualType != t` but `isOfType` test passes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-582143903
https://github.com/hail-is/hail/issues/8009#issuecomment-580354782:244,Deployability,release,releases,244,"we hardcode the scala version to 2.11 at the moment:. https://github.com/hail-is/hail/blob/9ef5ebb362e70633a8e29b81768ad92f2853e6cb/hail/build.gradle#L37-L38. so this is pretty much to be expected. We'll make this flexible when Google Dataproc releases a GA image using 2.12, but could certainly document it before then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-580354782
https://github.com/hail-is/hail/issues/8009#issuecomment-580354782:214,Modifiability,flexible,flexible,214,"we hardcode the scala version to 2.11 at the moment:. https://github.com/hail-is/hail/blob/9ef5ebb362e70633a8e29b81768ad92f2853e6cb/hail/build.gradle#L37-L38. so this is pretty much to be expected. We'll make this flexible when Google Dataproc releases a GA image using 2.12, but could certainly document it before then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-580354782
https://github.com/hail-is/hail/issues/8009#issuecomment-580692854:296,Deployability,release,release-,296,"Got it; for my own future reference I'm going to link to the [Cloud Dataproc Image version list](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions) and note that the 1.5 series, currently in [preview](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.5), does use Scala 2.12 as of December 2019 according to the [release notes](https://cloud.google.com/dataproc/docs/release-notes). Also, there's an [issue](https://issuetracker.google.com/issues/132603281) on Google Cloud's insanely janky issue tracker for the Scala version upgrade.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-580692854
https://github.com/hail-is/hail/issues/8009#issuecomment-580692854:368,Deployability,release,release,368,"Got it; for my own future reference I'm going to link to the [Cloud Dataproc Image version list](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions) and note that the 1.5 series, currently in [preview](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.5), does use Scala 2.12 as of December 2019 according to the [release notes](https://cloud.google.com/dataproc/docs/release-notes). Also, there's an [issue](https://issuetracker.google.com/issues/132603281) on Google Cloud's insanely janky issue tracker for the Scala version upgrade.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-580692854
https://github.com/hail-is/hail/issues/8009#issuecomment-580692854:422,Deployability,release,release-notes,422,"Got it; for my own future reference I'm going to link to the [Cloud Dataproc Image version list](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions) and note that the 1.5 series, currently in [preview](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.5), does use Scala 2.12 as of December 2019 according to the [release notes](https://cloud.google.com/dataproc/docs/release-notes). Also, there's an [issue](https://issuetracker.google.com/issues/132603281) on Google Cloud's insanely janky issue tracker for the Scala version upgrade.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-580692854
https://github.com/hail-is/hail/issues/8009#issuecomment-580692854:582,Deployability,upgrade,upgrade,582,"Got it; for my own future reference I'm going to link to the [Cloud Dataproc Image version list](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions) and note that the 1.5 series, currently in [preview](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.5), does use Scala 2.12 as of December 2019 according to the [release notes](https://cloud.google.com/dataproc/docs/release-notes). Also, there's an [issue](https://issuetracker.google.com/issues/132603281) on Google Cloud's insanely janky issue tracker for the Scala version upgrade.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-580692854
https://github.com/hail-is/hail/issues/8009#issuecomment-613655651:53,Deployability,release,release,53,"We'll probably switch to 2.12 when there's a PySpark release with 2.12, which there isn't in the 2.4 series (aside from one patch version (2.4.2, nothing else). This has been moved to Asana for task scheduling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-613655651
https://github.com/hail-is/hail/issues/8009#issuecomment-613655651:124,Deployability,patch,patch,124,"We'll probably switch to 2.12 when there's a PySpark release with 2.12, which there isn't in the 2.4 series (aside from one patch version (2.4.2, nothing else). This has been moved to Asana for task scheduling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-613655651
https://github.com/hail-is/hail/issues/8009#issuecomment-613655651:199,Energy Efficiency,schedul,scheduling,199,"We'll probably switch to 2.12 when there's a PySpark release with 2.12, which there isn't in the 2.4 series (aside from one patch version (2.4.2, nothing else). This has been moved to Asana for task scheduling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-613655651
https://github.com/hail-is/hail/issues/8009#issuecomment-613667435:79,Usability,clear,clear,79,"we're thinking that GitHub issues should just be bug reports / problems with a clear fix that can be addressed with a maximum of a few commits. Feature requests should be in the forum, development discuss in in dev.hail.is or Zulip dev channel.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-613667435
https://github.com/hail-is/hail/pull/8012#issuecomment-580367676:68,Integrability,wrap,wrapped,68,"```scala; def emit(mbLike: EmitMethodBuilderLike): Code[Unit] =; // wrapped methods can't contain uses of Recur; useValues(mbLike.mb, ir.pType, mbLike.emit.emit(ir, env, EmitRegion.default(mbLike.emit.mb), container, None)); ```. Issue is Emit running before inference",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8012#issuecomment-580367676
https://github.com/hail-is/hail/pull/8012#issuecomment-580371999:79,Modifiability,refactor,refactoring,79,"hold off on this for now, regardless. It'll put me in rebase hell with the agg refactoring I'm doing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8012#issuecomment-580371999
https://github.com/hail-is/hail/pull/8013#issuecomment-580526563:25,Availability,failure,failures,25,some inference assertion failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8013#issuecomment-580526563
https://github.com/hail-is/hail/pull/8013#issuecomment-580526563:15,Testability,assert,assertion,15,some inference assertion failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8013#issuecomment-580526563
https://github.com/hail-is/hail/pull/8018#issuecomment-580526141:6,Deployability,deploy,deployed,6,I dev deployed it and this looks good.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8018#issuecomment-580526141
https://github.com/hail-is/hail/issues/8027#issuecomment-581552954:781,Security,access,accessors,781,"Adding a printout to the requirement, shows that the partitioner's key is not getting trimmed.; ```; java.lang.IllegalArgumentException: requirement failed: struct{locus: locus<GRCh38>} struct{locus: locus<GRCh38>, alleles: array<str>}; ```. ```diff; diff --git a/hail/src/main/scala/is/hail/rvd/RVD.scala b/hail/src/main/scala/is/hail/rvd/RVD.scala; index 88fdc84b3..dcf9a5773 100644; --- a/hail/src/main/scala/is/hail/rvd/RVD.scala; +++ b/hail/src/main/scala/is/hail/rvd/RVD.scala; @@ -43,7 +43,7 @@ class RVD(; self =>; require(crdd.getNumPartitions == partitioner.numPartitions); ; - require(typ.kType.virtualType isIsomorphicTo partitioner.kType); + require(typ.kType.virtualType isIsomorphicTo partitioner.kType, s""${typ.kType.virtualType} ${partitioner.kType}""); ; // Basic accessors; ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8027#issuecomment-581552954
https://github.com/hail-is/hail/pull/8028#issuecomment-586726766:60,Testability,benchmark,benchmark,60,will experiment with automatically generating TMP nodes and benchmark that.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8028#issuecomment-586726766
https://github.com/hail-is/hail/pull/8028#issuecomment-589006380:571,Modifiability,extend,extends,571,"> This node can be used to group together multiple nodes with lowering implementations; > let us generate a TableValue that can go into the tail of relational functions that will take longer to lower. Sorry, I didn't sleep well last night and I must be slow today. I don't think I understand either of these. Can you give me examples?. In thinking about how this is intended to be used, I'm actually starting to formulate a different picture: what I think we want is. ```; case class DistributedArray(; contexts: IR, globals: IR, cname: String, gname: String, body: IR); extends TableIR; ```. where DistributedArray has the same signature as CollectDistributedArray, but is a TableIR instead of a (value)IR and should be able to be rendered as an RVD. In particular, this is something we can construct from a TableStage during the lowering process when we hit something that can't be lowered. In LowreTableIR we'd have:. ```; case TableToTableApply(child, f) =>; TableToTableApply(lower(child).toDistributedArray, f); ```. where lower(child) returns a TableStage.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8028#issuecomment-589006380
https://github.com/hail-is/hail/pull/8028#issuecomment-589016449:264,Usability,clear,clear,264,"> In my proposal, the body would have to have a stream type. It does:; https://github.com/hail-is/hail/pull/8028/files#diff-beb69e35856952c39b4b4929ac5fb987R973. This design is very similar to what you're looking for. The right structure for the node wasn't super clear (we aren't super far along in lowering, so the needs aren't obvious) but I think the core piece that we need in any kind of lowering intermediate is code-generated iterators, which are implemented here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8028#issuecomment-589016449
https://github.com/hail-is/hail/pull/8028#issuecomment-589017978:164,Integrability,bridg,bridge,164,> This design is very similar to what you're looking for. It has the wrong input type (table). That's why I'm confused about how it is going to used. It can't be a bridge to lowered stuff because the input and output are both non-lowered. Table collect (and friends) gets you from TableIR to (value)IR and bridges in one direction. We need a node that goes in the other direction and this isn't it. I'm not debating the need for the iterator stuff.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8028#issuecomment-589017978
https://github.com/hail-is/hail/pull/8028#issuecomment-589017978:306,Integrability,bridg,bridges,306,> This design is very similar to what you're looking for. It has the wrong input type (table). That's why I'm confused about how it is going to used. It can't be a bridge to lowered stuff because the input and output are both non-lowered. Table collect (and friends) gets you from TableIR to (value)IR and bridges in one direction. We need a node that goes in the other direction and this isn't it. I'm not debating the need for the iterator stuff.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8028#issuecomment-589017978
https://github.com/hail-is/hail/pull/8030#issuecomment-582661801:17,Deployability,install,installs,17,"The worker image installs `docker/requirements.txt` which pip installs `googlecloudprofiler` which requires gcc. I could probably figure out the actual requirements of the worker image and list those separately. That seems prudent anyway. However, in the long term, if we want to profiler the workers, we'd need build-essential. Installing build-essential adds 204MB.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8030#issuecomment-582661801
https://github.com/hail-is/hail/pull/8030#issuecomment-582661801:62,Deployability,install,installs,62,"The worker image installs `docker/requirements.txt` which pip installs `googlecloudprofiler` which requires gcc. I could probably figure out the actual requirements of the worker image and list those separately. That seems prudent anyway. However, in the long term, if we want to profiler the workers, we'd need build-essential. Installing build-essential adds 204MB.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8030#issuecomment-582661801
https://github.com/hail-is/hail/pull/8030#issuecomment-582661801:329,Deployability,Install,Installing,329,"The worker image installs `docker/requirements.txt` which pip installs `googlecloudprofiler` which requires gcc. I could probably figure out the actual requirements of the worker image and list those separately. That seems prudent anyway. However, in the long term, if we want to profiler the workers, we'd need build-essential. Installing build-essential adds 204MB.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8030#issuecomment-582661801
https://github.com/hail-is/hail/pull/8031#issuecomment-582095526:40,Testability,test,test,40,"yeah, saw the same thing after adding a test, thanks for catching!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8031#issuecomment-582095526
https://github.com/hail-is/hail/pull/8038#issuecomment-585830759:33,Testability,test,tests,33,"Finally got around to adding the tests, should be good now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8038#issuecomment-585830759
https://github.com/hail-is/hail/issues/8041#issuecomment-582174317:172,Safety,sanity check,sanity check,172,similar:; ```; >>> mt.aggregate_entries(hl.agg.fraction(mt.GQ > 20)); 0.97; >>> mt = mt.filter_entries(mt.GQ > 20); >>> mt.aggregate_entries(hl.agg.fraction(mt.GQ > 20)) # sanity check; 0.97; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8041#issuecomment-582174317
https://github.com/hail-is/hail/issues/8041#issuecomment-582177354:279,Deployability,update,update,279,"Cross-posting from Zulip:. Perhaps unfilter_entries should take a condition and values for the fields:. ```; mt = mt.unfilter_entries(hl.is_defined(mt2[mt.row_key, mt.col_key]); foo = mt2[mt.row_key, mt.col_key].foo); ```. This is relevant to a situation where Laurent wanted to update the filtered cells based on data from another matrix table.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8041#issuecomment-582177354
https://github.com/hail-is/hail/pull/8045#issuecomment-582620130:86,Testability,log,logs,86,I should also note that I've already done this and verified that my IP appears in the logs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8045#issuecomment-582620130
https://github.com/hail-is/hail/issues/8047#issuecomment-582646516:9,Safety,timeout,timeout,9,here's a timeout:; ```; # time curl -vvv https://ci.hail.is; * Rebuilt URL to: https://ci.hail.is/; * Trying 35.188.91.25...; * TCP_NODELAY set; * Connection failed; * connect to 35.188.91.25 port 443 failed: Operation timed out; * Failed to connect to ci.hail.is port 443: Operation timed out; * Closing connection 0; curl: (7) Failed to connect to ci.hail.is port 443: Operation timed out; curl -vvv https://ci.hail.is 0.01s user 0.01s system 0% cpu 1:15.36 total; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8047#issuecomment-582646516
https://github.com/hail-is/hail/issues/8047#issuecomment-582651289:366,Availability,downtime,downtime,366,"The last link has the right answer. For reasons not known to me, you must destroy the service and recreate the service to get correct behavior. You know you have correct behavior when the TCP Load Balancer in the GCP console shows most of your instances as unhealthy (because most of them are not hosting the service in question). This lead to at most 15 minutes of downtime and probably like 10 minutes, which seems unacceptable to me, but ðŸ¤·â€â™€",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8047#issuecomment-582651289
https://github.com/hail-is/hail/issues/8047#issuecomment-582651289:192,Performance,Load,Load,192,"The last link has the right answer. For reasons not known to me, you must destroy the service and recreate the service to get correct behavior. You know you have correct behavior when the TCP Load Balancer in the GCP console shows most of your instances as unhealthy (because most of them are not hosting the service in question). This lead to at most 15 minutes of downtime and probably like 10 minutes, which seems unacceptable to me, but ðŸ¤·â€â™€",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8047#issuecomment-582651289
https://github.com/hail-is/hail/pull/8049#issuecomment-592050430:54,Integrability,wrap,wrapper,54,I switched the docker retry function back to having a wrapper/curried because I got a clash with the kwargs for the name parameter in one function and figured this was safer then trying to rename the parameter to something that will never clash.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8049#issuecomment-592050430
https://github.com/hail-is/hail/pull/8049#issuecomment-592050430:168,Safety,safe,safer,168,I switched the docker retry function back to having a wrapper/curried because I got a clash with the kwargs for the name parameter in one function and figured this was safer then trying to rename the parameter to something that will never clash.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8049#issuecomment-592050430
https://github.com/hail-is/hail/pull/8050#issuecomment-583033913:94,Availability,checkpoint,checkpoint,94,"Alright, I'll look at that. I was copying pc_relate above, which does the balding nichols and checkpoint process as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583033913
https://github.com/hail-is/hail/pull/8050#issuecomment-583136594:225,Testability,Test,Tests,225,"I addressed the comment and made a data source that's a GB (defined like: . ```; n_rows = 60_000; n_cols = 4_000; mt = hl.utils.range_matrix_table(n_rows, n_cols); mt = mt.annotate_entries(unif = hl.rand_unif(0, 1)); ```; ). Tests only take like 20 seconds though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583136594
https://github.com/hail-is/hail/pull/8050#issuecomment-583409119:49,Testability,benchmark,benchmarks,49,"The java process that gets created in htop while benchmarks are running goes over 100% CPU usage. Seen it go up to 300%. So yeah, seems like veclib (Apple BLAS) is probably multithreading (or Spark is parallelizing stuff)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583409119
https://github.com/hail-is/hail/pull/8050#issuecomment-583412384:341,Modifiability,variab,variable,341,"Looks like we probably want to add the following to benchmark somewhere:. ```; export MKL_NUM_THREADS=1; export NUMEXPR_NUM_THREADS=1; export OPENBLAS_NUM_THREADS=1; export OMP_NUM_THREADS=1; export VECLIB_MAXIMUM_THREADS=1; ```. Trying to test if setting veclib lower fixes things, but apparently Apple caches the result of the environment variable somewhere so it's unclear whether me setting it is working",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583412384
https://github.com/hail-is/hail/pull/8050#issuecomment-583412384:304,Performance,cache,caches,304,"Looks like we probably want to add the following to benchmark somewhere:. ```; export MKL_NUM_THREADS=1; export NUMEXPR_NUM_THREADS=1; export OPENBLAS_NUM_THREADS=1; export OMP_NUM_THREADS=1; export VECLIB_MAXIMUM_THREADS=1; ```. Trying to test if setting veclib lower fixes things, but apparently Apple caches the result of the environment variable somewhere so it's unclear whether me setting it is working",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583412384
https://github.com/hail-is/hail/pull/8050#issuecomment-583412384:52,Testability,benchmark,benchmark,52,"Looks like we probably want to add the following to benchmark somewhere:. ```; export MKL_NUM_THREADS=1; export NUMEXPR_NUM_THREADS=1; export OPENBLAS_NUM_THREADS=1; export OMP_NUM_THREADS=1; export VECLIB_MAXIMUM_THREADS=1; ```. Trying to test if setting veclib lower fixes things, but apparently Apple caches the result of the environment variable somewhere so it's unclear whether me setting it is working",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583412384
https://github.com/hail-is/hail/pull/8050#issuecomment-583412384:240,Testability,test,test,240,"Looks like we probably want to add the following to benchmark somewhere:. ```; export MKL_NUM_THREADS=1; export NUMEXPR_NUM_THREADS=1; export OPENBLAS_NUM_THREADS=1; export OMP_NUM_THREADS=1; export VECLIB_MAXIMUM_THREADS=1; ```. Trying to test if setting veclib lower fixes things, but apparently Apple caches the result of the environment variable somewhere so it's unclear whether me setting it is working",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583412384
https://github.com/hail-is/hail/pull/8050#issuecomment-583414827:93,Testability,benchmark,benchmark,93,will go here:; https://github.com/hail-is/hail/blob/18b57caf7779310eb0cbc2a70f47eda06fb037c5/benchmark/scripts/benchmark_in_pipeline.py#L56. Can you add those? Thanks!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583414827
https://github.com/hail-is/hail/pull/8050#issuecomment-583417425:97,Testability,benchmark,benchmark,97,Why do we have to add them ourselves in shell rc for mac? Presumably we can also set them in the benchmark code when running benchmarks locally?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583417425
https://github.com/hail-is/hail/pull/8050#issuecomment-583417425:125,Testability,benchmark,benchmarks,125,Why do we have to add them ourselves in shell rc for mac? Presumably we can also set them in the benchmark code when running benchmarks locally?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583417425
https://github.com/hail-is/hail/pull/8050#issuecomment-583418657:28,Modifiability,variab,variables,28,I'm not sure how to set env variables for the Hail java process from Python. I'm sure it can be done,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583418657
https://github.com/hail-is/hail/pull/8050#issuecomment-583434753:116,Deployability,configurat,configuration,116,"Hmm, looks like it's something like `spark.executorEnv.FOO=...` based on this: https://spark.apache.org/docs/latest/configuration.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583434753
https://github.com/hail-is/hail/pull/8050#issuecomment-583434753:116,Modifiability,config,configuration,116,"Hmm, looks like it's something like `spark.executorEnv.FOO=...` based on this: https://spark.apache.org/docs/latest/configuration.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583434753
https://github.com/hail-is/hail/pull/8050#issuecomment-583451801:315,Availability,down,down,315,"Sure, did that, though I kind of want a way to verify that it worked. Watching top doesn't seem super scientific. Linear regression still only takes like 20 seconds on my laptop. Watching top I see that the cpu usage spikes to 300% for a second at the beginning of each of the benchmark iterations, then falls back down to somewhere between 100% and 110% for the duration of the 20 seconds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583451801
https://github.com/hail-is/hail/pull/8050#issuecomment-583451801:277,Testability,benchmark,benchmark,277,"Sure, did that, though I kind of want a way to verify that it worked. Watching top doesn't seem super scientific. Linear regression still only takes like 20 seconds on my laptop. Watching top I see that the cpu usage spikes to 300% for a second at the beginning of each of the benchmark iterations, then falls back down to somewhere between 100% and 110% for the duration of the 20 seconds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583451801
https://github.com/hail-is/hail/pull/8050#issuecomment-583453677:161,Energy Efficiency,schedul,scheduler,161,"Yeah, it's all in the same JVM process so those variables would have no effect: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala#L97",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583453677
https://github.com/hail-is/hail/pull/8050#issuecomment-583453677:48,Modifiability,variab,variables,48,"Yeah, it's all in the same JVM process so those variables would have no effect: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala#L97",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583453677
https://github.com/hail-is/hail/pull/8050#issuecomment-583459034:200,Deployability,pipeline,pipeline,200,"Yep, sigh. And I don't want to specify these args on the master node since that would break workflows that do use LMMs and do svd on master node. So I guess I'll add them specifically for the hail in pipeline docker based benchmarking system since we know that's running in local mode and actually want that to have only one core. Given that, should I still leave this executor arguments in place or pull them back out?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583459034
https://github.com/hail-is/hail/pull/8050#issuecomment-583459034:222,Testability,benchmark,benchmarking,222,"Yep, sigh. And I don't want to specify these args on the master node since that would break workflows that do use LMMs and do svd on master node. So I guess I'll add them specifically for the hail in pipeline docker based benchmarking system since we know that's running in local mode and actually want that to have only one core. Given that, should I still leave this executor arguments in place or pull them back out?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583459034
https://github.com/hail-is/hail/pull/8050#issuecomment-583459628:22,Testability,test,test,22,Use os.environ in the test (and remember to set it back to its old value it afterward),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583459628
https://github.com/hail-is/hail/pull/8050#issuecomment-583461086:52,Testability,benchmark,benchmark,52,"Ah, edit them specifically in the linear regression benchmark test? Is there a reason we'd want that as opposed to just setting them in the benchmark suite's dockerfile or something?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583461086
https://github.com/hail-is/hail/pull/8050#issuecomment-583461086:62,Testability,test,test,62,"Ah, edit them specifically in the linear regression benchmark test? Is there a reason we'd want that as opposed to just setting them in the benchmark suite's dockerfile or something?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583461086
https://github.com/hail-is/hail/pull/8050#issuecomment-583461086:140,Testability,benchmark,benchmark,140,"Ah, edit them specifically in the linear regression benchmark test? Is there a reason we'd want that as opposed to just setting them in the benchmark suite's dockerfile or something?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583461086
https://github.com/hail-is/hail/pull/8050#issuecomment-583462187:276,Energy Efficiency,allocate,allocate,276,"> Use os.environ in the test (and remember to set it back to its old value it afterward). This won't work. Hail has already been initialized. We definitely want to set these for every benchmark, at least for now -- this could be responsible for docker blowing limits (we only allocate 1.5 cores, I think)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583462187
https://github.com/hail-is/hail/pull/8050#issuecomment-583462187:24,Testability,test,test,24,"> Use os.environ in the test (and remember to set it back to its old value it afterward). This won't work. Hail has already been initialized. We definitely want to set these for every benchmark, at least for now -- this could be responsible for docker blowing limits (we only allocate 1.5 cores, I think)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583462187
https://github.com/hail-is/hail/pull/8050#issuecomment-583462187:184,Testability,benchmark,benchmark,184,"> Use os.environ in the test (and remember to set it back to its old value it afterward). This won't work. Hail has already been initialized. We definitely want to set these for every benchmark, at least for now -- this could be responsible for docker blowing limits (we only allocate 1.5 cores, I think)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583462187
https://github.com/hail-is/hail/pull/8050#issuecomment-583464027:7,Security,expose,expose,7,Can we expose a HailContext.setEnv then? That seems like the right thing to do,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583464027
https://github.com/hail-is/hail/pull/8050#issuecomment-583465417:26,Testability,benchmark,benchmarks,26,"we only use 1 core in the benchmarks anyway -- I think the right solution is to set the executor env by default, set them all in the docker script for benchmarks, and not worry too much about non-benchmark local mode for now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583465417
https://github.com/hail-is/hail/pull/8050#issuecomment-583465417:151,Testability,benchmark,benchmarks,151,"we only use 1 core in the benchmarks anyway -- I think the right solution is to set the executor env by default, set them all in the docker script for benchmarks, and not worry too much about non-benchmark local mode for now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583465417
https://github.com/hail-is/hail/pull/8050#issuecomment-583465417:196,Testability,benchmark,benchmark,196,"we only use 1 core in the benchmarks anyway -- I think the right solution is to set the executor env by default, set them all in the docker script for benchmarks, and not worry too much about non-benchmark local mode for now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583465417
https://github.com/hail-is/hail/pull/8050#issuecomment-583468345:123,Testability,benchmark,benchmark,123,I think I'm just setting the docker script ones for now in this PR. Anything else seems out of scope of adding this random benchmark,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583468345
https://github.com/hail-is/hail/pull/8050#issuecomment-583557445:95,Deployability,pipeline,pipeline,95,Ok @tpoterba I added the environment variables where I think you originally wanted them in the pipeline task.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583557445
https://github.com/hail-is/hail/pull/8050#issuecomment-583557445:37,Modifiability,variab,variables,37,Ok @tpoterba I added the environment variables where I think you originally wanted them in the pipeline task.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583557445
https://github.com/hail-is/hail/pull/8050#issuecomment-583572766:194,Deployability,release,released,194,"Yeah, let's make that a separate thing, I want to like SSH into other nodes and verify it's doing what I thought, and I was also reluctant to add a new option to the HailContext right before we released 0.2.32 in case it slid in first",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583572766
https://github.com/hail-is/hail/pull/8052#issuecomment-583226388:51,Deployability,deploy,deploying,51,"No state. I printed the session in the logs (I was deploying into prod to test while I had a broken cookie). My session had nothing set except that it was marked as created on January 7th. This despite that I saw a log statement from when I hit /login,m that clearly showed me session with all the right values. My guess is that there was some old signing key or somehow the session got corrupted so you can decode it to get an empty session and add new fields but they fail to be written back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052#issuecomment-583226388
https://github.com/hail-is/hail/pull/8052#issuecomment-583226388:39,Testability,log,logs,39,"No state. I printed the session in the logs (I was deploying into prod to test while I had a broken cookie). My session had nothing set except that it was marked as created on January 7th. This despite that I saw a log statement from when I hit /login,m that clearly showed me session with all the right values. My guess is that there was some old signing key or somehow the session got corrupted so you can decode it to get an empty session and add new fields but they fail to be written back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052#issuecomment-583226388
https://github.com/hail-is/hail/pull/8052#issuecomment-583226388:74,Testability,test,test,74,"No state. I printed the session in the logs (I was deploying into prod to test while I had a broken cookie). My session had nothing set except that it was marked as created on January 7th. This despite that I saw a log statement from when I hit /login,m that clearly showed me session with all the right values. My guess is that there was some old signing key or somehow the session got corrupted so you can decode it to get an empty session and add new fields but they fail to be written back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052#issuecomment-583226388
https://github.com/hail-is/hail/pull/8052#issuecomment-583226388:215,Testability,log,log,215,"No state. I printed the session in the logs (I was deploying into prod to test while I had a broken cookie). My session had nothing set except that it was marked as created on January 7th. This despite that I saw a log statement from when I hit /login,m that clearly showed me session with all the right values. My guess is that there was some old signing key or somehow the session got corrupted so you can decode it to get an empty session and add new fields but they fail to be written back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052#issuecomment-583226388
https://github.com/hail-is/hail/pull/8052#issuecomment-583226388:246,Testability,log,login,246,"No state. I printed the session in the logs (I was deploying into prod to test while I had a broken cookie). My session had nothing set except that it was marked as created on January 7th. This despite that I saw a log statement from when I hit /login,m that clearly showed me session with all the right values. My guess is that there was some old signing key or somehow the session got corrupted so you can decode it to get an empty session and add new fields but they fail to be written back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052#issuecomment-583226388
https://github.com/hail-is/hail/pull/8052#issuecomment-583226388:259,Usability,clear,clearly,259,"No state. I printed the session in the logs (I was deploying into prod to test while I had a broken cookie). My session had nothing set except that it was marked as created on January 7th. This despite that I saw a log statement from when I hit /login,m that clearly showed me session with all the right values. My guess is that there was some old signing key or somehow the session got corrupted so you can decode it to get an empty session and add new fields but they fail to be written back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8052#issuecomment-583226388
https://github.com/hail-is/hail/issues/8053#issuecomment-583445035:39,Safety,Timeout,Timeouts,39,So google just doesn't ever retry Read Timeouts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053#issuecomment-583445035
https://github.com/hail-is/hail/issues/8053#issuecomment-588390042:68,Availability,error,error,68,This should be fixed by #8103 etc. Reopen this issue if you see the error again.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8053#issuecomment-588390042
https://github.com/hail-is/hail/pull/8057#issuecomment-583558504:112,Testability,test,tests,112,"I can't easily tell whether the change to emitted code does not change behavior, looked a bit, so will wait for tests, but besides that, looks good",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8057#issuecomment-583558504
https://github.com/hail-is/hail/pull/8057#issuecomment-583559299:14,Performance,load,loads,14,"yeah, we have loads of tests go through this path. If Python tests pass, I'm quite satisfied this is correct!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8057#issuecomment-583559299
https://github.com/hail-is/hail/pull/8057#issuecomment-583559299:23,Testability,test,tests,23,"yeah, we have loads of tests go through this path. If Python tests pass, I'm quite satisfied this is correct!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8057#issuecomment-583559299
https://github.com/hail-is/hail/pull/8057#issuecomment-583559299:61,Testability,test,tests,61,"yeah, we have loads of tests go through this path. If Python tests pass, I'm quite satisfied this is correct!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8057#issuecomment-583559299
https://github.com/hail-is/hail/pull/8058#issuecomment-583565691:37,Integrability,rout,router,37,I copied `nginx.conf` from a running router and then trimmed out the comments and made our changes (see `access_log` and `log_format`).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8058#issuecomment-583565691
https://github.com/hail-is/hail/pull/8063#issuecomment-583773630:314,Availability,error,error,314,"Still contains debug messages, and needs rebase. All will be fixed after tests pass. Remaining tests not passing are:; <img width=""358"" alt=""Screenshot 2020-02-08 15 17 58"" src=""https://user-images.githubusercontent.com/5543229/74091636-8162d600-4a87-11ea-9750-f2804352d4a3.png"">. Each of these fails with a match error in Emit, either on MakeStream, or StreamRange",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583773630
https://github.com/hail-is/hail/pull/8063#issuecomment-583773630:21,Integrability,message,messages,21,"Still contains debug messages, and needs rebase. All will be fixed after tests pass. Remaining tests not passing are:; <img width=""358"" alt=""Screenshot 2020-02-08 15 17 58"" src=""https://user-images.githubusercontent.com/5543229/74091636-8162d600-4a87-11ea-9750-f2804352d4a3.png"">. Each of these fails with a match error in Emit, either on MakeStream, or StreamRange",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583773630
https://github.com/hail-is/hail/pull/8063#issuecomment-583773630:73,Testability,test,tests,73,"Still contains debug messages, and needs rebase. All will be fixed after tests pass. Remaining tests not passing are:; <img width=""358"" alt=""Screenshot 2020-02-08 15 17 58"" src=""https://user-images.githubusercontent.com/5543229/74091636-8162d600-4a87-11ea-9750-f2804352d4a3.png"">. Each of these fails with a match error in Emit, either on MakeStream, or StreamRange",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583773630
https://github.com/hail-is/hail/pull/8063#issuecomment-583773630:95,Testability,test,tests,95,"Still contains debug messages, and needs rebase. All will be fixed after tests pass. Remaining tests not passing are:; <img width=""358"" alt=""Screenshot 2020-02-08 15 17 58"" src=""https://user-images.githubusercontent.com/5543229/74091636-8162d600-4a87-11ea-9750-f2804352d4a3.png"">. Each of these fails with a match error in Emit, either on MakeStream, or StreamRange",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583773630
https://github.com/hail-is/hail/pull/8063#issuecomment-583795904:166,Availability,failure,failure,166,"Did my best, the behavior of this lowering functionality is complex, and it's hard to come up with a universal solution. The current issue I'm struggling with is the failure it testArrayAggContexts, which finds a ToArray(StreamRange()) being passed to EmitStream, instead of a StreamRange. TLDR: my ArrayAgg rule is stupid and fucked. Options: 1) Fix this rule, 2) (Seems not as good) In Emit have ArrayAgg needs to pass its child through Emit.emit a second time to match on ToArray, 3) make unstreamify more specific, such that ArrayAgg is allowed to take streams directly. ToArray definitely needs to wrap StreamRange in some cases (for instance MakeTuple(ToArray(StreamRange)), else get issues with the stream passed to SRVB. it would be helpful to have the intended (but currently applicable) design of stream/array semantics written down for all nodes (maybe it exists, I'll dig through design docs). Besides this 2 more failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583795904
https://github.com/hail-is/hail/pull/8063#issuecomment-583795904:838,Availability,down,down,838,"Did my best, the behavior of this lowering functionality is complex, and it's hard to come up with a universal solution. The current issue I'm struggling with is the failure it testArrayAggContexts, which finds a ToArray(StreamRange()) being passed to EmitStream, instead of a StreamRange. TLDR: my ArrayAgg rule is stupid and fucked. Options: 1) Fix this rule, 2) (Seems not as good) In Emit have ArrayAgg needs to pass its child through Emit.emit a second time to match on ToArray, 3) make unstreamify more specific, such that ArrayAgg is allowed to take streams directly. ToArray definitely needs to wrap StreamRange in some cases (for instance MakeTuple(ToArray(StreamRange)), else get issues with the stream passed to SRVB. it would be helpful to have the intended (but currently applicable) design of stream/array semantics written down for all nodes (maybe it exists, I'll dig through design docs). Besides this 2 more failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583795904
https://github.com/hail-is/hail/pull/8063#issuecomment-583795904:926,Availability,failure,failures,926,"Did my best, the behavior of this lowering functionality is complex, and it's hard to come up with a universal solution. The current issue I'm struggling with is the failure it testArrayAggContexts, which finds a ToArray(StreamRange()) being passed to EmitStream, instead of a StreamRange. TLDR: my ArrayAgg rule is stupid and fucked. Options: 1) Fix this rule, 2) (Seems not as good) In Emit have ArrayAgg needs to pass its child through Emit.emit a second time to match on ToArray, 3) make unstreamify more specific, such that ArrayAgg is allowed to take streams directly. ToArray definitely needs to wrap StreamRange in some cases (for instance MakeTuple(ToArray(StreamRange)), else get issues with the stream passed to SRVB. it would be helpful to have the intended (but currently applicable) design of stream/array semantics written down for all nodes (maybe it exists, I'll dig through design docs). Besides this 2 more failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583795904
https://github.com/hail-is/hail/pull/8063#issuecomment-583795904:603,Integrability,wrap,wrap,603,"Did my best, the behavior of this lowering functionality is complex, and it's hard to come up with a universal solution. The current issue I'm struggling with is the failure it testArrayAggContexts, which finds a ToArray(StreamRange()) being passed to EmitStream, instead of a StreamRange. TLDR: my ArrayAgg rule is stupid and fucked. Options: 1) Fix this rule, 2) (Seems not as good) In Emit have ArrayAgg needs to pass its child through Emit.emit a second time to match on ToArray, 3) make unstreamify more specific, such that ArrayAgg is allowed to take streams directly. ToArray definitely needs to wrap StreamRange in some cases (for instance MakeTuple(ToArray(StreamRange)), else get issues with the stream passed to SRVB. it would be helpful to have the intended (but currently applicable) design of stream/array semantics written down for all nodes (maybe it exists, I'll dig through design docs). Besides this 2 more failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583795904
https://github.com/hail-is/hail/pull/8063#issuecomment-583795904:177,Testability,test,testArrayAggContexts,177,"Did my best, the behavior of this lowering functionality is complex, and it's hard to come up with a universal solution. The current issue I'm struggling with is the failure it testArrayAggContexts, which finds a ToArray(StreamRange()) being passed to EmitStream, instead of a StreamRange. TLDR: my ArrayAgg rule is stupid and fucked. Options: 1) Fix this rule, 2) (Seems not as good) In Emit have ArrayAgg needs to pass its child through Emit.emit a second time to match on ToArray, 3) make unstreamify more specific, such that ArrayAgg is allowed to take streams directly. ToArray definitely needs to wrap StreamRange in some cases (for instance MakeTuple(ToArray(StreamRange)), else get issues with the stream passed to SRVB. it would be helpful to have the intended (but currently applicable) design of stream/array semantics written down for all nodes (maybe it exists, I'll dig through design docs). Besides this 2 more failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583795904
https://github.com/hail-is/hail/pull/8063#issuecomment-583797420:87,Testability,test,testArrayAggContexts,87,"now it looks like ArrayAgg isn't matched on in LowerArrayToStream, despite existing in testArrayAggContexts with an ArrayRange child. Will look more tomorrow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583797420
https://github.com/hail-is/hail/pull/8063#issuecomment-583803556:505,Availability,down,down,505,"> Options: 1) Fix this rule, 2) (Seems not as good) In Emit have ArrayAgg needs to pass its child through Emit.emit a second time to match on ToArray, 3) make unstreamify more specific, such that ArrayAgg is allowed to take streams directly. ToArray definitely needs to wrap StreamRange in some cases (for instance MakeTuple(ToArray(StreamRange)), else get issues with the stream passed to SRVB. it would be helpful to have the intended (but currently applicable) design of stream/array semantics written down for all nodes (maybe it exists, I'll dig through design docs). Besides this 2 more failures. Option 4: rebase on master, where ArrayAgg is not an emittable node (only RunAgg / RunAggScan) :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583803556
https://github.com/hail-is/hail/pull/8063#issuecomment-583803556:593,Availability,failure,failures,593,"> Options: 1) Fix this rule, 2) (Seems not as good) In Emit have ArrayAgg needs to pass its child through Emit.emit a second time to match on ToArray, 3) make unstreamify more specific, such that ArrayAgg is allowed to take streams directly. ToArray definitely needs to wrap StreamRange in some cases (for instance MakeTuple(ToArray(StreamRange)), else get issues with the stream passed to SRVB. it would be helpful to have the intended (but currently applicable) design of stream/array semantics written down for all nodes (maybe it exists, I'll dig through design docs). Besides this 2 more failures. Option 4: rebase on master, where ArrayAgg is not an emittable node (only RunAgg / RunAggScan) :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583803556
https://github.com/hail-is/hail/pull/8063#issuecomment-583803556:270,Integrability,wrap,wrap,270,"> Options: 1) Fix this rule, 2) (Seems not as good) In Emit have ArrayAgg needs to pass its child through Emit.emit a second time to match on ToArray, 3) make unstreamify more specific, such that ArrayAgg is allowed to take streams directly. ToArray definitely needs to wrap StreamRange in some cases (for instance MakeTuple(ToArray(StreamRange)), else get issues with the stream passed to SRVB. it would be helpful to have the intended (but currently applicable) design of stream/array semantics written down for all nodes (maybe it exists, I'll dig through design docs). Besides this 2 more failures. Option 4: rebase on master, where ArrayAgg is not an emittable node (only RunAgg / RunAggScan) :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583803556
https://github.com/hail-is/hail/pull/8063#issuecomment-583812669:307,Integrability,Wrap,WrappedArray,307,"After merge, addition of Let, RunAggScan, and ArrayFold2 all tests pass besides testMakeArrayWithDifferentRequiredeness (result mismatch). Will fix tomorrow. org.scalatest.exceptions.TestFailedException: t.valuesSimilar(res, expected, t.valuesSimilar$default$3, t.valuesSimilar$default$4) was false; result=WrappedArray(null, [0,null]); expect=WrappedArray(null, [2,WrappedArray(1)]); strategy=JvmCompile). However, I'm still concerned about the complexity of the lowering step, for instance, the Let should not be needed, but in practice was. Also need to understand why/whether the other IR, besides the child, of ArrayFold, and ArrayFold2 need/do not need to be streamified. Followed Streamify.ArrayFold here, and investigated some (for instance in ArrayFold2 streamifying the seq IR breaks typecheck), but should be more principled. edit: One difference from master run of this test is that we're generating MakeStream instead of MakeArray. for:. MakeTuple(ArrayBuffer((0,MakeArray(ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),0), GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),1)),array<struct{a: int32, b: array<int32>}>))))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583812669
https://github.com/hail-is/hail/pull/8063#issuecomment-583812669:344,Integrability,Wrap,WrappedArray,344,"After merge, addition of Let, RunAggScan, and ArrayFold2 all tests pass besides testMakeArrayWithDifferentRequiredeness (result mismatch). Will fix tomorrow. org.scalatest.exceptions.TestFailedException: t.valuesSimilar(res, expected, t.valuesSimilar$default$3, t.valuesSimilar$default$4) was false; result=WrappedArray(null, [0,null]); expect=WrappedArray(null, [2,WrappedArray(1)]); strategy=JvmCompile). However, I'm still concerned about the complexity of the lowering step, for instance, the Let should not be needed, but in practice was. Also need to understand why/whether the other IR, besides the child, of ArrayFold, and ArrayFold2 need/do not need to be streamified. Followed Streamify.ArrayFold here, and investigated some (for instance in ArrayFold2 streamifying the seq IR breaks typecheck), but should be more principled. edit: One difference from master run of this test is that we're generating MakeStream instead of MakeArray. for:. MakeTuple(ArrayBuffer((0,MakeArray(ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),0), GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),1)),array<struct{a: int32, b: array<int32>}>))))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583812669
https://github.com/hail-is/hail/pull/8063#issuecomment-583812669:366,Integrability,Wrap,WrappedArray,366,"After merge, addition of Let, RunAggScan, and ArrayFold2 all tests pass besides testMakeArrayWithDifferentRequiredeness (result mismatch). Will fix tomorrow. org.scalatest.exceptions.TestFailedException: t.valuesSimilar(res, expected, t.valuesSimilar$default$3, t.valuesSimilar$default$4) was false; result=WrappedArray(null, [0,null]); expect=WrappedArray(null, [2,WrappedArray(1)]); strategy=JvmCompile). However, I'm still concerned about the complexity of the lowering step, for instance, the Let should not be needed, but in practice was. Also need to understand why/whether the other IR, besides the child, of ArrayFold, and ArrayFold2 need/do not need to be streamified. Followed Streamify.ArrayFold here, and investigated some (for instance in ArrayFold2 streamifying the seq IR breaks typecheck), but should be more principled. edit: One difference from master run of this test is that we're generating MakeStream instead of MakeArray. for:. MakeTuple(ArrayBuffer((0,MakeArray(ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),0), GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),1)),array<struct{a: int32, b: array<int32>}>))))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583812669
https://github.com/hail-is/hail/pull/8063#issuecomment-583812669:61,Testability,test,tests,61,"After merge, addition of Let, RunAggScan, and ArrayFold2 all tests pass besides testMakeArrayWithDifferentRequiredeness (result mismatch). Will fix tomorrow. org.scalatest.exceptions.TestFailedException: t.valuesSimilar(res, expected, t.valuesSimilar$default$3, t.valuesSimilar$default$4) was false; result=WrappedArray(null, [0,null]); expect=WrappedArray(null, [2,WrappedArray(1)]); strategy=JvmCompile). However, I'm still concerned about the complexity of the lowering step, for instance, the Let should not be needed, but in practice was. Also need to understand why/whether the other IR, besides the child, of ArrayFold, and ArrayFold2 need/do not need to be streamified. Followed Streamify.ArrayFold here, and investigated some (for instance in ArrayFold2 streamifying the seq IR breaks typecheck), but should be more principled. edit: One difference from master run of this test is that we're generating MakeStream instead of MakeArray. for:. MakeTuple(ArrayBuffer((0,MakeArray(ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),0), GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),1)),array<struct{a: int32, b: array<int32>}>))))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583812669
https://github.com/hail-is/hail/pull/8063#issuecomment-583812669:80,Testability,test,testMakeArrayWithDifferentRequiredeness,80,"After merge, addition of Let, RunAggScan, and ArrayFold2 all tests pass besides testMakeArrayWithDifferentRequiredeness (result mismatch). Will fix tomorrow. org.scalatest.exceptions.TestFailedException: t.valuesSimilar(res, expected, t.valuesSimilar$default$3, t.valuesSimilar$default$4) was false; result=WrappedArray(null, [0,null]); expect=WrappedArray(null, [2,WrappedArray(1)]); strategy=JvmCompile). However, I'm still concerned about the complexity of the lowering step, for instance, the Let should not be needed, but in practice was. Also need to understand why/whether the other IR, besides the child, of ArrayFold, and ArrayFold2 need/do not need to be streamified. Followed Streamify.ArrayFold here, and investigated some (for instance in ArrayFold2 streamifying the seq IR breaks typecheck), but should be more principled. edit: One difference from master run of this test is that we're generating MakeStream instead of MakeArray. for:. MakeTuple(ArrayBuffer((0,MakeArray(ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),0), GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),1)),array<struct{a: int32, b: array<int32>}>))))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583812669
https://github.com/hail-is/hail/pull/8063#issuecomment-583812669:183,Testability,Test,TestFailedException,183,"After merge, addition of Let, RunAggScan, and ArrayFold2 all tests pass besides testMakeArrayWithDifferentRequiredeness (result mismatch). Will fix tomorrow. org.scalatest.exceptions.TestFailedException: t.valuesSimilar(res, expected, t.valuesSimilar$default$3, t.valuesSimilar$default$4) was false; result=WrappedArray(null, [0,null]); expect=WrappedArray(null, [2,WrappedArray(1)]); strategy=JvmCompile). However, I'm still concerned about the complexity of the lowering step, for instance, the Let should not be needed, but in practice was. Also need to understand why/whether the other IR, besides the child, of ArrayFold, and ArrayFold2 need/do not need to be streamified. Followed Streamify.ArrayFold here, and investigated some (for instance in ArrayFold2 streamifying the seq IR breaks typecheck), but should be more principled. edit: One difference from master run of this test is that we're generating MakeStream instead of MakeArray. for:. MakeTuple(ArrayBuffer((0,MakeArray(ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),0), GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),1)),array<struct{a: int32, b: array<int32>}>))))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583812669
https://github.com/hail-is/hail/pull/8063#issuecomment-583812669:882,Testability,test,test,882,"After merge, addition of Let, RunAggScan, and ArrayFold2 all tests pass besides testMakeArrayWithDifferentRequiredeness (result mismatch). Will fix tomorrow. org.scalatest.exceptions.TestFailedException: t.valuesSimilar(res, expected, t.valuesSimilar$default$3, t.valuesSimilar$default$4) was false; result=WrappedArray(null, [0,null]); expect=WrappedArray(null, [2,WrappedArray(1)]); strategy=JvmCompile). However, I'm still concerned about the complexity of the lowering step, for instance, the Let should not be needed, but in practice was. Also need to understand why/whether the other IR, besides the child, of ArrayFold, and ArrayFold2 need/do not need to be streamified. Followed Streamify.ArrayFold here, and investigated some (for instance in ArrayFold2 streamifying the seq IR breaks typecheck), but should be more principled. edit: One difference from master run of this test is that we're generating MakeStream instead of MakeArray. for:. MakeTuple(ArrayBuffer((0,MakeArray(ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),0), GetTupleElement(In(0,PCTuple[0:PCStruct{a:PInt32,b:PCArray[PInt32]},1:+PCStruct{a:+PInt32,b:+PCArray[PInt32]}]),1)),array<struct{a: int32, b: array<int32>}>))))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583812669
https://github.com/hail-is/hail/pull/8063#issuecomment-583813511:340,Availability,redundant,redundant,340,"Indeed in my streamify, forcing MakeArray to remain a MakeArray fixes the problem. Now to investigate why MakeStream is the wrong solution, and why the new streamify isn't handling this correctly. to be clear, this branch finds the MakeArray inside of the MakeTuple and generates a ToArray(MakeStream), which both seems not super wrong and redundant. But the fact that's it's a value issue, with an array reading garbage, also make it look like a requiredeness/ copy function issue (though this was previously tested)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511
https://github.com/hail-is/hail/pull/8063#issuecomment-583813511:340,Safety,redund,redundant,340,"Indeed in my streamify, forcing MakeArray to remain a MakeArray fixes the problem. Now to investigate why MakeStream is the wrong solution, and why the new streamify isn't handling this correctly. to be clear, this branch finds the MakeArray inside of the MakeTuple and generates a ToArray(MakeStream), which both seems not super wrong and redundant. But the fact that's it's a value issue, with an array reading garbage, also make it look like a requiredeness/ copy function issue (though this was previously tested)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511
https://github.com/hail-is/hail/pull/8063#issuecomment-583813511:510,Testability,test,tested,510,"Indeed in my streamify, forcing MakeArray to remain a MakeArray fixes the problem. Now to investigate why MakeStream is the wrong solution, and why the new streamify isn't handling this correctly. to be clear, this branch finds the MakeArray inside of the MakeTuple and generates a ToArray(MakeStream), which both seems not super wrong and redundant. But the fact that's it's a value issue, with an array reading garbage, also make it look like a requiredeness/ copy function issue (though this was previously tested)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511
https://github.com/hail-is/hail/pull/8063#issuecomment-583813511:203,Usability,clear,clear,203,"Indeed in my streamify, forcing MakeArray to remain a MakeArray fixes the problem. Now to investigate why MakeStream is the wrong solution, and why the new streamify isn't handling this correctly. to be clear, this branch finds the MakeArray inside of the MakeTuple and generates a ToArray(MakeStream), which both seems not super wrong and redundant. But the fact that's it's a value issue, with an array reading garbage, also make it look like a requiredeness/ copy function issue (though this was previously tested)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511
https://github.com/hail-is/hail/pull/8063#issuecomment-583897947:306,Safety,avoid,avoid,306,"duh, my issue is that my copy stuff only runs on MakeArray, so I'm just seeing requiredeness offset issues when I'm lowering to MakeStream. . toEmitTriplet in Emit needs to be modified to read the actual element types of the child IRs. It currently assumes uniform element pType's. edit: Personal notes to avoid comments in code. Currently trying version that only affects toEmitTriplet, note also however:. ```scala; case x@MakeStream(elements, t) =>; val e = coerce[PStreamable](x.pType).elementType; implicit val eP = TypedTriplet.pack(e); sequence(elements.map { ir => TypedTriplet(e, emitIR(ir, env)) }); .map(_.untyped); ````. assumes all element types are same, which can be trivially untrue (MakeStream(MakeTuple())",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583897947
https://github.com/hail-is/hail/pull/8063#issuecomment-583963692:276,Deployability,update,update,276,"Issue I ran into: I need to pass each child IR's ptype to the join point loop body. There isn't a very easy way to get one PType out of an IndexedSeq[PType] of ptypes. For instance, even though srvb.advance() runs inside the body of the JoinPoint loop, its staticIdx does not update (since the loop only iterates at runtime). I want to pass Code[IndexedSeq[PType]] but that isn't possible. Will work on tomorrow. . edit:. I think I need to do something like this to access individual ptypes (but within toEmitTriplet loop body): . ```scala; case x@MakeStream(elements, t) =>; val e = coerce[PStreamable](x.pType).elementType; implicit val eP = TypedTriplet.pack(e); sequence(elements.map { ir => TypedTriplet(e, emitIR(ir, env)) }); .map(_.untyped); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583963692
https://github.com/hail-is/hail/pull/8063#issuecomment-583963692:466,Security,access,access,466,"Issue I ran into: I need to pass each child IR's ptype to the join point loop body. There isn't a very easy way to get one PType out of an IndexedSeq[PType] of ptypes. For instance, even though srvb.advance() runs inside the body of the JoinPoint loop, its staticIdx does not update (since the loop only iterates at runtime). I want to pass Code[IndexedSeq[PType]] but that isn't possible. Will work on tomorrow. . edit:. I think I need to do something like this to access individual ptypes (but within toEmitTriplet loop body): . ```scala; case x@MakeStream(elements, t) =>; val e = coerce[PStreamable](x.pType).elementType; implicit val eP = TypedTriplet.pack(e); sequence(elements.map { ir => TypedTriplet(e, emitIR(ir, env)) }); .map(_.untyped); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583963692
https://github.com/hail-is/hail/pull/8063#issuecomment-584224191:357,Availability,Error,ErrorHandling,357,"IRSuite passes, but all tests do not. From test_docs:. ```. Java stack trace:; is.hail.utils.HailException: not a streamable IR: (ToArray; (ArrayMap __iruid_226; (ToStream; (ToArray; (GetTupleElement 0; (Ref __iruid_225)))); (MakeTuple (0 1); (GetField key; (Ref __iruid_226)); (GetTupleElement 0; (GetField value; (Ref __iruid_226)))))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:851); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584224191
https://github.com/hail-is/hail/pull/8063#issuecomment-584224191:383,Availability,Error,ErrorHandling,383,"IRSuite passes, but all tests do not. From test_docs:. ```. Java stack trace:; is.hail.utils.HailException: not a streamable IR: (ToArray; (ArrayMap __iruid_226; (ToStream; (ToArray; (GetTupleElement 0; (Ref __iruid_225)))); (MakeTuple (0 1); (GetField key; (Ref __iruid_226)); (GetTupleElement 0; (GetField value; (Ref __iruid_226)))))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:851); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584224191
https://github.com/hail-is/hail/pull/8063#issuecomment-584224191:24,Testability,test,tests,24,"IRSuite passes, but all tests do not. From test_docs:. ```. Java stack trace:; is.hail.utils.HailException: not a streamable IR: (ToArray; (ArrayMap __iruid_226; (ToStream; (ToArray; (GetTupleElement 0; (Ref __iruid_225)))); (MakeTuple (0 1); (GetField key; (Ref __iruid_226)); (GetTupleElement 0; (GetField value; (Ref __iruid_226)))))); 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:9); 	at is.hail.utils.package$.fatal(package.scala:74); 	at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:851); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584224191
https://github.com/hail-is/hail/pull/8063#issuecomment-584445732:224,Testability,test,testRangeRead,224,"Need to understand ToStream and ToArray invariants. For ToArray, I'm considering the following invariant: if child is stream, leave as is, if child is not stream, streamify. Explicit ToArray match needed to pass TableSuite::testRangeRead. A bunch of tests fail in Aggregators2Suite; in testDownsample case it's an issue with ToStream missing from FoldConstants. Fixed this with a _: ToStream => None. Other Aggregators2Suite issues are stranger, here's the testArrayElementsAgg:. Cannot find __iruid_39 in Map(__iruid_22 -> PCStruct{stream:PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]]}, __iruid_38 -> PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]]); java.lang.RuntimeException: Cannot find __iruid_39 in Map(__iruid_22 -> PCStruct{stream:PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]]}, __iruid_38 -> PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]])",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584445732
https://github.com/hail-is/hail/pull/8063#issuecomment-584445732:250,Testability,test,tests,250,"Need to understand ToStream and ToArray invariants. For ToArray, I'm considering the following invariant: if child is stream, leave as is, if child is not stream, streamify. Explicit ToArray match needed to pass TableSuite::testRangeRead. A bunch of tests fail in Aggregators2Suite; in testDownsample case it's an issue with ToStream missing from FoldConstants. Fixed this with a _: ToStream => None. Other Aggregators2Suite issues are stranger, here's the testArrayElementsAgg:. Cannot find __iruid_39 in Map(__iruid_22 -> PCStruct{stream:PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]]}, __iruid_38 -> PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]]); java.lang.RuntimeException: Cannot find __iruid_39 in Map(__iruid_22 -> PCStruct{stream:PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]]}, __iruid_38 -> PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]])",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584445732
https://github.com/hail-is/hail/pull/8063#issuecomment-584445732:286,Testability,test,testDownsample,286,"Need to understand ToStream and ToArray invariants. For ToArray, I'm considering the following invariant: if child is stream, leave as is, if child is not stream, streamify. Explicit ToArray match needed to pass TableSuite::testRangeRead. A bunch of tests fail in Aggregators2Suite; in testDownsample case it's an issue with ToStream missing from FoldConstants. Fixed this with a _: ToStream => None. Other Aggregators2Suite issues are stranger, here's the testArrayElementsAgg:. Cannot find __iruid_39 in Map(__iruid_22 -> PCStruct{stream:PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]]}, __iruid_38 -> PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]]); java.lang.RuntimeException: Cannot find __iruid_39 in Map(__iruid_22 -> PCStruct{stream:PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]]}, __iruid_38 -> PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]])",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584445732
https://github.com/hail-is/hail/pull/8063#issuecomment-584445732:457,Testability,test,testArrayElementsAgg,457,"Need to understand ToStream and ToArray invariants. For ToArray, I'm considering the following invariant: if child is stream, leave as is, if child is not stream, streamify. Explicit ToArray match needed to pass TableSuite::testRangeRead. A bunch of tests fail in Aggregators2Suite; in testDownsample case it's an issue with ToStream missing from FoldConstants. Fixed this with a _: ToStream => None. Other Aggregators2Suite issues are stranger, here's the testArrayElementsAgg:. Cannot find __iruid_39 in Map(__iruid_22 -> PCStruct{stream:PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]]}, __iruid_38 -> PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]]); java.lang.RuntimeException: Cannot find __iruid_39 in Map(__iruid_22 -> PCStruct{stream:PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]]}, __iruid_38 -> PCArray[PCArray[PCStruct{a:PCString,b:PInt64}]])",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584445732
https://github.com/hail-is/hail/pull/8063#issuecomment-584451828:23,Availability,error,error,23,"So the reason for that error is this rule is insufficient:. ```scala; case x if x.typ == TVoid =>; x.children.foreach(c => infer(c.asInstanceOf[IR])); PVoid; ```. We need to update the environment as well, in some cases, such as ArrayFor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584451828
https://github.com/hail-is/hail/pull/8063#issuecomment-584451828:174,Deployability,update,update,174,"So the reason for that error is this rule is insufficient:. ```scala; case x if x.typ == TVoid =>; x.children.foreach(c => infer(c.asInstanceOf[IR])); PVoid; ```. We need to update the environment as well, in some cases, such as ArrayFor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584451828
https://github.com/hail-is/hail/pull/8063#issuecomment-584453034:24,Testability,test,testLoweringMatrixMapColsWithAggFiltersAndLets,24,"Slightly weird match in testLoweringMatrixMapColsWithAggFiltersAndLets. We end up setting required on PVoid. Just thought it was interesting that this had not come up yet:. Example IR:. PVOID FOR NODE If(ApplyComparisonOp(LT(int32,int32),GetField(In(1,PCStruct{row_idx:PInt32,`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:PCArray[PCStruct{}]}),row_idx),I32(5)),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Cast(GetField(In(1,PCStruct{row_idx:PInt32,`the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:PCArray[PCStruct{}]}),row_idx),int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))),Begin(ArrayBuffer()))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584453034
https://github.com/hail-is/hail/pull/8063#issuecomment-586422358:102,Integrability,wrap,wrap,102,"Tests finally all pass. The boundary condition is too fragile (TContainer can be TDict, and we cannot wrap that in ToArray)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586422358
https://github.com/hail-is/hail/pull/8063#issuecomment-586422358:0,Testability,Test,Tests,0,"Tests finally all pass. The boundary condition is too fragile (TContainer can be TDict, and we cannot wrap that in ToArray)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586422358
https://github.com/hail-is/hail/pull/8063#issuecomment-586537288:72,Testability,test,test,72,"Can you describe the issue with TContainer nodes, and possibly create a test with an example that makes sure we streamify it properly?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586537288
https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:661,Integrability,wrap,wrap,661,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:722,Integrability,wrap,wrap,722,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:1091,Integrability,wrap,wrapping,1091,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:1196,Integrability,wrap,wrap-in-ToStream,1196,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:583,Performance,perform,perform,583,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:709,Performance,perform,perform,709,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:134,Safety,safe,safe,134,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:346,Safety,safe,safe,346,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:1275,Safety,avoid,avoid,1275,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:1020,Testability,test,test,1020,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:1069,Testability,test,test,1069,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:808,Usability,simpl,simplified,808,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403
https://github.com/hail-is/hail/pull/8063#issuecomment-586582159:634,Integrability,wrap,wrap,634,"> ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. Without a concrete example, I'm having some trouble understanding what this problem is. What IR nodes can return a container type that aren't included in the streamify match? Isn't it just ToSet/ToDict/GroupByKey? Calling `boundary` on a ToSet is no problem at all, I think. > ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). Can you write such an IR, and what it should return after streamify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159
https://github.com/hail-is/hail/pull/8063#issuecomment-586582159:695,Integrability,wrap,wrap,695,"> ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. Without a concrete example, I'm having some trouble understanding what this problem is. What IR nodes can return a container type that aren't included in the streamify match? Isn't it just ToSet/ToDict/GroupByKey? Calling `boundary` on a ToSet is no problem at all, I think. > ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). Can you write such an IR, and what it should return after streamify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159
https://github.com/hail-is/hail/pull/8063#issuecomment-586582159:1041,Integrability,wrap,wrapping,1041,"> ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. Without a concrete example, I'm having some trouble understanding what this problem is. What IR nodes can return a container type that aren't included in the streamify match? Isn't it just ToSet/ToDict/GroupByKey? Calling `boundary` on a ToSet is no problem at all, I think. > ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). Can you write such an IR, and what it should return after streamify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159
https://github.com/hail-is/hail/pull/8063#issuecomment-586582159:1146,Integrability,wrap,wrap-in-ToStream,1146,"> ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. Without a concrete example, I'm having some trouble understanding what this problem is. What IR nodes can return a container type that aren't included in the streamify match? Isn't it just ToSet/ToDict/GroupByKey? Calling `boundary` on a ToSet is no problem at all, I think. > ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). Can you write such an IR, and what it should return after streamify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159
https://github.com/hail-is/hail/pull/8063#issuecomment-586582159:556,Performance,perform,perform,556,"> ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. Without a concrete example, I'm having some trouble understanding what this problem is. What IR nodes can return a container type that aren't included in the streamify match? Isn't it just ToSet/ToDict/GroupByKey? Calling `boundary` on a ToSet is no problem at all, I think. > ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). Can you write such an IR, and what it should return after streamify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159
https://github.com/hail-is/hail/pull/8063#issuecomment-586582159:682,Performance,perform,perform,682,"> ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. Without a concrete example, I'm having some trouble understanding what this problem is. What IR nodes can return a container type that aren't included in the streamify match? Isn't it just ToSet/ToDict/GroupByKey? Calling `boundary` on a ToSet is no problem at all, I think. > ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). Can you write such an IR, and what it should return after streamify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159
https://github.com/hail-is/hail/pull/8063#issuecomment-586582159:107,Safety,safe,safe,107,"> ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. Without a concrete example, I'm having some trouble understanding what this problem is. What IR nodes can return a container type that aren't included in the streamify match? Isn't it just ToSet/ToDict/GroupByKey? Calling `boundary` on a ToSet is no problem at all, I think. > ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). Can you write such an IR, and what it should return after streamify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159
https://github.com/hail-is/hail/pull/8063#issuecomment-586582159:319,Safety,safe,safe,319,"> ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. Without a concrete example, I'm having some trouble understanding what this problem is. What IR nodes can return a container type that aren't included in the streamify match? Isn't it just ToSet/ToDict/GroupByKey? Calling `boundary` on a ToSet is no problem at all, I think. > ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). Can you write such an IR, and what it should return after streamify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159
https://github.com/hail-is/hail/pull/8063#issuecomment-586586489:19,Testability,test,test,19,"I will work on the test. . However, thereâ€™s a related issue: should we be designing the ToStream behavior in this lowering pass to comply with our type systemâ€™s invariant, or not?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586586489
https://github.com/hail-is/hail/pull/8063#issuecomment-586598280:1009,Availability,error,errors,1009,"Here are the alternatives that I see:. The original design:; ```scala; object LowerArrayToStream {; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]); streamified = ToArray(streamified). if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]); ToStream(node); else; node; }; }; }. private def streamify(node: IR): IR = {; node match {; //...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; }; }. def apply(node: IR): IR = boundary(node); }; ````. the above has plenty of errors, surrounding attempts to cast PCanonicalArray to PStream. This can be fixed using TContainer instead of TArray. But as soon as you do this, you need to make sure you're never generating ToArray(ToStream(something of type TDict or TSet)), which means you need the if check in the present PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586598280
https://github.com/hail-is/hail/pull/8063#issuecomment-586598280:402,Testability,assert,assert,402,"Here are the alternatives that I see:. The original design:; ```scala; object LowerArrayToStream {; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]); streamified = ToArray(streamified). if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]); ToStream(node); else; node; }; }; }. private def streamify(node: IR): IR = {; node match {; //...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; }; }. def apply(node: IR): IR = boundary(node); }; ````. the above has plenty of errors, surrounding attempts to cast PCanonicalArray to PStream. This can be fixed using TContainer instead of TArray. But as soon as you do this, you need to make sure you're never generating ToArray(ToStream(something of type TDict or TSet)), which means you need the if check in the present PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586598280
https://github.com/hail-is/hail/pull/8063#issuecomment-586598443:26,Availability,error,errors,26,"> the above has plenty of errors, surrounding attempts to cast PCanonicalArray to PStream. where do these errors appear?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586598443
https://github.com/hail-is/hail/pull/8063#issuecomment-586598443:106,Availability,error,errors,106,"> the above has plenty of errors, surrounding attempts to cast PCanonicalArray to PStream. where do these errors appear?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586598443
https://github.com/hail-is/hail/pull/8063#issuecomment-586599051:521,Availability,error,errors,521,"In InferPType, first 2 I looked at: ArrayRef, Coalesce (in getNestedElementPTypesOfSameType). ##### Experiment with ToStream removed in the case _ => condition in streamify:. ```scala; // as above, but catch all condition in streamify:; private def streamify(node: IR): IR = {; node match {; //...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren); x; }; }; ```. results in many errors in IRSuite, one of which is:. > is.hail.utils.HailException: not a streamable IR: (Literal Array[Int32] ""[3,null,7]""); > ...; > at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:850). ##### With `ToStream(x)` as the return of `case _ =>` and the rest same as above. 100 errors in IRSuite, first one I looked at:. > Caused by: java.lang.ClassCastException: is.hail.expr.types.virtual.TInterval cannot be cast to is.hail.expr.types.virtual.TIterable; > at is.hail.expr.ir.InferType$.apply(InferType.scala:95)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586599051
https://github.com/hail-is/hail/pull/8063#issuecomment-586599051:837,Availability,error,errors,837,"In InferPType, first 2 I looked at: ArrayRef, Coalesce (in getNestedElementPTypesOfSameType). ##### Experiment with ToStream removed in the case _ => condition in streamify:. ```scala; // as above, but catch all condition in streamify:; private def streamify(node: IR): IR = {; node match {; //...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren); x; }; }; ```. results in many errors in IRSuite, one of which is:. > is.hail.utils.HailException: not a streamable IR: (Literal Array[Int32] ""[3,null,7]""); > ...; > at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:850). ##### With `ToStream(x)` as the return of `case _ =>` and the rest same as above. 100 errors in IRSuite, first one I looked at:. > Caused by: java.lang.ClassCastException: is.hail.expr.types.virtual.TInterval cannot be cast to is.hail.expr.types.virtual.TIterable; > at is.hail.expr.ir.InferType$.apply(InferType.scala:95)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586599051
https://github.com/hail-is/hail/pull/8063#issuecomment-586599441:34,Availability,error,errors,34,"can we dig into just one of these errors? Can you get the IR before and after streamify, so we know what it's doing wrong?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586599441
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:1711,Availability,failure,failures,1711,".map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. IRSuite.testDictContains:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))); ...; before: ; ToArray(Ref(__iruid_56,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). After lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:6744,Availability,failure,failures,6744,"plyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). is.hail.utils.HailException: not a streamable IR: (GetTupleElement 0; (In PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]] 0)); ...; at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:850). ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }; ````. with . ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }. 4 failures in IRSuite, again testArrayAggScan:. Before Lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:2313,Integrability,Wrap,WrappedArray,2313,"ay<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). After lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ToStream(ArrayMap(ToStream(StreamRange(I32(0),I32(4),I32(1))),__iruid_304,Cast(Ref(__iruid_304,int32),int64))),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> Agg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:2425,Integrability,Wrap,WrappedArray,2425,"ay<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). After lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ToStream(ArrayMap(ToStream(StreamRange(I32(0),I32(4),I32(1))),__iruid_304,Cast(Ref(__iruid_304,int32),int64))),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> Agg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:3134,Integrability,Wrap,WrappedArray,3134,"m(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). After lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ToStream(ArrayMap(ToStream(StreamRange(I32(0),I32(4),I32(1))),__iruid_304,Cast(Ref(__iruid_304,int32),int64))),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). java.lang.ClassCastException: is.hail.expr.types.physical.PStream cannot be cast to is.hail.expr.types.physical.PContainer. 	at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:631). #### Let's try to return x from streamify catch-all case (instead of if(...) ToStream(x) else x). With:. ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ````. Again issue in testArrayAggScan (and 8 others in IRSuite):. Before Lower: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:3246,Integrability,Wrap,WrappedArray,3246,"m(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). After lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ToStream(ArrayMap(ToStream(StreamRange(I32(0),I32(4),I32(1))),__iruid_304,Cast(Ref(__iruid_304,int32),int64))),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). java.lang.ClassCastException: is.hail.expr.types.physical.PStream cannot be cast to is.hail.expr.types.physical.PContainer. 	at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:631). #### Let's try to return x from streamify catch-all case (instead of if(...) ToStream(x) else x). With:. ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ````. Again issue in testArrayAggScan (and 8 others in IRSuite):. Before Lower: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:4560,Integrability,Wrap,WrappedArray,4560,"al.PContainer. 	at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:631). #### Let's try to return x from streamify catch-all case (instead of if(...) ToStream(x) else x). With:. ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ````. Again issue in testArrayAggScan (and 8 others in IRSuite):. Before Lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). After lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(Cal",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:4919,Integrability,Wrap,WrappedArray,4919,"al.PContainer. 	at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:631). #### Let's try to return x from streamify catch-all case (instead of if(...) ToStream(x) else x). With:. ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ````. Again issue in testArrayAggScan (and 8 others in IRSuite):. Before Lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). After lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(Cal",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:5629,Integrability,Wrap,WrappedArray,5629,"(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). After lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). is.hail.utils.HailException: not a streamable IR: (GetTupleElement 0; (In PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]] 0)); ...; at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:850). ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }; ````. with . ```scala; private def toStream(node: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:5988,Integrability,Wrap,WrappedArray,5988,"(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). After lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). is.hail.utils.HailException: not a streamable IR: (GetTupleElement 0; (In PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]] 0)); ...; at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:850). ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }; ````. with . ```scala; private def toStream(node: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:7363,Integrability,Wrap,WrappedArray,7363,"eam$1(EmitStream.scala:850). ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }; ````. with . ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }. 4 failures in IRSuite, again testArrayAggScan:. Before Lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). After lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(Cal",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:7722,Integrability,Wrap,WrappedArray,7722,"eam$1(EmitStream.scala:850). ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }; ````. with . ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }. 4 failures in IRSuite, again testArrayAggScan:. Before Lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). After lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(Cal",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:8432,Integrability,Wrap,WrappedArray,8432,",ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). After lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). is.hail.utils.HailException: not a streamable IR: (GetTupleElement 0; (In PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]] 0))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:8791,Integrability,Wrap,WrappedArray,8791,",ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). After lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). is.hail.utils.HailException: not a streamable IR: (GetTupleElement 0; (In PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]] 0))",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:444,Testability,assert,assert,444,"### Experiment 1. ```scala; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]) {; streamified = ToArray(streamified); }. if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }. // in streamify; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. IRSuite.testDictContains:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))); ...; before: ; ToArray(Ref(__iruid_56,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:947,Testability,test,testDictContains,947,"### Experiment 1. ```scala; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]) {; streamified = ToArray(streamified); }. if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }. // in streamify; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. IRSuite.testDictContains:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))); ...; before: ; ToArray(Ref(__iruid_56,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:1293,Testability,Assert,AssertionError,1293," = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }. // in streamify; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. IRSuite.testDictContains:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))); ...; before: ; ToArray(Ref(__iruid_56,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:1309,Testability,assert,assertion,1309," = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }. // in streamify; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. IRSuite.testDictContains:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))); ...; before: ; ToArray(Ref(__iruid_56,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:1345,Testability,assert,assert,1345,"ore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }. // in streamify; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. IRSuite.testDictContains:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))); ...; before: ; ToArray(Ref(__iruid_56,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:1764,Testability,test,tests,1764,".map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. IRSuite.testDictContains:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))); ...; before: ; ToArray(Ref(__iruid_56,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). After lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:3942,Testability,test,testArrayAggScan,3942,"n(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). java.lang.ClassCastException: is.hail.expr.types.physical.PStream cannot be cast to is.hail.expr.types.physical.PContainer. 	at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:631). #### Let's try to return x from streamify catch-all case (instead of if(...) ToStream(x) else x). With:. ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ````. Again issue in testArrayAggScan (and 8 others in IRSuite):. Before Lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(Ca",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:6771,Testability,test,testArrayAggScan,6771,"plyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None)))))). is.hail.utils.HailException: not a streamable IR: (GetTupleElement 0; (In PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]] 0)); ...; at is.hail.expr.ir.EmitStream$.is$hail$expr$ir$EmitStream$$emitStream$1(EmitStream.scala:850). ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }; ````. with . ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TContainer]) {; ToStream(node); } else {; node; }; }; }; }. 4 failures in IRSuite, again testArrayAggScan:. Before Lower: ; MakeTuple(ArrayBuffer((0,RunAggScan(GetTupleElement(In(0,PCTuple[0:PCArray[PCStruct{x:PCCall,y:PInt32}]]),0),__iruid_400,Begin(ArrayBuffer(InitOp(0,ArrayBuffer(I32(2)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(GetField(Ref(__iruid_400,struct{x: call, y: int32}),x)),AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None),CallStats()))),Let(__iruid_401,ResultOp(0,WrappedArray(AggStateSignature(Map(CallStats() -> AggSignature(CallStats(),ArrayBuffer(int32),ArrayBuffer(call))),CallStats(),None))),ApplyBinaryPrimOp(Add(),GetField(Ref(__iruid_400,struct{x: call, y: int32}),y),GetField(GetTupleElement(Ref(__iruid_401,tuple(struct{AC: array<int32>, AF: array<float64>, AN: int32, homozygote_count: array<int32>})),0),AN))),WrappedArray(AggStateSignature(Map(CallStat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113
https://github.com/hail-is/hail/pull/8063#issuecomment-586610981:40,Integrability,wrap,wrap,40,"I read what you wrote as you wanting to wrap RunAggScan in ToStream. I don't have time now, but I will re-run the above with toStream around that node. However, the issues that originally precipitated this observation/fix had nothing to do with RunAggScan. They had to do with the fact that we cannot safely wrap ToStream on something that is TContainer, and then cast ToArray on that. Even if we don't find a bug now, this can't be safe, we need instead to handle each specific type of TContainer (ToArray, ToDict, ToSet)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586610981
https://github.com/hail-is/hail/pull/8063#issuecomment-586610981:308,Integrability,wrap,wrap,308,"I read what you wrote as you wanting to wrap RunAggScan in ToStream. I don't have time now, but I will re-run the above with toStream around that node. However, the issues that originally precipitated this observation/fix had nothing to do with RunAggScan. They had to do with the fact that we cannot safely wrap ToStream on something that is TContainer, and then cast ToArray on that. Even if we don't find a bug now, this can't be safe, we need instead to handle each specific type of TContainer (ToArray, ToDict, ToSet)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586610981
https://github.com/hail-is/hail/pull/8063#issuecomment-586610981:301,Safety,safe,safely,301,"I read what you wrote as you wanting to wrap RunAggScan in ToStream. I don't have time now, but I will re-run the above with toStream around that node. However, the issues that originally precipitated this observation/fix had nothing to do with RunAggScan. They had to do with the fact that we cannot safely wrap ToStream on something that is TContainer, and then cast ToArray on that. Even if we don't find a bug now, this can't be safe, we need instead to handle each specific type of TContainer (ToArray, ToDict, ToSet)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586610981
https://github.com/hail-is/hail/pull/8063#issuecomment-586610981:433,Safety,safe,safe,433,"I read what you wrote as you wanting to wrap RunAggScan in ToStream. I don't have time now, but I will re-run the above with toStream around that node. However, the issues that originally precipitated this observation/fix had nothing to do with RunAggScan. They had to do with the fact that we cannot safely wrap ToStream on something that is TContainer, and then cast ToArray on that. Even if we don't find a bug now, this can't be safe, we need instead to handle each specific type of TContainer (ToArray, ToDict, ToSet)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586610981
https://github.com/hail-is/hail/pull/8063#issuecomment-586612345:1489,Integrability,wrap,wrapping,1489,"```scala; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]) {; streamified = ToArray(streamified); }. if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. 4 failed in IRSuite, including testDictContains, as before:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))). before: ; ToArray(Ref(__iruid_2345,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_2345,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). wrapping `case x: ApplyIR => streamify(x.explicitNode)` doesn't fix. But the greater point, I think, is that if boundary can ever be called on something that was wrapped in ToStream, we cannot just blindly case ToArray on it. Need to cast ToArray, ToDict, ToSet, or do what I did. I prefer the separate ToArray, ToDict, ToSet, over what I have now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586612345
https://github.com/hail-is/hail/pull/8063#issuecomment-586612345:1651,Integrability,wrap,wrapped,1651,"```scala; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]) {; streamified = ToArray(streamified); }. if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. 4 failed in IRSuite, including testDictContains, as before:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))). before: ; ToArray(Ref(__iruid_2345,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_2345,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). wrapping `case x: ApplyIR => streamify(x.explicitNode)` doesn't fix. But the greater point, I think, is that if boundary can ever be called on something that was wrapped in ToStream, we cannot just blindly case ToArray on it. Need to cast ToArray, ToDict, ToSet, or do what I did. I prefer the separate ToArray, ToDict, ToSet, over what I have now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586612345
https://github.com/hail-is/hail/pull/8063#issuecomment-586612345:426,Testability,assert,assert,426,"```scala; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]) {; streamified = ToArray(streamified); }. if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. 4 failed in IRSuite, including testDictContains, as before:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))). before: ; ToArray(Ref(__iruid_2345,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_2345,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). wrapping `case x: ApplyIR => streamify(x.explicitNode)` doesn't fix. But the greater point, I think, is that if boundary can ever be called on something that was wrapped in ToStream, we cannot just blindly case ToArray on it. Need to cast ToArray, ToDict, ToSet, or do what I did. I prefer the separate ToArray, ToDict, ToSet, over what I have now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586612345
https://github.com/hail-is/hail/pull/8063#issuecomment-586612345:940,Testability,test,testDictContains,940,"```scala; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]) {; streamified = ToArray(streamified); }. if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. 4 failed in IRSuite, including testDictContains, as before:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))). before: ; ToArray(Ref(__iruid_2345,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_2345,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). wrapping `case x: ApplyIR => streamify(x.explicitNode)` doesn't fix. But the greater point, I think, is that if boundary can ever be called on something that was wrapped in ToStream, we cannot just blindly case ToArray on it. Need to cast ToArray, ToDict, ToSet, or do what I did. I prefer the separate ToArray, ToDict, ToSet, over what I have now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586612345
https://github.com/hail-is/hail/pull/8063#issuecomment-586612345:1296,Testability,Assert,AssertionError,1296,"```scala; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]) {; streamified = ToArray(streamified); }. if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. 4 failed in IRSuite, including testDictContains, as before:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))). before: ; ToArray(Ref(__iruid_2345,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_2345,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). wrapping `case x: ApplyIR => streamify(x.explicitNode)` doesn't fix. But the greater point, I think, is that if boundary can ever be called on something that was wrapped in ToStream, we cannot just blindly case ToArray on it. Need to cast ToArray, ToDict, ToSet, or do what I did. I prefer the separate ToArray, ToDict, ToSet, over what I have now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586612345
https://github.com/hail-is/hail/pull/8063#issuecomment-586612345:1312,Testability,assert,assertion,1312,"```scala; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]) {; streamified = ToArray(streamified); }. if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. 4 failed in IRSuite, including testDictContains, as before:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))). before: ; ToArray(Ref(__iruid_2345,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_2345,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). wrapping `case x: ApplyIR => streamify(x.explicitNode)` doesn't fix. But the greater point, I think, is that if boundary can ever be called on something that was wrapped in ToStream, we cannot just blindly case ToArray on it. Need to cast ToArray, ToDict, ToSet, or do what I did. I prefer the separate ToArray, ToDict, ToSet, over what I have now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586612345
https://github.com/hail-is/hail/pull/8063#issuecomment-586612345:1348,Testability,assert,assert,1348,"```scala; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]) {; streamified = ToArray(streamified); }. if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }; ...; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. 4 failed in IRSuite, including testDictContains, as before:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))). before: ; ToArray(Ref(__iruid_2345,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_2345,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). wrapping `case x: ApplyIR => streamify(x.explicitNode)` doesn't fix. But the greater point, I think, is that if boundary can ever be called on something that was wrapped in ToStream, we cannot just blindly case ToArray on it. Need to cast ToArray, ToDict, ToSet, or do what I did. I prefer the separate ToArray, ToDict, ToSet, over what I have now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586612345
https://github.com/hail-is/hail/pull/8063#issuecomment-586726532:24,Usability,simpl,simpler,24,I'm happy with how much simpler this is.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586726532
https://github.com/hail-is/hail/pull/8063#issuecomment-586753124:214,Availability,down,down,214,"test_ld_score_regression is failing now, caused by . ```scala; case GetTupleElement(o, idx) =>; infer(o); val t = coerce[PTuple](o.pType2); assert(idx >= 0 && idx < t.size); ```. in inferPType throwing. Will track down. I've seen this before, once. edit:. t.size 1, idx: 1; GetTupleElement(Ref(__iruid_28388,tuple(1:float64))),1)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586753124
https://github.com/hail-is/hail/pull/8063#issuecomment-586753124:140,Testability,assert,assert,140,"test_ld_score_regression is failing now, caused by . ```scala; case GetTupleElement(o, idx) =>; infer(o); val t = coerce[PTuple](o.pType2); assert(idx >= 0 && idx < t.size); ```. in inferPType throwing. Will track down. I've seen this before, once. edit:. t.size 1, idx: 1; GetTupleElement(Ref(__iruid_28388,tuple(1:float64))),1)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586753124
https://github.com/hail-is/hail/pull/8063#issuecomment-586754725:147,Performance,optimiz,optimization,147,"> Ready to look at. Remaining question: should we encode the fact that in EmitStream context, If and Let should have TStream children?. This is an optimization, not a requirement. We've also removed `If` stream emitters for now, because they were broken",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586754725
https://github.com/hail-is/hail/pull/8063#issuecomment-587144436:8,Testability,assert,assert,8,with no assert,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-587144436
https://github.com/hail-is/hail/pull/8063#issuecomment-587165563:529,Deployability,update,update,529,"Finding:. Before PruneDeadFields (after ForwardRelationalLets) we see the following Let:. <img width=""759"" alt=""Screenshot 2020-02-17 16 17 55"" src=""https://user-images.githubusercontent.com/5543229/74686967-1c069780-51a1-11ea-8acf-ef6c6d313c2f.png"">. After:; <img width=""419"" alt=""Screenshot 2020-02-17 16 14 08"" src=""https://user-images.githubusercontent.com/5543229/74686973-1f018800-51a1-11ea-8f98-f3b15a16dbfa.png"">. The MakeTuple is being modified, one field removed. If this is the correct action, then it is a bug to not update GetTupleElement to reflect the fact that the desired element came after the removed field, therefore requiring the index of that element to be shifted, or the type of the tuple to be shifted. Ah, I suppose what is happening is that this could be a problem with InferPType, since my MakeTuple inference should map the old index to the new (old: 1, new: 0), via fieldIdx. Ok, looking at this, it appears InferPType is getting the wrong MakeTuple, with 2 elements:. <img width=""998"" alt=""Screenshot 2020-02-17 16 30 31"" src=""https://user-images.githubusercontent.com/5543229/74687628-ee225280-51a2-11ea-9a9d-a3813f587d1d.png"">. If I fix this, I fix the problem. Somehow InferPType is getting the wrong IR. First thought, maybe we need a deepCopy, regardless of ir sharing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-587165563
https://github.com/hail-is/hail/pull/8067#issuecomment-584124068:9,Testability,benchmark,benchmarks,9,I'll run benchmarks when I get to Broad.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8067#issuecomment-584124068
https://github.com/hail-is/hail/pull/8067#issuecomment-585439143:15,Availability,failure,failures,15,do I have test failures? I thought it passed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8067#issuecomment-585439143
https://github.com/hail-is/hail/pull/8067#issuecomment-585439143:10,Testability,test,test,10,do I have test failures? I thought it passed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8067#issuecomment-585439143
https://github.com/hail-is/hail/pull/8067#issuecomment-585444507:57,Testability,benchmark,benchmark,57,"hmm, actually I'm going to dismiss your approval until I benchmark this. . No worries about being confused, I was the source (I opened 2 prs...)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8067#issuecomment-585444507
https://github.com/hail-is/hail/pull/8068#issuecomment-584231530:13,Testability,assert,assertions,13,failing some assertions,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8068#issuecomment-584231530
https://github.com/hail-is/hail/pull/8071#issuecomment-584740001:90,Usability,simpl,simplicity,90,"@tpoterba I just did some back of the envelope calculations on this, and while I like the simplicity of it I don't think it scales in a tenable way. I'll let you know when I've fixed that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8071#issuecomment-584740001
https://github.com/hail-is/hail/pull/8073#issuecomment-585372375:106,Availability,error,error,106,"@johnc1231 I uncommented some of the tests that I'd commented out before because they don't hit the prune error. I had to implement some stuff on PNDArray (mostly copy, and adding a case for setRequired) in order to make it work; let me know if those were missing for a reason and I'll take it out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8073#issuecomment-585372375
https://github.com/hail-is/hail/pull/8073#issuecomment-585372375:37,Testability,test,tests,37,"@johnc1231 I uncommented some of the tests that I'd commented out before because they don't hit the prune error. I had to implement some stuff on PNDArray (mostly copy, and adding a case for setRequired) in order to make it work; let me know if those were missing for a reason and I'll take it out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8073#issuecomment-585372375
https://github.com/hail-is/hail/issues/8076#issuecomment-584795090:210,Availability,down,down,210,"to answer (1), I think the right thing is for us to implement our own hashable immutable data structures (and use frozenset for sets, for instance) for results of Hail computations. I think we have yet to nail down whether this would be a breaking interface change, forcing us to wait until 0.3. To answer (2), you *may* be able to do `hl.stop(); hl.init()` to reset the session, but not sure this will work in every case. The driver should really only die for OOM and faults, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8076#issuecomment-584795090
https://github.com/hail-is/hail/issues/8076#issuecomment-584795090:469,Availability,fault,faults,469,"to answer (1), I think the right thing is for us to implement our own hashable immutable data structures (and use frozenset for sets, for instance) for results of Hail computations. I think we have yet to nail down whether this would be a breaking interface change, forcing us to wait until 0.3. To answer (2), you *may* be able to do `hl.stop(); hl.init()` to reset the session, but not sure this will work in every case. The driver should really only die for OOM and faults, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8076#issuecomment-584795090
https://github.com/hail-is/hail/issues/8076#issuecomment-584795090:248,Integrability,interface,interface,248,"to answer (1), I think the right thing is for us to implement our own hashable immutable data structures (and use frozenset for sets, for instance) for results of Hail computations. I think we have yet to nail down whether this would be a breaking interface change, forcing us to wait until 0.3. To answer (2), you *may* be able to do `hl.stop(); hl.init()` to reset the session, but not sure this will work in every case. The driver should really only die for OOM and faults, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8076#issuecomment-584795090
https://github.com/hail-is/hail/issues/8076#issuecomment-584795090:70,Security,hash,hashable,70,"to answer (1), I think the right thing is for us to implement our own hashable immutable data structures (and use frozenset for sets, for instance) for results of Hail computations. I think we have yet to nail down whether this would be a breaking interface change, forcing us to wait until 0.3. To answer (2), you *may* be able to do `hl.stop(); hl.init()` to reset the session, but not sure this will work in every case. The driver should really only die for OOM and faults, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8076#issuecomment-584795090
https://github.com/hail-is/hail/issues/8076#issuecomment-585938806:21,Availability,error,errors,21,"Hi @eric-czech,. The errors you're running into with aggregating sorted arrays should be fixed in versions 0.2.31 and later; if upgrading the version works for you, please let me know and I will close this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8076#issuecomment-585938806
https://github.com/hail-is/hail/issues/8078#issuecomment-584868077:34,Deployability,update,update,34,"Yeah, this was broken in a recent update to the gcloud libraries. For now, following the command to do:; ```; gcloud config set dataproc/region VALUE; ```; will generate the best user experience. We'll fix this for the next version though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8078#issuecomment-584868077
https://github.com/hail-is/hail/issues/8078#issuecomment-584868077:117,Modifiability,config,config,117,"Yeah, this was broken in a recent update to the gcloud libraries. For now, following the command to do:; ```; gcloud config set dataproc/region VALUE; ```; will generate the best user experience. We'll fix this for the next version though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8078#issuecomment-584868077
https://github.com/hail-is/hail/issues/8078#issuecomment-584868077:179,Usability,user experience,user experience,179,"Yeah, this was broken in a recent update to the gcloud libraries. For now, following the command to do:; ```; gcloud config set dataproc/region VALUE; ```; will generate the best user experience. We'll fix this for the next version though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8078#issuecomment-584868077
https://github.com/hail-is/hail/pull/8082#issuecomment-605028789:421,Testability,test,testing,421,"The bug here appears to be that ndarray serializes to JSON in a format that we cannot deserialize:; ```; E is.hail.utils.HailException: scala.MatchError: could not convert ""{flags: 0, offset: 0, shape: (2, 3), strides: (12, 4), data: [1,2,3,4,5,6]}"" to ndarray<int32, 2> in column ""nd""; ```; I guess the big question for this PR is what is the serialized representation of an ndarray? Once we decide on that we can start testing for round-trip ability. Site of necessary changes appears to be `AnnotationImpex.scala`. I propose `{shape, strides, data}`, does that seem right John?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8082#issuecomment-605028789
https://github.com/hail-is/hail/issues/8091#issuecomment-613654687:21,Energy Efficiency,schedul,scheduled,21,moved to asana to be scheduled.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8091#issuecomment-613654687
https://github.com/hail-is/hail/pull/8099#issuecomment-586380409:159,Modifiability,variab,variables,159,"In the course of this work I also fixed a problem with the staged code generated by the copyFromType methods -- the addresses to copy from were never bound to variables, so in nested types, we ended up duplicating a lot of code (an array of Tuple10s of Tuple10s of Tuple10s would duplicate the top `loadElement` 1000x!)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586380409
https://github.com/hail-is/hail/pull/8099#issuecomment-586380409:299,Performance,load,loadElement,299,"In the course of this work I also fixed a problem with the staged code generated by the copyFromType methods -- the addresses to copy from were never bound to variables, so in nested types, we ended up duplicating a lot of code (an array of Tuple10s of Tuple10s of Tuple10s would duplicate the top `loadElement` 1000x!)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586380409
https://github.com/hail-is/hail/pull/8099#issuecomment-586410150:161,Modifiability,variab,variables,161,"> In the course of this work I also fixed a problem with the staged code generated by the copyFromType methods -- the addresses to copy from were never bound to variables, so in nested types, we ended up duplicating a lot of code (an array of Tuple10s of Tuple10s of Tuple10s would duplicate the top `loadElement` 1000x!). Could you show me where I forgot to bind address returns? This was an oversight; I understand the cost of not binding address-generating code (Need to call Code.store and load that, instead of re-running the generating function)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586410150
https://github.com/hail-is/hail/pull/8099#issuecomment-586410150:301,Performance,load,loadElement,301,"> In the course of this work I also fixed a problem with the staged code generated by the copyFromType methods -- the addresses to copy from were never bound to variables, so in nested types, we ended up duplicating a lot of code (an array of Tuple10s of Tuple10s of Tuple10s would duplicate the top `loadElement` 1000x!). Could you show me where I forgot to bind address returns? This was an oversight; I understand the cost of not binding address-generating code (Need to call Code.store and load that, instead of re-running the generating function)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586410150
https://github.com/hail-is/hail/pull/8099#issuecomment-586410150:494,Performance,load,load,494,"> In the course of this work I also fixed a problem with the staged code generated by the copyFromType methods -- the addresses to copy from were never bound to variables, so in nested types, we ended up duplicating a lot of code (an array of Tuple10s of Tuple10s of Tuple10s would duplicate the top `loadElement` 1000x!). Could you show me where I forgot to bind address returns? This was an oversight; I understand the cost of not binding address-generating code (Need to call Code.store and load that, instead of re-running the generating function)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586410150
https://github.com/hail-is/hail/pull/8099#issuecomment-586413200:249,Modifiability,variab,variable,249,"Here's an example:. https://github.com/hail-is/hail/blob/6f5d1d9b511a1b5c91908a70f1afde909ec3226e/hail/src/main/scala/is/hail/expr/types/physical/PBaseStruct.scala#L346. We use `srcStructOffset` multiple times without binding, and then don't bind a variable before calling a copyFromType. I think the invariant is basically that if you get a Code[T], you must not use it more than once, since you're inlining arbitrary code. I've tried to enforce this model in my implementations",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586413200
https://github.com/hail-is/hail/pull/8099#issuecomment-586416114:322,Performance,load,load,322,"> I think the invariant is basically that if you get a Code[T], you must not use it more than once, since you're inlining arbitrary code. I've tried to enforce this model in my implementations. Yes that's definitely true, should not use more than once. In the example you linked, body is only being used once, calling the load on a specific srcField.index right? The ""loop"" here is statically unrolled (since iterating over a statically-known collection), each of these `body` invocations should only be called once I think",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586416114
https://github.com/hail-is/hail/pull/8099#issuecomment-586417359:241,Integrability,rout,routine,241,"> I thought in these cases the code would be evaluated (inner -> outer). Sorry, don't know what you mean here. Can you explain?. > ... added to the function stack. What actually happens?. There's no function stack - just because our codegen routine calls a new method doesn't mean the generated code has a method. We'd have to explicitly put a method boundary in. In the linked example, you're right that there is no generated loop, just a bunch of flat code. You're passing `loadElement(...)` as the `srcAddress` to a field's copyFromType method, which may use it multiple times, inlining load element (and doing a bunch of unnecessary pointer math / dereferences) each time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586417359
https://github.com/hail-is/hail/pull/8099#issuecomment-586417359:476,Performance,load,loadElement,476,"> I thought in these cases the code would be evaluated (inner -> outer). Sorry, don't know what you mean here. Can you explain?. > ... added to the function stack. What actually happens?. There's no function stack - just because our codegen routine calls a new method doesn't mean the generated code has a method. We'd have to explicitly put a method boundary in. In the linked example, you're right that there is no generated loop, just a bunch of flat code. You're passing `loadElement(...)` as the `srcAddress` to a field's copyFromType method, which may use it multiple times, inlining load element (and doing a bunch of unnecessary pointer math / dereferences) each time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586417359
https://github.com/hail-is/hail/pull/8099#issuecomment-586417359:590,Performance,load,load,590,"> I thought in these cases the code would be evaluated (inner -> outer). Sorry, don't know what you mean here. Can you explain?. > ... added to the function stack. What actually happens?. There's no function stack - just because our codegen routine calls a new method doesn't mean the generated code has a method. We'd have to explicitly put a method boundary in. In the linked example, you're right that there is no generated loop, just a bunch of flat code. You're passing `loadElement(...)` as the `srcAddress` to a field's copyFromType method, which may use it multiple times, inlining load element (and doing a bunch of unnecessary pointer math / dereferences) each time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586417359
https://github.com/hail-is/hail/pull/8099#issuecomment-586421823:721,Performance,load,loadElement,721,"> Sorry, don't know what you mean here. Can you explain?. I just mean that if I call. ```; In [4]: def foo(integer): ; ...: if(integer == 5): ; ...: print(""GOT 5"") ; ...: print(f""Added 1, got {integer + 1}"") ; ...: ; ...: def bar(): ; ...: foo(someComplicatedGeneratingFunctionThatReturnsAnInt()) ; ...: ; ...: def someComplicatedGeneratingFunctionThatReturnsAnInt(): ; ...: print(""Called complex function"") ; ...: return 5 ; ...: . In [5]: bar() ; Called complex function; GOT 5; Added 1, got 6; ```. Will evaluate someComplicatedGeneratingFunctionThatReturnsAnInt, and use the return value as the argument to foo. It won't store the function as an argument and lazily eval n times with no memoization. > You're passing loadElement(...) as the srcAddress to a field's copyFromType method, which may use it multiple times, inlining load element (and doing a bunch of unnecessary pointer math / dereferences) each time. Thanks for catching that, understood.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586421823
https://github.com/hail-is/hail/pull/8099#issuecomment-586421823:832,Performance,load,load,832,"> Sorry, don't know what you mean here. Can you explain?. I just mean that if I call. ```; In [4]: def foo(integer): ; ...: if(integer == 5): ; ...: print(""GOT 5"") ; ...: print(f""Added 1, got {integer + 1}"") ; ...: ; ...: def bar(): ; ...: foo(someComplicatedGeneratingFunctionThatReturnsAnInt()) ; ...: ; ...: def someComplicatedGeneratingFunctionThatReturnsAnInt(): ; ...: print(""Called complex function"") ; ...: return 5 ; ...: . In [5]: bar() ; Called complex function; GOT 5; Added 1, got 6; ```. Will evaluate someComplicatedGeneratingFunctionThatReturnsAnInt, and use the return value as the argument to foo. It won't store the function as an argument and lazily eval n times with no memoization. > You're passing loadElement(...) as the srcAddress to a field's copyFromType method, which may use it multiple times, inlining load element (and doing a bunch of unnecessary pointer math / dereferences) each time. Thanks for catching that, understood.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586421823
https://github.com/hail-is/hail/pull/8099#issuecomment-586471938:57,Safety,Unsafe,UnsafeIndexedSeq,57,"arrayCopyTest looks like it needs requiredeness upcast:. UnsafeIndexedSeq(8589934593, 12884901890, 17179869187, 21474836484, 25769803781, 30064771078, 34359738375, 38654705672, 9) did not equal Vector(1, 2, 3, 4, 5, 6, 7, 8, 9)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586471938
https://github.com/hail-is/hail/pull/8099#issuecomment-586546086:10,Testability,Assert,AssertionError,10,java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.types.physical.PCanonicalBaseStruct.constructAtAddress(PCanonicalBaseStruct.scala:213),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586546086
https://github.com/hail-is/hail/pull/8099#issuecomment-586546086:26,Testability,assert,assertion,26,java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.types.physical.PCanonicalBaseStruct.constructAtAddress(PCanonicalBaseStruct.scala:213),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586546086
https://github.com/hail-is/hail/pull/8099#issuecomment-586546086:62,Testability,assert,assert,62,java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.types.physical.PCanonicalBaseStruct.constructAtAddress(PCanonicalBaseStruct.scala:213),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586546086
https://github.com/hail-is/hail/issues/8106#issuecomment-597321135:126,Deployability,update,update,126,"Sorry no one answered this earlier (consider bugging us on hail.zulipchat.com or discuss.hail.is). The first thing I'd say is update to a newer version of hail and see if this still happens, as you're 15 versions behind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106#issuecomment-597321135
https://github.com/hail-is/hail/issues/8106#issuecomment-599765896:92,Availability,error,error,92,I ended up posting on the forum. I did update to the newest version. It did not generate an error this time. The job ran much further but hung with 4 tasks left. . John,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106#issuecomment-599765896
https://github.com/hail-is/hail/issues/8106#issuecomment-599765896:39,Deployability,update,update,39,I ended up posting on the forum. I did update to the newest version. It did not generate an error this time. The job ran much further but hung with 4 tasks left. . John,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8106#issuecomment-599765896
https://github.com/hail-is/hail/pull/8107#issuecomment-587687066:1051,Safety,detect,detect,1051,"@danking . The redirect was caused by lack of X-Forwarded-Host + X-Forwarded-Proto; * https://github.com/expressjs/express/blob/b8e50568af9c73ef1ade434e92c60d389868361d/lib/request.js#L429; * remoteAddress is the url from the last hop, not any forwarded address. > I'll revisit why ghost issues redirects with the new changes this afternoon.; Ghost doesn't read this header. They use X-Forwarded* headers, via Express: https://expressjs.com/en/guide/behind-proxies.html ; * Expresses finds ips using proxy-addr package (the getter is req.ips): https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/lib/request.js#L349; * The test: https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/test/req.ip.js; * proxy-addr does not support x-real-ip: https://github.com/jshttp/proxy-addr/issues/15. Ghost apparently uses X-Forwarded-For to rate limit malicious addresses:. * ""6. Include the X-Forwarded-For header, populated with the remote IP of the original request.; Without this, we aren't able to detect spam traffic patterns and your site risks being rate limited or incorrectly restricted.""; * https://ghost.org/faq/can-i-run-ghost-from-a-subdirectory/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8107#issuecomment-587687066
https://github.com/hail-is/hail/pull/8107#issuecomment-587687066:1094,Safety,risk,risks,1094,"@danking . The redirect was caused by lack of X-Forwarded-Host + X-Forwarded-Proto; * https://github.com/expressjs/express/blob/b8e50568af9c73ef1ade434e92c60d389868361d/lib/request.js#L429; * remoteAddress is the url from the last hop, not any forwarded address. > I'll revisit why ghost issues redirects with the new changes this afternoon.; Ghost doesn't read this header. They use X-Forwarded* headers, via Express: https://expressjs.com/en/guide/behind-proxies.html ; * Expresses finds ips using proxy-addr package (the getter is req.ips): https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/lib/request.js#L349; * The test: https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/test/req.ip.js; * proxy-addr does not support x-real-ip: https://github.com/jshttp/proxy-addr/issues/15. Ghost apparently uses X-Forwarded-For to rate limit malicious addresses:. * ""6. Include the X-Forwarded-For header, populated with the remote IP of the original request.; Without this, we aren't able to detect spam traffic patterns and your site risks being rate limited or incorrectly restricted.""; * https://ghost.org/faq/can-i-run-ghost-from-a-subdirectory/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8107#issuecomment-587687066
https://github.com/hail-is/hail/pull/8107#issuecomment-587687066:654,Testability,test,test,654,"@danking . The redirect was caused by lack of X-Forwarded-Host + X-Forwarded-Proto; * https://github.com/expressjs/express/blob/b8e50568af9c73ef1ade434e92c60d389868361d/lib/request.js#L429; * remoteAddress is the url from the last hop, not any forwarded address. > I'll revisit why ghost issues redirects with the new changes this afternoon.; Ghost doesn't read this header. They use X-Forwarded* headers, via Express: https://expressjs.com/en/guide/behind-proxies.html ; * Expresses finds ips using proxy-addr package (the getter is req.ips): https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/lib/request.js#L349; * The test: https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/test/req.ip.js; * proxy-addr does not support x-real-ip: https://github.com/jshttp/proxy-addr/issues/15. Ghost apparently uses X-Forwarded-For to rate limit malicious addresses:. * ""6. Include the X-Forwarded-For header, populated with the remote IP of the original request.; Without this, we aren't able to detect spam traffic patterns and your site risks being rate limited or incorrectly restricted.""; * https://ghost.org/faq/can-i-run-ghost-from-a-subdirectory/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8107#issuecomment-587687066
https://github.com/hail-is/hail/pull/8107#issuecomment-587687066:743,Testability,test,test,743,"@danking . The redirect was caused by lack of X-Forwarded-Host + X-Forwarded-Proto; * https://github.com/expressjs/express/blob/b8e50568af9c73ef1ade434e92c60d389868361d/lib/request.js#L429; * remoteAddress is the url from the last hop, not any forwarded address. > I'll revisit why ghost issues redirects with the new changes this afternoon.; Ghost doesn't read this header. They use X-Forwarded* headers, via Express: https://expressjs.com/en/guide/behind-proxies.html ; * Expresses finds ips using proxy-addr package (the getter is req.ips): https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/lib/request.js#L349; * The test: https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/test/req.ip.js; * proxy-addr does not support x-real-ip: https://github.com/jshttp/proxy-addr/issues/15. Ghost apparently uses X-Forwarded-For to rate limit malicious addresses:. * ""6. Include the X-Forwarded-For header, populated with the remote IP of the original request.; Without this, we aren't able to detect spam traffic patterns and your site risks being rate limited or incorrectly restricted.""; * https://ghost.org/faq/can-i-run-ghost-from-a-subdirectory/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8107#issuecomment-587687066
https://github.com/hail-is/hail/pull/8107#issuecomment-587687066:444,Usability,guid,guide,444,"@danking . The redirect was caused by lack of X-Forwarded-Host + X-Forwarded-Proto; * https://github.com/expressjs/express/blob/b8e50568af9c73ef1ade434e92c60d389868361d/lib/request.js#L429; * remoteAddress is the url from the last hop, not any forwarded address. > I'll revisit why ghost issues redirects with the new changes this afternoon.; Ghost doesn't read this header. They use X-Forwarded* headers, via Express: https://expressjs.com/en/guide/behind-proxies.html ; * Expresses finds ips using proxy-addr package (the getter is req.ips): https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/lib/request.js#L349; * The test: https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/test/req.ip.js; * proxy-addr does not support x-real-ip: https://github.com/jshttp/proxy-addr/issues/15. Ghost apparently uses X-Forwarded-For to rate limit malicious addresses:. * ""6. Include the X-Forwarded-For header, populated with the remote IP of the original request.; Without this, we aren't able to detect spam traffic patterns and your site risks being rate limited or incorrectly restricted.""; * https://ghost.org/faq/can-i-run-ghost-from-a-subdirectory/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8107#issuecomment-587687066
https://github.com/hail-is/hail/issues/8110#issuecomment-594592285:184,Energy Efficiency,efficient,efficient,184,"@patrick-schultz knows what i mean. if you have non-overlapping partitions, you just need to order them correctly and then no intra-partition merging is needed, so should be much more efficient (like an un-keyed union)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8110#issuecomment-594592285
https://github.com/hail-is/hail/pull/8115#issuecomment-588323242:277,Deployability,pipeline,pipelines,277,"@johnc1231 I've backed out the removal of _jbm etc. since it's not necessary for this change (I just wanted to convince myself that it was, in fact, unnecessary now) with the intention of opening a separate PR for it once Konrad and Jacob are no longer relying on it for their pipelines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8115#issuecomment-588323242
https://github.com/hail-is/hail/pull/8117#issuecomment-589741905:114,Integrability,protocol,protocol,114,Ghost does not respect the `X-Forwarded` headers. It should not have a `url` parameter but a `pathPrefix` and the protocol should be set from `X-Forwarded-Proto`. Thanks to this design bug we cannot test connectivity to the blog in PRs until the internal gateway is configured to use TLS. I'll revisit this PR when that happens.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8117#issuecomment-589741905
https://github.com/hail-is/hail/pull/8117#issuecomment-589741905:266,Modifiability,config,configured,266,Ghost does not respect the `X-Forwarded` headers. It should not have a `url` parameter but a `pathPrefix` and the protocol should be set from `X-Forwarded-Proto`. Thanks to this design bug we cannot test connectivity to the blog in PRs until the internal gateway is configured to use TLS. I'll revisit this PR when that happens.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8117#issuecomment-589741905
https://github.com/hail-is/hail/pull/8117#issuecomment-589741905:199,Testability,test,test,199,Ghost does not respect the `X-Forwarded` headers. It should not have a `url` parameter but a `pathPrefix` and the protocol should be set from `X-Forwarded-Proto`. Thanks to this design bug we cannot test connectivity to the blog in PRs until the internal gateway is configured to use TLS. I'll revisit this PR when that happens.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8117#issuecomment-589741905
https://github.com/hail-is/hail/pull/8123#issuecomment-588545378:100,Testability,test,tests,100,"looks very reasonable on first scan, will have time to look more carefully tomorrow. If this passes tests I'm pretty confident it's fine.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8123#issuecomment-588545378
https://github.com/hail-is/hail/pull/8124#issuecomment-588572988:15,Testability,test,test,15,pytest and jvm-test pass,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8124#issuecomment-588572988
https://github.com/hail-is/hail/pull/8126#issuecomment-589390319:185,Integrability,interface,interface,185,"Got rid of copyStreamable, and set getNestedElementPTypesOfSameType to use concrete constructors as we discussed at lunch. The child ptypes (elementType, fields) are always cast to the interface, not the concrete class, since this is the correct behavior in our restriction hierarchy (upcast to canonical). This is obviously still not quite right (need to check whether anything canonical, not just head), but that's outside the scope of this pr.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8126#issuecomment-589390319
https://github.com/hail-is/hail/pull/8127#issuecomment-589025030:313,Usability,simpl,simpler,313,"I don't like the proliferation of `.copy` methods -- I find it extremely hard to reason about what they're actually doing and where they should be used. I think it would be reasonable for each PType to implement a `setRequired` method, which seems to require the same amount of total code, but makes things a bit simpler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8127#issuecomment-589025030
https://github.com/hail-is/hail/pull/8127#issuecomment-589146282:315,Usability,simpl,simpler,315,"> I don't like the proliferation of `.copy` methods -- I find it extremely hard to reason about what they're actually doing and where they should be used. I think it would be reasonable for each PType to implement a `setRequired` method, which seems to require the same amount of total code, but makes things a bit simpler. Will do this. At the same time, would you mind helping me understand why you don't like .copy? This has come up before, and wasn't fully explained. The semantics seem straightforward to me, and for case classes require no new code (since case classes define this copy method): ""construct a new instance of this class, using constructor arguments whose defaults are the object's constructor arguments"". I'd like to understand how you see .copy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8127#issuecomment-589146282
https://github.com/hail-is/hail/pull/8129#issuecomment-589676137:711,Security,expose,expose,711,"> This doesn't track the length if known like the old code. That will make some things e.g. ToArray much more expensive. I think you should finish all the stream processing nodes to unblock lowering and then return to this. Right, I was waiting to get the hard parts right first. This should be easy to add back. > Finally, this seems a silent on the the question of region management. What's your picture here? The region management write had consumers passing regions to producers, but I don't see how that fits in here. I put the region management on hold when I realized there was no way to make a stream free any regions it owns when its consumer stops pulling from it. So the new stream design is just to expose those setup and finalization hooks, where creating and freeing regions can go. When I start moving the region management logic I had over to the new design, I'll have to see if regions have to be baked into streams, or if they can be orthogonal. My plan was to first try making `EmitStream` return `Stream[Region => EmitTriplet]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8129#issuecomment-589676137
https://github.com/hail-is/hail/pull/8129#issuecomment-589676137:839,Testability,log,logic,839,"> This doesn't track the length if known like the old code. That will make some things e.g. ToArray much more expensive. I think you should finish all the stream processing nodes to unblock lowering and then return to this. Right, I was waiting to get the hard parts right first. This should be easy to add back. > Finally, this seems a silent on the the question of region management. What's your picture here? The region management write had consumers passing regions to producers, but I don't see how that fits in here. I put the region management on hold when I realized there was no way to make a stream free any regions it owns when its consumer stops pulling from it. So the new stream design is just to expose those setup and finalization hooks, where creating and freeing regions can go. When I start moving the region management logic I had over to the new design, I'll have to see if regions have to be baked into streams, or if they can be orthogonal. My plan was to first try making `EmitStream` return `Stream[Region => EmitTriplet]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8129#issuecomment-589676137
https://github.com/hail-is/hail/pull/8129#issuecomment-589715873:108,Modifiability,refactor,refactoring,108,"I've added real tests, that check that setup and finalization are correctly paired. If it's okay to wait on refactoring COption (which I don't think will be any harder to do later), then this should be ready for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8129#issuecomment-589715873
https://github.com/hail-is/hail/pull/8129#issuecomment-589715873:16,Testability,test,tests,16,"I've added real tests, that check that setup and finalization are correctly paired. If it's okay to wait on refactoring COption (which I don't think will be any harder to do later), then this should be ready for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8129#issuecomment-589715873
https://github.com/hail-is/hail/pull/8132#issuecomment-589323266:100,Integrability,Interface,Interface,100,"Tim would you prefer RVD.unify to be separate from RVD.union, such that the caller controls upcast? Interface seems much simpler if RVD.union calls RVD.unify, but may result in unify being called too many times (if the caller unifies rvds, and doesn't realize that they also transitively call RVD.unify because some function the caller directly calls calls RVD.union)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8132#issuecomment-589323266
https://github.com/hail-is/hail/pull/8132#issuecomment-589323266:121,Usability,simpl,simpler,121,"Tim would you prefer RVD.unify to be separate from RVD.union, such that the caller controls upcast? Interface seems much simpler if RVD.union calls RVD.unify, but may result in unify being called too many times (if the caller unifies rvds, and doesn't realize that they also transitively call RVD.unify because some function the caller directly calls calls RVD.union)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8132#issuecomment-589323266
https://github.com/hail-is/hail/pull/8134#issuecomment-589502275:20,Testability,test,test,20,passes beside batch test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8134#issuecomment-589502275
https://github.com/hail-is/hail/pull/8136#issuecomment-591472185:16,Testability,test,test,16,"No, I wanted to test the PR build path. As you can see itâ€™s broken ðŸ˜‰",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8136#issuecomment-591472185
https://github.com/hail-is/hail/issues/8138#issuecomment-594938133:121,Integrability,message,message,121,"@patrick-schultz What should the behavior here be like?; During the execution of the above, we are getting the following message in the log:; ```; 2020-03-04 18:34:38 root: INFO: invalid partitioner: !lteqWithOverlap(-1)([[]-[]].right, [[]-[]].left); ```; These are empty range bounds, I feel like in this particular case, `lteqWithOverlap` should be true.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8138#issuecomment-594938133
https://github.com/hail-is/hail/issues/8138#issuecomment-594938133:136,Testability,log,log,136,"@patrick-schultz What should the behavior here be like?; During the execution of the above, we are getting the following message in the log:; ```; 2020-03-04 18:34:38 root: INFO: invalid partitioner: !lteqWithOverlap(-1)([[]-[]].right, [[]-[]].left); ```; These are empty range bounds, I feel like in this particular case, `lteqWithOverlap` should be true.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8138#issuecomment-594938133
https://github.com/hail-is/hail/issues/8138#issuecomment-595422967:24,Testability,assert,assertion,24,`RVDPartitioner` has an assertion that `allowedOverlap >= 0`. I think `lteqWithOverlap` assumes that as well. Can you figure out where the -1 is coming from?. Edit: I take it back. I did test the -1 case. Let me remember how this works.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8138#issuecomment-595422967
https://github.com/hail-is/hail/issues/8138#issuecomment-595422967:187,Testability,test,test,187,`RVDPartitioner` has an assertion that `allowedOverlap >= 0`. I think `lteqWithOverlap` assumes that as well. Can you figure out where the -1 is coming from?. Edit: I take it back. I did test the -1 case. Let me remember how this works.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8138#issuecomment-595422967
https://github.com/hail-is/hail/issues/8138#issuecomment-595460712:65,Testability,assert,assertion,65,"In this particular case, it comes from `repartition` in RVD, the assertion that is failing is this one. ```scala; require(newPartitioner.satisfiesAllowedOverlap(newPartitioner.kType.size - 1)); ```; In this case `kType` is empty, so `kType.size` is 0.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8138#issuecomment-595460712
https://github.com/hail-is/hail/issues/8138#issuecomment-597057008:136,Testability,log,logic,136,Thanks for digging in to this Chris. Sorry I haven't had much time to help. I think I was able to get my head back into the partitioner logic during my walk to work this morning. I'll spend some time looking at this after team meeting today.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8138#issuecomment-597057008
https://github.com/hail-is/hail/issues/8138#issuecomment-597279382:37,Testability,log,logic,37,"I think the interval and partitioner logic is all correct. I think the problem is that the `naiveCoalesce` implementation is wrong when the key is empty. In that case, it computes a new partitioner, which will just be fewer partitions with range bounds [[]-[]], then tries to repartition to that. It's right to refuse, because the repartitioner works under the assumption that each key is contained in at most one target partition's interval.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8138#issuecomment-597279382
https://github.com/hail-is/hail/pull/8142#issuecomment-589902885:25,Availability,Error,Error,25,"Pausing for a few hours. Error in testEmitLeftJoinDistinct is a bit strange. IR contains only constants, and appears correctly inferred as having required elements, but a missing element is found at emit time. <img width=""2270"" alt=""Screenshot 2020-02-21 20 38 52"" src=""https://user-images.githubusercontent.com/5543229/75083876-2f3f9d00-54ea-11ea-9bb8-45f9e708a50b.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8142#issuecomment-589902885
https://github.com/hail-is/hail/pull/8142#issuecomment-589902885:34,Testability,test,testEmitLeftJoinDistinct,34,"Pausing for a few hours. Error in testEmitLeftJoinDistinct is a bit strange. IR contains only constants, and appears correctly inferred as having required elements, but a missing element is found at emit time. <img width=""2270"" alt=""Screenshot 2020-02-21 20 38 52"" src=""https://user-images.githubusercontent.com/5543229/75083876-2f3f9d00-54ea-11ea-9bb8-45f9e708a50b.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8142#issuecomment-589902885
https://github.com/hail-is/hail/pull/8142#issuecomment-590029353:11,Availability,failure,failure,11,"Working on failure in takeByAnnotationAnnotation. Error is segmentation fault, triggered by presence of null values, in either key or value position of Row(). Reverting recent RVB and SRVB changes did not fix, so issue does not appear to be there (also no uses of .pType in these files)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8142#issuecomment-590029353
https://github.com/hail-is/hail/pull/8142#issuecomment-590029353:50,Availability,Error,Error,50,"Working on failure in takeByAnnotationAnnotation. Error is segmentation fault, triggered by presence of null values, in either key or value position of Row(). Reverting recent RVB and SRVB changes did not fix, so issue does not appear to be there (also no uses of .pType in these files)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8142#issuecomment-590029353
https://github.com/hail-is/hail/pull/8142#issuecomment-590029353:72,Availability,fault,fault,72,"Working on failure in takeByAnnotationAnnotation. Error is segmentation fault, triggered by presence of null values, in either key or value position of Row(). Reverting recent RVB and SRVB changes did not fix, so issue does not appear to be there (also no uses of .pType in these files)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8142#issuecomment-590029353
https://github.com/hail-is/hail/pull/8144#issuecomment-590144918:302,Integrability,wrap,wraps,302,"Physical values in the emitter are currently represented by a single scalar JVM value, a Code[T]. The goal of physical values is to make the representation of values more general. In addition, physical values carry a PType, which Code[T] doesn't. The PValue introduced here isn't a ""real"" one, it just wraps the previous Code[T] and carries the PType. I will move towards the more general picture in pieces.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8144#issuecomment-590144918
https://github.com/hail-is/hail/issues/8146#issuecomment-590927561:79,Availability,error,error,79,"Ok. What's going on here is like a proper pipe, we're consuming vep's standard error on a background thread and outputting it to System.err , then in the failure path, we're attempting to consume it again. This goes poorly. Fix incoming.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-590927561
https://github.com/hail-is/hail/issues/8146#issuecomment-590927561:154,Availability,failure,failure,154,"Ok. What's going on here is like a proper pipe, we're consuming vep's standard error on a background thread and outputting it to System.err , then in the failure path, we're attempting to consume it again. This goes poorly. Fix incoming.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-590927561
https://github.com/hail-is/hail/issues/8146#issuecomment-591009197:85,Availability,error,error,85,"FWIW it does go into the container logs which is how I've always pulled out the true error, but I'm not sure how to get that on every system. For future reference, there's an even more pernicious issue, which is that when running VEP with `-o STDOUT` it actually suppresses certain error messages too - and there's not much you can do about that unless you actually go in and run VEP manually without that, in the environment that hail uses.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-591009197
https://github.com/hail-is/hail/issues/8146#issuecomment-591009197:282,Availability,error,error,282,"FWIW it does go into the container logs which is how I've always pulled out the true error, but I'm not sure how to get that on every system. For future reference, there's an even more pernicious issue, which is that when running VEP with `-o STDOUT` it actually suppresses certain error messages too - and there's not much you can do about that unless you actually go in and run VEP manually without that, in the environment that hail uses.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-591009197
https://github.com/hail-is/hail/issues/8146#issuecomment-591009197:288,Integrability,message,messages,288,"FWIW it does go into the container logs which is how I've always pulled out the true error, but I'm not sure how to get that on every system. For future reference, there's an even more pernicious issue, which is that when running VEP with `-o STDOUT` it actually suppresses certain error messages too - and there's not much you can do about that unless you actually go in and run VEP manually without that, in the environment that hail uses.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-591009197
https://github.com/hail-is/hail/issues/8146#issuecomment-591009197:35,Testability,log,logs,35,"FWIW it does go into the container logs which is how I've always pulled out the true error, but I'm not sure how to get that on every system. For future reference, there's an even more pernicious issue, which is that when running VEP with `-o STDOUT` it actually suppresses certain error messages too - and there's not much you can do about that unless you actually go in and run VEP manually without that, in the environment that hail uses.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-591009197
https://github.com/hail-is/hail/issues/8146#issuecomment-591013808:118,Integrability,message,message,118,"This change will make it so that it no longer goes to the container logs, and instead will be placed in the exception message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-591013808
https://github.com/hail-is/hail/issues/8146#issuecomment-591013808:68,Testability,log,logs,68,"This change will make it so that it no longer goes to the container logs, and instead will be placed in the exception message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8146#issuecomment-591013808
https://github.com/hail-is/hail/pull/8148#issuecomment-590660567:284,Energy Efficiency,efficient,efficient,284,"@patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure? This PR gets ride of MakeArray but until MakeStream, I don't think this is viable. Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. When the stream consumer is smaller, we might consider inlining it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590660567
https://github.com/hail-is/hail/pull/8148#issuecomment-590660567:47,Integrability,wrap,wrap,47,"@patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure? This PR gets ride of MakeArray but until MakeStream, I don't think this is viable. Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. When the stream consumer is smaller, we might consider inlining it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590660567
https://github.com/hail-is/hail/pull/8148#issuecomment-590660567:76,Safety,avoid,avoid,76,"@patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure? This PR gets ride of MakeArray but until MakeStream, I don't think this is viable. Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. When the stream consumer is smaller, we might consider inlining it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590660567
https://github.com/hail-is/hail/pull/8148#issuecomment-590861391:706,Energy Efficiency,efficient,efficient,706,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391
https://github.com/hail-is/hail/pull/8148#issuecomment-590861391:49,Integrability,wrap,wrap,49,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391
https://github.com/hail-is/hail/pull/8148#issuecomment-590861391:172,Integrability,wrap,wrap,172,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391
https://github.com/hail-is/hail/pull/8148#issuecomment-590861391:499,Performance,perform,performance,499,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391
https://github.com/hail-is/hail/pull/8148#issuecomment-590861391:78,Safety,avoid,avoid,78,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391
https://github.com/hail-is/hail/pull/8148#issuecomment-590861391:977,Usability,simpl,simplify,977,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391
https://github.com/hail-is/hail/pull/8148#issuecomment-590972329:1022,Usability,simpl,simpler,1022,"Here are some slightly scattered thoughts:. You're pointing out that sometimes, it is better to unroll a stream, meaning the consumer code is repeated once per stream element. There is a more general version of this, where you want to concatenate several streams, and you might want to repeat the consumer code once per source stream. My instinct in cases like that is that both options (unroll the stream or not) should be explicitly representable in the IR, and the choice should be made before code generation. I think the fundamental issue you're pointing out is that MakeStream is most naturally a push stream (it wants to be the one driving the outer loop), and that because we don't support push streams we're forced to generate suboptimal code. But zip doesn't work for push streams. I think the only general solution is to support both push streams and pull streams explicitly, but that would add a significant amount of complexity. That may be worth doing at some point, but I definitely think we should get the simpler version working first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590972329
https://github.com/hail-is/hail/pull/8148#issuecomment-591932796:250,Modifiability,variab,variable,250,"> should be explicitly representable in the IR. How would that work with MakeArray?. > MakeStream is most naturally a push stream. I'm not sure I see that. If you don't duplicate the consumer, the push code is the same. The question is, do you use a variable + switch to track where you are in the MakeStream, or do you use the program counter via labels, where you get the latter from the former by code duplication.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-591932796
https://github.com/hail-is/hail/pull/8148#issuecomment-591935928:64,Integrability,wrap,wrapping,64,"I'm going to close this and revisit it after (1) we're handling wrapping for the stream code, and (2) we can generate roughly comparable code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-591935928
https://github.com/hail-is/hail/pull/8148#issuecomment-591962972:805,Integrability,wrap,wrap,805,">> should be explicitly representable in the IR; >; > How would that work with MakeArray?. I have some thoughts on that, but I think it would require a bigger change to how we represent streams in the IR. One idea is to put stream producers and stream consumers on equal footing. Right now, a stream consumer is a node with a stream child that produces a non-stream value, e.g. `ArrayFor(stream, name, body)`. We could let stream consumers be values themselves, `ArrayFor(name, body)`, with other nodes for connecting producers with consumers, e.g. `RunStream(stream, ArrayFor(name, body))`. Then I think we would need two `MakeStream`s, one that can be unrolled, and one that can be zipped. The former just directly takes a consumer and produces a value (and can choose to duplicate the consumer code or wrap it in a method). The later constructs a stream producer, which can be freely used in zips, etc. It would definitely require some thought how to optimize such a representation (is there a normal form?). > I'm not sure I see that. If you don't duplicate the consumer, the push code is the same. The question is, do you use a variable + switch to track where you are in the MakeStream, or do you use the program counter via labels, where you get the latter from the former by code duplication. I think the easiest way to see it is to think about how you would zip two `MakeStream`s, both of which represent their state using the program counter. You could do it by statically fusing them into a single `MakeStream`, but what if it was something more complicated like `Zip(Map(MakeStream(...), ...), Filter(MakeStream(...), ...))`? You can't represent a compound state with a single program counter. At a higher level, you can only do the optimization of unrolling the `MakeStream` loop if that is the outermost loop in the generated code. A producer wanting to create the outermost loop is what I mean by a push stream. By contrast, to be able to zip arbitrary streams, you want the consumer to",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-591962972
https://github.com/hail-is/hail/pull/8148#issuecomment-591962972:1133,Modifiability,variab,variable,1133,"h MakeArray?. I have some thoughts on that, but I think it would require a bigger change to how we represent streams in the IR. One idea is to put stream producers and stream consumers on equal footing. Right now, a stream consumer is a node with a stream child that produces a non-stream value, e.g. `ArrayFor(stream, name, body)`. We could let stream consumers be values themselves, `ArrayFor(name, body)`, with other nodes for connecting producers with consumers, e.g. `RunStream(stream, ArrayFor(name, body))`. Then I think we would need two `MakeStream`s, one that can be unrolled, and one that can be zipped. The former just directly takes a consumer and produces a value (and can choose to duplicate the consumer code or wrap it in a method). The later constructs a stream producer, which can be freely used in zips, etc. It would definitely require some thought how to optimize such a representation (is there a normal form?). > I'm not sure I see that. If you don't duplicate the consumer, the push code is the same. The question is, do you use a variable + switch to track where you are in the MakeStream, or do you use the program counter via labels, where you get the latter from the former by code duplication. I think the easiest way to see it is to think about how you would zip two `MakeStream`s, both of which represent their state using the program counter. You could do it by statically fusing them into a single `MakeStream`, but what if it was something more complicated like `Zip(Map(MakeStream(...), ...), Filter(MakeStream(...), ...))`? You can't represent a compound state with a single program counter. At a higher level, you can only do the optimization of unrolling the `MakeStream` loop if that is the outermost loop in the generated code. A producer wanting to create the outermost loop is what I mean by a push stream. By contrast, to be able to zip arbitrary streams, you want the consumer to create the outermost loop, and on each iteration ""pull"" from each producer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-591962972
https://github.com/hail-is/hail/pull/8148#issuecomment-591962972:954,Performance,optimiz,optimize,954,">> should be explicitly representable in the IR; >; > How would that work with MakeArray?. I have some thoughts on that, but I think it would require a bigger change to how we represent streams in the IR. One idea is to put stream producers and stream consumers on equal footing. Right now, a stream consumer is a node with a stream child that produces a non-stream value, e.g. `ArrayFor(stream, name, body)`. We could let stream consumers be values themselves, `ArrayFor(name, body)`, with other nodes for connecting producers with consumers, e.g. `RunStream(stream, ArrayFor(name, body))`. Then I think we would need two `MakeStream`s, one that can be unrolled, and one that can be zipped. The former just directly takes a consumer and produces a value (and can choose to duplicate the consumer code or wrap it in a method). The later constructs a stream producer, which can be freely used in zips, etc. It would definitely require some thought how to optimize such a representation (is there a normal form?). > I'm not sure I see that. If you don't duplicate the consumer, the push code is the same. The question is, do you use a variable + switch to track where you are in the MakeStream, or do you use the program counter via labels, where you get the latter from the former by code duplication. I think the easiest way to see it is to think about how you would zip two `MakeStream`s, both of which represent their state using the program counter. You could do it by statically fusing them into a single `MakeStream`, but what if it was something more complicated like `Zip(Map(MakeStream(...), ...), Filter(MakeStream(...), ...))`? You can't represent a compound state with a single program counter. At a higher level, you can only do the optimization of unrolling the `MakeStream` loop if that is the outermost loop in the generated code. A producer wanting to create the outermost loop is what I mean by a push stream. By contrast, to be able to zip arbitrary streams, you want the consumer to",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-591962972
https://github.com/hail-is/hail/pull/8148#issuecomment-591962972:1745,Performance,optimiz,optimization,1745,"h MakeArray?. I have some thoughts on that, but I think it would require a bigger change to how we represent streams in the IR. One idea is to put stream producers and stream consumers on equal footing. Right now, a stream consumer is a node with a stream child that produces a non-stream value, e.g. `ArrayFor(stream, name, body)`. We could let stream consumers be values themselves, `ArrayFor(name, body)`, with other nodes for connecting producers with consumers, e.g. `RunStream(stream, ArrayFor(name, body))`. Then I think we would need two `MakeStream`s, one that can be unrolled, and one that can be zipped. The former just directly takes a consumer and produces a value (and can choose to duplicate the consumer code or wrap it in a method). The later constructs a stream producer, which can be freely used in zips, etc. It would definitely require some thought how to optimize such a representation (is there a normal form?). > I'm not sure I see that. If you don't duplicate the consumer, the push code is the same. The question is, do you use a variable + switch to track where you are in the MakeStream, or do you use the program counter via labels, where you get the latter from the former by code duplication. I think the easiest way to see it is to think about how you would zip two `MakeStream`s, both of which represent their state using the program counter. You could do it by statically fusing them into a single `MakeStream`, but what if it was something more complicated like `Zip(Map(MakeStream(...), ...), Filter(MakeStream(...), ...))`? You can't represent a compound state with a single program counter. At a higher level, you can only do the optimization of unrolling the `MakeStream` loop if that is the outermost loop in the generated code. A producer wanting to create the outermost loop is what I mean by a push stream. By contrast, to be able to zip arbitrary streams, you want the consumer to create the outermost loop, and on each iteration ""pull"" from each producer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-591962972
https://github.com/hail-is/hail/pull/8149#issuecomment-590890096:36,Usability,simpl,simplify,36,I have a second PR coming that will simplify the makefiles but is not a functional change.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-590890096
https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:255,Energy Efficiency,schedul,scheduler,255,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:297,Energy Efficiency,schedul,schedule,297,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:481,Energy Efficiency,schedul,scheduling,481,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:458,Integrability,message,messages,458,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:65,Modifiability,config,config,65,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:395,Performance,perform,performance,395,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:417,Performance,load,load,417,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:425,Performance,latency,latency,425,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:470,Performance,latency,latency,470,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:13,Usability,clear,clear,13,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256
https://github.com/hail-is/hail/pull/8149#issuecomment-592059899:176,Energy Efficiency,schedul,scheduling,176,> I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job. This is a good idea. My current thinking for the autoscaler is to utilize multiple input parameters such as the ones you list to determine the optimal number of instances to create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-592059899
https://github.com/hail-is/hail/pull/8149#issuecomment-592059899:153,Integrability,message,messages,153,> I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job. This is a good idea. My current thinking for the autoscaler is to utilize multiple input parameters such as the ones you list to determine the optimal number of instances to create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-592059899
https://github.com/hail-is/hail/pull/8149#issuecomment-592059899:90,Performance,perform,performance,90,> I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job. This is a good idea. My current thinking for the autoscaler is to utilize multiple input parameters such as the ones you list to determine the optimal number of instances to create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-592059899
https://github.com/hail-is/hail/pull/8149#issuecomment-592059899:112,Performance,load,load,112,> I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job. This is a good idea. My current thinking for the autoscaler is to utilize multiple input parameters such as the ones you list to determine the optimal number of instances to create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-592059899
https://github.com/hail-is/hail/pull/8149#issuecomment-592059899:120,Performance,latency,latency,120,> I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job. This is a good idea. My current thinking for the autoscaler is to utilize multiple input parameters such as the ones you list to determine the optimal number of instances to create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-592059899
https://github.com/hail-is/hail/pull/8149#issuecomment-592059899:165,Performance,latency,latency,165,> I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job. This is a good idea. My current thinking for the autoscaler is to utilize multiple input parameters such as the ones you list to determine the optimal number of instances to create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-592059899
https://github.com/hail-is/hail/pull/8149#issuecomment-592061652:200,Energy Efficiency,reduce,reduce,200,"I agree. I think the regulation has to happen in two ways: if it receives too much work, it should reject the work so it doesn't get overloaded. If it is consistently getting too much work, it should reduce the size of the worker pool to generate less work for itself.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-592061652
https://github.com/hail-is/hail/pull/8149#issuecomment-592103445:38,Testability,log,log,38,"@cseed yes, if you do something like `log.info(""hi"", extra_field=31)` that will be easy to parse.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-592103445
https://github.com/hail-is/hail/pull/8150#issuecomment-590976833:37,Testability,test,test,37,I think we need to at least manually test it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8150#issuecomment-590976833
https://github.com/hail-is/hail/pull/8150#issuecomment-590993084:9,Testability,test,test,9,How do I test this? I don't know how to make vep fail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8150#issuecomment-590993084
https://github.com/hail-is/hail/pull/8150#issuecomment-591007231:23,Testability,test,test,23,"I just meant we should test that VEP still works on this branch. Like, do we have automated tests for VEP?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8150#issuecomment-591007231
https://github.com/hail-is/hail/pull/8150#issuecomment-591007231:92,Testability,test,tests,92,"I just meant we should test that VEP still works on this branch. Like, do we have automated tests for VEP?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8150#issuecomment-591007231
https://github.com/hail-is/hail/pull/8150#issuecomment-591010047:23,Testability,test,test-dataproc,23,"Oh, that's easy, `make test-dataproc`, doing that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8150#issuecomment-591010047
https://github.com/hail-is/hail/pull/8150#issuecomment-591026741:17,Testability,test,test-dataproc,17,@johnc1231 `make test-dataproc` succeeded,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8150#issuecomment-591026741
https://github.com/hail-is/hail/pull/8156#issuecomment-594139749:8,Testability,test,tests,8,Failing tests for real,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8156#issuecomment-594139749
https://github.com/hail-is/hail/pull/8156#issuecomment-594140056:10,Testability,test,tests,10,> Failing tests for real. it makes me sad we have to clarify :(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8156#issuecomment-594140056
https://github.com/hail-is/hail/pull/8156#issuecomment-594161055:20,Testability,test,test,20,I fixed the failing test by merging in #8142. This will have to wait for that to go in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8156#issuecomment-594161055
https://github.com/hail-is/hail/pull/8156#issuecomment-594495172:20,Availability,failure,failures,20,"Looks like this has failures and needs a rebase. Your PR stack is getting pretty high so let's keep the bottom moving. Also, I rebased my lir branch on Value[T] and now I'm passing the asm4s tests and most other tests are failing on joinpoint which I didn't port. So this stack is now a blocker for me to resume that thread.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8156#issuecomment-594495172
https://github.com/hail-is/hail/pull/8156#issuecomment-594495172:191,Testability,test,tests,191,"Looks like this has failures and needs a rebase. Your PR stack is getting pretty high so let's keep the bottom moving. Also, I rebased my lir branch on Value[T] and now I'm passing the asm4s tests and most other tests are failing on joinpoint which I didn't port. So this stack is now a blocker for me to resume that thread.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8156#issuecomment-594495172
https://github.com/hail-is/hail/pull/8156#issuecomment-594495172:212,Testability,test,tests,212,"Looks like this has failures and needs a rebase. Your PR stack is getting pretty high so let's keep the bottom moving. Also, I rebased my lir branch on Value[T] and now I'm passing the asm4s tests and most other tests are failing on joinpoint which I didn't port. So this stack is now a blocker for me to resume that thread.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8156#issuecomment-594495172
https://github.com/hail-is/hail/pull/8156#issuecomment-594495172:305,Usability,resume,resume,305,"Looks like this has failures and needs a rebase. Your PR stack is getting pretty high so let's keep the bottom moving. Also, I rebased my lir branch on Value[T] and now I'm passing the asm4s tests and most other tests are failing on joinpoint which I didn't port. So this stack is now a blocker for me to resume that thread.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8156#issuecomment-594495172
https://github.com/hail-is/hail/pull/8160#issuecomment-591515885:63,Safety,timeout,timeout,63,The inner docker retry function is catching the outer wait_for timeout :(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8160#issuecomment-591515885
https://github.com/hail-is/hail/pull/8164#issuecomment-591670074:6,Usability,clear,clear,6,"to be clear, the comment on default is a conversation starter, not a change request",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8164#issuecomment-591670074
https://github.com/hail-is/hail/pull/8171#issuecomment-592219318:301,Performance,optimiz,optimizer,301,"> I think we can also get rid of TIterable by refining the type casting in ToSet/ToDict/ToArray/ToStream nodes to reflect the new semantics. Yes, I will consider that in a later pass. > For python---the idea is that we're not really introducing any user-visible changes, right?. Correct. And yes, the optimizer should strip out any glue. > I'm a little nervous about the ToArray(ToStream(dict/set) idiom we're using here. OK, this is fixed. I added a CastToArray which takes a TContainer and does a no-op cast to array type. I use it wherever I can and I added a simplify rule to introduce it when possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8171#issuecomment-592219318
https://github.com/hail-is/hail/pull/8171#issuecomment-592219318:563,Usability,simpl,simplify,563,"> I think we can also get rid of TIterable by refining the type casting in ToSet/ToDict/ToArray/ToStream nodes to reflect the new semantics. Yes, I will consider that in a later pass. > For python---the idea is that we're not really introducing any user-visible changes, right?. Correct. And yes, the optimizer should strip out any glue. > I'm a little nervous about the ToArray(ToStream(dict/set) idiom we're using here. OK, this is fixed. I added a CastToArray which takes a TContainer and does a no-op cast to array type. I use it wherever I can and I added a simplify rule to introduce it when possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8171#issuecomment-592219318
https://github.com/hail-is/hail/pull/8171#issuecomment-592220580:429,Security,hash,hashtable,429,"Some more comments on this array casting business:; - We could have made Array{Ref, Len} work on any container. That would fix most uses of CastToArray, but it would still leave out the cast that we genuinely want to convert a set/dict to an array. I don't think we ever do that, but a user could with hl.array.; - It is partially an accident that dict, set and array use the same representation. We could imagine others, e.g. a hashtable for set. In which case, the cast wouldn't work, the ArrayLen probably would, and ArrayRef might or might not.; - Array{Ref, Len} could apply to streams, too (they'll need custom codegen tho).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8171#issuecomment-592220580
https://github.com/hail-is/hail/pull/8171#issuecomment-592681047:383,Testability,log,logic,383,"Looks good! ; - There's a couple of places where we use `hl.array()` for our own methods, e.g. `Table.to_matrix_table` and the vcf_combiner that I think are just conceptually ""turn this set/dict into an array, please"" as opposed to casting out of convenience (like we did for filter, etc. before this PR).; - I think as we add more representations for array/set/dict the CastToArray logic will just need to take into account the pType that it's casting to/from accordingly; it'll therefore still ""work"" but won't always return a sorted array or guarantee an ordering (which I think we shouldn't consider part of the semantics anyways); - I don't know that ArrayRef makes sense on general Set/Dict objects directly anyways, since they don't conceptually have a well-defined element order (I think a lot of languages don't let you index into sets/dicts by index, right?). I think I'd agree that ArrayLen should work on everything.; - I'm kind of in favor of introducing companion StreamRef/StreamLen nodes for ArrayRef/ArrayLen to work on streams, just because they'll look very distinct in the generated code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8171#issuecomment-592681047
https://github.com/hail-is/hail/pull/8178#issuecomment-599780653:121,Deployability,deploy,deploying,121,Can you refresh my memory on what this is about? I basically thought everything in vdc/ was complete bitrot. This is for deploying ... a new Hail Python package?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8178#issuecomment-599780653
https://github.com/hail-is/hail/pull/8178#issuecomment-599784846:4,Deployability,deploy,deploy,4,"The deploy service account was unnecessarily privileged and not actively used. AFAICT, it's used only by this make file to stand up a new hail vdc from scratch. I removed the deploy service account and modified this makefile to create it, use it, and then destroy it when finished. If this is all bitrot, then I suppose it doesn't matter. I tested that these new and modified targets work as expected. What did you use to set up Konrad's project?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8178#issuecomment-599784846
https://github.com/hail-is/hail/pull/8178#issuecomment-599784846:175,Deployability,deploy,deploy,175,"The deploy service account was unnecessarily privileged and not actively used. AFAICT, it's used only by this make file to stand up a new hail vdc from scratch. I removed the deploy service account and modified this makefile to create it, use it, and then destroy it when finished. If this is all bitrot, then I suppose it doesn't matter. I tested that these new and modified targets work as expected. What did you use to set up Konrad's project?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8178#issuecomment-599784846
https://github.com/hail-is/hail/pull/8178#issuecomment-599784846:341,Testability,test,tested,341,"The deploy service account was unnecessarily privileged and not actively used. AFAICT, it's used only by this make file to stand up a new hail vdc from scratch. I removed the deploy service account and modified this makefile to create it, use it, and then destroy it when finished. If this is all bitrot, then I suppose it doesn't matter. I tested that these new and modified targets work as expected. What did you use to set up Konrad's project?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8178#issuecomment-599784846
https://github.com/hail-is/hail/pull/8179#issuecomment-592277392:566,Integrability,depend,dependent,566,"Yes, exactly. We started by building something to compile instances of AsmFunctionN, that's what FunctionBuilder is. MethodBuilder is for adding auxiliary methods to the class being compiled which will be called from the apply method. In the old way, FunctionBuilder was central. Now I'm trying to tease things apart so they are analogous to the core Java constructs: class, method, field. FunctionBuilder will just have a class and a method build for the apply method. Splitting out ClassBuilder will also let us build multiple/auxiliary classes (something done by dependent functions).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8179#issuecomment-592277392
https://github.com/hail-is/hail/pull/8179#issuecomment-592278110:538,Integrability,wrap,wrapper,538,"> old world a MethodBuilder had a FunctionBuilder as a field, and now instead a FunctionBuilder has a ClassBuilder and also a way to make a MethodBuilder. Uh, so there are three pictures, the old way, this PR, and where I'm headed (the PR comment); - the old way, method builder had a function builder, because function builder was really also the class builder (for an AsmFunctionN); - now I added class builder and function builder (which was really building a class) has one; - in the final picture, function builder will be a simpler wrapper to class and method builders. Method builders will point back to their class.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8179#issuecomment-592278110
https://github.com/hail-is/hail/pull/8179#issuecomment-592278110:530,Usability,simpl,simpler,530,"> old world a MethodBuilder had a FunctionBuilder as a field, and now instead a FunctionBuilder has a ClassBuilder and also a way to make a MethodBuilder. Uh, so there are three pictures, the old way, this PR, and where I'm headed (the PR comment); - the old way, method builder had a function builder, because function builder was really also the class builder (for an AsmFunctionN); - now I added class builder and function builder (which was really building a class) has one; - in the final picture, function builder will be a simpler wrapper to class and method builders. Method builders will point back to their class.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8179#issuecomment-592278110
https://github.com/hail-is/hail/pull/8190#issuecomment-600700900:48,Availability,down,down,48,"OK, so. The issue seems to be that docker shuts down a connection when an invalid packet is received. https://github.com/moby/libnetwork/issues/1090. Why `git clone` isn't exiting with a non-zero code and thus bailing out of the function, I don't know.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8190#issuecomment-600700900
https://github.com/hail-is/hail/pull/8191#issuecomment-592571143:75,Integrability,depend,depends,75,ugh. the printed form of UnsafeRow is a public API? surely no one actually depends on this,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8191#issuecomment-592571143
https://github.com/hail-is/hail/pull/8191#issuecomment-592571143:25,Safety,Unsafe,UnsafeRow,25,ugh. the printed form of UnsafeRow is a public API? surely no one actually depends on this,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8191#issuecomment-592571143
https://github.com/hail-is/hail/pull/8191#issuecomment-592574334:33,Testability,test,tests,33,"OK, well. This PR modifies those tests to use the new printed form, which was a significant improvement to my development experience of the Shuffler. I also fixed tuples to print with parentheses instead of brackets.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8191#issuecomment-592574334
https://github.com/hail-is/hail/pull/8191#issuecomment-593005968:96,Safety,Unsafe,UnsafeRow,96,I think `format` is doing the wrong thing if you were able to affect python output by modifying UnsafeRow.toString. The format method should be using the Type.str method if objects are not strings or other formattable objects (numerics).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8191#issuecomment-593005968
https://github.com/hail-is/hail/pull/8193#issuecomment-592579928:288,Testability,log,logs,288,It's not in pulling. It seems like it's actually in show based on the stack trace. I assumed it was in `create` because this appeared in the stack trace `name=f'batch-{self.job.batch_id}-job-{self.job.job_id}-{self.name}'` and that's the only place that's used. Let me look at the worker logs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592579928
https://github.com/hail-is/hail/pull/8193#issuecomment-592582922:66,Availability,error,error,66,It was an old worker that didn't have the idempotent updates. The error was in create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592582922
https://github.com/hail-is/hail/pull/8193#issuecomment-592582922:53,Deployability,update,updates,53,It was an old worker that didn't have the idempotent updates. The error was in create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592582922
https://github.com/hail-is/hail/pull/8193#issuecomment-592584656:19,Availability,failure,failure,19,I checked the test failure should be fixed by #8131,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592584656
https://github.com/hail-is/hail/pull/8193#issuecomment-592584656:14,Testability,test,test,14,I checked the test failure should be fixed by #8131,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592584656
https://github.com/hail-is/hail/pull/8193#issuecomment-592590862:33,Availability,error,error,33,I still don't understand how the error you posted above relates to the changes in this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592590862
https://github.com/hail-is/hail/pull/8194#issuecomment-592560456:16,Deployability,deploy,deploy,16,Tested with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8194#issuecomment-592560456
https://github.com/hail-is/hail/pull/8194#issuecomment-592560456:0,Testability,Test,Tested,0,Tested with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8194#issuecomment-592560456
https://github.com/hail-is/hail/pull/8203#issuecomment-592744446:136,Availability,error,errors,136,"Since RegionPool cleans its memory via PhantomReferences now, it is not AutoCloseable. In fact, I'm not sure how we avoided double free errors in the past. I made all the tests not use `using`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8203#issuecomment-592744446
https://github.com/hail-is/hail/pull/8203#issuecomment-592744446:116,Safety,avoid,avoided,116,"Since RegionPool cleans its memory via PhantomReferences now, it is not AutoCloseable. In fact, I'm not sure how we avoided double free errors in the past. I made all the tests not use `using`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8203#issuecomment-592744446
https://github.com/hail-is/hail/pull/8203#issuecomment-592744446:171,Testability,test,tests,171,"Since RegionPool cleans its memory via PhantomReferences now, it is not AutoCloseable. In fact, I'm not sure how we avoided double free errors in the past. I made all the tests not use `using`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8203#issuecomment-592744446
https://github.com/hail-is/hail/pull/8203#issuecomment-604461227:33,Energy Efficiency,reduce,reduce,33,closing until it is necessary to reduce PR burden,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8203#issuecomment-604461227
https://github.com/hail-is/hail/pull/8205#issuecomment-594214674:23,Availability,down,down,23,"So I still need to sit down and actually dig through this, but my initial thoughts:. We should have shuffler tests that exercise the shuffler directly (without going through RVD/TableIR). I don't think they need to be totally comprehensive, or be necessarily independent of all hail infrastructure (e.g. encoders/decoders/types/regions, which you're using in the shuffler), but should exercise whatever the basic interface between the shuffle service and the rest of the world is going to be, independent of the specific implementation that shuffles an RVD. To that end, I'd also advocate for defining a backend-agnostic shuffler interface to shuffle TableIRs/RVDs; instead of needing to check the context flag everywhere you might shuffle something, we'd define what an RVD shuffle looks like independent of whether or not we're using the shuffle service or a Spark shuffle, and then the shuffler interface would call the correct implementation depending on what shuffler if was supposed to use. (I'm not absolutely set on the second thing happening immediately, but I do think it's longer-term the right way to think about how the shuffler plugs in, and it feels like it should be a pretty minimal change from what you have right now to get a possibly-imperfect-but-workable interface in.). In short, this feels like two+ separate PRs to me, in that it's composed of pretty distinct, substantial changes that involve a lot of non-trivial choices:; - the actual implementation of the shuffle service, and; - a client for Hail IRs to interact with the shuffle service + wiring things up to actually go through it. and I'm happy to review both pieces of it, but it might be easier + quicker to get the shuffler itself in if you broke that change out separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205#issuecomment-594214674
https://github.com/hail-is/hail/pull/8205#issuecomment-594214674:413,Integrability,interface,interface,413,"So I still need to sit down and actually dig through this, but my initial thoughts:. We should have shuffler tests that exercise the shuffler directly (without going through RVD/TableIR). I don't think they need to be totally comprehensive, or be necessarily independent of all hail infrastructure (e.g. encoders/decoders/types/regions, which you're using in the shuffler), but should exercise whatever the basic interface between the shuffle service and the rest of the world is going to be, independent of the specific implementation that shuffles an RVD. To that end, I'd also advocate for defining a backend-agnostic shuffler interface to shuffle TableIRs/RVDs; instead of needing to check the context flag everywhere you might shuffle something, we'd define what an RVD shuffle looks like independent of whether or not we're using the shuffle service or a Spark shuffle, and then the shuffler interface would call the correct implementation depending on what shuffler if was supposed to use. (I'm not absolutely set on the second thing happening immediately, but I do think it's longer-term the right way to think about how the shuffler plugs in, and it feels like it should be a pretty minimal change from what you have right now to get a possibly-imperfect-but-workable interface in.). In short, this feels like two+ separate PRs to me, in that it's composed of pretty distinct, substantial changes that involve a lot of non-trivial choices:; - the actual implementation of the shuffle service, and; - a client for Hail IRs to interact with the shuffle service + wiring things up to actually go through it. and I'm happy to review both pieces of it, but it might be easier + quicker to get the shuffler itself in if you broke that change out separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205#issuecomment-594214674
https://github.com/hail-is/hail/pull/8205#issuecomment-594214674:630,Integrability,interface,interface,630,"So I still need to sit down and actually dig through this, but my initial thoughts:. We should have shuffler tests that exercise the shuffler directly (without going through RVD/TableIR). I don't think they need to be totally comprehensive, or be necessarily independent of all hail infrastructure (e.g. encoders/decoders/types/regions, which you're using in the shuffler), but should exercise whatever the basic interface between the shuffle service and the rest of the world is going to be, independent of the specific implementation that shuffles an RVD. To that end, I'd also advocate for defining a backend-agnostic shuffler interface to shuffle TableIRs/RVDs; instead of needing to check the context flag everywhere you might shuffle something, we'd define what an RVD shuffle looks like independent of whether or not we're using the shuffle service or a Spark shuffle, and then the shuffler interface would call the correct implementation depending on what shuffler if was supposed to use. (I'm not absolutely set on the second thing happening immediately, but I do think it's longer-term the right way to think about how the shuffler plugs in, and it feels like it should be a pretty minimal change from what you have right now to get a possibly-imperfect-but-workable interface in.). In short, this feels like two+ separate PRs to me, in that it's composed of pretty distinct, substantial changes that involve a lot of non-trivial choices:; - the actual implementation of the shuffle service, and; - a client for Hail IRs to interact with the shuffle service + wiring things up to actually go through it. and I'm happy to review both pieces of it, but it might be easier + quicker to get the shuffler itself in if you broke that change out separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205#issuecomment-594214674
https://github.com/hail-is/hail/pull/8205#issuecomment-594214674:898,Integrability,interface,interface,898,"So I still need to sit down and actually dig through this, but my initial thoughts:. We should have shuffler tests that exercise the shuffler directly (without going through RVD/TableIR). I don't think they need to be totally comprehensive, or be necessarily independent of all hail infrastructure (e.g. encoders/decoders/types/regions, which you're using in the shuffler), but should exercise whatever the basic interface between the shuffle service and the rest of the world is going to be, independent of the specific implementation that shuffles an RVD. To that end, I'd also advocate for defining a backend-agnostic shuffler interface to shuffle TableIRs/RVDs; instead of needing to check the context flag everywhere you might shuffle something, we'd define what an RVD shuffle looks like independent of whether or not we're using the shuffle service or a Spark shuffle, and then the shuffler interface would call the correct implementation depending on what shuffler if was supposed to use. (I'm not absolutely set on the second thing happening immediately, but I do think it's longer-term the right way to think about how the shuffler plugs in, and it feels like it should be a pretty minimal change from what you have right now to get a possibly-imperfect-but-workable interface in.). In short, this feels like two+ separate PRs to me, in that it's composed of pretty distinct, substantial changes that involve a lot of non-trivial choices:; - the actual implementation of the shuffle service, and; - a client for Hail IRs to interact with the shuffle service + wiring things up to actually go through it. and I'm happy to review both pieces of it, but it might be easier + quicker to get the shuffler itself in if you broke that change out separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205#issuecomment-594214674
https://github.com/hail-is/hail/pull/8205#issuecomment-594214674:946,Integrability,depend,depending,946,"So I still need to sit down and actually dig through this, but my initial thoughts:. We should have shuffler tests that exercise the shuffler directly (without going through RVD/TableIR). I don't think they need to be totally comprehensive, or be necessarily independent of all hail infrastructure (e.g. encoders/decoders/types/regions, which you're using in the shuffler), but should exercise whatever the basic interface between the shuffle service and the rest of the world is going to be, independent of the specific implementation that shuffles an RVD. To that end, I'd also advocate for defining a backend-agnostic shuffler interface to shuffle TableIRs/RVDs; instead of needing to check the context flag everywhere you might shuffle something, we'd define what an RVD shuffle looks like independent of whether or not we're using the shuffle service or a Spark shuffle, and then the shuffler interface would call the correct implementation depending on what shuffler if was supposed to use. (I'm not absolutely set on the second thing happening immediately, but I do think it's longer-term the right way to think about how the shuffler plugs in, and it feels like it should be a pretty minimal change from what you have right now to get a possibly-imperfect-but-workable interface in.). In short, this feels like two+ separate PRs to me, in that it's composed of pretty distinct, substantial changes that involve a lot of non-trivial choices:; - the actual implementation of the shuffle service, and; - a client for Hail IRs to interact with the shuffle service + wiring things up to actually go through it. and I'm happy to review both pieces of it, but it might be easier + quicker to get the shuffler itself in if you broke that change out separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205#issuecomment-594214674
https://github.com/hail-is/hail/pull/8205#issuecomment-594214674:1277,Integrability,interface,interface,1277,"So I still need to sit down and actually dig through this, but my initial thoughts:. We should have shuffler tests that exercise the shuffler directly (without going through RVD/TableIR). I don't think they need to be totally comprehensive, or be necessarily independent of all hail infrastructure (e.g. encoders/decoders/types/regions, which you're using in the shuffler), but should exercise whatever the basic interface between the shuffle service and the rest of the world is going to be, independent of the specific implementation that shuffles an RVD. To that end, I'd also advocate for defining a backend-agnostic shuffler interface to shuffle TableIRs/RVDs; instead of needing to check the context flag everywhere you might shuffle something, we'd define what an RVD shuffle looks like independent of whether or not we're using the shuffle service or a Spark shuffle, and then the shuffler interface would call the correct implementation depending on what shuffler if was supposed to use. (I'm not absolutely set on the second thing happening immediately, but I do think it's longer-term the right way to think about how the shuffler plugs in, and it feels like it should be a pretty minimal change from what you have right now to get a possibly-imperfect-but-workable interface in.). In short, this feels like two+ separate PRs to me, in that it's composed of pretty distinct, substantial changes that involve a lot of non-trivial choices:; - the actual implementation of the shuffle service, and; - a client for Hail IRs to interact with the shuffle service + wiring things up to actually go through it. and I'm happy to review both pieces of it, but it might be easier + quicker to get the shuffler itself in if you broke that change out separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205#issuecomment-594214674
https://github.com/hail-is/hail/pull/8205#issuecomment-594214674:109,Testability,test,tests,109,"So I still need to sit down and actually dig through this, but my initial thoughts:. We should have shuffler tests that exercise the shuffler directly (without going through RVD/TableIR). I don't think they need to be totally comprehensive, or be necessarily independent of all hail infrastructure (e.g. encoders/decoders/types/regions, which you're using in the shuffler), but should exercise whatever the basic interface between the shuffle service and the rest of the world is going to be, independent of the specific implementation that shuffles an RVD. To that end, I'd also advocate for defining a backend-agnostic shuffler interface to shuffle TableIRs/RVDs; instead of needing to check the context flag everywhere you might shuffle something, we'd define what an RVD shuffle looks like independent of whether or not we're using the shuffle service or a Spark shuffle, and then the shuffler interface would call the correct implementation depending on what shuffler if was supposed to use. (I'm not absolutely set on the second thing happening immediately, but I do think it's longer-term the right way to think about how the shuffler plugs in, and it feels like it should be a pretty minimal change from what you have right now to get a possibly-imperfect-but-workable interface in.). In short, this feels like two+ separate PRs to me, in that it's composed of pretty distinct, substantial changes that involve a lot of non-trivial choices:; - the actual implementation of the shuffle service, and; - a client for Hail IRs to interact with the shuffle service + wiring things up to actually go through it. and I'm happy to review both pieces of it, but it might be easier + quicker to get the shuffler itself in if you broke that change out separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8205#issuecomment-594214674
https://github.com/hail-is/hail/pull/8207#issuecomment-593374823:86,Availability,down,down,86,"Note, when I made this change, I ran into a bug where the label in whileLoop got laid down twice. I found the two cuprits:; - one was && and ||, which duplicated the code on different code paths so was conceptually correct but could lead to code explosion. Either way, I rewrote them.; - and the other was in checkedConvertFrom which actually executed its input twice. I will propose some code changes to deal with this reuse issue. In general, I want the picture that Code[_] cannot be placed in multiple locations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8207#issuecomment-593374823
https://github.com/hail-is/hail/pull/8207#issuecomment-593417586:481,Modifiability,variab,variables,481,"> otherwise we need to wait until EmitStream1 is ripped out. That will be very soon. > I will propose some code changes to deal with this reuse issue. In general, I want the picture that Code[_] cannot be placed in multiple locations. Absolutely. I have some WIP from before I shifted focus to streams that makes a `Code[_]` throw an exception at the site where it is used a second time. But it's tangled up with some other changes I was experimenting with, mostly to enable using variables which could be either locals or fields, with the decision being made after the complete `Code[_]` is assembled. I'd be happy to revisit that when lowering is unblocked, or let you do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8207#issuecomment-593417586
https://github.com/hail-is/hail/pull/8210#issuecomment-593504954:188,Availability,down,down,188,"I'd be ok with taking it off the nav bar if there was some other link to it that was reasonable and findable. Python is a good example. They always show newest thing, but they have a drop down at the top that lets you switch: https://docs.python.org/3/library/re.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8210#issuecomment-593504954
https://github.com/hail-is/hail/pull/8218#issuecomment-599765193:254,Availability,down,download,254,Wow this is supremely annoying. https://github.com/jupyter/notebook/issues/3397 Jupyter is basically known to be broken for the normal use case of most Asyncio libraries. The recommended fix is to use a third party monkey patch. I'll revisit this if 1kg download continue to be a frequent issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8218#issuecomment-599765193
https://github.com/hail-is/hail/pull/8218#issuecomment-599765193:222,Deployability,patch,patch,222,Wow this is supremely annoying. https://github.com/jupyter/notebook/issues/3397 Jupyter is basically known to be broken for the normal use case of most Asyncio libraries. The recommended fix is to use a third party monkey patch. I'll revisit this if 1kg download continue to be a frequent issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8218#issuecomment-599765193
https://github.com/hail-is/hail/pull/8220#issuecomment-593643614:81,Availability,recover,recover,81,Do we need to manually delete the stopped but not deleted instances for batch to recover?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8220#issuecomment-593643614
https://github.com/hail-is/hail/pull/8220#issuecomment-593643614:81,Safety,recover,recover,81,Do we need to manually delete the stopped but not deleted instances for batch to recover?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8220#issuecomment-593643614
https://github.com/hail-is/hail/pull/8220#issuecomment-593645975:105,Availability,recover,recover,105,@danking I made a fix that should make it so we don't need to manually delete instances and batch should recover.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8220#issuecomment-593645975
https://github.com/hail-is/hail/pull/8220#issuecomment-593645975:105,Safety,recover,recover,105,@danking I made a fix that should make it so we don't need to manually delete instances and batch should recover.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8220#issuecomment-593645975
https://github.com/hail-is/hail/pull/8230#issuecomment-597107650:8,Testability,test,tests,8,Failing tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8230#issuecomment-597107650
https://github.com/hail-is/hail/pull/8231#issuecomment-594308654:5,Availability,failure,failures,5,test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8231#issuecomment-594308654
https://github.com/hail-is/hail/pull/8231#issuecomment-594308654:0,Testability,test,test,0,test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8231#issuecomment-594308654
https://github.com/hail-is/hail/pull/8236#issuecomment-594546138:61,Modifiability,variab,variables,61,Would previously pull out address and load length into local variables even if we just need to pass along pointer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8236#issuecomment-594546138
https://github.com/hail-is/hail/pull/8236#issuecomment-594546138:38,Performance,load,load,38,Would previously pull out address and load length into local variables even if we just need to pass along pointer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8236#issuecomment-594546138
https://github.com/hail-is/hail/pull/8237#issuecomment-605073218:45,Availability,error,error,45,This seems to have failed due to exactly the error it is supposed to fix. The gear in use by CI is not the gear fixed here. I'll bump.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8237#issuecomment-605073218
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:347,Availability,down,downsampled,347,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:436,Availability,down,downsample,436,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:949,Availability,down,downsampled,949,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:963,Availability,down,downsampled,963,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:996,Availability,down,downsampled,996,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:1314,Availability,down,downsampled,1314,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:1340,Availability,down,downsampled,1340,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:1354,Availability,down,downsampled,1354,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:1385,Availability,down,downsampled,1385,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:1411,Availability,down,downsampled,1411,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:1437,Availability,down,downsampled,1437,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:1451,Availability,down,downsampled,1451,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:1478,Availability,down,downsampled,1478,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:1674,Availability,down,downed,1674,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:1721,Availability,down,downed,1721,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594740307:1404,Modifiability,extend,extend,1404,"Easily reproduced. Here's a modification of the above code that doesn't rely on any external data:. ```; def normalize_contig(contig):; return contig. def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.filter(; mt.Pvalue > p_threshold,; hl.agg.downsample(; x,; y,; label=[; normalize_contig(mt.locus.contig),; hl.str(mt.locus.position),; hl.str(mt.Pvalue),; ],; n_divisions=n_divisions; ); ),; unbinned=hl.agg.filter(; mt.Pvalue <= p_threshold,; hl.agg.collect(hl.struct(; pval=mt.Pvalue,; chrom=normalize_contig(mt.locus.contig),; pos=mt.locus.position,; ac=mt.AC,; af=mt.AF,; an=mt.AN,; alleles=mt.alleles,; beta=mt.BETA,; consequence=hl.if_else(; hl.is_defined(mt.annotation),; mt.annotation,; ""N/A""; ),; gene_name=mt.gene,; is_binned=False; ); ); ); ). downsampled = downsampled.select_cols(; binned=downsampled.binned.map(; lambda a_bin: hl.struct(; pval=hl.float64(a_bin[2][2]),; chrom=a_bin[2][0],; pos=hl.int32(a_bin[2][1]),; ac=hl.literal(0.0),; af=hl.literal(0.0),; an=hl.literal(0),; alleles=hl.literal(['N', 'A']),; beta=hl.literal(0.0),; consequence=""N/A"",; gene_name=""N/A"",; is_binned=True,. ); ),; unbinned=downsampled.unbinned,; ). downsampled = downsampled.select_cols(; data=downsampled.binned.extend(downsampled.unbinned); ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1), BETA=hl.rand_unif(0, 1), annotation=""Foo"", gene=""Geney"", AN = 1, AC = 1.0, AF = 1.0); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594740307
https://github.com/hail-is/hail/issues/8240#issuecomment-594754052:227,Availability,down,downsampled,227,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
https://github.com/hail-is/hail/issues/8240#issuecomment-594754052:274,Availability,down,downsample,274,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
https://github.com/hail-is/hail/issues/8240#issuecomment-594754052:354,Availability,down,downsampled,354,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
https://github.com/hail-is/hail/issues/8240#issuecomment-594754052:368,Availability,down,downsampled,368,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
https://github.com/hail-is/hail/issues/8240#issuecomment-594754052:395,Availability,down,downsampled,395,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
https://github.com/hail-is/hail/issues/8240#issuecomment-594754052:506,Availability,down,downed,506,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
https://github.com/hail-is/hail/issues/8240#issuecomment-594754052:553,Availability,down,downed,553,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
https://github.com/hail-is/hail/issues/8240#issuecomment-594754052:609,Availability,down,downsample,609,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
https://github.com/hail-is/hail/issues/8240#issuecomment-594754052:634,Security,access,accessing,634,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
https://github.com/hail-is/hail/issues/8240#issuecomment-594754052:15,Usability,simpl,simpler,15,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
https://github.com/hail-is/hail/issues/8240#issuecomment-594754052:644,Usability,clear,cleared,644,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052
https://github.com/hail-is/hail/pull/8243#issuecomment-594854003:64,Availability,failure,failures,64,ready to approve once tests passing (test_hail_java has current failures),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8243#issuecomment-594854003
https://github.com/hail-is/hail/pull/8243#issuecomment-594854003:22,Testability,test,tests,22,ready to approve once tests passing (test_hail_java has current failures),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8243#issuecomment-594854003
https://github.com/hail-is/hail/pull/8245#issuecomment-600899413:478,Availability,error,errorTransformer,478,"This PR implements the beginning of the EmitCode and CodeBuilder change discussed here: https://dev.hail.is/t/on-the-subject-of-emittriplet/183/7. The codegen rule for ArrayRef is now the delightfully clear and concise:. ```; EmitCode.fromI(mb) { cb =>; emit(a).toI(cb).flatMap(cb) { (ac) =>; emit(i).toI(cb).flatMap(cb) { (ic) =>; val av = ac.asIndexable.memoize(cb, ""aref_a""); val iv = cb.memoize(ic.tcode[Int], ""aref_i""). cb.ifx(iv < 0 || iv >= av.loadLength(), {; cb._fatal(errorTransformer(; const(""array index out of bounds: index=""); .concat(iv.toS); .concat("", length=""); .concat(av.loadLength().toS))); }); av.loadElement(cb, iv); }; }; }; ```. Summary of changes:. Introduce CodeBuilder. CodeBuilder allows for the imperative, sequential construction of code. The idea is that it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413
https://github.com/hail-is/hail/pull/8245#issuecomment-600899413:2418,Availability,avail,available,2418,"it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the imperative style (yet), I wrote routines to convert them back and forth: `EmitCode.fromI { cb => ... }` provides a CodeBuilder and converts a resulting IEmitCode back to an EmitCode, and EmitCode.toI(cb) does the opposite. There is also `(Emit)CodeBuilder.scoped` that will run a code builder function and collect the code as a Code[Unit]. A second idea introduced in this PR is that some PType operations may only be available on a PSettable/PValue, rather than on PCode. The motivation is the canonical array ref implementation, which involves a lot of duplicate code generation. In the current setup, we can have a compound PValue, and this introduces a PCanonicalIndexableValue that has three fields: the base array address, the length and the elements addresss. FYI @patrick-schultz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413
https://github.com/hail-is/hail/pull/8245#issuecomment-600899413:2032,Integrability,rout,routines,2032,"it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the imperative style (yet), I wrote routines to convert them back and forth: `EmitCode.fromI { cb => ... }` provides a CodeBuilder and converts a resulting IEmitCode back to an EmitCode, and EmitCode.toI(cb) does the opposite. There is also `(Emit)CodeBuilder.scoped` that will run a code builder function and collect the code as a Code[Unit]. A second idea introduced in this PR is that some PType operations may only be available on a PSettable/PValue, rather than on PCode. The motivation is the canonical array ref implementation, which involves a lot of duplicate code generation. In the current setup, we can have a compound PValue, and this introduces a PCanonicalIndexableValue that has three fields: the base array address, the length and the elements addresss. FYI @patrick-schultz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413
https://github.com/hail-is/hail/pull/8245#issuecomment-600899413:451,Performance,load,loadLength,451,"This PR implements the beginning of the EmitCode and CodeBuilder change discussed here: https://dev.hail.is/t/on-the-subject-of-emittriplet/183/7. The codegen rule for ArrayRef is now the delightfully clear and concise:. ```; EmitCode.fromI(mb) { cb =>; emit(a).toI(cb).flatMap(cb) { (ac) =>; emit(i).toI(cb).flatMap(cb) { (ic) =>; val av = ac.asIndexable.memoize(cb, ""aref_a""); val iv = cb.memoize(ic.tcode[Int], ""aref_i""). cb.ifx(iv < 0 || iv >= av.loadLength(), {; cb._fatal(errorTransformer(; const(""array index out of bounds: index=""); .concat(iv.toS); .concat("", length=""); .concat(av.loadLength().toS))); }); av.loadElement(cb, iv); }; }; }; ```. Summary of changes:. Introduce CodeBuilder. CodeBuilder allows for the imperative, sequential construction of code. The idea is that it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413
https://github.com/hail-is/hail/pull/8245#issuecomment-600899413:591,Performance,load,loadLength,591,"This PR implements the beginning of the EmitCode and CodeBuilder change discussed here: https://dev.hail.is/t/on-the-subject-of-emittriplet/183/7. The codegen rule for ArrayRef is now the delightfully clear and concise:. ```; EmitCode.fromI(mb) { cb =>; emit(a).toI(cb).flatMap(cb) { (ac) =>; emit(i).toI(cb).flatMap(cb) { (ic) =>; val av = ac.asIndexable.memoize(cb, ""aref_a""); val iv = cb.memoize(ic.tcode[Int], ""aref_i""). cb.ifx(iv < 0 || iv >= av.loadLength(), {; cb._fatal(errorTransformer(; const(""array index out of bounds: index=""); .concat(iv.toS); .concat("", length=""); .concat(av.loadLength().toS))); }); av.loadElement(cb, iv); }; }; }; ```. Summary of changes:. Introduce CodeBuilder. CodeBuilder allows for the imperative, sequential construction of code. The idea is that it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413
https://github.com/hail-is/hail/pull/8245#issuecomment-600899413:619,Performance,load,loadElement,619,"This PR implements the beginning of the EmitCode and CodeBuilder change discussed here: https://dev.hail.is/t/on-the-subject-of-emittriplet/183/7. The codegen rule for ArrayRef is now the delightfully clear and concise:. ```; EmitCode.fromI(mb) { cb =>; emit(a).toI(cb).flatMap(cb) { (ac) =>; emit(i).toI(cb).flatMap(cb) { (ic) =>; val av = ac.asIndexable.memoize(cb, ""aref_a""); val iv = cb.memoize(ic.tcode[Int], ""aref_i""). cb.ifx(iv < 0 || iv >= av.loadLength(), {; cb._fatal(errorTransformer(; const(""array index out of bounds: index=""); .concat(iv.toS); .concat("", length=""); .concat(av.loadLength().toS))); }); av.loadElement(cb, iv); }; }; }; ```. Summary of changes:. Introduce CodeBuilder. CodeBuilder allows for the imperative, sequential construction of code. The idea is that it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413
https://github.com/hail-is/hail/pull/8245#issuecomment-600899413:201,Usability,clear,clear,201,"This PR implements the beginning of the EmitCode and CodeBuilder change discussed here: https://dev.hail.is/t/on-the-subject-of-emittriplet/183/7. The codegen rule for ArrayRef is now the delightfully clear and concise:. ```; EmitCode.fromI(mb) { cb =>; emit(a).toI(cb).flatMap(cb) { (ac) =>; emit(i).toI(cb).flatMap(cb) { (ic) =>; val av = ac.asIndexable.memoize(cb, ""aref_a""); val iv = cb.memoize(ic.tcode[Int], ""aref_i""). cb.ifx(iv < 0 || iv >= av.loadLength(), {; cb._fatal(errorTransformer(; const(""array index out of bounds: index=""); .concat(iv.toS); .concat("", length=""); .concat(av.loadLength().toS))); }); av.loadElement(cb, iv); }; }; }; ```. Summary of changes:. Introduce CodeBuilder. CodeBuilder allows for the imperative, sequential construction of code. The idea is that it is the imperative analog of Code[Unit]. In this analogy, a function returning Code[Unit] becomes a function that takes a CodeBuilder, alternatively, a Code[Unit] can become a CodeLabel: the place to jump to run a given computation. In addition to CodeBuilder, I have a imperative implementation of EmitCode that is similar to the proposal in the dev post: IEmitCode. Under the above analog, the proposal in the dev post would become:. ```; trait IEmitCode {; def apply(missing: (CodeBuilder) => Unit, present: (CodeBuilder, PValue) => Unit): Unit; }; ```. However, I took the additional step of ""defunctionalizing"" this picture by using labels instead of functions of code, giving:. ```; case class IEmitCode(Lmissing: CodeLabel, Lpresent: CodeLabel, pc: PCode) {; ...; }; ```. In this model, the emit function will become: `Emit.emit(cb: CodeBuilder, ...): IEmitCode`. The discipline is after calling `emit`, the contents of the code builder, when executed, will jump to one of `Lmissing` or `Lpresent` (labels which are not defined yet) and the consumer can define those labels, and use the expression `pc` in the code after the `Lpresent` label only. Because obviously I haven't converted everything to the i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-600899413
https://github.com/hail-is/hail/pull/8245#issuecomment-601652867:7,Performance,perform,performance,7,"FYI, I performance tested this + https://github.com/hail-is/hail/pull/8326 and gained a few percent:. ```; Harmonic mean: 91.0%; Geometric mean: 95.3%; Arithmetic mean: 96.8%; Median: 97.8%; ```. So this should go in and we leave the memoization optimization as a TODO.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-601652867
https://github.com/hail-is/hail/pull/8245#issuecomment-601652867:246,Performance,optimiz,optimization,246,"FYI, I performance tested this + https://github.com/hail-is/hail/pull/8326 and gained a few percent:. ```; Harmonic mean: 91.0%; Geometric mean: 95.3%; Arithmetic mean: 96.8%; Median: 97.8%; ```. So this should go in and we leave the memoization optimization as a TODO.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-601652867
https://github.com/hail-is/hail/pull/8245#issuecomment-601652867:19,Testability,test,tested,19,"FYI, I performance tested this + https://github.com/hail-is/hail/pull/8326 and gained a few percent:. ```; Harmonic mean: 91.0%; Geometric mean: 95.3%; Arithmetic mean: 96.8%; Median: 97.8%; ```. So this should go in and we leave the memoization optimization as a TODO.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8245#issuecomment-601652867
https://github.com/hail-is/hail/pull/8246#issuecomment-595892761:0,Testability,test,test,0,"test time seemed a little bit lower, but not as much as I'd hoping (and probably not statistically lower)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8246#issuecomment-595892761
https://github.com/hail-is/hail/pull/8250#issuecomment-595827209:131,Integrability,wrap,wrapping,131,"Tim, put a WIP on this to block merge. The fix is to add the correct ptype for the recur branch in Emit.emit. However, this has us wrapping PValue(pt, Code._empty), which seems odd (a type and no corresponding value). @catoverdrive could you comment? Should Recur only take type PVoid and infer type PVoid, is the current system correct, or would you prefer an altogether different solution to: https://github.com/hail-is/hail/pull/8250/commits/8df5a452e2e123ffabf42e277164ce3e6c367517",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8250#issuecomment-595827209
https://github.com/hail-is/hail/pull/8257#issuecomment-595910770:79,Testability,assert,assertions,79,"the virtual type is serialized without requiredness now. earlier versions have assertions that `decodedPType.virtualType == encodedVirtualType`, and this assertion fails on new files",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8257#issuecomment-595910770
https://github.com/hail-is/hail/pull/8257#issuecomment-595910770:154,Testability,assert,assertion,154,"the virtual type is serialized without requiredness now. earlier versions have assertions that `decodedPType.virtualType == encodedVirtualType`, and this assertion fails on new files",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8257#issuecomment-595910770
https://github.com/hail-is/hail/pull/8258#issuecomment-596109210:93,Availability,error,error,93,"Because we're running the bash script that is generated with +e so it will fail on the first error. To try and implement always_run would require implementing something more complicated in the local backend and not just building up a script. Because you'd have to check whether the parents succeeded for each task. It's not out of the question, but I don't think it's worth doing right now. Hence, the NotImplementedError. Same reasoning for timeout.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8258#issuecomment-596109210
https://github.com/hail-is/hail/pull/8258#issuecomment-596109210:442,Safety,timeout,timeout,442,"Because we're running the bash script that is generated with +e so it will fail on the first error. To try and implement always_run would require implementing something more complicated in the local backend and not just building up a script. Because you'd have to check whether the parents succeeded for each task. It's not out of the question, but I don't think it's worth doing right now. Hence, the NotImplementedError. Same reasoning for timeout.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8258#issuecomment-596109210
https://github.com/hail-is/hail/pull/8264#issuecomment-597047221:195,Modifiability,refactor,refactor,195,"`hl.foo.bar.baz` looks at the __init__files, which work differently from imports. I think you could do `from hail.experimental.import_gtf import _load_gencode_gtf`. One of the tasks for 0.3 is a refactor that prevents everything from being imported and exposed as `hl.blah.blah.blah`, which leads to super long import times.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8264#issuecomment-597047221
https://github.com/hail-is/hail/pull/8264#issuecomment-597047221:253,Security,expose,exposed,253,"`hl.foo.bar.baz` looks at the __init__files, which work differently from imports. I think you could do `from hail.experimental.import_gtf import _load_gencode_gtf`. One of the tasks for 0.3 is a refactor that prevents everything from being imported and exposed as `hl.blah.blah.blah`, which leads to super long import times.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8264#issuecomment-597047221
https://github.com/hail-is/hail/pull/8265#issuecomment-600357758:104,Usability,guid,guidelines,104,"Sorry I didn't get to this, Nik! I've been trying to think if we should revisit our experimental module guidelines in light of the questions we're getting on Zulip, but I think things are probably fine.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8265#issuecomment-600357758
https://github.com/hail-is/hail/pull/8265#issuecomment-601360575:14,Availability,error,error,14,"got a doctest error:; ```; ______________ [doctest] hail.experimental.ldscsim.get_cov_matrix ______________; 035 trait 3 & trait 4 :math:`r_g` = 0.6; 036 To obtain the covariance matrix corresponding to this scenario :math:`h^2` values are; 037 ordered according to user specification and :math:`r_g` values are ordered by the ; 038 order in which the corresponding genetic covariance terms will appear in the ; 039 covariance matrix, reading lines in the upper triangular matrix from left to; 040 right, top to bottom (read first row left to right, read second row left to ; 041 right, etc.), exluding the diagonal.; 042 ; 043 >>> cov_matrix, rg = get_cov_matrix(h2=[0.1, 0.3, 0.2, 0.6], rg=[0.4, 0.3, 0.1, 0.2, 0.15, 0.6]); 044 >>> print(cov_matrix); Differences (unified diff with -expected +actual):; @@ -1,4 +1,4 @@; -array([[0.1 , 0.06928203, 0.04242641, 0.0244949 ],; - [0.06928203, 0.3 , 0.04898979, 0.06363961],; - [0.04242641, 0.04898979, 0.2 , 0.2078461 ],; - [0.0244949 , 0.06363961, 0.2078461 , 0.6 ]]; +[[0.1 0.06928203 0.04242641 0.0244949 ]; + [0.06928203 0.3 0.04898979 0.06363961]; + [0.04242641 0.04898979 0.2 0.2078461 ]; + [0.0244949 0.06363961 0.2078461 0.6 ]]. /hail/python/hail/experimental/ldscsim.py:44: DocTestFailure. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8265#issuecomment-601360575
https://github.com/hail-is/hail/pull/8265#issuecomment-601433785:38,Availability,failure,failure,38,"I think you'll still have the doctest failure. It just returns list of lists now ,right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8265#issuecomment-601433785
https://github.com/hail-is/hail/pull/8265#issuecomment-601452175:40,Availability,failure,failure,40,"> I think you'll still have the doctest failure. It just returns list of lists now ,right?. I'm not sure I follow. What returns a list of lists?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8265#issuecomment-601452175
https://github.com/hail-is/hail/pull/8265#issuecomment-601665796:100,Availability,failure,failure,100,"[this comment](https://github.com/hail-is/hail/pull/8265#issuecomment-601360575) describes the test failure -- it looks like something was a numpy array, but is now a nested list.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8265#issuecomment-601665796
https://github.com/hail-is/hail/pull/8265#issuecomment-601665796:95,Testability,test,test,95,"[this comment](https://github.com/hail-is/hail/pull/8265#issuecomment-601360575) describes the test failure -- it looks like something was a numpy array, but is now a nested list.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8265#issuecomment-601665796
https://github.com/hail-is/hail/pull/8265#issuecomment-604460146:57,Testability,test,tests,57,"Ugh, sorry, this has been failing unreliable CI/services tests. I'll keep bumping until it goes in",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8265#issuecomment-604460146
https://github.com/hail-is/hail/pull/8268#issuecomment-596564071:36,Availability,failure,failure,36,"Adding ""WIP"" tag since I had a test failure from `make test-dataproc`. Seems like I have a mix of reference genomes somewhere.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8268#issuecomment-596564071
https://github.com/hail-is/hail/pull/8268#issuecomment-596564071:31,Testability,test,test,31,"Adding ""WIP"" tag since I had a test failure from `make test-dataproc`. Seems like I have a mix of reference genomes somewhere.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8268#issuecomment-596564071
https://github.com/hail-is/hail/pull/8268#issuecomment-596564071:55,Testability,test,test-dataproc,55,"Adding ""WIP"" tag since I had a test failure from `make test-dataproc`. Seems like I have a mix of reference genomes somewhere.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8268#issuecomment-596564071
https://github.com/hail-is/hail/pull/8268#issuecomment-596593742:122,Testability,test,tests,122,"Ah, the mix of reference genomes is coming from me running all the ""cluster_tests"" for GRCh38 cluster as well. One of the tests hardcodes 37",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8268#issuecomment-596593742
https://github.com/hail-is/hail/pull/8268#issuecomment-596626657:8,Testability,test,tests,8,"Ok, all tests passing now. Removing WIP",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8268#issuecomment-596626657
https://github.com/hail-is/hail/pull/8272#issuecomment-597259336:728,Safety,avoid,avoid,728,"That was nasty to debug. For some reason, Scala has both `Ordering.on` (on the `Ordering` class) and `Ordering.by` (on the companion object) to translate an ordering on `U` to an ordering on `T` along a function `f: T => U`. `on` defines `compare`:; ```; def on[U](f: U => T): Ordering[U] = new Ordering[U] {; def compare(x: U, y: U) = outer.compare(f(x), f(y)); }; ```; but `by` mysteriously bases the new ordering only on the original `lt`:; ```; def by[T, S](f: T => S)(implicit ord: Ordering[S]): Ordering[T] =; fromLessThan((x, y) => ord.lt(f(x), f(y))). def fromLessThan[T](cmp: (T, T) => Boolean): Ordering[T] = new Ordering[T] {; def compare(x: T, y: T) = if (cmp(x, y)) -1 else if (cmp(y, x)) 1 else 0; // overrides to avoid multiple comparisons; override def lt(x: T, y: T): Boolean = cmp(x, y); override def gt(x: T, y: T): Boolean = cmp(y, x); override def gteq(x: T, y: T): Boolean = !cmp(x, y); override def lteq(x: T, y: T): Boolean = !cmp(y, x); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8272#issuecomment-597259336
https://github.com/hail-is/hail/pull/8281#issuecomment-597307671:270,Availability,error,errors,270,"Added a region argument, and mandated that users have either configured a region or are using `--region`. I believe older versions of gcloud assume the region is the default region for your project if you don't have anything specified, which causes users to have random errors when they update gcloud that they ask about on Zulip. This way, everyone gets a good error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671
https://github.com/hail-is/hail/pull/8281#issuecomment-597307671:362,Availability,error,error,362,"Added a region argument, and mandated that users have either configured a region or are using `--region`. I believe older versions of gcloud assume the region is the default region for your project if you don't have anything specified, which causes users to have random errors when they update gcloud that they ask about on Zulip. This way, everyone gets a good error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671
https://github.com/hail-is/hail/pull/8281#issuecomment-597307671:287,Deployability,update,update,287,"Added a region argument, and mandated that users have either configured a region or are using `--region`. I believe older versions of gcloud assume the region is the default region for your project if you don't have anything specified, which causes users to have random errors when they update gcloud that they ask about on Zulip. This way, everyone gets a good error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671
https://github.com/hail-is/hail/pull/8281#issuecomment-597307671:368,Integrability,message,message,368,"Added a region argument, and mandated that users have either configured a region or are using `--region`. I believe older versions of gcloud assume the region is the default region for your project if you don't have anything specified, which causes users to have random errors when they update gcloud that they ask about on Zulip. This way, everyone gets a good error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671
https://github.com/hail-is/hail/pull/8281#issuecomment-597307671:61,Modifiability,config,configured,61,"Added a region argument, and mandated that users have either configured a region or are using `--region`. I believe older versions of gcloud assume the region is the default region for your project if you don't have anything specified, which causes users to have random errors when they update gcloud that they ask about on Zulip. This way, everyone gets a good error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671
https://github.com/hail-is/hail/pull/8282#issuecomment-598486137:98,Availability,error,error,98,test/hail/methods/test_impex.py::BGENTests::test_import_bgen_no_entries FAILED. a deserialization error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8282#issuecomment-598486137
https://github.com/hail-is/hail/pull/8282#issuecomment-598486137:0,Testability,test,test,0,test/hail/methods/test_impex.py::BGENTests::test_import_bgen_no_entries FAILED. a deserialization error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8282#issuecomment-598486137
https://github.com/hail-is/hail/pull/8282#issuecomment-611255794:25,Availability,error,error,25,Looks like a compilation error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8282#issuecomment-611255794
https://github.com/hail-is/hail/pull/8286#issuecomment-597354835:11,Testability,test,test,11,I'll add a test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8286#issuecomment-597354835
https://github.com/hail-is/hail/pull/8286#issuecomment-597807644:37,Testability,test,test,37,Looks like there's a typo in the new test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8286#issuecomment-597807644
https://github.com/hail-is/hail/pull/8287#issuecomment-597830451:171,Usability,clear,clear,171,"RegionPool does have a finalize method, so presumably that will be called on the driver if needed (I don't really know if or when java calls finalizers). I implemented a `clear()` method that reclaims all RegionMemory owned by the pool, but leaves the pool ready to be used by the next task.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8287#issuecomment-597830451
https://github.com/hail-is/hail/pull/8292#issuecomment-597808897:70,Testability,benchmark,benchmarks,70,Adding the WIP label to prevent merging until Tim has given the ok on benchmarks,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8292#issuecomment-597808897
https://github.com/hail-is/hail/pull/8292#issuecomment-598094208:83,Testability,benchmark,benchmark,83,that's almost certainly within the (considerable) measurement noise. I'll run that benchmark locally to make sure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8292#issuecomment-598094208
https://github.com/hail-is/hail/pull/8297#issuecomment-600361269:5,Availability,failure,failures,5,test failures were caused by PType inference bugs that are now fixed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8297#issuecomment-600361269
https://github.com/hail-is/hail/pull/8297#issuecomment-600361269:0,Testability,test,test,0,test failures were caused by PType inference bugs that are now fixed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8297#issuecomment-600361269
https://github.com/hail-is/hail/pull/8306#issuecomment-598781546:5,Availability,failure,failures,5,test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8306#issuecomment-598781546
https://github.com/hail-is/hail/pull/8306#issuecomment-598781546:0,Testability,test,test,0,test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8306#issuecomment-598781546
https://github.com/hail-is/hail/pull/8308#issuecomment-606016409:12,Safety,safe,safety,12,"As an extra safety step, I slapped the stacked PR tag on it which should prevent merging.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8308#issuecomment-606016409
https://github.com/hail-is/hail/pull/8315#issuecomment-600173212:30,Testability,test,tests,30,"Agh, failing one of my memory tests. Will dig in to it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8315#issuecomment-600173212
https://github.com/hail-is/hail/pull/8315#issuecomment-604459713:21,Testability,benchmark,benchmarks,21,"I'm just waiting for benchmarks to run, but otherwise I think so",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8315#issuecomment-604459713
https://github.com/hail-is/hail/pull/8315#issuecomment-604542228:256,Availability,down,down,256,"`large_range_matrix_table_sum()` failed in the benchmarks, looking into that. When I ran it locally, seemed to be allocating more memory than I would think, so there's probably a leak there. Otherwise, I think this is safe to review while I track this one down (and maybe you'll catch the cause of this). ```; 2020-03-26 12:41:14 root: INFO: RegionPool: REPORT_THRESHOLD: 16.0M allocated (792.0K blocks / 15.3M chunks), thread 70: Executor task launch worker for task 10; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8315#issuecomment-604542228
https://github.com/hail-is/hail/pull/8315#issuecomment-604542228:378,Energy Efficiency,allocate,allocated,378,"`large_range_matrix_table_sum()` failed in the benchmarks, looking into that. When I ran it locally, seemed to be allocating more memory than I would think, so there's probably a leak there. Otherwise, I think this is safe to review while I track this one down (and maybe you'll catch the cause of this). ```; 2020-03-26 12:41:14 root: INFO: RegionPool: REPORT_THRESHOLD: 16.0M allocated (792.0K blocks / 15.3M chunks), thread 70: Executor task launch worker for task 10; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8315#issuecomment-604542228
https://github.com/hail-is/hail/pull/8315#issuecomment-604542228:218,Safety,safe,safe,218,"`large_range_matrix_table_sum()` failed in the benchmarks, looking into that. When I ran it locally, seemed to be allocating more memory than I would think, so there's probably a leak there. Otherwise, I think this is safe to review while I track this one down (and maybe you'll catch the cause of this). ```; 2020-03-26 12:41:14 root: INFO: RegionPool: REPORT_THRESHOLD: 16.0M allocated (792.0K blocks / 15.3M chunks), thread 70: Executor task launch worker for task 10; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8315#issuecomment-604542228
https://github.com/hail-is/hail/pull/8315#issuecomment-604542228:47,Testability,benchmark,benchmarks,47,"`large_range_matrix_table_sum()` failed in the benchmarks, looking into that. When I ran it locally, seemed to be allocating more memory than I would think, so there's probably a leak there. Otherwise, I think this is safe to review while I track this one down (and maybe you'll catch the cause of this). ```; 2020-03-26 12:41:14 root: INFO: RegionPool: REPORT_THRESHOLD: 16.0M allocated (792.0K blocks / 15.3M chunks), thread 70: Executor task launch worker for task 10; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8315#issuecomment-604542228
https://github.com/hail-is/hail/pull/8315#issuecomment-609962053:44,Testability,benchmark,benchmarks,44,"I believe I've addressed your comments, the benchmarks against 0.2.34 look normal, somehow there are no conflicts at this exact moment. Once #8474 goes in let's get this in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8315#issuecomment-609962053
https://github.com/hail-is/hail/pull/8317#issuecomment-600572344:131,Modifiability,variab,variables,131,"Just a bit of background: until now, to satisfy the JVM bytecode verifier, in certain (many) situations we had to initialize local variables to default values. The recent PR https://github.com/hail-is/hail/pull/8312 added a local initializer as part of lir (the bytecode-level IR) that uses a dataflow analysis to compute the minimal required set of initializations at the top of the function.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8317#issuecomment-600572344
https://github.com/hail-is/hail/issues/8318#issuecomment-600212727:13,Integrability,depend,depend,13,"We currently depend on bokeh version 1.2, so I'm not surprised that bokeh 2.0 doesn't work with Hail's plotting lib. Going to close this as a non-issue since it's a versioning mismatch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8318#issuecomment-600212727
https://github.com/hail-is/hail/issues/8325#issuecomment-603567150:134,Availability,error,error,134,"Just to be clear, this pipeline was what I wrote when trying to replicate the bug Duncan was seeing, but it hit a different assertion error than the one he was hitting. He was hitting ""local in the wrong method builder"" problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150
https://github.com/hail-is/hail/issues/8325#issuecomment-603567150:23,Deployability,pipeline,pipeline,23,"Just to be clear, this pipeline was what I wrote when trying to replicate the bug Duncan was seeing, but it hit a different assertion error than the one he was hitting. He was hitting ""local in the wrong method builder"" problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150
https://github.com/hail-is/hail/issues/8325#issuecomment-603567150:124,Testability,assert,assertion,124,"Just to be clear, this pipeline was what I wrote when trying to replicate the bug Duncan was seeing, but it hit a different assertion error than the one he was hitting. He was hitting ""local in the wrong method builder"" problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150
https://github.com/hail-is/hail/issues/8325#issuecomment-603567150:11,Usability,clear,clear,11,"Just to be clear, this pipeline was what I wrote when trying to replicate the bug Duncan was seeing, but it hit a different assertion error than the one he was hitting. He was hitting ""local in the wrong method builder"" problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150
https://github.com/hail-is/hail/issues/8325#issuecomment-603568619:16,Availability,error,errors,16,I saw different errors if a `show()` vs `_force_count_rows()` was the last line.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603568619
https://github.com/hail-is/hail/pull/8333#issuecomment-612943506:54,Deployability,update,update,54,"This is a month out of date and CI has hit the status update limit, closing. Please push a fresh commit SHA when you're ready for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8333#issuecomment-612943506
https://github.com/hail-is/hail/pull/8335#issuecomment-602304418:33,Availability,failure,failure,33,"I think I squashed all the bugs, failure is from the unrelated test_ci issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8335#issuecomment-602304418
https://github.com/hail-is/hail/pull/8336#issuecomment-602797692:0,Deployability,Deploy,Deploy,0,"Deploy happened, closing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8336#issuecomment-602797692
https://github.com/hail-is/hail/pull/8345#issuecomment-603569487:21,Testability,test,tests,21,Failing all the hail tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8345#issuecomment-603569487
https://github.com/hail-is/hail/pull/8348#issuecomment-603354428:188,Energy Efficiency,adapt,adapting,188,I'm having trouble with finding examples of codegen to improve for these types. It seems that a lot of our way of generating code here is to apply functions from our function registry and adapting them to use PCode seems to be a much larger project. I think I need to change how those work before I can do the rest of the PCode changes in such a way that benefits our code changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8348#issuecomment-603354428
https://github.com/hail-is/hail/pull/8348#issuecomment-603354428:188,Modifiability,adapt,adapting,188,I'm having trouble with finding examples of codegen to improve for these types. It seems that a lot of our way of generating code here is to apply functions from our function registry and adapting them to use PCode seems to be a much larger project. I think I need to change how those work before I can do the rest of the PCode changes in such a way that benefits our code changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8348#issuecomment-603354428
https://github.com/hail-is/hail/issues/8349#issuecomment-617327917:254,Deployability,release,release,254,@astheeggeggs Thanks for the bug report. It lead to finding a rather serious bug. See https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375 for more details on what other regressions could have been affected. A new release should go out today with the fix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8349#issuecomment-617327917
https://github.com/hail-is/hail/issues/8349#issuecomment-617334301:402,Deployability,release,release,402,"Great! No worries!. On Tue, 21 Apr 2020, 19:12 Patrick Schultz, <notifications@github.com>; wrote:. > @astheeggeggs <https://github.com/astheeggeggs> Thanks for the bug; > report. It lead to finding a rather serious bug. See; > https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; > for more details on what other regressions could have been affected. A new; > release should go out today with the fix.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/8349#issuecomment-617327917>, or; > unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABVQA76SRJ64CRAO36BG2GLRNXO2FANCNFSM4LS2ZGUA>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8349#issuecomment-617334301
https://github.com/hail-is/hail/pull/8351#issuecomment-603808832:44,Usability,clear,clear,44,"I thought about it a bit, and I think it is clear what to do (ultimately) for registered functions: we need a variant that takes PCode instead of Code[_], and then we can begin to migrate the functions incrementally. A separate thread of work, obviously.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8351#issuecomment-603808832
https://github.com/hail-is/hail/issues/8352#issuecomment-603528845:8,Deployability,deploy,deployed,8,Hail is deployed to the Python package index: https://pypi.org/project/hail/. Adding a conda recipe isn't a high-priority task right now. What are the reasons that a PyPI package is insufficient?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8352#issuecomment-603528845
https://github.com/hail-is/hail/pull/8361#issuecomment-604091281:93,Performance,concurren,concurrent,93,"This week, subsequent PRs will address:; - Implement `stop` which frees memory.; - Test many concurrent clients.; - Implement TableKeyBy and TableOrderBy with this (hidden by a hail context flag).; - Package this as a k8s service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-604091281
https://github.com/hail-is/hail/pull/8361#issuecomment-604091281:83,Testability,Test,Test,83,"This week, subsequent PRs will address:; - Implement `stop` which frees memory.; - Test many concurrent clients.; - Implement TableKeyBy and TableOrderBy with this (hidden by a hail context flag).; - Package this as a k8s service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-604091281
https://github.com/hail-is/hail/pull/8361#issuecomment-609933937:176,Integrability,interface,interface,176,"Thanks for the thoughtful review @catoverdrive!. @cseed , your review is requested of the use of TLS. @cseed , I'm also curious of your thoughts about documenting the shuffler interface. I could eventually write a README file in the package that reflects the final outcome of the dev forum posts. We could also leave the dev forum as the only documentation. What do you think?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609933937
https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:361,Deployability,pipeline,pipelines,361,"> Slight question about the shuffle ID/client/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052
https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:1153,Deployability,pipeline,pipeline,1153,"ent/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleClient` objects around to access old data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052
https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:1725,Integrability,interface,interfaces---the,1725,"ent/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleClient` objects around to access old data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052
https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:398,Performance,concurren,concurrent,398,"> Slight question about the shuffle ID/client/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052
https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:1099,Performance,concurren,concurrent,1099,"ent/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleClient` objects around to access old data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052
https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:895,Security,secur,secure,895,"> Slight question about the shuffle ID/client/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052
https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:1034,Security,access,access,1034,"> Slight question about the shuffle ID/client/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052
https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:1571,Security,access,access,1571,"ent/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleClient` objects around to access old data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052
https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:1657,Security,access,access,1657,"ent/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleClient` objects around to access old data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052
https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:1866,Security,access,accessing,1866,"ent/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleClient` objects around to access old data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052
https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:2026,Security,access,access,2026,"ent/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleClient` objects around to access old data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052
https://github.com/hail-is/hail/pull/8362#issuecomment-616599742:28,Performance,optimiz,optimize,28,"I had a messy rebase. I ran optimize imports to clean up the import mess. It made a lot of changes to the imports, my apologies. It shouldn't affect the correctness in anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8362#issuecomment-616599742
https://github.com/hail-is/hail/pull/8365#issuecomment-604286431:14,Deployability,update,update,14,Still need to update a use case or two.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8365#issuecomment-604286431
https://github.com/hail-is/hail/pull/8365#issuecomment-605101201:37,Integrability,wrap,wrapped,37,"For intervals, a lot of the usage is wrapped up in ordering. The lift of ordering to CodeBuilder/PCode is another large project all on its own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8365#issuecomment-605101201
https://github.com/hail-is/hail/pull/8368#issuecomment-607588305:244,Integrability,depend,dependencies,244,"This is all passing except check_batch is running in an image on python 3.6 and the worker is on python 3.7. The process initializer isn't exposed until 3.7. I think I need to change the base image, but am worried about breaking Hail and other dependencies. Otherwise, I can just pass the key file path and the project and create the credentials and gcs client each time a function is called rather than once per process. This will make it slower for small files.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8368#issuecomment-607588305
https://github.com/hail-is/hail/pull/8368#issuecomment-607588305:139,Security,expose,exposed,139,"This is all passing except check_batch is running in an image on python 3.6 and the worker is on python 3.7. The process initializer isn't exposed until 3.7. I think I need to change the base image, but am worried about breaking Hail and other dependencies. Otherwise, I can just pass the key file path and the project and create the credentials and gcs client each time a function is called rather than once per process. This will make it slower for small files.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8368#issuecomment-607588305
https://github.com/hail-is/hail/pull/8370#issuecomment-604766994:16,Deployability,update,update,16,"This is just an update of #7951. Merged into a more recent master. . I still need to study it to see the filesystem burden. It does seem that the comb-ops are being distributed, but I ran it on gnomAD just to try it, and it failed with HDFS out of space during combops.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8370#issuecomment-604766994
https://github.com/hail-is/hail/pull/8371#issuecomment-607822257:105,Testability,test,tests,105,"The rebase was a little messy. I got it compiling, but there will probably be some minor issues with the tests. It was passing before, so I expect the overall code to be sound.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371#issuecomment-607822257
https://github.com/hail-is/hail/pull/8371#issuecomment-607958840:23,Availability,failure,failures,23,"Looks like some Python failures. It will take me a little while to track them down, but they all look minor and of two forms I understand:; - RVDType being constructed with the row type being optional,; - and incorrect type signatures when building emit methods due to mismatched missingness.; Go ahead and review while I fix them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371#issuecomment-607958840
https://github.com/hail-is/hail/pull/8371#issuecomment-607958840:78,Availability,down,down,78,"Looks like some Python failures. It will take me a little while to track them down, but they all look minor and of two forms I understand:; - RVDType being constructed with the row type being optional,; - and incorrect type signatures when building emit methods due to mismatched missingness.; Go ahead and review while I fix them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371#issuecomment-607958840
https://github.com/hail-is/hail/pull/8371#issuecomment-609070509:831,Availability,down,downcast,831,"A miracle. It finally passed. That was a real slog. I pushed a bunch of non-trivial changes, so it is probably good if you give a skeptical, fresh look. Summary of new changes:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and global etypes are present at the toplevel, and setRequired(true) if they are coming from encoders written by previous versions; - rename PType.copyFromType to PType.copyFromAdddres. Modify it so it can ""downcast"": convert to a PType with greater requiredness. This is used in converting TableValues to MatrixValues to satisfy the requiredness assertions. Let me know if you have any questions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371#issuecomment-609070509
https://github.com/hail-is/hail/pull/8371#issuecomment-609070509:613,Testability,assert,assert,613,"A miracle. It finally passed. That was a real slog. I pushed a bunch of non-trivial changes, so it is probably good if you give a skeptical, fresh look. Summary of new changes:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and global etypes are present at the toplevel, and setRequired(true) if they are coming from encoders written by previous versions; - rename PType.copyFromType to PType.copyFromAdddres. Modify it so it can ""downcast"": convert to a PType with greater requiredness. This is used in converting TableValues to MatrixValues to satisfy the requiredness assertions. Let me know if you have any questions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371#issuecomment-609070509
https://github.com/hail-is/hail/pull/8371#issuecomment-609070509:971,Testability,assert,assertions,971,"A miracle. It finally passed. That was a real slog. I pushed a bunch of non-trivial changes, so it is probably good if you give a skeptical, fresh look. Summary of new changes:; - added PType.literalPType that infers PTypes from Scala literals, use in a few places (emit for Literal, BroadcastRegionValue constructor from annotation, etc.); - require Table global and row types to be required; - same for MatrixValue, but also cols and entries (the entries array, not individual entries, which an be missing); - Don't upcast globals in TableKeyBy and TableOrderBy; - added EType setRequired; - AbstractCodecSpecs assert row and global etypes are present at the toplevel, and setRequired(true) if they are coming from encoders written by previous versions; - rename PType.copyFromType to PType.copyFromAdddres. Modify it so it can ""downcast"": convert to a PType with greater requiredness. This is used in converting TableValues to MatrixValues to satisfy the requiredness assertions. Let me know if you have any questions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8371#issuecomment-609070509
https://github.com/hail-is/hail/pull/8375#issuecomment-605098820:19,Availability,error,error,19,"Not sure what this error is: . deepest = 'HailException: block matrix must have at least one row'; full = 'is.hail.utils.HailException: block matrix must have at least one row\n\tat is.hail.utils.ErrorHandling$class.fatal(Er...ava:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: HailException: block matrix must have at least one row; E ; E Java stack trace:; E is.hail.utils.HailException: block matrix must have at least one row",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8375#issuecomment-605098820
https://github.com/hail-is/hail/pull/8375#issuecomment-605098820:196,Availability,Error,ErrorHandling,196,"Not sure what this error is: . deepest = 'HailException: block matrix must have at least one row'; full = 'is.hail.utils.HailException: block matrix must have at least one row\n\tat is.hail.utils.ErrorHandling$class.fatal(Er...ava:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: HailException: block matrix must have at least one row; E ; E Java stack trace:; E is.hail.utils.HailException: block matrix must have at least one row",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8375#issuecomment-605098820
https://github.com/hail-is/hail/pull/8375#issuecomment-605098820:794,Availability,Error,Error,794,"Not sure what this error is: . deepest = 'HailException: block matrix must have at least one row'; full = 'is.hail.utils.HailException: block matrix must have at least one row\n\tat is.hail.utils.ErrorHandling$class.fatal(Er...ava:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: HailException: block matrix must have at least one row; E ; E Java stack trace:; E is.hail.utils.HailException: block matrix must have at least one row",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8375#issuecomment-605098820
https://github.com/hail-is/hail/pull/8375#issuecomment-605098820:437,Integrability,protocol,protocol,437,"Not sure what this error is: . deepest = 'HailException: block matrix must have at least one row'; full = 'is.hail.utils.HailException: block matrix must have at least one row\n\tat is.hail.utils.ErrorHandling$class.fatal(Er...ava:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: HailException: block matrix must have at least one row; E ; E Java stack trace:; E is.hail.utils.HailException: block matrix must have at least one row",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8375#issuecomment-605098820
https://github.com/hail-is/hail/pull/8377#issuecomment-605284219:10,Testability,test,tests,10,Added the tests you suggested,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8377#issuecomment-605284219
https://github.com/hail-is/hail/pull/8380#issuecomment-605417622:6,Testability,test,tests,6,"While tests pass, I would like your eyes on the requiredeness choices.I thought that if the result was present (EmitCode.m evaluates to false), and if that EmitCode.m was calculated by using an inner PType, the inner PTypes could be assumed to be non-missing. This seems logical, but failed in the commented function",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8380#issuecomment-605417622
https://github.com/hail-is/hail/pull/8380#issuecomment-605417622:271,Testability,log,logical,271,"While tests pass, I would like your eyes on the requiredeness choices.I thought that if the result was present (EmitCode.m evaluates to false), and if that EmitCode.m was calculated by using an inner PType, the inner PTypes could be assumed to be non-missing. This seems logical, but failed in the commented function",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8380#issuecomment-605417622
https://github.com/hail-is/hail/pull/8387#issuecomment-605674200:210,Deployability,update,updates,210,"I found a linux that doesn't have `<execinfo.h>` as part of `libc`, thus requiring an external library. This is mostly so I can work on my desktop again, which had gotten to a pretty screwed up state regarding updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8387#issuecomment-605674200
https://github.com/hail-is/hail/pull/8394#issuecomment-606100914:5,Availability,failure,failures,5,"test failures related to locals, ugh.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8394#issuecomment-606100914
https://github.com/hail-is/hail/pull/8394#issuecomment-606100914:0,Testability,test,test,0,"test failures related to locals, ugh.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8394#issuecomment-606100914
https://github.com/hail-is/hail/pull/8398#issuecomment-607436509:60,Usability,simpl,simpler,60,As I started to get attempt_ids working in the DB it seemed simpler to go with your approach. I implemented that instead.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8398#issuecomment-607436509
https://github.com/hail-is/hail/pull/8398#issuecomment-609908738:22,Deployability,update,update,22,I now unconditionally update. The update system will either find another PR that was started in the meantime or it will start a new batch,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8398#issuecomment-609908738
https://github.com/hail-is/hail/pull/8398#issuecomment-609908738:34,Deployability,update,update,34,I now unconditionally update. The update system will either find another PR that was started in the meantime or it will start a new batch,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8398#issuecomment-609908738
https://github.com/hail-is/hail/pull/8402#issuecomment-606678817:28,Deployability,deploy,deploying,28,Protection re-enabled. Hand deploying CI now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8402#issuecomment-606678817
https://github.com/hail-is/hail/pull/8402#issuecomment-606698835:23,Availability,error,error,23,There was a subsequent error https://github.com/hail-is/hail/pull/8403,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8402#issuecomment-606698835
https://github.com/hail-is/hail/pull/8402#issuecomment-606699423:29,Deployability,deploy,deploying,29,"#8403 was force merged, hand deploying again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8402#issuecomment-606699423
https://github.com/hail-is/hail/pull/8402#issuecomment-606704529:5,Deployability,deploy,deploy,5,Hand deploy succeeded. CI appears OK now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8402#issuecomment-606704529
https://github.com/hail-is/hail/pull/8405#issuecomment-606756521:14,Deployability,update,update,14,Still need to update ci.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8405#issuecomment-606756521
https://github.com/hail-is/hail/issues/8423#issuecomment-607430617:1585,Availability,error,error,1585,"hail 0.2.33, python 3.7.3. `hl.init(sc); Traceback (most recent call last):; File ""<console>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in init; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 290, in init; _optimizer_iterations,_backend); File ""<decorator-gen-1212>"", line 2, in __init__; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 121, in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1286, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/transforms/_java_utils.py"", line 237, in wrapper; return func(*args, **kwargs); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/protocol.py"", line 332, in get_return_value; format(target_id, ""."", name, value)); py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; py4j.Py4JException: Method apply([class org.apache.spark.SparkContext, class java.lang.String, class scala.None$, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8423#issuecomment-607430617
https://github.com/hail-is/hail/issues/8423#issuecomment-607430617:303,Integrability,wrap,wrapper,303,"hail 0.2.33, python 3.7.3. `hl.init(sc); Traceback (most recent call last):; File ""<console>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in init; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 290, in init; _optimizer_iterations,_backend); File ""<decorator-gen-1212>"", line 2, in __init__; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 121, in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1286, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/transforms/_java_utils.py"", line 237, in wrapper; return func(*args, **kwargs); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/protocol.py"", line 332, in get_return_value; format(target_id, ""."", name, value)); py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; py4j.Py4JException: Method apply([class org.apache.spark.SparkContext, class java.lang.String, class scala.None$, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8423#issuecomment-607430617
https://github.com/hail-is/hail/issues/8423#issuecomment-607430617:718,Integrability,wrap,wrapper,718,"hail 0.2.33, python 3.7.3. `hl.init(sc); Traceback (most recent call last):; File ""<console>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in init; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 290, in init; _optimizer_iterations,_backend); File ""<decorator-gen-1212>"", line 2, in __init__; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 121, in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1286, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/transforms/_java_utils.py"", line 237, in wrapper; return func(*args, **kwargs); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/protocol.py"", line 332, in get_return_value; format(target_id, ""."", name, value)); py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; py4j.Py4JException: Method apply([class org.apache.spark.SparkContext, class java.lang.String, class scala.None$, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8423#issuecomment-607430617
https://github.com/hail-is/hail/issues/8423#issuecomment-607430617:1328,Integrability,wrap,wrapper,1328,"hail 0.2.33, python 3.7.3. `hl.init(sc); Traceback (most recent call last):; File ""<console>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in init; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 290, in init; _optimizer_iterations,_backend); File ""<decorator-gen-1212>"", line 2, in __init__; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 121, in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1286, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/transforms/_java_utils.py"", line 237, in wrapper; return func(*args, **kwargs); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/protocol.py"", line 332, in get_return_value; format(target_id, ""."", name, value)); py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; py4j.Py4JException: Method apply([class org.apache.spark.SparkContext, class java.lang.String, class scala.None$, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8423#issuecomment-607430617
https://github.com/hail-is/hail/issues/8423#issuecomment-607430617:1474,Integrability,protocol,protocol,1474,"hail 0.2.33, python 3.7.3. `hl.init(sc); Traceback (most recent call last):; File ""<console>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in init; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 290, in init; _optimizer_iterations,_backend); File ""<decorator-gen-1212>"", line 2, in __init__; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 121, in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1286, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/transforms/_java_utils.py"", line 237, in wrapper; return func(*args, **kwargs); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/protocol.py"", line 332, in get_return_value; format(target_id, ""."", name, value)); py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; py4j.Py4JException: Method apply([class org.apache.spark.SparkContext, class java.lang.String, class scala.None$, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8423#issuecomment-607430617
https://github.com/hail-is/hail/issues/8423#issuecomment-607430617:1562,Integrability,protocol,protocol,1562,"hail 0.2.33, python 3.7.3. `hl.init(sc); Traceback (most recent call last):; File ""<console>"", line 1, in <module>; File ""<decorator-gen-1214>"", line 2, in init; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 290, in init; _optimizer_iterations,_backend); File ""<decorator-gen-1212>"", line 2, in __init__; File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/typecheck/check.py"", line 585, in wrapper; return __original_func(*args_, **kwargs_); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/hail/context.py"", line 121, in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/java_gateway.py"", line 1286, in __call__; answer, self.gateway_client, self.target_id, self.name); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/transforms/_java_utils.py"", line 237, in wrapper; return func(*args, **kwargs); File ""/tmp/conda-9cc77aa7-66f1-4cac-870e-53c3df210a16/real/envs/conda-env/lib/python3.7/site-packages/py4j/protocol.py"", line 332, in get_return_value; format(target_id, ""."", name, value)); py4j.protocol.Py4JError: An error occurred while calling z:is.hail.HailContext.apply. Trace:; py4j.Py4JException: Method apply([class org.apache.spark.SparkContext, class java.lang.String, class scala.None$, class java.lang.String, class java.lang.String, class java.lang.Boolean, class java.lang.Boolean, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer]) does not exist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8423#issuecomment-607430617
https://github.com/hail-is/hail/issues/8423#issuecomment-607431839:12,Deployability,install,install,12,how did you install Hail? `pip install hail` or from source?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8423#issuecomment-607431839
https://github.com/hail-is/hail/issues/8423#issuecomment-607431839:31,Deployability,install,install,31,how did you install Hail? `pip install hail` or from source?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8423#issuecomment-607431839
https://github.com/hail-is/hail/issues/8423#issuecomment-607438752:41,Deployability,install,installation,41,please refer to this page for up-to-date installation instructions: https://hail.is/docs/0.2/getting_started.html,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8423#issuecomment-607438752
https://github.com/hail-is/hail/pull/8424#issuecomment-607411084:40,Deployability,deploy,deploying,40,Force merging due to broken batch. Hand deploying CI now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8424#issuecomment-607411084
https://github.com/hail-is/hail/pull/8424#issuecomment-607417090:5,Deployability,deploy,deploy,5,Hand deploy successful. Monitoring logs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8424#issuecomment-607417090
https://github.com/hail-is/hail/pull/8424#issuecomment-607417090:24,Energy Efficiency,Monitor,Monitoring,24,Hand deploy successful. Monitoring logs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8424#issuecomment-607417090
https://github.com/hail-is/hail/pull/8424#issuecomment-607417090:35,Testability,log,logs,35,Hand deploy successful. Monitoring logs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8424#issuecomment-607417090
https://github.com/hail-is/hail/pull/8428#issuecomment-607460857:19,Deployability,deploy,deploying,19,"force merged, hand deploying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8428#issuecomment-607460857
https://github.com/hail-is/hail/pull/8428#issuecomment-607465228:0,Deployability,deploy,deployed,0,deployed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8428#issuecomment-607465228
https://github.com/hail-is/hail/pull/8431#issuecomment-617756822:31,Integrability,depend,dependency,31,Can this move forward now? The dependency went in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8431#issuecomment-617756822
https://github.com/hail-is/hail/pull/8433#issuecomment-607558384:38,Availability,error,errors,38,> Then I will PR a change that raises errors if we try to start a service with plaintext connections or unverified connections. You should also modify the CloudSQL instance to only accept TLS connections. That's an option.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8433#issuecomment-607558384
https://github.com/hail-is/hail/pull/8434#issuecomment-607868552:0,Testability,Test,Tests,0,"Tests are failing, you missed a `_jhc` somewhere",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8434#issuecomment-607868552
https://github.com/hail-is/hail/pull/8436#issuecomment-607598560:8,Testability,test,tests,8,failing tests (outer join) due to requiredeness,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8436#issuecomment-607598560
https://github.com/hail-is/hail/pull/8436#issuecomment-609932494:23,Deployability,release,release,23,"Nice work. Let's block release on this, clearly it was bugged before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8436#issuecomment-609932494
https://github.com/hail-is/hail/pull/8436#issuecomment-609932494:40,Usability,clear,clearly,40,"Nice work. Let's block release on this, clearly it was bugged before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8436#issuecomment-609932494
https://github.com/hail-is/hail/pull/8440#issuecomment-609845317:177,Deployability,deploy,deploy,177,"The CI tests were failing due to not enough disk space on the workers. I increased the size to 20 GB, but this requires a migration. I'll need to setup a VM with the ability to deploy to have this new image pre-built. The last time I did a migration from the VM, it took a long time to deploy despite pre-caching the image, so I'll want to build a new VM with more cores to see if that helps. We should wait until after Konrad's jobs are done to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-609845317
https://github.com/hail-is/hail/pull/8440#issuecomment-609845317:286,Deployability,deploy,deploy,286,"The CI tests were failing due to not enough disk space on the workers. I increased the size to 20 GB, but this requires a migration. I'll need to setup a VM with the ability to deploy to have this new image pre-built. The last time I did a migration from the VM, it took a long time to deploy despite pre-caching the image, so I'll want to build a new VM with more cores to see if that helps. We should wait until after Konrad's jobs are done to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-609845317
https://github.com/hail-is/hail/pull/8440#issuecomment-609845317:7,Testability,test,tests,7,"The CI tests were failing due to not enough disk space on the workers. I increased the size to 20 GB, but this requires a migration. I'll need to setup a VM with the ability to deploy to have this new image pre-built. The last time I did a migration from the VM, it took a long time to deploy despite pre-caching the image, so I'll want to build a new VM with more cores to see if that helps. We should wait until after Konrad's jobs are done to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-609845317
https://github.com/hail-is/hail/pull/8440#issuecomment-609899181:33,Deployability,deploy,deploy,33,I just built the image using dev deploy of this branch to check its size and it's only grown by 8MB. What else is causing disk usage growth?. ```; # docker image ls gcr.io/hail-vdc/ci-intermediate:anrd6xyjsrnd; REPOSITORY TAG IMAGE ID CREATED SIZE; gcr.io/hail-vdc/ci-intermediate anrd6xyjsrnd 2a479e5a34c4 53 seconds ago 363MB; # docker image ls gcr.io/hail-vdc/batch-worker:g76daybmi5g1 ; REPOSITORY TAG IMAGE ID CREATED SIZE; gcr.io/hail-vdc/batch-worker g76daybmi5g1 6782b39e4d31 45 hours ago 355MB; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-609899181
https://github.com/hail-is/hail/pull/8440#issuecomment-614876259:132,Energy Efficiency,schedul,scheduled,132,"I'm not sure why this is still failing. It looks like the CI jobs are all succeeding, but the last jobs in the batch aren't getting scheduled. Building the base image on the worker is taking 2 minutes 25 seconds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-614876259
https://github.com/hail-is/hail/pull/8440#issuecomment-614882682:154,Availability,down,down,154,"ok. Figured it out. The issue is that the build of the new image finally succeeded after 18 minutes, but the test timed out and batch must have been shut down already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-614882682
https://github.com/hail-is/hail/pull/8440#issuecomment-614882682:109,Testability,test,test,109,"ok. Figured it out. The issue is that the build of the new image finally succeeded after 18 minutes, but the test timed out and batch must have been shut down already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-614882682
https://github.com/hail-is/hail/pull/8440#issuecomment-615284738:65,Safety,timeout,timeout,65,"@danking This is finally passing. I had to change the test_ci to timeout after 30 minutes. Do you want to do the migration? If not, then I'll figure out how to try and make a faster VM later today.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-615284738
https://github.com/hail-is/hail/pull/8443#issuecomment-609062840:82,Testability,test,test,82,"should pass now. I reverted the requiredness change in TableRange, so I think the test would pass on master now, but will fail as soon as Cotton's meatball goes in (Without my fix)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8443#issuecomment-609062840
https://github.com/hail-is/hail/pull/8443#issuecomment-609863055:54,Testability,test,tests,54,scooped! Cotton must have fixed this in order to make tests pass after rowType is always required.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8443#issuecomment-609863055
https://github.com/hail-is/hail/pull/8443#issuecomment-609863468:28,Testability,test,test,28,"actually, I want to add the test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8443#issuecomment-609863468
https://github.com/hail-is/hail/pull/8446#issuecomment-608498258:79,Modifiability,variab,variable,79,"The only thing that may be a good change is to add a more descriptive name for variable creation and initialization than `memoize`. Because you're right, it doesn't really memoize. That's outside the scope of this PR however.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8446#issuecomment-608498258
https://github.com/hail-is/hail/pull/8448#issuecomment-608331163:5,Testability,test,tested,5,"Hmm, tested locally and this didn't work. Will investigate. Marking WIP.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8448#issuecomment-608331163
https://github.com/hail-is/hail/pull/8448#issuecomment-608339475:29,Testability,test,testng,29,"You can't exclude a class in testng.xml: https://stackoverflow.com/questions/12157846/how-to-exclude-class-in-testng. I moved the GoogleStorageFS tests to a separate package and excluded that. Also renamed the test classes ""BlahSuite"" instead of ""TestBlah"" for consistency.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8448#issuecomment-608339475
https://github.com/hail-is/hail/pull/8448#issuecomment-608339475:110,Testability,test,testng,110,"You can't exclude a class in testng.xml: https://stackoverflow.com/questions/12157846/how-to-exclude-class-in-testng. I moved the GoogleStorageFS tests to a separate package and excluded that. Also renamed the test classes ""BlahSuite"" instead of ""TestBlah"" for consistency.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8448#issuecomment-608339475
https://github.com/hail-is/hail/pull/8448#issuecomment-608339475:146,Testability,test,tests,146,"You can't exclude a class in testng.xml: https://stackoverflow.com/questions/12157846/how-to-exclude-class-in-testng. I moved the GoogleStorageFS tests to a separate package and excluded that. Also renamed the test classes ""BlahSuite"" instead of ""TestBlah"" for consistency.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8448#issuecomment-608339475
https://github.com/hail-is/hail/pull/8448#issuecomment-608339475:210,Testability,test,test,210,"You can't exclude a class in testng.xml: https://stackoverflow.com/questions/12157846/how-to-exclude-class-in-testng. I moved the GoogleStorageFS tests to a separate package and excluded that. Also renamed the test classes ""BlahSuite"" instead of ""TestBlah"" for consistency.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8448#issuecomment-608339475
https://github.com/hail-is/hail/pull/8448#issuecomment-608339475:247,Testability,Test,TestBlah,247,"You can't exclude a class in testng.xml: https://stackoverflow.com/questions/12157846/how-to-exclude-class-in-testng. I moved the GoogleStorageFS tests to a separate package and excluded that. Also renamed the test classes ""BlahSuite"" instead of ""TestBlah"" for consistency.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8448#issuecomment-608339475
https://github.com/hail-is/hail/pull/8452#issuecomment-608620168:43,Deployability,install,install,43,@johnc1231 Thoughts on including; ```; pip install gnomad; ```. The package name isn't actually included anywhere on that page.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8452#issuecomment-608620168
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:19,Deployability,Pipeline,Pipeline,19,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:81,Deployability,pipeline,pipeline,81,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:109,Deployability,Update,Update,109,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:178,Deployability,Update,Update,178,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:240,Deployability,Update,Update,240,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:380,Deployability,pipeline,pipelines,380,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:427,Deployability,pipeline,pipeline,427,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:496,Deployability,Update,Update,496,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:530,Deployability,Update,Update,530,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:650,Deployability,Update,Update,650,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:703,Deployability,pipeline,pipeline,703,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:811,Deployability,release,release,811,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:436,Testability,test,test,436,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:473,Testability,test,test,473,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:503,Testability,test,tests,503,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:537,Testability,benchmark,benchmark,537,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606
https://github.com/hail-is/hail/pull/8453#issuecomment-611733313:93,Deployability,release,release,93,"> I did NOT change the navbar to point to the batch docs since it won't exist until the next release. Could you explain this further? The navbar will also not be deployed until the next release, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-611733313
https://github.com/hail-is/hail/pull/8453#issuecomment-611733313:162,Deployability,deploy,deployed,162,"> I did NOT change the navbar to point to the batch docs since it won't exist until the next release. Could you explain this further? The navbar will also not be deployed until the next release, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-611733313
https://github.com/hail-is/hail/pull/8453#issuecomment-611733313:186,Deployability,release,release,186,"> I did NOT change the navbar to point to the batch docs since it won't exist until the next release. Could you explain this further? The navbar will also not be deployed until the next release, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-611733313
https://github.com/hail-is/hail/pull/8453#issuecomment-612086823:62,Deployability,deploy,deployed,62,"> Could you explain this further? The navbar will also not be deployed until the next release, right?. No, the navbar is for all of the UI pages that aren't part of the site.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-612086823
https://github.com/hail-is/hail/pull/8453#issuecomment-612086823:86,Deployability,release,release,86,"> Could you explain this further? The navbar will also not be deployed until the next release, right?. No, the navbar is for all of the UI pages that aren't part of the site.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-612086823
https://github.com/hail-is/hail/pull/8453#issuecomment-612087215:14,Deployability,update,updated,14,"That will get updated once this PR goes in, while the docs and the client users use (batch not pipeline anymore) will be dependent on upgrading to the next PIP release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-612087215
https://github.com/hail-is/hail/pull/8453#issuecomment-612087215:95,Deployability,pipeline,pipeline,95,"That will get updated once this PR goes in, while the docs and the client users use (batch not pipeline anymore) will be dependent on upgrading to the next PIP release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-612087215
https://github.com/hail-is/hail/pull/8453#issuecomment-612087215:160,Deployability,release,release,160,"That will get updated once this PR goes in, while the docs and the client users use (batch not pipeline anymore) will be dependent on upgrading to the next PIP release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-612087215
https://github.com/hail-is/hail/pull/8453#issuecomment-612087215:121,Integrability,depend,dependent,121,"That will get updated once this PR goes in, while the docs and the client users use (batch not pipeline anymore) will be dependent on upgrading to the next PIP release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-612087215
https://github.com/hail-is/hail/pull/8453#issuecomment-612155649:10,Deployability,update,update,10,And let's update the navbar since we're making the above change.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-612155649
https://github.com/hail-is/hail/pull/8455#issuecomment-609911680:341,Availability,robust,robust,341,"> How about underlining it?. Nice suggestion. I hadn't done this because placing the underline on the li increased the height of the element, again requiring negative margin. But, on display: inline elements, border doesn't affect height, so the solution is just to place it there. CSS. I've also improved the javascript function to be more robust; before it would have considered /docs/0.2/index.html as home. It now matches the `pathname` of the anchor tag and location. This is just the right way to do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8455#issuecomment-609911680
https://github.com/hail-is/hail/pull/8467#issuecomment-609470292:322,Integrability,depend,depends,322,"I think it can still be a little tighter (though it's already a definite improvement over before). How does this sound -- it avoids 'key value' while still being precise:. ```; Tables are joined at rows that have the same value of non-missing key fields.; The inclusion of a row with no matching key in the opposite table depends on the; join strategy:. - **inner** -- Only rows with a matching key in the opposite table are included; in the resulting table.; - **left** -- All rows from the left table are included in the resulting table.; If a row in the left table has no match in the right table, then the fields; derived from the right table will be missing.; - **right** -- All rows from the right table are included in the resulting table.; If a row in the right table has no match in the left table, then the fields; derived from the left table will be missing.; - **outer** -- All rows are included in the resulting table. If a row in the right; table has no match in the left table, then the fields derived from the left; table will be missing. If a row in the right table has no match in the left table,; then the fields derived from the left table will be missing.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8467#issuecomment-609470292
https://github.com/hail-is/hail/pull/8467#issuecomment-609470292:125,Safety,avoid,avoids,125,"I think it can still be a little tighter (though it's already a definite improvement over before). How does this sound -- it avoids 'key value' while still being precise:. ```; Tables are joined at rows that have the same value of non-missing key fields.; The inclusion of a row with no matching key in the opposite table depends on the; join strategy:. - **inner** -- Only rows with a matching key in the opposite table are included; in the resulting table.; - **left** -- All rows from the left table are included in the resulting table.; If a row in the left table has no match in the right table, then the fields; derived from the right table will be missing.; - **right** -- All rows from the right table are included in the resulting table.; If a row in the right table has no match in the left table, then the fields; derived from the left table will be missing.; - **outer** -- All rows are included in the resulting table. If a row in the right; table has no match in the left table, then the fields derived from the left; table will be missing. If a row in the right table has no match in the left table,; then the fields derived from the left table will be missing.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8467#issuecomment-609470292
https://github.com/hail-is/hail/pull/8467#issuecomment-609471408:399,Integrability,depend,depends,399,"I like it, in particular I like the use of the adjective ""opposite"". Small proposed tweak to the first sentence, because that sounds similar to ""Both tables must have the same number of keys"", which is included in the description of keys below the portion we're changing. ```; Tables are joined at rows whose key fields have equal values.; The inclusion of a row with no match in the opposite table depends on the; join strategy:. - **inner** -- Only rows with a matching key in the opposite table are included; in the resulting table.; - **left** -- All rows from the left table are included in the resulting table.; If a row in the left table has no match in the right table, then the fields; derived from the right table will be missing.; - **right** -- All rows from the right table are included in the resulting table.; If a row in the right table has no match in the left table, then the fields; derived from the left table will be missing.; - **outer** -- All rows are included in the resulting table. If a row in the right; table has no match in the left table, then the fields derived from the left; table will be missing. If a row in the right table has no match in the left table,; then the fields derived from the left table will be missing.; ```. Below this I had added ""Missing (NA) keys never match"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8467#issuecomment-609471408
https://github.com/hail-is/hail/issues/8469#issuecomment-609464676:25,Deployability,release,release,25,0.2.35 is a bit of a dud release,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8469#issuecomment-609464676
https://github.com/hail-is/hail/pull/8471#issuecomment-609873949:52,Testability,assert,asserts,52,I'm gonna kill this in favor of something that also asserts the status is success.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8471#issuecomment-609873949
https://github.com/hail-is/hail/pull/8476#issuecomment-610034709:674,Modifiability,extend,extend,674,"> I think I agree with you if it is your intention for this javascript to only be used on pages served at hail.is?. So right now it's only used on pages hosted at hail.is/ (everything in the www folder). Longer term we should unify these efforts, so that www (hail.is), docs, workshop, notebook, etc have a unified appearance. Blog is different, since we're using a CMS (content management) solution, although I could be put under a common umbrella as well. One option is to use NextJS, which is my preferred React solution: https://ghost.org/docs/api/v3/nextjs/, although I'm not sure that is a solution that would get buy-in. I could make a solution in python as well and extend our current system as far as it will go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8476#issuecomment-610034709
https://github.com/hail-is/hail/pull/8485#issuecomment-610590716:88,Testability,log,log,88,Totally unrelated: what ends up in `/home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log`? What happens to the usual Spark/Hail master logs?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610590716
https://github.com/hail-is/hail/pull/8485#issuecomment-610590716:138,Testability,log,logs,138,Totally unrelated: what ends up in `/home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log`? What happens to the usual Spark/Hail master logs?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610590716
https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:518,Availability,error,error,518,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050
https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:594,Availability,error,errors,594,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050
https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:747,Deployability,pipeline,pipelines,747,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050
https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:903,Performance,optimiz,optimization,903,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050
https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:70,Testability,log,log,70,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050
https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:119,Testability,log,logs,119,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050
https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:219,Testability,log,logs,219,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050
https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:245,Testability,log,log,245,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050
https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:362,Testability,log,logs,362,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050
https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:416,Testability,log,logs,416,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050
https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:449,Testability,log,logs,449,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050
https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:820,Testability,log,logs,820,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050
https://github.com/hail-is/hail/pull/8490#issuecomment-610644390:41,Integrability,rout,router,41,"I have forthcoming changes that make the router only accept HTTPS. I've been testing this in my dev namespace. Unfortunately, my batch workers cannot speak to my batch instance anymore. The workers speak to internal-gateway who then tries to proxy to my router over HTTP, but nobody is listening on that port. As long as there is a mix of HTTP-only and HTTPS-only routers, internal-gateway needs a way to know which protocol to use with which router. It's temporary because I intend everyone to speak HTTPS.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8490#issuecomment-610644390
https://github.com/hail-is/hail/pull/8490#issuecomment-610644390:254,Integrability,rout,router,254,"I have forthcoming changes that make the router only accept HTTPS. I've been testing this in my dev namespace. Unfortunately, my batch workers cannot speak to my batch instance anymore. The workers speak to internal-gateway who then tries to proxy to my router over HTTP, but nobody is listening on that port. As long as there is a mix of HTTP-only and HTTPS-only routers, internal-gateway needs a way to know which protocol to use with which router. It's temporary because I intend everyone to speak HTTPS.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8490#issuecomment-610644390
https://github.com/hail-is/hail/pull/8490#issuecomment-610644390:364,Integrability,rout,routers,364,"I have forthcoming changes that make the router only accept HTTPS. I've been testing this in my dev namespace. Unfortunately, my batch workers cannot speak to my batch instance anymore. The workers speak to internal-gateway who then tries to proxy to my router over HTTP, but nobody is listening on that port. As long as there is a mix of HTTP-only and HTTPS-only routers, internal-gateway needs a way to know which protocol to use with which router. It's temporary because I intend everyone to speak HTTPS.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8490#issuecomment-610644390
https://github.com/hail-is/hail/pull/8490#issuecomment-610644390:416,Integrability,protocol,protocol,416,"I have forthcoming changes that make the router only accept HTTPS. I've been testing this in my dev namespace. Unfortunately, my batch workers cannot speak to my batch instance anymore. The workers speak to internal-gateway who then tries to proxy to my router over HTTP, but nobody is listening on that port. As long as there is a mix of HTTP-only and HTTPS-only routers, internal-gateway needs a way to know which protocol to use with which router. It's temporary because I intend everyone to speak HTTPS.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8490#issuecomment-610644390
https://github.com/hail-is/hail/pull/8490#issuecomment-610644390:443,Integrability,rout,router,443,"I have forthcoming changes that make the router only accept HTTPS. I've been testing this in my dev namespace. Unfortunately, my batch workers cannot speak to my batch instance anymore. The workers speak to internal-gateway who then tries to proxy to my router over HTTP, but nobody is listening on that port. As long as there is a mix of HTTP-only and HTTPS-only routers, internal-gateway needs a way to know which protocol to use with which router. It's temporary because I intend everyone to speak HTTPS.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8490#issuecomment-610644390
https://github.com/hail-is/hail/pull/8490#issuecomment-610644390:77,Testability,test,testing,77,"I have forthcoming changes that make the router only accept HTTPS. I've been testing this in my dev namespace. Unfortunately, my batch workers cannot speak to my batch instance anymore. The workers speak to internal-gateway who then tries to proxy to my router over HTTP, but nobody is listening on that port. As long as there is a mix of HTTP-only and HTTPS-only routers, internal-gateway needs a way to know which protocol to use with which router. It's temporary because I intend everyone to speak HTTPS.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8490#issuecomment-610644390
https://github.com/hail-is/hail/pull/8496#issuecomment-612919449:53,Performance,queue,queued,53,Reassigning this since Arcturus has a lot of reviews queued up. I think it's a relatively straightforward one though.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8496#issuecomment-612919449
https://github.com/hail-is/hail/pull/8497#issuecomment-610732635:37,Availability,error,error,37,"I tested this by adding an assertion error that would get hit just before the while loop.With the old code, we always hit the assertion, with the new code, we do not.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8497#issuecomment-610732635
https://github.com/hail-is/hail/pull/8497#issuecomment-610732635:2,Testability,test,tested,2,"I tested this by adding an assertion error that would get hit just before the while loop.With the old code, we always hit the assertion, with the new code, we do not.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8497#issuecomment-610732635
https://github.com/hail-is/hail/pull/8497#issuecomment-610732635:27,Testability,assert,assertion,27,"I tested this by adding an assertion error that would get hit just before the while loop.With the old code, we always hit the assertion, with the new code, we do not.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8497#issuecomment-610732635
https://github.com/hail-is/hail/pull/8497#issuecomment-610732635:126,Testability,assert,assertion,126,"I tested this by adding an assertion error that would get hit just before the while loop.With the old code, we always hit the assertion, with the new code, we do not.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8497#issuecomment-610732635
https://github.com/hail-is/hail/pull/8498#issuecomment-610999840:5,Deployability,deploy,deploying,5,hand deploying,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8498#issuecomment-610999840
https://github.com/hail-is/hail/pull/8498#issuecomment-611001507:0,Deployability,deploy,deployed,0,deployed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8498#issuecomment-611001507
https://github.com/hail-is/hail/pull/8500#issuecomment-611043999:5,Deployability,deploy,deploying,5,hand deploying now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8500#issuecomment-611043999
https://github.com/hail-is/hail/pull/8500#issuecomment-611051863:3,Deployability,deploy,deployed,3,CI deployed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8500#issuecomment-611051863
https://github.com/hail-is/hail/pull/8504#issuecomment-611711990:243,Deployability,deploy,deploy,243,"I suppose we have lost to stand up a working CI (b/c it must talk to batcH) without a working router. Or, really, we've lost the ability to do that without copying the service definition from the router's list. I'm OK with this. I agree `make deploy` not also setting up the service definition is out of sync with our previous behavior. I could engineer some system to have one definition but two uses of the `Service` definition but it just doesn't feel important enough to justify it. We rarely if ever deploy the services by hand without deploying the router first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504#issuecomment-611711990
https://github.com/hail-is/hail/pull/8504#issuecomment-611711990:505,Deployability,deploy,deploy,505,"I suppose we have lost to stand up a working CI (b/c it must talk to batcH) without a working router. Or, really, we've lost the ability to do that without copying the service definition from the router's list. I'm OK with this. I agree `make deploy` not also setting up the service definition is out of sync with our previous behavior. I could engineer some system to have one definition but two uses of the `Service` definition but it just doesn't feel important enough to justify it. We rarely if ever deploy the services by hand without deploying the router first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504#issuecomment-611711990
https://github.com/hail-is/hail/pull/8504#issuecomment-611711990:541,Deployability,deploy,deploying,541,"I suppose we have lost to stand up a working CI (b/c it must talk to batcH) without a working router. Or, really, we've lost the ability to do that without copying the service definition from the router's list. I'm OK with this. I agree `make deploy` not also setting up the service definition is out of sync with our previous behavior. I could engineer some system to have one definition but two uses of the `Service` definition but it just doesn't feel important enough to justify it. We rarely if ever deploy the services by hand without deploying the router first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504#issuecomment-611711990
https://github.com/hail-is/hail/pull/8504#issuecomment-611711990:94,Integrability,rout,router,94,"I suppose we have lost to stand up a working CI (b/c it must talk to batcH) without a working router. Or, really, we've lost the ability to do that without copying the service definition from the router's list. I'm OK with this. I agree `make deploy` not also setting up the service definition is out of sync with our previous behavior. I could engineer some system to have one definition but two uses of the `Service` definition but it just doesn't feel important enough to justify it. We rarely if ever deploy the services by hand without deploying the router first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504#issuecomment-611711990
https://github.com/hail-is/hail/pull/8504#issuecomment-611711990:196,Integrability,rout,router,196,"I suppose we have lost to stand up a working CI (b/c it must talk to batcH) without a working router. Or, really, we've lost the ability to do that without copying the service definition from the router's list. I'm OK with this. I agree `make deploy` not also setting up the service definition is out of sync with our previous behavior. I could engineer some system to have one definition but two uses of the `Service` definition but it just doesn't feel important enough to justify it. We rarely if ever deploy the services by hand without deploying the router first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504#issuecomment-611711990
https://github.com/hail-is/hail/pull/8504#issuecomment-611711990:555,Integrability,rout,router,555,"I suppose we have lost to stand up a working CI (b/c it must talk to batcH) without a working router. Or, really, we've lost the ability to do that without copying the service definition from the router's list. I'm OK with this. I agree `make deploy` not also setting up the service definition is out of sync with our previous behavior. I could engineer some system to have one definition but two uses of the `Service` definition but it just doesn't feel important enough to justify it. We rarely if ever deploy the services by hand without deploying the router first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8504#issuecomment-611711990
https://github.com/hail-is/hail/pull/8511#issuecomment-611277147:65,Availability,error,error,65,"```; # xsltproc -o fuckyou.html --html fuckyou.xslt; compilation error: file fuckyou.xslt line 2 element stylesheet; xsltParseStylesheetProcess : document is not a stylesheet; # vim fuckyou.xslt ; # cat fuckyou.xslt ; <?xml version=""1.0"" encoding=""ISO-8859-15""?>; <xsl:stylesheet version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform"">; </xsl:stylesheet>; # xsltproc -o fuckyou.html --html fuckyou.xslt; ```. www.w3.org definitely speaks TLS based on `curl`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8511#issuecomment-611277147
https://github.com/hail-is/hail/pull/8511#issuecomment-613040116:91,Testability,test,tests,91,"@johnc1231 you already peeked at this, so do you mind properly reviewing it? It passes the tests now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8511#issuecomment-613040116
https://github.com/hail-is/hail/pull/8513#issuecomment-611765252:27,Deployability,configurat,configuration,27,I'm beginning to think the configuration stuff is confusing because it ignores the distinction between clients and servers.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513#issuecomment-611765252
https://github.com/hail-is/hail/pull/8513#issuecomment-611765252:27,Modifiability,config,configuration,27,I'm beginning to think the configuration stuff is confusing because it ignores the distinction between clients and servers.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8513#issuecomment-611765252
https://github.com/hail-is/hail/pull/8518#issuecomment-611792254:87,Testability,test,tests,87,"Let's decide on the implemented functions that we want here, and then I will write the tests for those functions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8518#issuecomment-611792254
https://github.com/hail-is/hail/pull/8520#issuecomment-612108804:116,Testability,test,test,116,"There is a PruneDeadFields rule. The default case for rebuild should work, right?. I did forget the PruneDeadFields test. I'll add that to the checklist too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8520#issuecomment-612108804
https://github.com/hail-is/hail/pull/8520#issuecomment-612126069:24,Deployability,update,update,24,"Added Pruner tests, and update the add-ir-checklist.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8520#issuecomment-612126069
https://github.com/hail-is/hail/pull/8520#issuecomment-612126069:13,Testability,test,tests,13,"Added Pruner tests, and update the add-ir-checklist.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8520#issuecomment-612126069
https://github.com/hail-is/hail/pull/8520#issuecomment-612907568:13,Testability,test,tests,13,"Failing Java tests (also, looking at the `test_hail_java` log, looks like we print a lot during this stream stuff?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8520#issuecomment-612907568
https://github.com/hail-is/hail/pull/8520#issuecomment-612907568:58,Testability,log,log,58,"Failing Java tests (also, looking at the `test_hail_java` log, looks like we print a lot during this stream stuff?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8520#issuecomment-612907568
https://github.com/hail-is/hail/pull/8520#issuecomment-612909469:150,Testability,test,testing,150,"Just fixed, I forgot the parser rule. Yeah, I left in some printing in the stream stuff that I should probably at least flag to turn off in automated testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8520#issuecomment-612909469
https://github.com/hail-is/hail/pull/8522#issuecomment-615506715:3,Testability,test,tests,3,so tests pass; happy to throw this behind a feature flag anyway,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8522#issuecomment-615506715
https://github.com/hail-is/hail/pull/8522#issuecomment-615541561:28,Testability,test,tests,28,"Should be ready for a look, tests pass, added construction test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8522#issuecomment-615541561
https://github.com/hail-is/hail/pull/8522#issuecomment-615541561:59,Testability,test,test,59,"Should be ready for a look, tests pass, added construction test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8522#issuecomment-615541561
https://github.com/hail-is/hail/pull/8523#issuecomment-611790941:5,Deployability,deploy,deploying,5,hand deploying now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8523#issuecomment-611790941
https://github.com/hail-is/hail/pull/8523#issuecomment-611791434:117,Availability,error,error,117,hand deploying the last working master that is:; ```; * | | | | 49ec05df2 - (7 hours ago) [query] Throw a validation error for queries that read/write same path (#8327) - Tim Poterba (HEAD); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8523#issuecomment-611791434
https://github.com/hail-is/hail/pull/8523#issuecomment-611791434:5,Deployability,deploy,deploying,5,hand deploying the last working master that is:; ```; * | | | | 49ec05df2 - (7 hours ago) [query] Throw a validation error for queries that read/write same path (#8327) - Tim Poterba (HEAD); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8523#issuecomment-611791434
https://github.com/hail-is/hail/pull/8523#issuecomment-611791434:106,Security,validat,validation,106,hand deploying the last working master that is:; ```; * | | | | 49ec05df2 - (7 hours ago) [query] Throw a validation error for queries that read/write same path (#8327) - Tim Poterba (HEAD); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8523#issuecomment-611791434
https://github.com/hail-is/hail/pull/8523#issuecomment-611792189:5,Deployability,deploy,deploy,5,hand deploy successful,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8523#issuecomment-611792189
https://github.com/hail-is/hail/pull/8524#issuecomment-611796466:5,Deployability,deploy,deploying,5,hand deploying latest version,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8524#issuecomment-611796466
https://github.com/hail-is/hail/pull/8524#issuecomment-611797230:0,Deployability,deploy,deploy,0,deploy successful,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8524#issuecomment-611797230
https://github.com/hail-is/hail/pull/8533#issuecomment-613147506:129,Deployability,deploy,deploy,129,"OK, so. I added test_dataproc as a separate build step and pulled that code out of the makefile into a script. I'm running a dev-deploy test of it here: ~~https://ci.hail.is/batches/32232~~. I added a commit to this branch (not present in the aforementioned dev deploy) that exits 0 if the git tag already exists. This prevents running the Dataproc tests on every master commit. I broadened the scope to include `dev`. This means that developers can run a deploy with `hailctl dev deploy hail-is/hail:master deploy`. I haven't tested the deploy step. I volunteer to do the next deploy and deal with whatever is broken. EDIT: Copying data between jobs loses the chmod settings so I had to switch away from `./`, that was why the last batch failed. https://ci.hail.is/batches/32237",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506
https://github.com/hail-is/hail/pull/8533#issuecomment-613147506:262,Deployability,deploy,deploy,262,"OK, so. I added test_dataproc as a separate build step and pulled that code out of the makefile into a script. I'm running a dev-deploy test of it here: ~~https://ci.hail.is/batches/32232~~. I added a commit to this branch (not present in the aforementioned dev deploy) that exits 0 if the git tag already exists. This prevents running the Dataproc tests on every master commit. I broadened the scope to include `dev`. This means that developers can run a deploy with `hailctl dev deploy hail-is/hail:master deploy`. I haven't tested the deploy step. I volunteer to do the next deploy and deal with whatever is broken. EDIT: Copying data between jobs loses the chmod settings so I had to switch away from `./`, that was why the last batch failed. https://ci.hail.is/batches/32237",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506
https://github.com/hail-is/hail/pull/8533#issuecomment-613147506:456,Deployability,deploy,deploy,456,"OK, so. I added test_dataproc as a separate build step and pulled that code out of the makefile into a script. I'm running a dev-deploy test of it here: ~~https://ci.hail.is/batches/32232~~. I added a commit to this branch (not present in the aforementioned dev deploy) that exits 0 if the git tag already exists. This prevents running the Dataproc tests on every master commit. I broadened the scope to include `dev`. This means that developers can run a deploy with `hailctl dev deploy hail-is/hail:master deploy`. I haven't tested the deploy step. I volunteer to do the next deploy and deal with whatever is broken. EDIT: Copying data between jobs loses the chmod settings so I had to switch away from `./`, that was why the last batch failed. https://ci.hail.is/batches/32237",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506
https://github.com/hail-is/hail/pull/8533#issuecomment-613147506:481,Deployability,deploy,deploy,481,"OK, so. I added test_dataproc as a separate build step and pulled that code out of the makefile into a script. I'm running a dev-deploy test of it here: ~~https://ci.hail.is/batches/32232~~. I added a commit to this branch (not present in the aforementioned dev deploy) that exits 0 if the git tag already exists. This prevents running the Dataproc tests on every master commit. I broadened the scope to include `dev`. This means that developers can run a deploy with `hailctl dev deploy hail-is/hail:master deploy`. I haven't tested the deploy step. I volunteer to do the next deploy and deal with whatever is broken. EDIT: Copying data between jobs loses the chmod settings so I had to switch away from `./`, that was why the last batch failed. https://ci.hail.is/batches/32237",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506
https://github.com/hail-is/hail/pull/8533#issuecomment-613147506:508,Deployability,deploy,deploy,508,"OK, so. I added test_dataproc as a separate build step and pulled that code out of the makefile into a script. I'm running a dev-deploy test of it here: ~~https://ci.hail.is/batches/32232~~. I added a commit to this branch (not present in the aforementioned dev deploy) that exits 0 if the git tag already exists. This prevents running the Dataproc tests on every master commit. I broadened the scope to include `dev`. This means that developers can run a deploy with `hailctl dev deploy hail-is/hail:master deploy`. I haven't tested the deploy step. I volunteer to do the next deploy and deal with whatever is broken. EDIT: Copying data between jobs loses the chmod settings so I had to switch away from `./`, that was why the last batch failed. https://ci.hail.is/batches/32237",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506
https://github.com/hail-is/hail/pull/8533#issuecomment-613147506:538,Deployability,deploy,deploy,538,"OK, so. I added test_dataproc as a separate build step and pulled that code out of the makefile into a script. I'm running a dev-deploy test of it here: ~~https://ci.hail.is/batches/32232~~. I added a commit to this branch (not present in the aforementioned dev deploy) that exits 0 if the git tag already exists. This prevents running the Dataproc tests on every master commit. I broadened the scope to include `dev`. This means that developers can run a deploy with `hailctl dev deploy hail-is/hail:master deploy`. I haven't tested the deploy step. I volunteer to do the next deploy and deal with whatever is broken. EDIT: Copying data between jobs loses the chmod settings so I had to switch away from `./`, that was why the last batch failed. https://ci.hail.is/batches/32237",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506
https://github.com/hail-is/hail/pull/8533#issuecomment-613147506:578,Deployability,deploy,deploy,578,"OK, so. I added test_dataproc as a separate build step and pulled that code out of the makefile into a script. I'm running a dev-deploy test of it here: ~~https://ci.hail.is/batches/32232~~. I added a commit to this branch (not present in the aforementioned dev deploy) that exits 0 if the git tag already exists. This prevents running the Dataproc tests on every master commit. I broadened the scope to include `dev`. This means that developers can run a deploy with `hailctl dev deploy hail-is/hail:master deploy`. I haven't tested the deploy step. I volunteer to do the next deploy and deal with whatever is broken. EDIT: Copying data between jobs loses the chmod settings so I had to switch away from `./`, that was why the last batch failed. https://ci.hail.is/batches/32237",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506
https://github.com/hail-is/hail/pull/8533#issuecomment-613147506:136,Testability,test,test,136,"OK, so. I added test_dataproc as a separate build step and pulled that code out of the makefile into a script. I'm running a dev-deploy test of it here: ~~https://ci.hail.is/batches/32232~~. I added a commit to this branch (not present in the aforementioned dev deploy) that exits 0 if the git tag already exists. This prevents running the Dataproc tests on every master commit. I broadened the scope to include `dev`. This means that developers can run a deploy with `hailctl dev deploy hail-is/hail:master deploy`. I haven't tested the deploy step. I volunteer to do the next deploy and deal with whatever is broken. EDIT: Copying data between jobs loses the chmod settings so I had to switch away from `./`, that was why the last batch failed. https://ci.hail.is/batches/32237",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506
https://github.com/hail-is/hail/pull/8533#issuecomment-613147506:349,Testability,test,tests,349,"OK, so. I added test_dataproc as a separate build step and pulled that code out of the makefile into a script. I'm running a dev-deploy test of it here: ~~https://ci.hail.is/batches/32232~~. I added a commit to this branch (not present in the aforementioned dev deploy) that exits 0 if the git tag already exists. This prevents running the Dataproc tests on every master commit. I broadened the scope to include `dev`. This means that developers can run a deploy with `hailctl dev deploy hail-is/hail:master deploy`. I haven't tested the deploy step. I volunteer to do the next deploy and deal with whatever is broken. EDIT: Copying data between jobs loses the chmod settings so I had to switch away from `./`, that was why the last batch failed. https://ci.hail.is/batches/32237",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506
https://github.com/hail-is/hail/pull/8533#issuecomment-613147506:527,Testability,test,tested,527,"OK, so. I added test_dataproc as a separate build step and pulled that code out of the makefile into a script. I'm running a dev-deploy test of it here: ~~https://ci.hail.is/batches/32232~~. I added a commit to this branch (not present in the aforementioned dev deploy) that exits 0 if the git tag already exists. This prevents running the Dataproc tests on every master commit. I broadened the scope to include `dev`. This means that developers can run a deploy with `hailctl dev deploy hail-is/hail:master deploy`. I haven't tested the deploy step. I volunteer to do the next deploy and deal with whatever is broken. EDIT: Copying data between jobs loses the chmod settings so I had to switch away from `./`, that was why the last batch failed. https://ci.hail.is/batches/32237",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506
https://github.com/hail-is/hail/pull/8536#issuecomment-613042838:0,Testability,test,test,0,test module doesn't compile,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8536#issuecomment-613042838
https://github.com/hail-is/hail/pull/8537#issuecomment-612563478:21,Testability,test,tests,21,@cseed it looks like tests are failing with `I == Z`. Is there another place you need to carry through the Boolean as Int thing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8537#issuecomment-612563478
https://github.com/hail-is/hail/issues/8545#issuecomment-613143370:98,Energy Efficiency,efficient,efficient,98,"Hail only accepts BGEN files with 8-bit probabilities, which is the most common (and a very space-efficient) representation. You can generate this yourself using the `-bgen-bits 8` argument in qctool. Hope this helps!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8545#issuecomment-613143370
https://github.com/hail-is/hail/pull/8550#issuecomment-613603023:131,Deployability,deploy,deploy,131,"This PR does not move us closer to testing and releasing built-once binaries. This is an important goal for me, but automating the deploy process as it exists is a prerequisite.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550#issuecomment-613603023
https://github.com/hail-is/hail/pull/8550#issuecomment-613603023:35,Testability,test,testing,35,"This PR does not move us closer to testing and releasing built-once binaries. This is an important goal for me, but automating the deploy process as it exists is a prerequisite.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550#issuecomment-613603023
https://github.com/hail-is/hail/pull/8550#issuecomment-613603462:83,Deployability,release,release,83,"OK, cool. Thanks for adding the dataproc tests! I'll approve this once the current release goes in, then we'll try this out for the next release, 0.2.38.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550#issuecomment-613603462
https://github.com/hail-is/hail/pull/8550#issuecomment-613603462:137,Deployability,release,release,137,"OK, cool. Thanks for adding the dataproc tests! I'll approve this once the current release goes in, then we'll try this out for the next release, 0.2.38.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550#issuecomment-613603462
https://github.com/hail-is/hail/pull/8550#issuecomment-613603462:41,Testability,test,tests,41,"OK, cool. Thanks for adding the dataproc tests! I'll approve this once the current release goes in, then we'll try this out for the next release, 0.2.38.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8550#issuecomment-613603462
https://github.com/hail-is/hail/pull/8552#issuecomment-613639601:67,Safety,avoid,avoided,67,"That's probably more code complication for CI that could be easily avoided by just sticking to the ""WIP"" tag.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8552#issuecomment-613639601
https://github.com/hail-is/hail/pull/8552#issuecomment-613658889:83,Deployability,deploy,deploy,83,Yes we could do that. I added the WIP tag. Sometimes PR testing is easier than dev deploy,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8552#issuecomment-613658889
https://github.com/hail-is/hail/pull/8552#issuecomment-613658889:56,Testability,test,testing,56,Yes we could do that. I added the WIP tag. Sometimes PR testing is easier than dev deploy,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8552#issuecomment-613658889
https://github.com/hail-is/hail/pull/8552#issuecomment-613688867:43,Integrability,message,message,43,I added an asana task for CI to use the PR message for single commit PRs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8552#issuecomment-613688867
https://github.com/hail-is/hail/pull/8552#issuecomment-613698193:153,Integrability,message,messages,153,"There's some arguments that might let us do that: https://developer.github.com/v3/pulls/#merge-a-pull-request-merge-button I'm not sure exactly what the messages look like. We could at least grab the PR message and supply it as ""additional"" text.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8552#issuecomment-613698193
https://github.com/hail-is/hail/pull/8552#issuecomment-613698193:203,Integrability,message,message,203,"There's some arguments that might let us do that: https://developer.github.com/v3/pulls/#merge-a-pull-request-merge-button I'm not sure exactly what the messages look like. We could at least grab the PR message and supply it as ""additional"" text.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8552#issuecomment-613698193
https://github.com/hail-is/hail/pull/8557#issuecomment-614089797:82,Testability,test,tests,82,"Apologies, I should have caught this in review. This at least lets you filter the tests, but they should also be properly executable. I'll fix that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8557#issuecomment-614089797
https://github.com/hail-is/hail/pull/8559#issuecomment-614205652:103,Modifiability,config,config,103,I verified that I can now run the batch tests on my laptop from the hail directory with:; ```; hailctl config set batch/billing_project hail # only needed once; make pytest PYTEST_ARGS='-k BatchTests'; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559#issuecomment-614205652
https://github.com/hail-is/hail/pull/8559#issuecomment-614205652:40,Testability,test,tests,40,I verified that I can now run the batch tests on my laptop from the hail directory with:; ```; hailctl config set batch/billing_project hail # only needed once; make pytest PYTEST_ARGS='-k BatchTests'; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8559#issuecomment-614205652
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:585,Deployability,Deploy,Deploy,585,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:630,Deployability,deploy,deploy,630,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:701,Deployability,upgrade,upgrade,701,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:914,Deployability,deploy,deploy,914,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:89,Integrability,interface,interface,89,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:137,Integrability,interface,interface,137,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:1241,Modifiability,config,config,1241,"such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDelaySeconds: 5; periodSeconds: 5; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:1859,Modifiability,config,configure-pod-container,1859,"such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDelaySeconds: 5; periodSeconds: 5; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:1883,Modifiability,config,configure-liveness-readiness-startup-probes,1883,"such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDelaySeconds: 5; periodSeconds: 5; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:191,Security,secur,secure,191,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:569,Security,certificate,certificate,569,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:751,Security,certificate,certificate,751,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:810,Security,certificate,certificates,810,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:846,Security,certificate,certificate,846,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:1450,Security,authoriz,authorization,1450,"such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDelaySeconds: 5; periodSeconds: 5; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:544,Usability,simpl,simpler,544,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805
https://github.com/hail-is/hail/pull/8561#issuecomment-615210085:61,Usability,clear,clear,61,"Also, thanks for that detailed write up. That was incredibly clear and instructive.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615210085
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:1894,Availability,error,error,1894," for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. crea",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2841,Availability,downtime,downtime,2841,"deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:1837,Deployability,deploy,deployment,1837,"wever, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a n",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2291,Deployability,update,update,2291,"ion strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2322,Deployability,configurat,configuration,2322,"ion strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2378,Deployability,configurat,configuration,2378,"ion strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2564,Deployability,deploy,deploy,2564,"ert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Dev",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2981,Deployability,update,update,2981," the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3297,Deployability,deploy,deploy,3297,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3468,Deployability,deploy,deploy,3468,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:4120,Deployability,deploy,deploy,4120,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3392,Integrability,rout,router-resolver,3392,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:333,Modifiability,rewrite,rewrite,333,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2322,Modifiability,config,configuration,2322,"ion strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2378,Modifiability,config,configuration,2378,"ion strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2339,Performance,load,load,2339,"ion strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3731,Performance,load,load,3731,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:84,Security,secur,security,84,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:127,Security,secur,security,127,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:158,Security,secur,security,158,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:226,Security,secur,security,226,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:374,Security,certificate,certificate,374,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:503,Security,certificate,certificate,503,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:595,Security,certificate,certificates,595,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:693,Security,certificate,certificate,693,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:732,Security,certificate,certificate,732,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:780,Security,secur,secure,780,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:1007,Security,certificate,certificate,1007,"] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little har",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:1043,Security,certificate,certificate,1043,"] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little har",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:1175,Security,certificate,certificates,1175,"] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little har",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:1205,Security,certificate,certificates,1205,"] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little har",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2042,Security,certificate,certificate,2042,"ally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. up",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3096,Security,certificate,certificates,3096,"cating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3255,Security,certificate,certificate,3255,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3750,Security,certificate,certificates,3750,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3953,Security,certificate,certificate,3953,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:4095,Security,certificate,certificates,4095,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:4164,Security,certificate,certificates,4164,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3840,Testability,test,tests,3840,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:1664,Usability,simpl,simplest,1664,"icate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243
https://github.com/hail-is/hail/pull/8561#issuecomment-617911061:600,Availability,downtime,downtime,600,"@cseed ; I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. I've updated the PR description with this text so it ends up in the squashed commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061
https://github.com/hail-is/hail/pull/8561#issuecomment-617911061:584,Deployability,deploy,deploys,584,"@cseed ; I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. I've updated the PR description with this text so it ends up in the squashed commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061
https://github.com/hail-is/hail/pull/8561#issuecomment-617911061:970,Deployability,update,updated,970,"@cseed ; I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. I've updated the PR description with this text so it ends up in the squashed commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061
https://github.com/hail-is/hail/pull/8561#issuecomment-617911061:48,Modifiability,config,config-hail-root,48,"@cseed ; I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. I've updated the PR description with this text so it ends up in the squashed commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061
https://github.com/hail-is/hail/pull/8561#issuecomment-617911061:616,Safety,avoid,avoid,616,"@cseed ; I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. I've updated the PR description with this text so it ends up in the squashed commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061
https://github.com/hail-is/hail/pull/8561#issuecomment-617911061:434,Security,certificate,certificate,434,"@cseed ; I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. I've updated the PR description with this text so it ends up in the squashed commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061
https://github.com/hail-is/hail/pull/8561#issuecomment-617911061:535,Security,certificate,certificate,535,"@cseed ; I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. I've updated the PR description with this text so it ends up in the squashed commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061
https://github.com/hail-is/hail/pull/8563#issuecomment-618474871:67,Testability,test,tests,67,"The auth service is used extensively, as you mention. I also added tests for my new functionality. I doubt the PR would pass if auth wasn't correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8563#issuecomment-618474871
https://github.com/hail-is/hail/pull/8563#issuecomment-621317831:0,Testability,Test,Tests,0,Tests are failing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8563#issuecomment-621317831
https://github.com/hail-is/hail/pull/8564#issuecomment-614850752:20,Testability,test,tests,20,"This passed all its tests, but I'll wait for a review to give it a bump.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8564#issuecomment-614850752
https://github.com/hail-is/hail/pull/8565#issuecomment-615221048:13,Testability,test,tests,13,Failing hail tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8565#issuecomment-615221048
https://github.com/hail-is/hail/pull/8565#issuecomment-615229346:24,Availability,failure,failures,24,"I'm pretty sure all the failures are from a bug I fixed in #8564. Was waiting for that to merge, but I can rebase to double check.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8565#issuecomment-615229346
https://github.com/hail-is/hail/pull/8571#issuecomment-615334218:20,Integrability,depend,dependency,20,"Thanks, the lsmtree dependency change unbreaks Google Dataproc compatibility so 'm tagging this high-prio.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8571#issuecomment-615334218
https://github.com/hail-is/hail/pull/8571#issuecomment-615348697:29,Integrability,depend,dependencies,29,"actually, the non-transitive dependencies is breaking the tests that use the LSM tree. Will sort out those issues.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8571#issuecomment-615348697
https://github.com/hail-is/hail/pull/8571#issuecomment-615348697:58,Testability,test,tests,58,"actually, the non-transitive dependencies is breaking the tests that use the LSM tree. Will sort out those issues.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8571#issuecomment-615348697
https://github.com/hail-is/hail/pull/8571#issuecomment-615453991:32,Integrability,depend,dependencies,32,I think I've fixed the dataproc dependencies: https://github.com/hail-is/hail/pull/8576,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8571#issuecomment-615453991
https://github.com/hail-is/hail/pull/8571#issuecomment-615454573:35,Integrability,depend,dependency,35,I reverted the LSM tree transitive dependency change.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8571#issuecomment-615454573
https://github.com/hail-is/hail/pull/8574#issuecomment-615356354:13,Testability,test,tests,13,some failing tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8574#issuecomment-615356354
https://github.com/hail-is/hail/pull/8574#issuecomment-615356793:8,Testability,test,tests,8,failing tests are from a problem in the parent PR that I fixed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8574#issuecomment-615356793
https://github.com/hail-is/hail/pull/8575#issuecomment-615433780:93,Safety,safe,safe,93,"It uses `hasMissingValues` now, which already has a test. I'm beefing up that test now to be safe. `test_linreg` fails if you swap the intercept with the other covariate, I can add that too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8575#issuecomment-615433780
https://github.com/hail-is/hail/pull/8575#issuecomment-615433780:52,Testability,test,test,52,"It uses `hasMissingValues` now, which already has a test. I'm beefing up that test now to be safe. `test_linreg` fails if you swap the intercept with the other covariate, I can add that too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8575#issuecomment-615433780
https://github.com/hail-is/hail/pull/8575#issuecomment-615433780:78,Testability,test,test,78,"It uses `hasMissingValues` now, which already has a test. I'm beefing up that test now to be safe. `test_linreg` fails if you swap the intercept with the other covariate, I can add that too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8575#issuecomment-615433780
https://github.com/hail-is/hail/pull/8575#issuecomment-615439810:8,Testability,test,test,8,"Yeah, a test of linreg is what I meant, thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8575#issuecomment-615439810
https://github.com/hail-is/hail/pull/8576#issuecomment-615459260:0,Availability,error,error,0,"error in test java:; ```; [[TestNGClassFinder]] Unable to read methods on class is.hail.relocated.com.indeed.util.varexport.servlet.ViewExportedVariablesServlet - unable to resolve class reference freemarker/template/ObjectWrapper; Exception in thread ""main"" java.lang.NoClassDefFoundError: freemarker/template/ObjectWrapper; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-615459260
https://github.com/hail-is/hail/pull/8576#issuecomment-615459260:9,Testability,test,test,9,"error in test java:; ```; [[TestNGClassFinder]] Unable to read methods on class is.hail.relocated.com.indeed.util.varexport.servlet.ViewExportedVariablesServlet - unable to resolve class reference freemarker/template/ObjectWrapper; Exception in thread ""main"" java.lang.NoClassDefFoundError: freemarker/template/ObjectWrapper; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-615459260
https://github.com/hail-is/hail/pull/8576#issuecomment-615459260:28,Testability,Test,TestNGClassFinder,28,"error in test java:; ```; [[TestNGClassFinder]] Unable to read methods on class is.hail.relocated.com.indeed.util.varexport.servlet.ViewExportedVariablesServlet - unable to resolve class reference freemarker/template/ObjectWrapper; Exception in thread ""main"" java.lang.NoClassDefFoundError: freemarker/template/ObjectWrapper; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-615459260
https://github.com/hail-is/hail/pull/8576#issuecomment-615468636:23,Testability,test,tests,23,It passed the dataproc tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-615468636
https://github.com/hail-is/hail/pull/8576#issuecomment-615476103:24,Testability,Test,Testing,24,ok maybe that fixes it? Testing locally doesn't seem to encounter the issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-615476103
https://github.com/hail-is/hail/pull/8576#issuecomment-615890461:17,Testability,test,tests,17,failing shuffler tests with UnsatisfiedLinkError.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-615890461
https://github.com/hail-is/hail/pull/8576#issuecomment-616207938:334,Availability,error,error,334,"ugh so this is really annoying. Shading libraries with native libraries doesn't work out of the box. The SO file that indeed ships has a symbol like this:; ```; 0000000000001440 T _Java_com_indeed_util_mmap_MMapBuffer_mmap; ```; But after relocation, the java counterpart's package doesn't match the package you see above. Ergo, link error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-616207938
https://github.com/hail-is/hail/pull/8576#issuecomment-616208715:52,Modifiability,plugin,plugin,52,Somewhat surprising that no one has written a shade plugin that renames SO symbols ðŸ¤· . That's clearly the correct answer here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-616208715
https://github.com/hail-is/hail/pull/8576#issuecomment-616208715:94,Usability,clear,clearly,94,Somewhat surprising that no one has written a shade plugin that renames SO symbols ðŸ¤· . That's clearly the correct answer here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8576#issuecomment-616208715
https://github.com/hail-is/hail/pull/8577#issuecomment-616211238:36,Availability,failure,failure,36,CI deploy is broken due to checkout failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8577#issuecomment-616211238
https://github.com/hail-is/hail/pull/8577#issuecomment-616211238:3,Deployability,deploy,deploy,3,CI deploy is broken due to checkout failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8577#issuecomment-616211238
https://github.com/hail-is/hail/pull/8581#issuecomment-617726597:84,Availability,down,downstream,84,"Thanks Alex, appreciated. I know it's a big change, but it is needed for a bunch of downstream stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8581#issuecomment-617726597
https://github.com/hail-is/hail/pull/8583#issuecomment-616612347:24,Deployability,deploy,deploy,24,I will need to manually deploy this when it lands.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8583#issuecomment-616612347
https://github.com/hail-is/hail/pull/8592#issuecomment-617353169:20,Testability,benchmark,benchmarks,20,Put WIP label since benchmarks are running but honestly correctness is probably more important than whatever the benchmarks tell us here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8592#issuecomment-617353169
https://github.com/hail-is/hail/pull/8592#issuecomment-617353169:113,Testability,benchmark,benchmarks,113,Put WIP label since benchmarks are running but honestly correctness is probably more important than whatever the benchmarks tell us here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8592#issuecomment-617353169
https://github.com/hail-is/hail/pull/8599#issuecomment-617899349:430,Integrability,Wrap,WrappedArray,430," ```; MakeStruct(WrappedArray((elements,Let(__iruid_5,GetField(Literal(struct{rows: array<struct{a: int32, b: str}>, global: struct{x: str}},[ArrayBuffer([0,row0]),[global]]),rows),ToArray(StreamMap(StreamRange(Let(__iruid_6,If(ApplyComparisonOp(LT(int32,int32),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),I32(16)),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),I32(16)),Let(__iruid_7,ApplyBinaryPrimOp(RoundToNegInfDivide(),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),Ref(__iruid_6,int32)),Let(__iruid_8,ApplyBinaryPrimOp(Subtract(),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),ApplyBinaryPrimOp(Multiply(),Ref(__iruid_7,int32),Ref(__iruid_6,int32))),If(ApplyComparisonOp(GTEQ(int32,int32),Ref(__iruid_6,int32),I32(1)),If(ApplyComparisonOp(GT(int32,int32),Ref(__iruid_8,int32),I32(0)),If(ApplyComparisonOp(LT(int32,int32),Ref(__iruid_8,int32),I32(1)),ApplyBinaryPrimOp(Add(),ApplyBinaryPrimOp(Multiply(),Ref(__iruid_7,int32),I32(1)),Ref(__iruid_8,int32)),ApplyBinaryPrimOp(Multiply(),ApplyBinaryPrimOp(Add(),Ref(__iruid_7,int32),I32(1)),I32(1))),ApplyBinaryPrimOp(Multiply(),Ref(__iruid_7,int32),I32(1))),I32(0))))),ApplyBinaryPrimOp(Add(),Let(__iruid_6,If(ApplyComparisonOp(LT(int32,int32),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),I32(16)),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),I32(16)),Let(__iruid_7,ApplyBinaryPrimOp(RoundToNegInfDivide(),ArrayLen(Ref(__iruid_5,array<struct{a: int32, b: str}>)),Ref(__iruid_6,int32)),Let(__iruid_8,ApplyBinaryPrimOp(Subtract(),ArrayLen(Ref(__iruid_5,array<struct{a: i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8599#issuecomment-617899349
https://github.com/hail-is/hail/pull/8599#issuecomment-618680964:25,Modifiability,refactor,refactoring,25,Closing this in favor of refactoring `TableStage` to do the whole right thing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8599#issuecomment-618680964
https://github.com/hail-is/hail/pull/8602#issuecomment-617920311:124,Performance,optimiz,optimizer,124,high level comment -- I'm not sure that the Requiredness class is as ergonomic as I'd like for use in several places in the optimizer/compiler. Can you PR that without the inference stuff so we can look at it independently?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8602#issuecomment-617920311
https://github.com/hail-is/hail/pull/8602#issuecomment-622126422:180,Testability,test,tests,180,"@tpoterba This should be ready for a look. It doesn't currently support InitOp, etc., but I will follow up with that once it's finished. Let me know if you want me to push TableIR tests to this PR or to split out those nodes into a separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8602#issuecomment-622126422
https://github.com/hail-is/hail/pull/8602#issuecomment-622471229:235,Testability,log,logic,235,"ok, I ended up pulling the Table stuff out of this PR (will push up another PR for it shortly) and hooking it in to InferPTypes in this PR (it won't handle stuff like RunAgg, etc. so I'll probably add a fallback to the original ptypes logic for now.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8602#issuecomment-622471229
https://github.com/hail-is/hail/pull/8603#issuecomment-618375915:8,Availability,error,errors,8,compile errors,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8603#issuecomment-618375915
https://github.com/hail-is/hail/pull/8621#issuecomment-620162545:125,Testability,test,test,125,"Alright, adjusted this to also make sure we don't pass a 0 into `dgeqrf` as the `LDA` argument and also to make sure that we test multiplying when only the inner dimension is a 0. Should be good for review now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8621#issuecomment-620162545
https://github.com/hail-is/hail/pull/8623#issuecomment-619208491:126,Deployability,deploy,deploy,126,"this image itself is failing for build reasons, I will merge. That should unstick everything by removing these steps from the deploy and pr paths.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8623#issuecomment-619208491
https://github.com/hail-is/hail/pull/8626#issuecomment-619252781:72,Testability,test,tests,72,I'm not 100% confident in the glob code yet. Do you have any additional tests to add?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8626#issuecomment-619252781
https://github.com/hail-is/hail/pull/8629#issuecomment-619252768:198,Availability,error,error,198,"Oops, sorry. Although I really blame PruneSuite. It does a bunch of serious work on construction, and basically makes the tests unusable if there are any bugs and testng silently bails with a fatal error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8629#issuecomment-619252768
https://github.com/hail-is/hail/pull/8629#issuecomment-619252768:122,Testability,test,tests,122,"Oops, sorry. Although I really blame PruneSuite. It does a bunch of serious work on construction, and basically makes the tests unusable if there are any bugs and testng silently bails with a fatal error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8629#issuecomment-619252768
https://github.com/hail-is/hail/pull/8629#issuecomment-619252768:163,Testability,test,testng,163,"Oops, sorry. Although I really blame PruneSuite. It does a bunch of serious work on construction, and basically makes the tests unusable if there are any bugs and testng silently bails with a fatal error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8629#issuecomment-619252768
https://github.com/hail-is/hail/pull/8633#issuecomment-619997334:20,Testability,test,tests,20,Failing the NDArray tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8633#issuecomment-619997334
https://github.com/hail-is/hail/pull/8633#issuecomment-620202391:0,Testability,Test,Tests,0,Tests pass now!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8633#issuecomment-620202391
https://github.com/hail-is/hail/pull/8634#issuecomment-629587797:0,Deployability,Update,Updates,0,"Updates made, site updated with changes matching what you should see if you dev deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8634#issuecomment-629587797
https://github.com/hail-is/hail/pull/8634#issuecomment-629587797:19,Deployability,update,updated,19,"Updates made, site updated with changes matching what you should see if you dev deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8634#issuecomment-629587797
https://github.com/hail-is/hail/pull/8634#issuecomment-629587797:80,Deployability,deploy,deploy,80,"Updates made, site updated with changes matching what you should see if you dev deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8634#issuecomment-629587797
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:1023,Availability,down,down,1023,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:1062,Availability,error,error,1062,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:1188,Availability,down,down,1188,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:1393,Availability,error,error,1393,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:1219,Deployability,continuous,continuous,1219,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:451,Integrability,contract,contract,451,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:503,Integrability,contract,contract,503,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:1331,Security,expose,exposed,1331,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:1717,Security,validat,validation,1717,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:154,Testability,assert,assert,154,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:1407,Testability,assert,assertion,1407,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:1823,Testability,assert,assertion,1823,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:1954,Testability,assert,assertion,1954,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:271,Usability,simpl,simple,271,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940
https://github.com/hail-is/hail/pull/8649#issuecomment-623771435:475,Testability,assert,assertion,475,"OK, thanks for the pushback. This is ready for another look. Here is my understanding of the semantics of partitioners and preserved keys:. 1. A TableStage from a lowered TableIR must have a partitioner that begins with the table key, but the partitioner can have a longer key. That partitioner need not be strict; consumers that require strictness like TableAggregateByKey or TableDistinct will strictify their lowered children. 2. The lowering rule for TableKeyBy makes no assertion about the number of preserved key fields. It is possible for client code to pass a TableKeyBy IR with 0 preserved key fields, which will result in a correct but possibly unperformant execution. We will document this when we document the IR system in several months.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-623771435
https://github.com/hail-is/hail/pull/8655#issuecomment-620651570:90,Testability,assert,assertion,90,"I was never clear on what `sign == 0` meant, but it appears not to be used: the tightened assertion passes everything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8655#issuecomment-620651570
https://github.com/hail-is/hail/pull/8655#issuecomment-620651570:12,Usability,clear,clear,12,"I was never clear on what `sign == 0` meant, but it appears not to be used: the tightened assertion passes everything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8655#issuecomment-620651570
https://github.com/hail-is/hail/pull/8656#issuecomment-621225819:41,Availability,error,errors,41,All python tests are failing with import errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8656#issuecomment-621225819
https://github.com/hail-is/hail/pull/8656#issuecomment-621225819:11,Testability,test,tests,11,All python tests are failing with import errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8656#issuecomment-621225819
https://github.com/hail-is/hail/pull/8658#issuecomment-621293435:120,Modifiability,refactor,refactor,120,"(@johnc1231 I snuck the LowerBlockMatrixIR change in here that I was using for illustration purposes, since it's just a refactor to make use of some legibility improvements, but I'm happy to back that out into a separate PR.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8658#issuecomment-621293435
https://github.com/hail-is/hail/pull/8665#issuecomment-621860471:11,Testability,test,tested,11,This isn't tested as thoroughly as I would like because I don't have a way to collect a keyed thing or unkey it in the lowerer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8665#issuecomment-621860471
https://github.com/hail-is/hail/pull/8665#issuecomment-621867329:63,Integrability,depend,depend,63,Maybe ought to wait for #8649 to go in so I can add tests that depend on keying.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8665#issuecomment-621867329
https://github.com/hail-is/hail/pull/8665#issuecomment-621867329:52,Testability,test,tests,52,Maybe ought to wait for #8649 to go in so I can add tests that depend on keying.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8665#issuecomment-621867329
https://github.com/hail-is/hail/pull/8671#issuecomment-621635408:391,Performance,load,loadInt,391,"With this change, only one test in `testTakeBy` fails. This is the generated code for the lead up to the invocation of the SeqOp method.; ```; /// THIS PR; public __m81wrapped(Lis/hail/annotations/Region;JJ)V; L0; LDC 0; LSTORE 6; LDC 0; LSTORE 8; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 8; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // we fail here; ISTORE 10; ILOAD 10; ISTORE 11; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // or here; ISTORE 12; ALOAD 0; ALOAD 0; ALOAD 1; ILOAD 11; INVOKEVIRTUAL __C44CompiledWithAggs.__m82str (Lis/hail/annotations/Region;I)J; LDC 9999; ILOAD 12; ISUB; INVOKEVIRTUAL __C44CompiledWithAggs.__m67take_by_seqop (JI)V; RETURN; L1; LOCALVARIABLE get_tup_elem_o J L0 L1 6; LOCALVARIABLE get_tup_elem_o J L0 L1 8; LOCALVARIABLE invoke I L0 L1 10; LOCALVARIABLE local11 I L0 L1 11; LOCALVARIABLE invoke I L0 L1 12; MAXSTACK = 5; MAXLOCALS = 13; ```. ```; /// PREVIOUS VERSION; // access flags 0x1; public __m85wrapped(Lis/hail/annotations/Region;JJ)V; L0; GOTO L1; L1; FRAME SAME; LLOAD 4; LSTORE 6; GOTO L2; L2; FRAME APPEND [J]; LDC 0; ISTORE 8; GOTO L3; L3; FRAME APPEND [I]; ALOAD 0; ILOAD 8; PUTFIELD __C46CompiledWithAggs.__f78vm : Z; GOTO L4; L4; FRAME SAME; LLOAD 4; LSTORE 9; GOTO L5; L5; FRAME APPEND [J]; LDC 0; ISTORE 11; GOTO L6; L6; FRAME APPEND [I]; ALOAD 0; ILOAD 11; PUTFIELD __C46CompiledWithAggs.__f77km : Z; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; IFNE L7; GOTO L8; L8; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 12; ILOAD 12; ISTORE 13; ALOAD 0; ALOAD 1; ILOAD 13; INVOKEVIRTUAL __C46CompiledWithAggs.__m86str (Lis/hail/annotations/Region;I)J; LSTORE 14; GOTO L9; L9; FRAME APPEND [T T J]; ALOAD 0; GETFIELD __C46Compiled",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408
https://github.com/hail-is/hail/pull/8671#issuecomment-621635408:596,Performance,load,loadInt,596,"With this change, only one test in `testTakeBy` fails. This is the generated code for the lead up to the invocation of the SeqOp method.; ```; /// THIS PR; public __m81wrapped(Lis/hail/annotations/Region;JJ)V; L0; LDC 0; LSTORE 6; LDC 0; LSTORE 8; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 8; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // we fail here; ISTORE 10; ILOAD 10; ISTORE 11; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // or here; ISTORE 12; ALOAD 0; ALOAD 0; ALOAD 1; ILOAD 11; INVOKEVIRTUAL __C44CompiledWithAggs.__m82str (Lis/hail/annotations/Region;I)J; LDC 9999; ILOAD 12; ISUB; INVOKEVIRTUAL __C44CompiledWithAggs.__m67take_by_seqop (JI)V; RETURN; L1; LOCALVARIABLE get_tup_elem_o J L0 L1 6; LOCALVARIABLE get_tup_elem_o J L0 L1 8; LOCALVARIABLE invoke I L0 L1 10; LOCALVARIABLE local11 I L0 L1 11; LOCALVARIABLE invoke I L0 L1 12; MAXSTACK = 5; MAXLOCALS = 13; ```. ```; /// PREVIOUS VERSION; // access flags 0x1; public __m85wrapped(Lis/hail/annotations/Region;JJ)V; L0; GOTO L1; L1; FRAME SAME; LLOAD 4; LSTORE 6; GOTO L2; L2; FRAME APPEND [J]; LDC 0; ISTORE 8; GOTO L3; L3; FRAME APPEND [I]; ALOAD 0; ILOAD 8; PUTFIELD __C46CompiledWithAggs.__f78vm : Z; GOTO L4; L4; FRAME SAME; LLOAD 4; LSTORE 9; GOTO L5; L5; FRAME APPEND [J]; LDC 0; ISTORE 11; GOTO L6; L6; FRAME APPEND [I]; ALOAD 0; ILOAD 11; PUTFIELD __C46CompiledWithAggs.__f77km : Z; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; IFNE L7; GOTO L8; L8; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 12; ILOAD 12; ISTORE 13; ALOAD 0; ALOAD 1; ILOAD 13; INVOKEVIRTUAL __C46CompiledWithAggs.__m86str (Lis/hail/annotations/Region;I)J; LSTORE 14; GOTO L9; L9; FRAME APPEND [T T J]; ALOAD 0; GETFIELD __C46Compiled",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408
https://github.com/hail-is/hail/pull/8671#issuecomment-621635408:1771,Performance,load,loadInt,1771,op (JI)V; RETURN; L1; LOCALVARIABLE get_tup_elem_o J L0 L1 6; LOCALVARIABLE get_tup_elem_o J L0 L1 8; LOCALVARIABLE invoke I L0 L1 10; LOCALVARIABLE local11 I L0 L1 11; LOCALVARIABLE invoke I L0 L1 12; MAXSTACK = 5; MAXLOCALS = 13; ```. ```; /// PREVIOUS VERSION; // access flags 0x1; public __m85wrapped(Lis/hail/annotations/Region;JJ)V; L0; GOTO L1; L1; FRAME SAME; LLOAD 4; LSTORE 6; GOTO L2; L2; FRAME APPEND [J]; LDC 0; ISTORE 8; GOTO L3; L3; FRAME APPEND [I]; ALOAD 0; ILOAD 8; PUTFIELD __C46CompiledWithAggs.__f78vm : Z; GOTO L4; L4; FRAME SAME; LLOAD 4; LSTORE 9; GOTO L5; L5; FRAME APPEND [J]; LDC 0; ISTORE 11; GOTO L6; L6; FRAME APPEND [I]; ALOAD 0; ILOAD 11; PUTFIELD __C46CompiledWithAggs.__f77km : Z; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; IFNE L7; GOTO L8; L8; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 12; ILOAD 12; ISTORE 13; ALOAD 0; ALOAD 1; ILOAD 13; INVOKEVIRTUAL __C46CompiledWithAggs.__m86str (Lis/hail/annotations/Region;I)J; LSTORE 14; GOTO L9; L9; FRAME APPEND [T T J]; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f77km : Z; IFNE L10; GOTO L11; L11; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 9; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 16; LDC 9999; ILOAD 16; ISUB; ISTORE 17; GOTO L12; L12; FRAME APPEND [T I]; ALOAD 0; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; LLOAD 14; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f77km : Z; ILOAD 17; INVOKEVIRTUAL __C46CompiledWithAggs.__m69take_by_seqop (ZJZI)V; RETURN; L10; FRAME CHOP 2; LDC 0; ISTORE 17; GOTO L12; L7; FRAME CHOP 3; LDC 0; LSTORE 14; GOTO L9; L13; LOCALVARIABLE get_tup_elem_o J L0 L13 6; LOCALVARIABLE bool Z L0 L13 8; LOCALVARIABLE get_tup_elem_o J L0 L13 9; LOCALVARIABLE bool Z L0 L13 11; LOCALVARIABLE invoke I L0 L13 12; LOCALVARIABLE local13 I L0 L13 13; LOCALVAR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408
https://github.com/hail-is/hail/pull/8671#issuecomment-621635408:2203,Performance,load,loadInt,2203,e I L0 L1 10; LOCALVARIABLE local11 I L0 L1 11; LOCALVARIABLE invoke I L0 L1 12; MAXSTACK = 5; MAXLOCALS = 13; ```. ```; /// PREVIOUS VERSION; // access flags 0x1; public __m85wrapped(Lis/hail/annotations/Region;JJ)V; L0; GOTO L1; L1; FRAME SAME; LLOAD 4; LSTORE 6; GOTO L2; L2; FRAME APPEND [J]; LDC 0; ISTORE 8; GOTO L3; L3; FRAME APPEND [I]; ALOAD 0; ILOAD 8; PUTFIELD __C46CompiledWithAggs.__f78vm : Z; GOTO L4; L4; FRAME SAME; LLOAD 4; LSTORE 9; GOTO L5; L5; FRAME APPEND [J]; LDC 0; ISTORE 11; GOTO L6; L6; FRAME APPEND [I]; ALOAD 0; ILOAD 11; PUTFIELD __C46CompiledWithAggs.__f77km : Z; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; IFNE L7; GOTO L8; L8; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 12; ILOAD 12; ISTORE 13; ALOAD 0; ALOAD 1; ILOAD 13; INVOKEVIRTUAL __C46CompiledWithAggs.__m86str (Lis/hail/annotations/Region;I)J; LSTORE 14; GOTO L9; L9; FRAME APPEND [T T J]; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f77km : Z; IFNE L10; GOTO L11; L11; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 9; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 16; LDC 9999; ILOAD 16; ISUB; ISTORE 17; GOTO L12; L12; FRAME APPEND [T I]; ALOAD 0; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; LLOAD 14; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f77km : Z; ILOAD 17; INVOKEVIRTUAL __C46CompiledWithAggs.__m69take_by_seqop (ZJZI)V; RETURN; L10; FRAME CHOP 2; LDC 0; ISTORE 17; GOTO L12; L7; FRAME CHOP 3; LDC 0; LSTORE 14; GOTO L9; L13; LOCALVARIABLE get_tup_elem_o J L0 L13 6; LOCALVARIABLE bool Z L0 L13 8; LOCALVARIABLE get_tup_elem_o J L0 L13 9; LOCALVARIABLE bool Z L0 L13 11; LOCALVARIABLE invoke I L0 L13 12; LOCALVARIABLE local13 I L0 L13 13; LOCALVARIABLE mux J L0 L13 14; LOCALVARIABLE invoke I L0 L13 16; LOCALVARIABLE mux I L0 L13 17; MAXSTACK = 6; MAXLOCALS = 18; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408
https://github.com/hail-is/hail/pull/8671#issuecomment-621635408:1093,Security,access,access,1093,on;JJ)V; L0; LDC 0; LSTORE 6; LDC 0; LSTORE 8; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 8; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // we fail here; ISTORE 10; ILOAD 10; ISTORE 11; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // or here; ISTORE 12; ALOAD 0; ALOAD 0; ALOAD 1; ILOAD 11; INVOKEVIRTUAL __C44CompiledWithAggs.__m82str (Lis/hail/annotations/Region;I)J; LDC 9999; ILOAD 12; ISUB; INVOKEVIRTUAL __C44CompiledWithAggs.__m67take_by_seqop (JI)V; RETURN; L1; LOCALVARIABLE get_tup_elem_o J L0 L1 6; LOCALVARIABLE get_tup_elem_o J L0 L1 8; LOCALVARIABLE invoke I L0 L1 10; LOCALVARIABLE local11 I L0 L1 11; LOCALVARIABLE invoke I L0 L1 12; MAXSTACK = 5; MAXLOCALS = 13; ```. ```; /// PREVIOUS VERSION; // access flags 0x1; public __m85wrapped(Lis/hail/annotations/Region;JJ)V; L0; GOTO L1; L1; FRAME SAME; LLOAD 4; LSTORE 6; GOTO L2; L2; FRAME APPEND [J]; LDC 0; ISTORE 8; GOTO L3; L3; FRAME APPEND [I]; ALOAD 0; ILOAD 8; PUTFIELD __C46CompiledWithAggs.__f78vm : Z; GOTO L4; L4; FRAME SAME; LLOAD 4; LSTORE 9; GOTO L5; L5; FRAME APPEND [J]; LDC 0; ISTORE 11; GOTO L6; L6; FRAME APPEND [I]; ALOAD 0; ILOAD 11; PUTFIELD __C46CompiledWithAggs.__f77km : Z; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; IFNE L7; GOTO L8; L8; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 12; ILOAD 12; ISTORE 13; ALOAD 0; ALOAD 1; ILOAD 13; INVOKEVIRTUAL __C46CompiledWithAggs.__m86str (Lis/hail/annotations/Region;I)J; LSTORE 14; GOTO L9; L9; FRAME APPEND [T T J]; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f77km : Z; IFNE L10; GOTO L11; L11; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 9; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408
https://github.com/hail-is/hail/pull/8671#issuecomment-621635408:27,Testability,test,test,27,"With this change, only one test in `testTakeBy` fails. This is the generated code for the lead up to the invocation of the SeqOp method.; ```; /// THIS PR; public __m81wrapped(Lis/hail/annotations/Region;JJ)V; L0; LDC 0; LSTORE 6; LDC 0; LSTORE 8; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 8; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // we fail here; ISTORE 10; ILOAD 10; ISTORE 11; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // or here; ISTORE 12; ALOAD 0; ALOAD 0; ALOAD 1; ILOAD 11; INVOKEVIRTUAL __C44CompiledWithAggs.__m82str (Lis/hail/annotations/Region;I)J; LDC 9999; ILOAD 12; ISUB; INVOKEVIRTUAL __C44CompiledWithAggs.__m67take_by_seqop (JI)V; RETURN; L1; LOCALVARIABLE get_tup_elem_o J L0 L1 6; LOCALVARIABLE get_tup_elem_o J L0 L1 8; LOCALVARIABLE invoke I L0 L1 10; LOCALVARIABLE local11 I L0 L1 11; LOCALVARIABLE invoke I L0 L1 12; MAXSTACK = 5; MAXLOCALS = 13; ```. ```; /// PREVIOUS VERSION; // access flags 0x1; public __m85wrapped(Lis/hail/annotations/Region;JJ)V; L0; GOTO L1; L1; FRAME SAME; LLOAD 4; LSTORE 6; GOTO L2; L2; FRAME APPEND [J]; LDC 0; ISTORE 8; GOTO L3; L3; FRAME APPEND [I]; ALOAD 0; ILOAD 8; PUTFIELD __C46CompiledWithAggs.__f78vm : Z; GOTO L4; L4; FRAME SAME; LLOAD 4; LSTORE 9; GOTO L5; L5; FRAME APPEND [J]; LDC 0; ISTORE 11; GOTO L6; L6; FRAME APPEND [I]; ALOAD 0; ILOAD 11; PUTFIELD __C46CompiledWithAggs.__f77km : Z; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; IFNE L7; GOTO L8; L8; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 12; ILOAD 12; ISTORE 13; ALOAD 0; ALOAD 1; ILOAD 13; INVOKEVIRTUAL __C46CompiledWithAggs.__m86str (Lis/hail/annotations/Region;I)J; LSTORE 14; GOTO L9; L9; FRAME APPEND [T T J]; ALOAD 0; GETFIELD __C46Compiled",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408
https://github.com/hail-is/hail/pull/8671#issuecomment-621635408:36,Testability,test,testTakeBy,36,"With this change, only one test in `testTakeBy` fails. This is the generated code for the lead up to the invocation of the SeqOp method.; ```; /// THIS PR; public __m81wrapped(Lis/hail/annotations/Region;JJ)V; L0; LDC 0; LSTORE 6; LDC 0; LSTORE 8; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 8; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // we fail here; ISTORE 10; ILOAD 10; ISTORE 11; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // or here; ISTORE 12; ALOAD 0; ALOAD 0; ALOAD 1; ILOAD 11; INVOKEVIRTUAL __C44CompiledWithAggs.__m82str (Lis/hail/annotations/Region;I)J; LDC 9999; ILOAD 12; ISUB; INVOKEVIRTUAL __C44CompiledWithAggs.__m67take_by_seqop (JI)V; RETURN; L1; LOCALVARIABLE get_tup_elem_o J L0 L1 6; LOCALVARIABLE get_tup_elem_o J L0 L1 8; LOCALVARIABLE invoke I L0 L1 10; LOCALVARIABLE local11 I L0 L1 11; LOCALVARIABLE invoke I L0 L1 12; MAXSTACK = 5; MAXLOCALS = 13; ```. ```; /// PREVIOUS VERSION; // access flags 0x1; public __m85wrapped(Lis/hail/annotations/Region;JJ)V; L0; GOTO L1; L1; FRAME SAME; LLOAD 4; LSTORE 6; GOTO L2; L2; FRAME APPEND [J]; LDC 0; ISTORE 8; GOTO L3; L3; FRAME APPEND [I]; ALOAD 0; ILOAD 8; PUTFIELD __C46CompiledWithAggs.__f78vm : Z; GOTO L4; L4; FRAME SAME; LLOAD 4; LSTORE 9; GOTO L5; L5; FRAME APPEND [J]; LDC 0; ISTORE 11; GOTO L6; L6; FRAME APPEND [I]; ALOAD 0; ILOAD 11; PUTFIELD __C46CompiledWithAggs.__f77km : Z; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; IFNE L7; GOTO L8; L8; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 12; ILOAD 12; ISTORE 13; ALOAD 0; ALOAD 1; ILOAD 13; INVOKEVIRTUAL __C46CompiledWithAggs.__m86str (Lis/hail/annotations/Region;I)J; LSTORE 14; GOTO L9; L9; FRAME APPEND [T T J]; ALOAD 0; GETFIELD __C46Compiled",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408
https://github.com/hail-is/hail/pull/8671#issuecomment-622046822:25,Testability,test,testTakeBy,25,"I got `Aggregators2Suite.testTakeBy` to pass, so we're probably good?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8671#issuecomment-622046822
https://github.com/hail-is/hail/pull/8679#issuecomment-622011492:54,Energy Efficiency,power,powers,54,I read the assignment wrong. It should be in integral powers.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8679#issuecomment-622011492
https://github.com/hail-is/hail/pull/8679#issuecomment-622567618:27,Testability,test,test,27,I confirmed the test_utils test I added is being run.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8679#issuecomment-622567618
https://github.com/hail-is/hail/pull/8692#issuecomment-623511128:13,Testability,test,test,13,Failing java test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8692#issuecomment-623511128
https://github.com/hail-is/hail/pull/8700#issuecomment-624157539:39,Availability,fault,fault,39,"Thanks, and sorry about that! Github's fault.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624157539
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1084,Performance,load,loadClass,1084,"g.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.jav",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1154,Performance,load,loadClass,1154,"InfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1210,Performance,load,loadClass,1210," - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1337,Performance,load,loadClass,1337,"eption in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundExcepti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2526,Performance,load,loadClass,2526,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2596,Performance,load,loadClass,2596,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2652,Performance,load,loadClass,2652,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:612,Security,secur,security,612,"not passing tests:; ```; + java -cp 'hail-test.jar:/spark-2.4.0-bin-hadoop2.7/jars/*' org.testng.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:621,Security,Secur,SecureClassLoader,621,"not passing tests:; ```; + java -cp 'hail-test.jar:/spark-2.4.0-bin-hadoop2.7/jars/*' org.testng.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:651,Security,Secur,SecureClassLoader,651,"not passing tests:; ```; + java -cp 'hail-test.jar:/spark-2.4.0-bin-hadoop2.7/jars/*' org.testng.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:774,Security,access,access,774,"not passing tests:; ```; + java -cp 'hail-test.jar:/spark-2.4.0-bin-hadoop2.7/jars/*' org.testng.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:939,Security,secur,security,939,"not passing tests:; ```; + java -cp 'hail-test.jar:/spark-2.4.0-bin-hadoop2.7/jars/*' org.testng.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:948,Security,Access,AccessController,948,"not passing tests:; ```; + java -cp 'hail-test.jar:/spark-2.4.0-bin-hadoop2.7/jars/*' org.testng.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:12,Testability,test,tests,12,"not passing tests:; ```; + java -cp 'hail-test.jar:/spark-2.4.0-bin-hadoop2.7/jars/*' org.testng.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:42,Testability,test,test,42,"not passing tests:; ```; + java -cp 'hail-test.jar:/spark-2.4.0-bin-hadoop2.7/jars/*' org.testng.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:90,Testability,test,testng,90,"not passing tests:; ```; + java -cp 'hail-test.jar:/spark-2.4.0-bin-hadoop2.7/jars/*' org.testng.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:97,Testability,Test,TestNG,97,"not passing tests:; ```; + java -cp 'hail-test.jar:/spark-2.4.0-bin-hadoop2.7/jars/*' org.testng.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:122,Testability,Log,LogTestListener,122,"not passing tests:; ```; + java -cp 'hail-test.jar:/spark-2.4.0-bin-hadoop2.7/jars/*' org.testng.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:138,Testability,test,testng-build,138,"not passing tests:; ```; + java -cp 'hail-test.jar:/spark-2.4.0-bin-hadoop2.7/jars/*' org.testng.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1251,Testability,test,testng,1251,"nce is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:13",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1317,Testability,test,testng,1317,"ctIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1374,Testability,test,testng,1374,ng.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apach,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1437,Testability,test,testng,1437,ath3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1504,Testability,test,testng,1504,oader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1571,Testability,test,testng,1571,Class(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Lau,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1578,Testability,Test,TestRunner,1578,ssLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$App,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1601,Testability,Test,TestRunner,1601,); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1631,Testability,test,testng,1631,ader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1638,Testability,Test,TestRunner,1638,neClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.Clas,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1654,Testability,Test,TestRunner,1654,ClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1684,Testability,test,testng,1684,a.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	..,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1691,Testability,Test,TestRunner,1691,ClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1707,Testability,Test,TestRunner,1707,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1737,Testability,test,testng,1737,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1744,Testability,Test,TestRunner,1744,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1762,Testability,Test,TestRunner,1762,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1792,Testability,test,testng,1792,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1881,Testability,test,testng,1881,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1936,Testability,test,testng,1936,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1993,Testability,test,testng,1993,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2000,Testability,Test,TestNG,2000,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2025,Testability,Test,TestNG,2025,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2052,Testability,test,testng,2052,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2059,Testability,Test,TestNG,2059,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2085,Testability,Test,TestNG,2085,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2112,Testability,test,testng,2112,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2119,Testability,Test,TestNG,2119,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2143,Testability,Test,TestNG,2143,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2170,Testability,test,testng,2170,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2177,Testability,Test,TestNG,2177,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2188,Testability,Test,TestNG,2188,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2215,Testability,test,testng,2215,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2222,Testability,Test,TestNG,2222,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2241,Testability,Test,TestNG,2241,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2268,Testability,test,testng,2268,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2275,Testability,Test,TestNG,2275,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2287,Testability,Test,TestNG,2287,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460
https://github.com/hail-is/hail/pull/8710#issuecomment-624411877:10,Deployability,deploy,deploying,10,"Tested by deploying it, I know it works. No one else is awake.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8710#issuecomment-624411877
https://github.com/hail-is/hail/pull/8710#issuecomment-624411877:0,Testability,Test,Tested,0,"Tested by deploying it, I know it works. No one else is awake.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8710#issuecomment-624411877
https://github.com/hail-is/hail/pull/8725#issuecomment-706201983:78,Deployability,update,updates,78,Trying to debug a CI issue and this PR's status has exceeded the number of CI updates due to it being rather old. I'll reopen once I've figured out CI's issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8725#issuecomment-706201983
https://github.com/hail-is/hail/pull/8726#issuecomment-629596529:48,Testability,test,testng,48,This whole stack of PRs is currently plagued by testng hell. I will resolve Monday. I assure you the tests work when not run using the test jar. I'll probably back out my use of the buggy TestNG feature (it's in LSMSuite in a previous PR).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8726#issuecomment-629596529
https://github.com/hail-is/hail/pull/8726#issuecomment-629596529:101,Testability,test,tests,101,This whole stack of PRs is currently plagued by testng hell. I will resolve Monday. I assure you the tests work when not run using the test jar. I'll probably back out my use of the buggy TestNG feature (it's in LSMSuite in a previous PR).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8726#issuecomment-629596529
https://github.com/hail-is/hail/pull/8726#issuecomment-629596529:135,Testability,test,test,135,This whole stack of PRs is currently plagued by testng hell. I will resolve Monday. I assure you the tests work when not run using the test jar. I'll probably back out my use of the buggy TestNG feature (it's in LSMSuite in a previous PR).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8726#issuecomment-629596529
https://github.com/hail-is/hail/pull/8726#issuecomment-629596529:188,Testability,Test,TestNG,188,This whole stack of PRs is currently plagued by testng hell. I will resolve Monday. I assure you the tests work when not run using the test jar. I'll probably back out my use of the buggy TestNG feature (it's in LSMSuite in a previous PR).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8726#issuecomment-629596529
https://github.com/hail-is/hail/pull/8726#issuecomment-650380852:636,Deployability,deploy,deploying,636,"I attempted to use the TLS stuff and I've decided against it for this PR. It appears that the batch tests do not work locally at all. The whole essence of this PR is getting the shuffler IR tested and into the mainline. I started making the fixes necessary to support local testing of a local server and when that proved complicated investigated how BatchClientSuite works locally. It seems that using the existing TLS stuff would require fixing all the TLS stuff to allow for at least local->remote testing, if not local->local testing. This PR is already very complex, I'd like to get it merged so we can move forward separately with deploying and eventually harmonizing with the existing TLS infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8726#issuecomment-650380852
https://github.com/hail-is/hail/pull/8726#issuecomment-650380852:100,Testability,test,tests,100,"I attempted to use the TLS stuff and I've decided against it for this PR. It appears that the batch tests do not work locally at all. The whole essence of this PR is getting the shuffler IR tested and into the mainline. I started making the fixes necessary to support local testing of a local server and when that proved complicated investigated how BatchClientSuite works locally. It seems that using the existing TLS stuff would require fixing all the TLS stuff to allow for at least local->remote testing, if not local->local testing. This PR is already very complex, I'd like to get it merged so we can move forward separately with deploying and eventually harmonizing with the existing TLS infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8726#issuecomment-650380852
https://github.com/hail-is/hail/pull/8726#issuecomment-650380852:190,Testability,test,tested,190,"I attempted to use the TLS stuff and I've decided against it for this PR. It appears that the batch tests do not work locally at all. The whole essence of this PR is getting the shuffler IR tested and into the mainline. I started making the fixes necessary to support local testing of a local server and when that proved complicated investigated how BatchClientSuite works locally. It seems that using the existing TLS stuff would require fixing all the TLS stuff to allow for at least local->remote testing, if not local->local testing. This PR is already very complex, I'd like to get it merged so we can move forward separately with deploying and eventually harmonizing with the existing TLS infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8726#issuecomment-650380852
https://github.com/hail-is/hail/pull/8726#issuecomment-650380852:274,Testability,test,testing,274,"I attempted to use the TLS stuff and I've decided against it for this PR. It appears that the batch tests do not work locally at all. The whole essence of this PR is getting the shuffler IR tested and into the mainline. I started making the fixes necessary to support local testing of a local server and when that proved complicated investigated how BatchClientSuite works locally. It seems that using the existing TLS stuff would require fixing all the TLS stuff to allow for at least local->remote testing, if not local->local testing. This PR is already very complex, I'd like to get it merged so we can move forward separately with deploying and eventually harmonizing with the existing TLS infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8726#issuecomment-650380852
https://github.com/hail-is/hail/pull/8726#issuecomment-650380852:500,Testability,test,testing,500,"I attempted to use the TLS stuff and I've decided against it for this PR. It appears that the batch tests do not work locally at all. The whole essence of this PR is getting the shuffler IR tested and into the mainline. I started making the fixes necessary to support local testing of a local server and when that proved complicated investigated how BatchClientSuite works locally. It seems that using the existing TLS stuff would require fixing all the TLS stuff to allow for at least local->remote testing, if not local->local testing. This PR is already very complex, I'd like to get it merged so we can move forward separately with deploying and eventually harmonizing with the existing TLS infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8726#issuecomment-650380852
https://github.com/hail-is/hail/pull/8726#issuecomment-650380852:529,Testability,test,testing,529,"I attempted to use the TLS stuff and I've decided against it for this PR. It appears that the batch tests do not work locally at all. The whole essence of this PR is getting the shuffler IR tested and into the mainline. I started making the fixes necessary to support local testing of a local server and when that proved complicated investigated how BatchClientSuite works locally. It seems that using the existing TLS stuff would require fixing all the TLS stuff to allow for at least local->remote testing, if not local->local testing. This PR is already very complex, I'd like to get it merged so we can move forward separately with deploying and eventually harmonizing with the existing TLS infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8726#issuecomment-650380852
https://github.com/hail-is/hail/pull/8738#issuecomment-629281049:211,Deployability,pipeline,pipeline,211,"> I think I'm going to choose not to support RelationalLetTable at this point; I think we lift relational lets before we lower MatrixTables, so we would never need to support it currently in our normal lowering pipeline. This isn't true -- we generate relational lets in the MatrixLowering pass!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8738#issuecomment-629281049
https://github.com/hail-is/hail/pull/8757#issuecomment-630316303:12,Testability,assert,assertion,12,This is the assertion I'd hit: https://github.com/hail-is/hail/blob/master/hail/src/main/scala/is/hail/rvd/RVD.scala#L117,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8757#issuecomment-630316303
https://github.com/hail-is/hail/pull/8759#issuecomment-628002023:194,Deployability,deploy,deploy,194,"I addressed all your comments except making the decision how this should be structured between MySQL and Python. Once we make that decision, then I'll make the changes and double check with dev deploy that the costs are still the same and the resource aggregation is correct. Sound good?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8759#issuecomment-628002023
https://github.com/hail-is/hail/pull/8759#issuecomment-629431482:135,Testability,test,tested,135,"@danking This should be ready for a second look. I think it's pretty close to being done and I'm happy with how the code turned out. I tested it with the checks on and everything passed with 0.1% maximum difference. I hand computed one example from the database and got the same exact cost as msec_mcpu to the limit of the database precision which was 10 decimal places. . And I ran the tests first with master and found that cost about $0.0018 and then running it with the new branch cost around $0.0021 (the test job timed out right before it completed). So I'm happy I got the same order of magnitude both ways. If you have any additional ideas for checks, would be happy to put that in. I think we should leave the checking code in for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8759#issuecomment-629431482
https://github.com/hail-is/hail/pull/8759#issuecomment-629431482:387,Testability,test,tests,387,"@danking This should be ready for a second look. I think it's pretty close to being done and I'm happy with how the code turned out. I tested it with the checks on and everything passed with 0.1% maximum difference. I hand computed one example from the database and got the same exact cost as msec_mcpu to the limit of the database precision which was 10 decimal places. . And I ran the tests first with master and found that cost about $0.0018 and then running it with the new branch cost around $0.0021 (the test job timed out right before it completed). So I'm happy I got the same order of magnitude both ways. If you have any additional ideas for checks, would be happy to put that in. I think we should leave the checking code in for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8759#issuecomment-629431482
https://github.com/hail-is/hail/pull/8759#issuecomment-629431482:510,Testability,test,test,510,"@danking This should be ready for a second look. I think it's pretty close to being done and I'm happy with how the code turned out. I tested it with the checks on and everything passed with 0.1% maximum difference. I hand computed one example from the database and got the same exact cost as msec_mcpu to the limit of the database precision which was 10 decimal places. . And I ran the tests first with master and found that cost about $0.0018 and then running it with the new branch cost around $0.0021 (the test job timed out right before it completed). So I'm happy I got the same order of magnitude both ways. If you have any additional ideas for checks, would be happy to put that in. I think we should leave the checking code in for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8759#issuecomment-629431482
https://github.com/hail-is/hail/pull/8760#issuecomment-626929606:29,Deployability,deploy,deploy,29,I tested this works with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8760#issuecomment-626929606
https://github.com/hail-is/hail/pull/8760#issuecomment-626929606:2,Testability,test,tested,2,I tested this works with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8760#issuecomment-626929606
https://github.com/hail-is/hail/pull/8760#issuecomment-649874088:12,Deployability,deploy,deploy,12,I'll do the deploy now!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8760#issuecomment-649874088
https://github.com/hail-is/hail/pull/8775#issuecomment-627904282:99,Integrability,depend,dependency,99,"OK, this is passing. Interval is in utils but has a bad import to expr.types that creates a cyclic dependency. I fixed this by allowing a lambda as a typechecker which returns a type or typechecker to defer the import. It looks like this:. ```; @typecheck_method(start=anytype,; end=anytype,; includes_start=bool,; includes_end=bool,; point_type=nullable(lambda: hl.expr.types.hail_type)); def __init__(self, start, end, includes_start=True, includes_end=False, point_type=None):; ...; ```. Tho I think eventually we should move struct and interval out of utils.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8775#issuecomment-627904282
https://github.com/hail-is/hail/pull/8776#issuecomment-628108350:73,Usability,Simpl,Simplify,73,Going to change this to be stacked on #8783. That will let me write the `Simplify` rules the way they ought to be written,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8776#issuecomment-628108350
https://github.com/hail-is/hail/pull/8778#issuecomment-631468022:15,Availability,failure,failures,15,still got test failures: https://ci.hail.is/batches/48405/jobs/45,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8778#issuecomment-631468022
https://github.com/hail-is/hail/pull/8778#issuecomment-631468022:10,Testability,test,test,10,still got test failures: https://ci.hail.is/batches/48405/jobs/45,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8778#issuecomment-631468022
https://github.com/hail-is/hail/pull/8783#issuecomment-628105025:131,Integrability,depend,depend,131,"I feel like I probably ought to have more tests, but wasn't sure what else to include. I didn't want to make the `StreamLen` tests depend on the correctness of more complicated IR nodes like `StreamZip`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8783#issuecomment-628105025
https://github.com/hail-is/hail/pull/8783#issuecomment-628105025:42,Testability,test,tests,42,"I feel like I probably ought to have more tests, but wasn't sure what else to include. I didn't want to make the `StreamLen` tests depend on the correctness of more complicated IR nodes like `StreamZip`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8783#issuecomment-628105025
https://github.com/hail-is/hail/pull/8783#issuecomment-628105025:125,Testability,test,tests,125,"I feel like I probably ought to have more tests, but wasn't sure what else to include. I didn't want to make the `StreamLen` tests depend on the correctness of more complicated IR nodes like `StreamZip`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8783#issuecomment-628105025
https://github.com/hail-is/hail/pull/8795#issuecomment-630925598:70,Deployability,release,released,70,@danking I added a change log for Batch. I assume this change will be released in the next version 0.2.42.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8795#issuecomment-630925598
https://github.com/hail-is/hail/pull/8795#issuecomment-630925598:26,Testability,log,log,26,@danking I added a change log for Batch. I assume this change will be released in the next version 0.2.42.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8795#issuecomment-630925598
https://github.com/hail-is/hail/pull/8798#issuecomment-628891104:31,Deployability,deploy,deployed,31,I semi-tested this as follows: deployed in auth and created a user. The logic then failed with 403 Forbidden because the dev namespace gsa can't create service accounts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8798#issuecomment-628891104
https://github.com/hail-is/hail/pull/8798#issuecomment-628891104:7,Testability,test,tested,7,I semi-tested this as follows: deployed in auth and created a user. The logic then failed with 403 Forbidden because the dev namespace gsa can't create service accounts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8798#issuecomment-628891104
https://github.com/hail-is/hail/pull/8798#issuecomment-628891104:72,Testability,log,logic,72,I semi-tested this as follows: deployed in auth and created a user. The logic then failed with 403 Forbidden because the dev namespace gsa can't create service accounts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8798#issuecomment-628891104
https://github.com/hail-is/hail/pull/8801#issuecomment-629588732:25,Testability,Test,TestNG,25,ugh. inexplicable NPE in TestNG with that version. Switching to the lowest version known to work.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8801#issuecomment-629588732
https://github.com/hail-is/hail/pull/8801#issuecomment-629593668:145,Availability,error,error,145,Welp. OK. Looks like we're stuck on 6.8.21 forever. I have no idea why the CI is NPE'ing. My local system gives all manner of other inexplicable error messages (mostly about class loading). Things are fine when done through `./gradlew test` though. It's just the test jar that seems broken.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8801#issuecomment-629593668
https://github.com/hail-is/hail/pull/8801#issuecomment-629593668:151,Integrability,message,messages,151,Welp. OK. Looks like we're stuck on 6.8.21 forever. I have no idea why the CI is NPE'ing. My local system gives all manner of other inexplicable error messages (mostly about class loading). Things are fine when done through `./gradlew test` though. It's just the test jar that seems broken.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8801#issuecomment-629593668
https://github.com/hail-is/hail/pull/8801#issuecomment-629593668:180,Performance,load,loading,180,Welp. OK. Looks like we're stuck on 6.8.21 forever. I have no idea why the CI is NPE'ing. My local system gives all manner of other inexplicable error messages (mostly about class loading). Things are fine when done through `./gradlew test` though. It's just the test jar that seems broken.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8801#issuecomment-629593668
https://github.com/hail-is/hail/pull/8801#issuecomment-629593668:235,Testability,test,test,235,Welp. OK. Looks like we're stuck on 6.8.21 forever. I have no idea why the CI is NPE'ing. My local system gives all manner of other inexplicable error messages (mostly about class loading). Things are fine when done through `./gradlew test` though. It's just the test jar that seems broken.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8801#issuecomment-629593668
https://github.com/hail-is/hail/pull/8801#issuecomment-629593668:263,Testability,test,test,263,Welp. OK. Looks like we're stuck on 6.8.21 forever. I have no idea why the CI is NPE'ing. My local system gives all manner of other inexplicable error messages (mostly about class loading). Things are fine when done through `./gradlew test` though. It's just the test jar that seems broken.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8801#issuecomment-629593668
https://github.com/hail-is/hail/pull/8801#issuecomment-629593684:15,Testability,test,test,15,I'll change my test somehow.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8801#issuecomment-629593684
https://github.com/hail-is/hail/pull/8803#issuecomment-629593748:14,Testability,test,tests,14,"I promise the tests all work. It's this TestNG nonsense. I'll back out my use of DataProvider, which is apparently breaking TestNG 6.8.21, but any newer version of TestNG breaks our test jar.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8803#issuecomment-629593748
https://github.com/hail-is/hail/pull/8803#issuecomment-629593748:40,Testability,Test,TestNG,40,"I promise the tests all work. It's this TestNG nonsense. I'll back out my use of DataProvider, which is apparently breaking TestNG 6.8.21, but any newer version of TestNG breaks our test jar.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8803#issuecomment-629593748
https://github.com/hail-is/hail/pull/8803#issuecomment-629593748:124,Testability,Test,TestNG,124,"I promise the tests all work. It's this TestNG nonsense. I'll back out my use of DataProvider, which is apparently breaking TestNG 6.8.21, but any newer version of TestNG breaks our test jar.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8803#issuecomment-629593748
https://github.com/hail-is/hail/pull/8803#issuecomment-629593748:164,Testability,Test,TestNG,164,"I promise the tests all work. It's this TestNG nonsense. I'll back out my use of DataProvider, which is apparently breaking TestNG 6.8.21, but any newer version of TestNG breaks our test jar.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8803#issuecomment-629593748
https://github.com/hail-is/hail/pull/8803#issuecomment-629593748:182,Testability,test,test,182,"I promise the tests all work. It's this TestNG nonsense. I'll back out my use of DataProvider, which is apparently breaking TestNG 6.8.21, but any newer version of TestNG breaks our test jar.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8803#issuecomment-629593748
https://github.com/hail-is/hail/pull/8805#issuecomment-629258538:4,Deployability,deploy,deploy,4,Dev deploy still coming up...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8805#issuecomment-629258538
https://github.com/hail-is/hail/pull/8805#issuecomment-629377509:60,Testability,log,log,60,Hmm. I thought we made you a full fledged developer. If you log in via https://auth.hail.is/login first does it work?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8805#issuecomment-629377509
https://github.com/hail-is/hail/pull/8805#issuecomment-629377509:92,Testability,log,login,92,Hmm. I thought we made you a full fledged developer. If you log in via https://auth.hail.is/login first does it work?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8805#issuecomment-629377509
https://github.com/hail-is/hail/pull/8805#issuecomment-629377969:4,Deployability,deploy,deploy,4,dev deploy should be up for whoever reviews: https://internal.hail.is/dking/batch,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8805#issuecomment-629377969
https://github.com/hail-is/hail/pull/8805#issuecomment-629408828:24,Testability,log,log,24,"blah right, you have to log into *my* auth.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8805#issuecomment-629408828
https://github.com/hail-is/hail/pull/8805#issuecomment-629409014:88,Deployability,deploy,deployed,88,"Well, I promise you it looks fine. We should figure out letting other devs log into dev deployed auth.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8805#issuecomment-629409014
https://github.com/hail-is/hail/pull/8805#issuecomment-629409014:75,Testability,log,log,75,"Well, I promise you it looks fine. We should figure out letting other devs log into dev deployed auth.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8805#issuecomment-629409014
https://github.com/hail-is/hail/pull/8806#issuecomment-629406978:23,Energy Efficiency,charge,charge,23,thanks for leading the charge to fix this stuff fast!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8806#issuecomment-629406978
https://github.com/hail-is/hail/pull/8818#issuecomment-630437136:284,Testability,test,tested,284,![Screen Shot 2020-05-18 at 5 07 59 PM](https://user-images.githubusercontent.com/106194/82259993-7d6a0e00-992a-11ea-8c3c-2e675fa61c42.png); ![Screen Shot 2020-05-18 at 5 07 53 PM](https://user-images.githubusercontent.com/106194/82259994-7d6a0e00-992a-11ea-9eee-7a03e83d12db.png). I tested it by making these changes in my browser on the live CI website.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8818#issuecomment-630437136
https://github.com/hail-is/hail/pull/8826#issuecomment-631055949:5,Deployability,deploy,deploying,5,hand deploying,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8826#issuecomment-631055949
https://github.com/hail-is/hail/pull/8826#issuecomment-631058177:0,Deployability,deploy,deployed,0,deployed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8826#issuecomment-631058177
https://github.com/hail-is/hail/pull/8829#issuecomment-631652614:49,Integrability,depend,dependsOn,49,Very good point. I moved the check to be on the `dependsOn` array of names rather than on the deps array of steps,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8829#issuecomment-631652614
https://github.com/hail-is/hail/pull/8831#issuecomment-631100639:5,Deployability,deploy,deploying,5,hand deploying,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8831#issuecomment-631100639
https://github.com/hail-is/hail/pull/8833#issuecomment-631473428:86,Modifiability,config,config,86,"This seems like the right approach. The other option is to expose the password in the config which we are trying to avoid. So ... even this is not idempotent. The current version of batch doesn't guarantee that children (or multiple retries of the same child) get the same or consistent set of files from their parents. This is because, while an attempt may have succeeded, there may be another attempt that is pending that succeeds and overwrites the outputs of the original successful attempt. I fixed this in my google batch backend by storing attempt outputs in per-attempt directory: /path/to/scratch/files/job_id/attempt_id/file. Then each child got the successful attempt (there could be multiple, but the driver selected one and only one) for each parent and that was used to localize the inputs. So this good enough and we should plan to add attempt consistency to Batch in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631473428
https://github.com/hail-is/hail/pull/8833#issuecomment-631473428:116,Safety,avoid,avoid,116,"This seems like the right approach. The other option is to expose the password in the config which we are trying to avoid. So ... even this is not idempotent. The current version of batch doesn't guarantee that children (or multiple retries of the same child) get the same or consistent set of files from their parents. This is because, while an attempt may have succeeded, there may be another attempt that is pending that succeeds and overwrites the outputs of the original successful attempt. I fixed this in my google batch backend by storing attempt outputs in per-attempt directory: /path/to/scratch/files/job_id/attempt_id/file. Then each child got the successful attempt (there could be multiple, but the driver selected one and only one) for each parent and that was used to localize the inputs. So this good enough and we should plan to add attempt consistency to Batch in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631473428
https://github.com/hail-is/hail/pull/8833#issuecomment-631473428:59,Security,expose,expose,59,"This seems like the right approach. The other option is to expose the password in the config which we are trying to avoid. So ... even this is not idempotent. The current version of batch doesn't guarantee that children (or multiple retries of the same child) get the same or consistent set of files from their parents. This is because, while an attempt may have succeeded, there may be another attempt that is pending that succeeds and overwrites the outputs of the original successful attempt. I fixed this in my google batch backend by storing attempt outputs in per-attempt directory: /path/to/scratch/files/job_id/attempt_id/file. Then each child got the successful attempt (there could be multiple, but the driver selected one and only one) for each parent and that was used to localize the inputs. So this good enough and we should plan to add attempt consistency to Batch in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631473428
https://github.com/hail-is/hail/pull/8833#issuecomment-631473428:70,Security,password,password,70,"This seems like the right approach. The other option is to expose the password in the config which we are trying to avoid. So ... even this is not idempotent. The current version of batch doesn't guarantee that children (or multiple retries of the same child) get the same or consistent set of files from their parents. This is because, while an attempt may have succeeded, there may be another attempt that is pending that succeeds and overwrites the outputs of the original successful attempt. I fixed this in my google batch backend by storing attempt outputs in per-attempt directory: /path/to/scratch/files/job_id/attempt_id/file. Then each child got the successful attempt (there could be multiple, but the driver selected one and only one) for each parent and that was used to localize the inputs. So this good enough and we should plan to add attempt consistency to Batch in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631473428
https://github.com/hail-is/hail/pull/8833#issuecomment-631672719:291,Security,password,passwords,291,"@jigold that's a great suggestion. To test that this code is acutally idempotent, I wanted to do this:. Duplicate the test database createDatabase task in `ci/test/resources/build.yaml` (but with a new step name). The jobs have the same parents so they race to run first. They have distinct passwords, so the resulting secret will non-deterministically have the wrong password (e.g. A creates the user, A writes its password first, B ignores already created user, then B writes its password second), but if everything but the secret works, then I'm confident repeated attempts sharing the same secret should work!. Unfortunately, I can't test CI itself in this way because test CIs (whether in dev or in a PR) are not permitted to create databases. I took your suggestion and copied the SQL query out and ran it twice instead. That worked fine. Everything was created once.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631672719
https://github.com/hail-is/hail/pull/8833#issuecomment-631672719:368,Security,password,password,368,"@jigold that's a great suggestion. To test that this code is acutally idempotent, I wanted to do this:. Duplicate the test database createDatabase task in `ci/test/resources/build.yaml` (but with a new step name). The jobs have the same parents so they race to run first. They have distinct passwords, so the resulting secret will non-deterministically have the wrong password (e.g. A creates the user, A writes its password first, B ignores already created user, then B writes its password second), but if everything but the secret works, then I'm confident repeated attempts sharing the same secret should work!. Unfortunately, I can't test CI itself in this way because test CIs (whether in dev or in a PR) are not permitted to create databases. I took your suggestion and copied the SQL query out and ran it twice instead. That worked fine. Everything was created once.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631672719
https://github.com/hail-is/hail/pull/8833#issuecomment-631672719:416,Security,password,password,416,"@jigold that's a great suggestion. To test that this code is acutally idempotent, I wanted to do this:. Duplicate the test database createDatabase task in `ci/test/resources/build.yaml` (but with a new step name). The jobs have the same parents so they race to run first. They have distinct passwords, so the resulting secret will non-deterministically have the wrong password (e.g. A creates the user, A writes its password first, B ignores already created user, then B writes its password second), but if everything but the secret works, then I'm confident repeated attempts sharing the same secret should work!. Unfortunately, I can't test CI itself in this way because test CIs (whether in dev or in a PR) are not permitted to create databases. I took your suggestion and copied the SQL query out and ran it twice instead. That worked fine. Everything was created once.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631672719
https://github.com/hail-is/hail/pull/8833#issuecomment-631672719:482,Security,password,password,482,"@jigold that's a great suggestion. To test that this code is acutally idempotent, I wanted to do this:. Duplicate the test database createDatabase task in `ci/test/resources/build.yaml` (but with a new step name). The jobs have the same parents so they race to run first. They have distinct passwords, so the resulting secret will non-deterministically have the wrong password (e.g. A creates the user, A writes its password first, B ignores already created user, then B writes its password second), but if everything but the secret works, then I'm confident repeated attempts sharing the same secret should work!. Unfortunately, I can't test CI itself in this way because test CIs (whether in dev or in a PR) are not permitted to create databases. I took your suggestion and copied the SQL query out and ran it twice instead. That worked fine. Everything was created once.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631672719
https://github.com/hail-is/hail/pull/8833#issuecomment-631672719:38,Testability,test,test,38,"@jigold that's a great suggestion. To test that this code is acutally idempotent, I wanted to do this:. Duplicate the test database createDatabase task in `ci/test/resources/build.yaml` (but with a new step name). The jobs have the same parents so they race to run first. They have distinct passwords, so the resulting secret will non-deterministically have the wrong password (e.g. A creates the user, A writes its password first, B ignores already created user, then B writes its password second), but if everything but the secret works, then I'm confident repeated attempts sharing the same secret should work!. Unfortunately, I can't test CI itself in this way because test CIs (whether in dev or in a PR) are not permitted to create databases. I took your suggestion and copied the SQL query out and ran it twice instead. That worked fine. Everything was created once.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631672719
https://github.com/hail-is/hail/pull/8833#issuecomment-631672719:118,Testability,test,test,118,"@jigold that's a great suggestion. To test that this code is acutally idempotent, I wanted to do this:. Duplicate the test database createDatabase task in `ci/test/resources/build.yaml` (but with a new step name). The jobs have the same parents so they race to run first. They have distinct passwords, so the resulting secret will non-deterministically have the wrong password (e.g. A creates the user, A writes its password first, B ignores already created user, then B writes its password second), but if everything but the secret works, then I'm confident repeated attempts sharing the same secret should work!. Unfortunately, I can't test CI itself in this way because test CIs (whether in dev or in a PR) are not permitted to create databases. I took your suggestion and copied the SQL query out and ran it twice instead. That worked fine. Everything was created once.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631672719
https://github.com/hail-is/hail/pull/8833#issuecomment-631672719:159,Testability,test,test,159,"@jigold that's a great suggestion. To test that this code is acutally idempotent, I wanted to do this:. Duplicate the test database createDatabase task in `ci/test/resources/build.yaml` (but with a new step name). The jobs have the same parents so they race to run first. They have distinct passwords, so the resulting secret will non-deterministically have the wrong password (e.g. A creates the user, A writes its password first, B ignores already created user, then B writes its password second), but if everything but the secret works, then I'm confident repeated attempts sharing the same secret should work!. Unfortunately, I can't test CI itself in this way because test CIs (whether in dev or in a PR) are not permitted to create databases. I took your suggestion and copied the SQL query out and ran it twice instead. That worked fine. Everything was created once.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631672719
https://github.com/hail-is/hail/pull/8833#issuecomment-631672719:638,Testability,test,test,638,"@jigold that's a great suggestion. To test that this code is acutally idempotent, I wanted to do this:. Duplicate the test database createDatabase task in `ci/test/resources/build.yaml` (but with a new step name). The jobs have the same parents so they race to run first. They have distinct passwords, so the resulting secret will non-deterministically have the wrong password (e.g. A creates the user, A writes its password first, B ignores already created user, then B writes its password second), but if everything but the secret works, then I'm confident repeated attempts sharing the same secret should work!. Unfortunately, I can't test CI itself in this way because test CIs (whether in dev or in a PR) are not permitted to create databases. I took your suggestion and copied the SQL query out and ran it twice instead. That worked fine. Everything was created once.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631672719
https://github.com/hail-is/hail/pull/8833#issuecomment-631672719:673,Testability,test,test,673,"@jigold that's a great suggestion. To test that this code is acutally idempotent, I wanted to do this:. Duplicate the test database createDatabase task in `ci/test/resources/build.yaml` (but with a new step name). The jobs have the same parents so they race to run first. They have distinct passwords, so the resulting secret will non-deterministically have the wrong password (e.g. A creates the user, A writes its password first, B ignores already created user, then B writes its password second), but if everything but the secret works, then I'm confident repeated attempts sharing the same secret should work!. Unfortunately, I can't test CI itself in this way because test CIs (whether in dev or in a PR) are not permitted to create databases. I took your suggestion and copied the SQL query out and ran it twice instead. That worked fine. Everything was created once.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631672719
https://github.com/hail-is/hail/pull/8834#issuecomment-632136987:59,Deployability,pipeline,pipelines,59,This should be split into two PRs so as not to break users pipelines until we release the new version of hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8834#issuecomment-632136987
https://github.com/hail-is/hail/pull/8834#issuecomment-632136987:78,Deployability,release,release,78,This should be split into two PRs so as not to break users pipelines until we release the new version of hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8834#issuecomment-632136987
https://github.com/hail-is/hail/pull/8834#issuecomment-634121647:29,Deployability,release,release,29,"This can go in after we do a release and we notify the batch users about the change. After that, we should delete all the per-user buckets.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8834#issuecomment-634121647
https://github.com/hail-is/hail/pull/8834#issuecomment-645084393:77,Testability,log,logic,77,"@jigold OK, this is ready for review again. Just ripping out the user bucket logic now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8834#issuecomment-645084393
https://github.com/hail-is/hail/pull/8840#issuecomment-631733744:108,Security,access,access,108,I tested the commands by creating a service account with 0 permissions and making sure that I could give it access and could read/write to the bucket and gcr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8840#issuecomment-631733744
https://github.com/hail-is/hail/pull/8840#issuecomment-631733744:2,Testability,test,tested,2,I tested the commands by creating a service account with 0 permissions and making sure that I could give it access and could read/write to the bucket and gcr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8840#issuecomment-631733744
https://github.com/hail-is/hail/pull/8844#issuecomment-631683697:191,Integrability,interface,interface,191,"Actually, the change log doesn't make sense as the version of the service isn't related to the documentation/hail pip version. Thoughts on removing the change log here and just having client interface changes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8844#issuecomment-631683697
https://github.com/hail-is/hail/pull/8844#issuecomment-631683697:21,Testability,log,log,21,"Actually, the change log doesn't make sense as the version of the service isn't related to the documentation/hail pip version. Thoughts on removing the change log here and just having client interface changes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8844#issuecomment-631683697
https://github.com/hail-is/hail/pull/8844#issuecomment-631683697:159,Testability,log,log,159,"Actually, the change log doesn't make sense as the version of the service isn't related to the documentation/hail pip version. Thoughts on removing the change log here and just having client interface changes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8844#issuecomment-631683697
https://github.com/hail-is/hail/pull/8850#issuecomment-632344001:99,Deployability,deploy,deploy,99,I made the timeout for the standing workers to be 5 minutes for the test scope and 2 hours for the deploy scope.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8850#issuecomment-632344001
https://github.com/hail-is/hail/pull/8850#issuecomment-632344001:11,Safety,timeout,timeout,11,I made the timeout for the standing workers to be 5 minutes for the test scope and 2 hours for the deploy scope.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8850#issuecomment-632344001
https://github.com/hail-is/hail/pull/8850#issuecomment-632344001:68,Testability,test,test,68,I made the timeout for the standing workers to be 5 minutes for the test scope and 2 hours for the deploy scope.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8850#issuecomment-632344001
https://github.com/hail-is/hail/pull/8850#issuecomment-635382758:242,Usability,simpl,simplicity,242,"Eh, as you point out in the billing changes it's 0.02208 USD per core per hour, that's 2 USD per day. It's probably a bit more if we have a full SSD for a 4 core, but still, this is extremely far away from our biggest cost center. I vote for simplicity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8850#issuecomment-635382758
https://github.com/hail-is/hail/pull/8850#issuecomment-635432823:59,Deployability,update,update,59,"Depending on which PR of #8844 goes in first, I'll need to update the docs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8850#issuecomment-635432823
https://github.com/hail-is/hail/pull/8850#issuecomment-635432823:0,Integrability,Depend,Depending,0,"Depending on which PR of #8844 goes in first, I'll need to update the docs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8850#issuecomment-635432823
https://github.com/hail-is/hail/pull/8859#issuecomment-634159069:110,Testability,test,test,110,"I backed off the support for treating deep NAs as nonequal. That makes the change simpler, and also easier to test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8859#issuecomment-634159069
https://github.com/hail-is/hail/pull/8859#issuecomment-634159069:82,Usability,simpl,simpler,82,"I backed off the support for treating deep NAs as nonequal. That makes the change simpler, and also easier to test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8859#issuecomment-634159069
https://github.com/hail-is/hail/pull/8864#issuecomment-634173911:15,Performance,perform,performance,15,"If I change my performance test to write using fast LZ4, the time drops to 26.2s and the profile looks like:. ![Screen Shot 2020-05-26 at 1 43 34 PM](https://user-images.githubusercontent.com/106194/82932751-e5d27400-9f56-11ea-9356-086f57d58fba.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-634173911
https://github.com/hail-is/hail/pull/8864#issuecomment-634173911:27,Testability,test,test,27,"If I change my performance test to write using fast LZ4, the time drops to 26.2s and the profile looks like:. ![Screen Shot 2020-05-26 at 1 43 34 PM](https://user-images.githubusercontent.com/106194/82932751-e5d27400-9f56-11ea-9356-086f57d58fba.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-634173911
https://github.com/hail-is/hail/pull/8864#issuecomment-637177344:1180,Energy Efficiency,reduce,reduce,1180,"@patrick-schultz, sorry, I missed this in-line comment. For the sake of unifying the discussion, I'll reply in PR comment so we can continue in one thread for both discussions (which I think are intimately related). > Right, but this is on the generic key_by path, and this is no longer an obvious optimization in all cases. I think my real question is: what is the new semantics for key_by? If I want to change my key from [A, B] to [B], then probably it will shuffle and choose balanced partitions, keeping roughly the same amount of parallelism, but if the existing partitioner satisfies a somewhat obscure condition that I don't have much control over, it will instead coalesce partitions.; >; > What if we gate this behavior behind a flag on TableKeyBy, and expose a way to opt in to the optimization in python?. This behavior is only accessible when TableKeyBy isSorted=true. If you've used a hidden field (only accessible through a) my newly exposed `_key_by_assert_sorted` or b) writing IR yourself) to assert that your dataset is already in the order of the new key, I'm certain you would *not* want to shuffle. Moreover, switching from `[A, B]` to `[B]` could very well reduce your effective parallelism even after a shuffle because keys are not permitted to be split across partitions. Consider a dataset keyed by `[Locus, Alleles, Gene]` with 10k partitions. If we re-key to `[Gene]` and we only have 1000 genes annotated, we'll lose partitions. In fact, in the 1:1 partition case (the case we're optimizing here) you *must* lose parallelism because each partition contains exactly one value for the key `B`. Indeed, each partition contains only one record at all!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344
https://github.com/hail-is/hail/pull/8864#issuecomment-637177344:298,Performance,optimiz,optimization,298,"@patrick-schultz, sorry, I missed this in-line comment. For the sake of unifying the discussion, I'll reply in PR comment so we can continue in one thread for both discussions (which I think are intimately related). > Right, but this is on the generic key_by path, and this is no longer an obvious optimization in all cases. I think my real question is: what is the new semantics for key_by? If I want to change my key from [A, B] to [B], then probably it will shuffle and choose balanced partitions, keeping roughly the same amount of parallelism, but if the existing partitioner satisfies a somewhat obscure condition that I don't have much control over, it will instead coalesce partitions.; >; > What if we gate this behavior behind a flag on TableKeyBy, and expose a way to opt in to the optimization in python?. This behavior is only accessible when TableKeyBy isSorted=true. If you've used a hidden field (only accessible through a) my newly exposed `_key_by_assert_sorted` or b) writing IR yourself) to assert that your dataset is already in the order of the new key, I'm certain you would *not* want to shuffle. Moreover, switching from `[A, B]` to `[B]` could very well reduce your effective parallelism even after a shuffle because keys are not permitted to be split across partitions. Consider a dataset keyed by `[Locus, Alleles, Gene]` with 10k partitions. If we re-key to `[Gene]` and we only have 1000 genes annotated, we'll lose partitions. In fact, in the 1:1 partition case (the case we're optimizing here) you *must* lose parallelism because each partition contains exactly one value for the key `B`. Indeed, each partition contains only one record at all!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344
https://github.com/hail-is/hail/pull/8864#issuecomment-637177344:793,Performance,optimiz,optimization,793,"@patrick-schultz, sorry, I missed this in-line comment. For the sake of unifying the discussion, I'll reply in PR comment so we can continue in one thread for both discussions (which I think are intimately related). > Right, but this is on the generic key_by path, and this is no longer an obvious optimization in all cases. I think my real question is: what is the new semantics for key_by? If I want to change my key from [A, B] to [B], then probably it will shuffle and choose balanced partitions, keeping roughly the same amount of parallelism, but if the existing partitioner satisfies a somewhat obscure condition that I don't have much control over, it will instead coalesce partitions.; >; > What if we gate this behavior behind a flag on TableKeyBy, and expose a way to opt in to the optimization in python?. This behavior is only accessible when TableKeyBy isSorted=true. If you've used a hidden field (only accessible through a) my newly exposed `_key_by_assert_sorted` or b) writing IR yourself) to assert that your dataset is already in the order of the new key, I'm certain you would *not* want to shuffle. Moreover, switching from `[A, B]` to `[B]` could very well reduce your effective parallelism even after a shuffle because keys are not permitted to be split across partitions. Consider a dataset keyed by `[Locus, Alleles, Gene]` with 10k partitions. If we re-key to `[Gene]` and we only have 1000 genes annotated, we'll lose partitions. In fact, in the 1:1 partition case (the case we're optimizing here) you *must* lose parallelism because each partition contains exactly one value for the key `B`. Indeed, each partition contains only one record at all!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344
https://github.com/hail-is/hail/pull/8864#issuecomment-637177344:1509,Performance,optimiz,optimizing,1509,"@patrick-schultz, sorry, I missed this in-line comment. For the sake of unifying the discussion, I'll reply in PR comment so we can continue in one thread for both discussions (which I think are intimately related). > Right, but this is on the generic key_by path, and this is no longer an obvious optimization in all cases. I think my real question is: what is the new semantics for key_by? If I want to change my key from [A, B] to [B], then probably it will shuffle and choose balanced partitions, keeping roughly the same amount of parallelism, but if the existing partitioner satisfies a somewhat obscure condition that I don't have much control over, it will instead coalesce partitions.; >; > What if we gate this behavior behind a flag on TableKeyBy, and expose a way to opt in to the optimization in python?. This behavior is only accessible when TableKeyBy isSorted=true. If you've used a hidden field (only accessible through a) my newly exposed `_key_by_assert_sorted` or b) writing IR yourself) to assert that your dataset is already in the order of the new key, I'm certain you would *not* want to shuffle. Moreover, switching from `[A, B]` to `[B]` could very well reduce your effective parallelism even after a shuffle because keys are not permitted to be split across partitions. Consider a dataset keyed by `[Locus, Alleles, Gene]` with 10k partitions. If we re-key to `[Gene]` and we only have 1000 genes annotated, we'll lose partitions. In fact, in the 1:1 partition case (the case we're optimizing here) you *must* lose parallelism because each partition contains exactly one value for the key `B`. Indeed, each partition contains only one record at all!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344
https://github.com/hail-is/hail/pull/8864#issuecomment-637177344:763,Security,expose,expose,763,"@patrick-schultz, sorry, I missed this in-line comment. For the sake of unifying the discussion, I'll reply in PR comment so we can continue in one thread for both discussions (which I think are intimately related). > Right, but this is on the generic key_by path, and this is no longer an obvious optimization in all cases. I think my real question is: what is the new semantics for key_by? If I want to change my key from [A, B] to [B], then probably it will shuffle and choose balanced partitions, keeping roughly the same amount of parallelism, but if the existing partitioner satisfies a somewhat obscure condition that I don't have much control over, it will instead coalesce partitions.; >; > What if we gate this behavior behind a flag on TableKeyBy, and expose a way to opt in to the optimization in python?. This behavior is only accessible when TableKeyBy isSorted=true. If you've used a hidden field (only accessible through a) my newly exposed `_key_by_assert_sorted` or b) writing IR yourself) to assert that your dataset is already in the order of the new key, I'm certain you would *not* want to shuffle. Moreover, switching from `[A, B]` to `[B]` could very well reduce your effective parallelism even after a shuffle because keys are not permitted to be split across partitions. Consider a dataset keyed by `[Locus, Alleles, Gene]` with 10k partitions. If we re-key to `[Gene]` and we only have 1000 genes annotated, we'll lose partitions. In fact, in the 1:1 partition case (the case we're optimizing here) you *must* lose parallelism because each partition contains exactly one value for the key `B`. Indeed, each partition contains only one record at all!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344
https://github.com/hail-is/hail/pull/8864#issuecomment-637177344:840,Security,access,accessible,840,"@patrick-schultz, sorry, I missed this in-line comment. For the sake of unifying the discussion, I'll reply in PR comment so we can continue in one thread for both discussions (which I think are intimately related). > Right, but this is on the generic key_by path, and this is no longer an obvious optimization in all cases. I think my real question is: what is the new semantics for key_by? If I want to change my key from [A, B] to [B], then probably it will shuffle and choose balanced partitions, keeping roughly the same amount of parallelism, but if the existing partitioner satisfies a somewhat obscure condition that I don't have much control over, it will instead coalesce partitions.; >; > What if we gate this behavior behind a flag on TableKeyBy, and expose a way to opt in to the optimization in python?. This behavior is only accessible when TableKeyBy isSorted=true. If you've used a hidden field (only accessible through a) my newly exposed `_key_by_assert_sorted` or b) writing IR yourself) to assert that your dataset is already in the order of the new key, I'm certain you would *not* want to shuffle. Moreover, switching from `[A, B]` to `[B]` could very well reduce your effective parallelism even after a shuffle because keys are not permitted to be split across partitions. Consider a dataset keyed by `[Locus, Alleles, Gene]` with 10k partitions. If we re-key to `[Gene]` and we only have 1000 genes annotated, we'll lose partitions. In fact, in the 1:1 partition case (the case we're optimizing here) you *must* lose parallelism because each partition contains exactly one value for the key `B`. Indeed, each partition contains only one record at all!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344
https://github.com/hail-is/hail/pull/8864#issuecomment-637177344:918,Security,access,accessible,918,"@patrick-schultz, sorry, I missed this in-line comment. For the sake of unifying the discussion, I'll reply in PR comment so we can continue in one thread for both discussions (which I think are intimately related). > Right, but this is on the generic key_by path, and this is no longer an obvious optimization in all cases. I think my real question is: what is the new semantics for key_by? If I want to change my key from [A, B] to [B], then probably it will shuffle and choose balanced partitions, keeping roughly the same amount of parallelism, but if the existing partitioner satisfies a somewhat obscure condition that I don't have much control over, it will instead coalesce partitions.; >; > What if we gate this behavior behind a flag on TableKeyBy, and expose a way to opt in to the optimization in python?. This behavior is only accessible when TableKeyBy isSorted=true. If you've used a hidden field (only accessible through a) my newly exposed `_key_by_assert_sorted` or b) writing IR yourself) to assert that your dataset is already in the order of the new key, I'm certain you would *not* want to shuffle. Moreover, switching from `[A, B]` to `[B]` could very well reduce your effective parallelism even after a shuffle because keys are not permitted to be split across partitions. Consider a dataset keyed by `[Locus, Alleles, Gene]` with 10k partitions. If we re-key to `[Gene]` and we only have 1000 genes annotated, we'll lose partitions. In fact, in the 1:1 partition case (the case we're optimizing here) you *must* lose parallelism because each partition contains exactly one value for the key `B`. Indeed, each partition contains only one record at all!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344
https://github.com/hail-is/hail/pull/8864#issuecomment-637177344:949,Security,expose,exposed,949,"@patrick-schultz, sorry, I missed this in-line comment. For the sake of unifying the discussion, I'll reply in PR comment so we can continue in one thread for both discussions (which I think are intimately related). > Right, but this is on the generic key_by path, and this is no longer an obvious optimization in all cases. I think my real question is: what is the new semantics for key_by? If I want to change my key from [A, B] to [B], then probably it will shuffle and choose balanced partitions, keeping roughly the same amount of parallelism, but if the existing partitioner satisfies a somewhat obscure condition that I don't have much control over, it will instead coalesce partitions.; >; > What if we gate this behavior behind a flag on TableKeyBy, and expose a way to opt in to the optimization in python?. This behavior is only accessible when TableKeyBy isSorted=true. If you've used a hidden field (only accessible through a) my newly exposed `_key_by_assert_sorted` or b) writing IR yourself) to assert that your dataset is already in the order of the new key, I'm certain you would *not* want to shuffle. Moreover, switching from `[A, B]` to `[B]` could very well reduce your effective parallelism even after a shuffle because keys are not permitted to be split across partitions. Consider a dataset keyed by `[Locus, Alleles, Gene]` with 10k partitions. If we re-key to `[Gene]` and we only have 1000 genes annotated, we'll lose partitions. In fact, in the 1:1 partition case (the case we're optimizing here) you *must* lose parallelism because each partition contains exactly one value for the key `B`. Indeed, each partition contains only one record at all!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344
https://github.com/hail-is/hail/pull/8864#issuecomment-637177344:1011,Testability,assert,assert,1011,"@patrick-schultz, sorry, I missed this in-line comment. For the sake of unifying the discussion, I'll reply in PR comment so we can continue in one thread for both discussions (which I think are intimately related). > Right, but this is on the generic key_by path, and this is no longer an obvious optimization in all cases. I think my real question is: what is the new semantics for key_by? If I want to change my key from [A, B] to [B], then probably it will shuffle and choose balanced partitions, keeping roughly the same amount of parallelism, but if the existing partitioner satisfies a somewhat obscure condition that I don't have much control over, it will instead coalesce partitions.; >; > What if we gate this behavior behind a flag on TableKeyBy, and expose a way to opt in to the optimization in python?. This behavior is only accessible when TableKeyBy isSorted=true. If you've used a hidden field (only accessible through a) my newly exposed `_key_by_assert_sorted` or b) writing IR yourself) to assert that your dataset is already in the order of the new key, I'm certain you would *not* want to shuffle. Moreover, switching from `[A, B]` to `[B]` could very well reduce your effective parallelism even after a shuffle because keys are not permitted to be split across partitions. Consider a dataset keyed by `[Locus, Alleles, Gene]` with 10k partitions. If we re-key to `[Gene]` and we only have 1000 genes annotated, we'll lose partitions. In fact, in the 1:1 partition case (the case we're optimizing here) you *must* lose parallelism because each partition contains exactly one value for the key `B`. Indeed, each partition contains only one record at all!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344
https://github.com/hail-is/hail/pull/8864#issuecomment-637177972:149,Safety,avoid,avoid,149,"The behavior of this PR has been restored to my initial intentions: in the case of a `TableKeyBy(..., isSorted=true)`, if the partitions are 1:1, we avoid a costly shuffle, perhaps at the risk of loss of parallelism.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177972
https://github.com/hail-is/hail/pull/8864#issuecomment-637177972:188,Safety,risk,risk,188,"The behavior of this PR has been restored to my initial intentions: in the case of a `TableKeyBy(..., isSorted=true)`, if the partitions are 1:1, we avoid a costly shuffle, perhaps at the risk of loss of parallelism.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177972
https://github.com/hail-is/hail/pull/8864#issuecomment-640909201:65,Testability,test,tests,65,@johnc1231 It should be now. I expect this latest commit to pass tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-640909201
https://github.com/hail-is/hail/pull/8865#issuecomment-634292347:88,Availability,down,down,88,"I'm fine with calling it whatever. `die` could be mistaken for something that will shut down hail, at the same time, that's what the IR node is called.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-634292347
https://github.com/hail-is/hail/pull/8865#issuecomment-634380327:37,Availability,error,error,37,"> slight nitpick---can we call this ""error"" or ""raise_error"" or something less dire sounding than ""die""?. or some version of `except` for parallelism with python?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-634380327
https://github.com/hail-is/hail/pull/8865#issuecomment-641489296:104,Testability,test,test,104,"Btw, I actually don't understand how one would use this function (not clear to me from the docs nor the test)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-641489296
https://github.com/hail-is/hail/pull/8865#issuecomment-641489296:70,Usability,clear,clear,70,"Btw, I actually don't understand how one would use this function (not clear to me from the docs nor the test)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-641489296
https://github.com/hail-is/hail/pull/8865#issuecomment-641495584:95,Availability,Error,Error,95,"I think it's useful if you want to write something like:. ```; hl.if_else(x < y, ...., hl.die(""Error: x must be less than y"")); ```. Otherwise you can't do input validation on expressions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-641495584
https://github.com/hail-is/hail/pull/8865#issuecomment-641495584:162,Security,validat,validation,162,"I think it's useful if you want to write something like:. ```; hl.if_else(x < y, ...., hl.die(""Error: x must be less than y"")); ```. Otherwise you can't do input validation on expressions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-641495584
https://github.com/hail-is/hail/pull/8866#issuecomment-635292105:5,Availability,error,error,5,what error do you get now?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8866#issuecomment-635292105
https://github.com/hail-is/hail/pull/8867#issuecomment-634712363:0,Deployability,Update,Updated,0,Updated changelog,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8867#issuecomment-634712363
https://github.com/hail-is/hail/pull/8867#issuecomment-634763312:15,Usability,clear,clearer,15,"I tried adding clearer names, but I thought that talking about ""new"" vs ""old"" was relatively clear. I may be too deep in it to judge these days.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8867#issuecomment-634763312
https://github.com/hail-is/hail/pull/8867#issuecomment-634763312:93,Usability,clear,clear,93,"I tried adding clearer names, but I thought that talking about ""new"" vs ""old"" was relatively clear. I may be too deep in it to judge these days.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8867#issuecomment-634763312
https://github.com/hail-is/hail/pull/8867#issuecomment-634788030:25,Usability,clear,clear,25,"Alright, think names are clear now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8867#issuecomment-634788030
https://github.com/hail-is/hail/pull/8867#issuecomment-634848956:29,Testability,test,test,29,@danking fyi- looks like the test case you added is failing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8867#issuecomment-634848956
https://github.com/hail-is/hail/pull/8876#issuecomment-634996787:33,Integrability,message,message,33,"fix looks good, want a changelog message tho",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8876#issuecomment-634996787
https://github.com/hail-is/hail/pull/8878#issuecomment-635336858:0,Availability,Failure,Failures,0,Failures:; - a Java test I created that should fail to make sure the memory checks are happening; - Python test_ld_score; - A docs test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878#issuecomment-635336858
https://github.com/hail-is/hail/pull/8878#issuecomment-635336858:20,Testability,test,test,20,Failures:; - a Java test I created that should fail to make sure the memory checks are happening; - Python test_ld_score; - A docs test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878#issuecomment-635336858
https://github.com/hail-is/hail/pull/8878#issuecomment-635336858:131,Testability,test,test,131,Failures:; - a Java test I created that should fail to make sure the memory checks are happening; - Python test_ld_score; - A docs test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878#issuecomment-635336858
https://github.com/hail-is/hail/pull/8878#issuecomment-663084550:16,Testability,test,test,16,"@cseed The only test failing now is that Java test, so we should be at a point where the next thing to think about is how to have a separate ""debug"" build of Hail that uses the checked allocator. And then we'd specify that the failing test should only run on the checked allocator.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878#issuecomment-663084550
https://github.com/hail-is/hail/pull/8878#issuecomment-663084550:46,Testability,test,test,46,"@cseed The only test failing now is that Java test, so we should be at a point where the next thing to think about is how to have a separate ""debug"" build of Hail that uses the checked allocator. And then we'd specify that the failing test should only run on the checked allocator.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878#issuecomment-663084550
https://github.com/hail-is/hail/pull/8878#issuecomment-663084550:235,Testability,test,test,235,"@cseed The only test failing now is that Java test, so we should be at a point where the next thing to think about is how to have a separate ""debug"" build of Hail that uses the checked allocator. And then we'd specify that the failing test should only run on the checked allocator.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8878#issuecomment-663084550
https://github.com/hail-is/hail/pull/8903#issuecomment-637175674:78,Testability,test,test,78,"Thanks @danking. And no worries, I was mainly interested in being able to see test output vs having tests automatically start.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8903#issuecomment-637175674
https://github.com/hail-is/hail/pull/8903#issuecomment-637175674:100,Testability,test,tests,100,"Thanks @danking. And no worries, I was mainly interested in being able to see test output vs having tests automatically start.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8903#issuecomment-637175674
https://github.com/hail-is/hail/pull/8917#issuecomment-638942141:103,Integrability,message,message,103,"Okay, I think all the changes are now directly related to the main changes described in the initial PR message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8917#issuecomment-638942141
https://github.com/hail-is/hail/pull/8920#issuecomment-638964132:333,Deployability,deploy,deploy,333,"> Ah, I commented on both PRs by accident. My bad, sorry! In the future, keeping each PR to a single commit can help the reviewer identify the right changes to review for stacked PRs. Please clarify. Unless I misunderstand, 1 commit per PR isnâ€™t something that we currently hold ourselves to. For instance the PR you issued for site deploy has something like 10 commits. Maybe we can specify this in a design doc, much like John and you did with the PR message? This would be a useful place to specify related issues, like settling on rebasing vs merging. . My understanding of our use of the stacked label is that the dependent PR isnâ€™t meant to be reviewed until the child is merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638964132
https://github.com/hail-is/hail/pull/8920#issuecomment-638964132:453,Integrability,message,message,453,"> Ah, I commented on both PRs by accident. My bad, sorry! In the future, keeping each PR to a single commit can help the reviewer identify the right changes to review for stacked PRs. Please clarify. Unless I misunderstand, 1 commit per PR isnâ€™t something that we currently hold ourselves to. For instance the PR you issued for site deploy has something like 10 commits. Maybe we can specify this in a design doc, much like John and you did with the PR message? This would be a useful place to specify related issues, like settling on rebasing vs merging. . My understanding of our use of the stacked label is that the dependent PR isnâ€™t meant to be reviewed until the child is merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638964132
https://github.com/hail-is/hail/pull/8920#issuecomment-638964132:619,Integrability,depend,dependent,619,"> Ah, I commented on both PRs by accident. My bad, sorry! In the future, keeping each PR to a single commit can help the reviewer identify the right changes to review for stacked PRs. Please clarify. Unless I misunderstand, 1 commit per PR isnâ€™t something that we currently hold ourselves to. For instance the PR you issued for site deploy has something like 10 commits. Maybe we can specify this in a design doc, much like John and you did with the PR message? This would be a useful place to specify related issues, like settling on rebasing vs merging. . My understanding of our use of the stacked label is that the dependent PR isnâ€™t meant to be reviewed until the child is merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638964132
https://github.com/hail-is/hail/pull/8920#issuecomment-638969261:24,Usability,clear,clear,24,"I don't think we have a clear policy on this. When I'm making stacked changes, I use one-commit per PR so that they can be reviewed independently. You're welcome to take either approach.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638969261
https://github.com/hail-is/hail/pull/8920#issuecomment-638971103:26,Deployability,deploy,deployed,26,"> Minor comments. Is this deployed anywhere so I can take a look?. http://34.207.246.132 has the latest, which is this PR + the tutorial page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638971103
https://github.com/hail-is/hail/pull/8920#issuecomment-638971130:26,Usability,clear,clear,26,"> I don't think we have a clear policy on this. When I'm making stacked changes, I use one-commit per PR so that they can be reviewed independently. You're welcome to take either approach. I think the policy we've been roughly sticking to on the compilers team is that people don't need to make one commit per PR, but also shouldn't expect that stacked PRs are reviewed until the parent goes in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638971130
https://github.com/hail-is/hail/pull/8920#issuecomment-638971214:10,Deployability,deploy,deploy,10,I can dev deploy it as well,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638971214
https://github.com/hail-is/hail/pull/8920#issuecomment-640990714:106,Integrability,depend,dependent,106,I'm just going to close the other PR so that we get the overall in (I think here you've approved both the dependent PR and the stacked content),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-640990714
https://github.com/hail-is/hail/pull/8923#issuecomment-639030337:55,Deployability,deploy,deployed,55,"Another issue I noticed: viz.min.js isn't found in the deployed internal copy. Assuming the vendors directory was copied, you may have deployed before adding execute bit to that folder.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639030337
https://github.com/hail-is/hail/pull/8923#issuecomment-639030337:135,Deployability,deploy,deployed,135,"Another issue I noticed: viz.min.js isn't found in the deployed internal copy. Assuming the vendors directory was copied, you may have deployed before adding execute bit to that folder.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639030337
https://github.com/hail-is/hail/pull/8923#issuecomment-639035804:157,Deployability,deploy,deploy,157,"Hmm. this works: https://internal.hail.is/dking/site/vendors/vanta/viz.min.js, where is it not getting loaded correctly? I probably need another rule in dev deploy's site's nginx to fix the path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639035804
https://github.com/hail-is/hail/pull/8923#issuecomment-639035804:103,Performance,load,loaded,103,"Hmm. this works: https://internal.hail.is/dking/site/vendors/vanta/viz.min.js, where is it not getting loaded correctly? I probably need another rule in dev deploy's site's nginx to fix the path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639035804
https://github.com/hail-is/hail/pull/8923#issuecomment-639040701:2,Deployability,update,updated,2,I updated the dev deploy too.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639040701
https://github.com/hail-is/hail/pull/8923#issuecomment-639040701:18,Deployability,deploy,deploy,18,I updated the dev deploy too.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639040701
https://github.com/hail-is/hail/pull/8923#issuecomment-639057471:6,Usability,clear,clear,6,"To be clear, not the link you provided, what the page (https://internal.hail.is/dking/site/index.html) pulls.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639057471
https://github.com/hail-is/hail/pull/8923#issuecomment-639077704:12,Deployability,deploy,deployed,12,"OK, so. Dev deployed `site` uses the same docs and www as non-dev-deployed site. Unfortunately, this means we have a root-URL issue. I use [an nginx `sub_filter`](https://github.com/hail-is/hail/blob/master/site/site.sh) rule to fix links and anchors. I didn't add any rules for javascript modules. The right fix is to move to a system that can build for different environments and set the root URL properly. Until we switch to that hypothetical new system, I'll add a rule that properly rewrites JS module imports. We can't fix this with symlinks or any kind of redirection. The root issue is that we, the users, are outside of the system and the way we specify to whom we're talking is `internal.hail.is/NAMESPACE/SERVICE/`. Cotton wanted to do `service.namespace.internal.hail.is` but there were some TLS wildcard issues. It is perhaps worth revisiting this at some point because it is a perennial issue for us.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639077704
https://github.com/hail-is/hail/pull/8923#issuecomment-639077704:66,Deployability,deploy,deployed,66,"OK, so. Dev deployed `site` uses the same docs and www as non-dev-deployed site. Unfortunately, this means we have a root-URL issue. I use [an nginx `sub_filter`](https://github.com/hail-is/hail/blob/master/site/site.sh) rule to fix links and anchors. I didn't add any rules for javascript modules. The right fix is to move to a system that can build for different environments and set the root URL properly. Until we switch to that hypothetical new system, I'll add a rule that properly rewrites JS module imports. We can't fix this with symlinks or any kind of redirection. The root issue is that we, the users, are outside of the system and the way we specify to whom we're talking is `internal.hail.is/NAMESPACE/SERVICE/`. Cotton wanted to do `service.namespace.internal.hail.is` but there were some TLS wildcard issues. It is perhaps worth revisiting this at some point because it is a perennial issue for us.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639077704
https://github.com/hail-is/hail/pull/8923#issuecomment-639077704:488,Modifiability,rewrite,rewrites,488,"OK, so. Dev deployed `site` uses the same docs and www as non-dev-deployed site. Unfortunately, this means we have a root-URL issue. I use [an nginx `sub_filter`](https://github.com/hail-is/hail/blob/master/site/site.sh) rule to fix links and anchors. I didn't add any rules for javascript modules. The right fix is to move to a system that can build for different environments and set the root URL properly. Until we switch to that hypothetical new system, I'll add a rule that properly rewrites JS module imports. We can't fix this with symlinks or any kind of redirection. The root issue is that we, the users, are outside of the system and the way we specify to whom we're talking is `internal.hail.is/NAMESPACE/SERVICE/`. Cotton wanted to do `service.namespace.internal.hail.is` but there were some TLS wildcard issues. It is perhaps worth revisiting this at some point because it is a perennial issue for us.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639077704
https://github.com/hail-is/hail/pull/8923#issuecomment-639135078:245,Deployability,deploy,deploys,245,"OK, so that was way more pain than I expected. Apparently when you're importing a JS module which needs authentication, you must specify the `crossorigin` attribute to the `script` tag. If you lack that attribute, no headers are sent. Since dev deploys are limited to developer access only (even though there is no in-dev-namespace authentication), this obviously doesn't work. Why does the word `crossorigin` mean send headers to the same origin? Who knows! Anyway. I don't think that bug should block this PR. That bug is an underlying issue with dev deploy. The fix for the module import bug is here: https://github.com/hail-is/hail/pull/8928",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639135078
https://github.com/hail-is/hail/pull/8923#issuecomment-639135078:553,Deployability,deploy,deploy,553,"OK, so that was way more pain than I expected. Apparently when you're importing a JS module which needs authentication, you must specify the `crossorigin` attribute to the `script` tag. If you lack that attribute, no headers are sent. Since dev deploys are limited to developer access only (even though there is no in-dev-namespace authentication), this obviously doesn't work. Why does the word `crossorigin` mean send headers to the same origin? Who knows! Anyway. I don't think that bug should block this PR. That bug is an underlying issue with dev deploy. The fix for the module import bug is here: https://github.com/hail-is/hail/pull/8928",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639135078
https://github.com/hail-is/hail/pull/8923#issuecomment-639135078:104,Security,authenticat,authentication,104,"OK, so that was way more pain than I expected. Apparently when you're importing a JS module which needs authentication, you must specify the `crossorigin` attribute to the `script` tag. If you lack that attribute, no headers are sent. Since dev deploys are limited to developer access only (even though there is no in-dev-namespace authentication), this obviously doesn't work. Why does the word `crossorigin` mean send headers to the same origin? Who knows! Anyway. I don't think that bug should block this PR. That bug is an underlying issue with dev deploy. The fix for the module import bug is here: https://github.com/hail-is/hail/pull/8928",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639135078
https://github.com/hail-is/hail/pull/8923#issuecomment-639135078:278,Security,access,access,278,"OK, so that was way more pain than I expected. Apparently when you're importing a JS module which needs authentication, you must specify the `crossorigin` attribute to the `script` tag. If you lack that attribute, no headers are sent. Since dev deploys are limited to developer access only (even though there is no in-dev-namespace authentication), this obviously doesn't work. Why does the word `crossorigin` mean send headers to the same origin? Who knows! Anyway. I don't think that bug should block this PR. That bug is an underlying issue with dev deploy. The fix for the module import bug is here: https://github.com/hail-is/hail/pull/8928",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639135078
https://github.com/hail-is/hail/pull/8923#issuecomment-639135078:332,Security,authenticat,authentication,332,"OK, so that was way more pain than I expected. Apparently when you're importing a JS module which needs authentication, you must specify the `crossorigin` attribute to the `script` tag. If you lack that attribute, no headers are sent. Since dev deploys are limited to developer access only (even though there is no in-dev-namespace authentication), this obviously doesn't work. Why does the word `crossorigin` mean send headers to the same origin? Who knows! Anyway. I don't think that bug should block this PR. That bug is an underlying issue with dev deploy. The fix for the module import bug is here: https://github.com/hail-is/hail/pull/8928",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639135078
https://github.com/hail-is/hail/pull/8923#issuecomment-639136314:63,Deployability,deploy,deploy,63,I also pushed a commit to make it harder to accidentally `make deploy` into the main namespace. `site`'s `make deploy` now requires a `NAMESPACE` argument to be set.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639136314
https://github.com/hail-is/hail/pull/8923#issuecomment-639136314:111,Deployability,deploy,deploy,111,I also pushed a commit to make it harder to accidentally `make deploy` into the main namespace. `site`'s `make deploy` now requires a `NAMESPACE` argument to be set.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639136314
https://github.com/hail-is/hail/pull/8928#issuecomment-639218007:535,Deployability,configurat,configurations,535,"OK, so, this is apparently an issue where browsers have not yet really implemented the standard correctly. From my reading of [HTML 2.6.4](https://html.spec.whatwg.org/#cors-settings-attributes), `crossorigin=""anonymous""` ought to be sufficient for requests to the same origin as the page containing the script tag. Jake Archibald has an informative [blog post](https://jakearchibald.com/2017/es-modules-in-browsers/) about modules. He links to a [demo](https://module-script-tests-sreyfhwvpq.now.sh/cookie-page) of three cross origin configurations. The three options are:; ```; <script type=""module"" src=""cookie-script""></script>; <script type=""module"" crossorigin="""" src=""cookie-script?1""></script>; <script type=""module"" crossorigin=""use-credentials"" src=""cookie-script?2""></script>; ```; I'm using Safari Version 13.1 (14609.1.20.111.8). I usually only see the very last script working. However, inexplicably, I have seen the first one very rarely work. All I've been doing is refreshing here and there as I try to understand this. The Safari inspector confirms that the cookie is only sent with thee last option. So, anyway, I'm adding `crossorigin=""use-credentials""`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8928#issuecomment-639218007
https://github.com/hail-is/hail/pull/8928#issuecomment-639218007:535,Modifiability,config,configurations,535,"OK, so, this is apparently an issue where browsers have not yet really implemented the standard correctly. From my reading of [HTML 2.6.4](https://html.spec.whatwg.org/#cors-settings-attributes), `crossorigin=""anonymous""` ought to be sufficient for requests to the same origin as the page containing the script tag. Jake Archibald has an informative [blog post](https://jakearchibald.com/2017/es-modules-in-browsers/) about modules. He links to a [demo](https://module-script-tests-sreyfhwvpq.now.sh/cookie-page) of three cross origin configurations. The three options are:; ```; <script type=""module"" src=""cookie-script""></script>; <script type=""module"" crossorigin="""" src=""cookie-script?1""></script>; <script type=""module"" crossorigin=""use-credentials"" src=""cookie-script?2""></script>; ```; I'm using Safari Version 13.1 (14609.1.20.111.8). I usually only see the very last script working. However, inexplicably, I have seen the first one very rarely work. All I've been doing is refreshing here and there as I try to understand this. The Safari inspector confirms that the cookie is only sent with thee last option. So, anyway, I'm adding `crossorigin=""use-credentials""`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8928#issuecomment-639218007
https://github.com/hail-is/hail/pull/8928#issuecomment-639218007:476,Testability,test,tests-sreyfhwvpq,476,"OK, so, this is apparently an issue where browsers have not yet really implemented the standard correctly. From my reading of [HTML 2.6.4](https://html.spec.whatwg.org/#cors-settings-attributes), `crossorigin=""anonymous""` ought to be sufficient for requests to the same origin as the page containing the script tag. Jake Archibald has an informative [blog post](https://jakearchibald.com/2017/es-modules-in-browsers/) about modules. He links to a [demo](https://module-script-tests-sreyfhwvpq.now.sh/cookie-page) of three cross origin configurations. The three options are:; ```; <script type=""module"" src=""cookie-script""></script>; <script type=""module"" crossorigin="""" src=""cookie-script?1""></script>; <script type=""module"" crossorigin=""use-credentials"" src=""cookie-script?2""></script>; ```; I'm using Safari Version 13.1 (14609.1.20.111.8). I usually only see the very last script working. However, inexplicably, I have seen the first one very rarely work. All I've been doing is refreshing here and there as I try to understand this. The Safari inspector confirms that the cookie is only sent with thee last option. So, anyway, I'm adding `crossorigin=""use-credentials""`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8928#issuecomment-639218007
https://github.com/hail-is/hail/pull/8928#issuecomment-639218266:7,Deployability,deploy,deploy,7,My dev deploy now has the light up graph ðŸ¥³,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8928#issuecomment-639218266
https://github.com/hail-is/hail/pull/8941#issuecomment-641617946:4,Testability,test,test,4,"The test I changed catches this now. I think that's sufficient, we shouldn't need a new test if an existing test can be improved to have better coverage.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8941#issuecomment-641617946
https://github.com/hail-is/hail/pull/8941#issuecomment-641617946:88,Testability,test,test,88,"The test I changed catches this now. I think that's sufficient, we shouldn't need a new test if an existing test can be improved to have better coverage.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8941#issuecomment-641617946
https://github.com/hail-is/hail/pull/8941#issuecomment-641617946:108,Testability,test,test,108,"The test I changed catches this now. I think that's sufficient, we shouldn't need a new test if an existing test can be improved to have better coverage.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8941#issuecomment-641617946
https://github.com/hail-is/hail/issues/8944#issuecomment-649050351:74,Testability,test,testing,74,"Glad to hear we're not crazy! Please let us know if there's any external; testing we can do to help. On Wed, Jun 24, 2020 at 1:21 PM John Compitello <notifications@github.com>; wrote:. > Confirmed that I'm able to replicate this, also seeing 354 as the cutoff.; > Looking into it.; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/8944#issuecomment-649049500>, or; > unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AAARLWJCO43V5BGEELQSWE3RYJN3BANCNFSM4N2T543Q>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-649050351
https://github.com/hail-is/hail/issues/8944#issuecomment-652065734:47,Availability,error,error,47,"Haven't figured it out yet, but reproduced the error with a simpler pipeline that just uses one annotate instead of `sample_qc`:. ```; P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = mt.annotate_cols(n_called = hl.agg.filter(hl.is_defined(mt.GT), hl.agg.count())); mt = mt.filter_cols(mt.n_called > 0); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception as e:; print(""\n[FAIL] with "", N, ""partitions""); raise e; break; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734
https://github.com/hail-is/hail/issues/8944#issuecomment-652065734:68,Deployability,pipeline,pipeline,68,"Haven't figured it out yet, but reproduced the error with a simpler pipeline that just uses one annotate instead of `sample_qc`:. ```; P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = mt.annotate_cols(n_called = hl.agg.filter(hl.is_defined(mt.GT), hl.agg.count())); mt = mt.filter_cols(mt.n_called > 0); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception as e:; print(""\n[FAIL] with "", N, ""partitions""); raise e; break; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734
https://github.com/hail-is/hail/issues/8944#issuecomment-652065734:60,Usability,simpl,simpler,60,"Haven't figured it out yet, but reproduced the error with a simpler pipeline that just uses one annotate instead of `sample_qc`:. ```; P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = mt.annotate_cols(n_called = hl.agg.filter(hl.is_defined(mt.GT), hl.agg.count())); mt = mt.filter_cols(mt.n_called > 0); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception as e:; print(""\n[FAIL] with "", N, ""partitions""); raise e; break; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734
https://github.com/hail-is/hail/issues/8944#issuecomment-665775696:32,Availability,error,error,32,"Thanks a lot for reporting this error, Tim found the fix, and this detailed report was very helpful.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-665775696
https://github.com/hail-is/hail/pull/8952#issuecomment-661894256:103,Energy Efficiency,allocate,allocated,103,"is that a reuse? If the region is freed in between, one might expect the value in the second row to be allocated in the same spot.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8952#issuecomment-661894256
https://github.com/hail-is/hail/pull/8952#issuecomment-666412422:42,Availability,error,error,42,I remember you said this was hitting some error/segfault -- anything you need from me to unblock?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8952#issuecomment-666412422
https://github.com/hail-is/hail/pull/8952#issuecomment-667098599:39,Availability,failure,failure,39,"This one isn't a segfault, it's a test failure where somehow for each ndarray in the table, row i + 1 is getting the data array from row i. Cloud workshop and ndarray aggregator pushed this down my priority list, but I will continue to try and fix it and then get back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8952#issuecomment-667098599
https://github.com/hail-is/hail/pull/8952#issuecomment-667098599:190,Availability,down,down,190,"This one isn't a segfault, it's a test failure where somehow for each ndarray in the table, row i + 1 is getting the data array from row i. Cloud workshop and ndarray aggregator pushed this down my priority list, but I will continue to try and fix it and then get back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8952#issuecomment-667098599
https://github.com/hail-is/hail/pull/8952#issuecomment-667098599:34,Testability,test,test,34,"This one isn't a segfault, it's a test failure where somehow for each ndarray in the table, row i + 1 is getting the data array from row i. Cloud workshop and ndarray aggregator pushed this down my priority list, but I will continue to try and fix it and then get back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8952#issuecomment-667098599
https://github.com/hail-is/hail/pull/8956#issuecomment-644207590:182,Performance,load,load,182,"> These files aren't actually referenced in the docs. The docs refer directly to /navbar.css. This is used in layout.html. Doc shares the site navbar. We could modify layout.html to load this using jquery, append to head.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644207590
https://github.com/hail-is/hail/pull/8956#issuecomment-644213411:130,Deployability,deploy,deployed,130,"I'm not sure I understand. `hail/python/hail/docs/_templates/layout.html` references `/navbar.css` which should be present on the deployed site. I might misunderstand `conf.py`, but, AFAICT, this makes unused copies of navbar.css and the PNG: https://hail.is/docs/0.2/navbar.css and https://hail.is/docs/0.2/hail-logo-cropped.png. The page at https://hail.is/docs/0.2/ loads `hail.is/navbar.css`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644213411
https://github.com/hail-is/hail/pull/8956#issuecomment-644213411:369,Performance,load,loads,369,"I'm not sure I understand. `hail/python/hail/docs/_templates/layout.html` references `/navbar.css` which should be present on the deployed site. I might misunderstand `conf.py`, but, AFAICT, this makes unused copies of navbar.css and the PNG: https://hail.is/docs/0.2/navbar.css and https://hail.is/docs/0.2/hail-logo-cropped.png. The page at https://hail.is/docs/0.2/ loads `hail.is/navbar.css`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644213411
https://github.com/hail-is/hail/pull/8956#issuecomment-644213411:313,Testability,log,logo-cropped,313,"I'm not sure I understand. `hail/python/hail/docs/_templates/layout.html` references `/navbar.css` which should be present on the deployed site. I might misunderstand `conf.py`, but, AFAICT, this makes unused copies of navbar.css and the PNG: https://hail.is/docs/0.2/navbar.css and https://hail.is/docs/0.2/hail-logo-cropped.png. The page at https://hail.is/docs/0.2/ loads `hail.is/navbar.css`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644213411
https://github.com/hail-is/hail/pull/8956#issuecomment-644228546:132,Deployability,deploy,deployed,132,"> I'm not sure I understand. `hail/python/hail/docs/_templates/layout.html` references `/navbar.css` which should be present on the deployed site. I might misunderstand `conf.py`, but, AFAICT, this makes unused copies of navbar.css and the PNG: https://hail.is/docs/0.2/navbar.css and https://hail.is/docs/0.2/hail-logo-cropped.png. The page at https://hail.is/docs/0.2/ loads `hail.is/navbar.css`. Sorry, ignore, I misread the comment. Agreed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644228546
https://github.com/hail-is/hail/pull/8956#issuecomment-644228546:371,Performance,load,loads,371,"> I'm not sure I understand. `hail/python/hail/docs/_templates/layout.html` references `/navbar.css` which should be present on the deployed site. I might misunderstand `conf.py`, but, AFAICT, this makes unused copies of navbar.css and the PNG: https://hail.is/docs/0.2/navbar.css and https://hail.is/docs/0.2/hail-logo-cropped.png. The page at https://hail.is/docs/0.2/ loads `hail.is/navbar.css`. Sorry, ignore, I misread the comment. Agreed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644228546
https://github.com/hail-is/hail/pull/8956#issuecomment-644228546:315,Testability,log,logo-cropped,315,"> I'm not sure I understand. `hail/python/hail/docs/_templates/layout.html` references `/navbar.css` which should be present on the deployed site. I might misunderstand `conf.py`, but, AFAICT, this makes unused copies of navbar.css and the PNG: https://hail.is/docs/0.2/navbar.css and https://hail.is/docs/0.2/hail-logo-cropped.png. The page at https://hail.is/docs/0.2/ loads `hail.is/navbar.css`. Sorry, ignore, I misread the comment. Agreed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644228546
https://github.com/hail-is/hail/pull/8957#issuecomment-644136378:88,Testability,test,testing,88,"Cotton and I discussed several weeks ago that we should build a debugging allocator for testing/debugging issues like the above. You were a natural candidate to work on this, given how you were digging pretty deeply into region stuff, and even started working on something similar. Shall we set up a chat in the next day or two to talk about this? Sounds like it's now a high-prio need.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8957#issuecomment-644136378
https://github.com/hail-is/hail/pull/8957#issuecomment-645097424:223,Availability,failure,failure,223,"Thanks! To clarify your other question, while the local backend segfault with this, I think it is caused by existing memory issues that show up in the Spark backend tests. I conjecture when you fix those, the local backend failure will go away. If not, we can flip to see who debugs it :-)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8957#issuecomment-645097424
https://github.com/hail-is/hail/pull/8957#issuecomment-645097424:165,Testability,test,tests,165,"Thanks! To clarify your other question, while the local backend segfault with this, I think it is caused by existing memory issues that show up in the Spark backend tests. I conjecture when you fix those, the local backend failure will go away. If not, we can flip to see who debugs it :-)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8957#issuecomment-645097424
https://github.com/hail-is/hail/pull/8958#issuecomment-645533486:532,Safety,safe,safest,532,"Actually, a slightly longer answer:. The parent relationship is essentially a representation of the JVM stack. Each X corresponds to 1 JVM bytecode (except NewInstanceX which is fused). During emit, children are pushed on the stack left-to-right by necessity. Slightly more generally, there are two distinct questions here: what the implementation does, and what it guarantees. For what it guarantees, there are two options: fixed (like Java, which evaluates things left-to-right), and undefined (like C). In general, I think it is safest not to rely on the order of evaluation. I'm not aware of where we do. ; Our backend is simple enough and the mapping to the JVM concrete enough that I don't see any reason why we'd have reason to deviate from left-to-right. So I guess I fall somewhere in between (don't rely on it, but violate it).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645533486
https://github.com/hail-is/hail/pull/8958#issuecomment-645533486:626,Usability,simpl,simple,626,"Actually, a slightly longer answer:. The parent relationship is essentially a representation of the JVM stack. Each X corresponds to 1 JVM bytecode (except NewInstanceX which is fused). During emit, children are pushed on the stack left-to-right by necessity. Slightly more generally, there are two distinct questions here: what the implementation does, and what it guarantees. For what it guarantees, there are two options: fixed (like Java, which evaluates things left-to-right), and undefined (like C). In general, I think it is safest not to rely on the order of evaluation. I'm not aware of where we do. ; Our backend is simple enough and the mapping to the JVM concrete enough that I don't see any reason why we'd have reason to deviate from left-to-right. So I guess I fall somewhere in between (don't rely on it, but violate it).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645533486
https://github.com/hail-is/hail/pull/8958#issuecomment-645546246:28,Safety,safe,safest,28,"> In general, I think it is safest not to rely on the order of evaluation. I'm not aware of where we do. Unless I'm missing something, this code splitter does rely on the evaluation order, which is why I asked about it. If a statement `x` has large children, it moves them to execute before `x`, evaluated in the `children` array order (more precisely, in the post-order traversal order), storing their results in locals. I think it's possible to avoid relying on order of evaluation, if instead of only splitting out sequences of statements, we can also split out a `ValueX` directly, replacing it by a call to new method which returns the result. I don't have strong feelings about whether the order should be guaranteed, just want to be clear which it is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645546246
https://github.com/hail-is/hail/pull/8958#issuecomment-645546246:447,Safety,avoid,avoid,447,"> In general, I think it is safest not to rely on the order of evaluation. I'm not aware of where we do. Unless I'm missing something, this code splitter does rely on the evaluation order, which is why I asked about it. If a statement `x` has large children, it moves them to execute before `x`, evaluated in the `children` array order (more precisely, in the post-order traversal order), storing their results in locals. I think it's possible to avoid relying on order of evaluation, if instead of only splitting out sequences of statements, we can also split out a `ValueX` directly, replacing it by a call to new method which returns the result. I don't have strong feelings about whether the order should be guaranteed, just want to be clear which it is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645546246
https://github.com/hail-is/hail/pull/8958#issuecomment-645546246:740,Usability,clear,clear,740,"> In general, I think it is safest not to rely on the order of evaluation. I'm not aware of where we do. Unless I'm missing something, this code splitter does rely on the evaluation order, which is why I asked about it. If a statement `x` has large children, it moves them to execute before `x`, evaluated in the `children` array order (more precisely, in the post-order traversal order), storing their results in locals. I think it's possible to avoid relying on order of evaluation, if instead of only splitting out sequences of statements, we can also split out a `ValueX` directly, replacing it by a call to new method which returns the result. I don't have strong feelings about whether the order should be guaranteed, just want to be clear which it is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645546246
https://github.com/hail-is/hail/pull/8958#issuecomment-645561070:310,Usability,clear,clear,310,"> Unless I'm missing something, this code splitter does rely on the evaluation order. I don't understand. I think I must be missing something. Maybe there is a bug I don't see. I'm particularly confused by your use of ""rely"". I think splitter, by lifting things out into separate statements (where Block has a clear order of evaluation) implements an order of evaluation. Now, you could ask if the order implemented by splitting is the same as the order of evaluation of the children of the parent. What I'm saying is the the order of evaluation of the children should be undefined, so any choice of splitting out large children is valid (but all orderings being valid, we should prefer left-to-right). Not, I'm just talking about the order of the evaluation of the children. Children are always evaluated before parents (as necessitated by the data flow), and blocks are executed linearly in order.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645561070
https://github.com/hail-is/hail/pull/8960#issuecomment-644205887:184,Security,secur,security,184,"Also, FYI - @cseed I had to change the permissions of the worker container to privileged and SYS_ADMIN. This doesn't apply to the user containers. But still we should double check the security implications of this change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8960#issuecomment-644205887
https://github.com/hail-is/hail/pull/8960#issuecomment-644218677:26,Testability,test,test,26,"And we plan to enable the test long-term in the follow up PR, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8960#issuecomment-644218677
https://github.com/hail-is/hail/pull/8960#issuecomment-644225287:26,Testability,test,test,26,I added a front-end batch test. I think we should delete the test I added in this PR in favor of the one in #8961 once we know it's working. This PR as it stands passed the tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8960#issuecomment-644225287
https://github.com/hail-is/hail/pull/8960#issuecomment-644225287:61,Testability,test,test,61,I added a front-end batch test. I think we should delete the test I added in this PR in favor of the one in #8961 once we know it's working. This PR as it stands passed the tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8960#issuecomment-644225287
https://github.com/hail-is/hail/pull/8960#issuecomment-644225287:173,Testability,test,tests,173,I added a front-end batch test. I think we should delete the test I added in this PR in favor of the one in #8961 once we know it's working. This PR as it stands passed the tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8960#issuecomment-644225287
https://github.com/hail-is/hail/pull/8961#issuecomment-644384619:59,Deployability,deploy,deploy,59,I'm going to temporarily close this while I debug with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8961#issuecomment-644384619
https://github.com/hail-is/hail/pull/8963#issuecomment-650180949:94,Testability,Benchmark,Benchmarks,94,This is still 2-5% slower than the current main line branch. I have a few more things to try. Benchmarks where the change is >20%:. ```; $ hail-bench compare ./0.2.47-3f8cff262dcf.json ./0.2.47-b2acae4f478f.json ; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; matrix_table_entries_table_no_key 580.3% 56.955 330.503; table_aggregate_array_sum 528.0% 9.103 48.066; table_big_aggregate_compile_and_execute 185.0% 13.325 24.648; per_row_stats_star_star 154.0% 8.474 13.046; table_aggregate_linreg 148.6% 48.593 72.212; matrix_table_filter_entries_unfilter 140.8% 8.581 12.085; matrix_table_array_arithmetic 135.8% 10.208 13.862; matrix_table_many_aggs_col_wise 133.7% 34.847 46.576; ...; full_combiner_chr22 33.4% 1644.907 548.608; ----------------------; Geometric mean: 104.8%; Median: 101.9%; ```. FYI @tpoterba @chrisvittal The combiner improvement persists across multiple runs and is real. That's quite nice!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650180949
https://github.com/hail-is/hail/pull/8963#issuecomment-650180949:214,Testability,Benchmark,Benchmark,214,This is still 2-5% slower than the current main line branch. I have a few more things to try. Benchmarks where the change is >20%:. ```; $ hail-bench compare ./0.2.47-3f8cff262dcf.json ./0.2.47-b2acae4f478f.json ; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; matrix_table_entries_table_no_key 580.3% 56.955 330.503; table_aggregate_array_sum 528.0% 9.103 48.066; table_big_aggregate_compile_and_execute 185.0% 13.325 24.648; per_row_stats_star_star 154.0% 8.474 13.046; table_aggregate_linreg 148.6% 48.593 72.212; matrix_table_filter_entries_unfilter 140.8% 8.581 12.085; matrix_table_array_arithmetic 135.8% 10.208 13.862; matrix_table_many_aggs_col_wise 133.7% 34.847 46.576; ...; full_combiner_chr22 33.4% 1644.907 548.608; ----------------------; Geometric mean: 104.8%; Median: 101.9%; ```. FYI @tpoterba @chrisvittal The combiner improvement persists across multiple runs and is real. That's quite nice!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650180949
https://github.com/hail-is/hail/pull/8963#issuecomment-650638060:69,Integrability,wrap,wrapping,69,Phew! I finally got to parity. I'm impressed how well the old method wrapping logic worked. Benchmarks with >20% change:. ```; $ hail-bench compare ./0.2.47-3f8cff262dcf-1.json 0.2.47-63dccdda2a44.json ; Failed benchmarks in run 1:; pc_relate_big; large_range_matrix_table_sum; Failed benchmarks in run 2:; pc_relate_big; large_range_matrix_table_sum; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; table_big_aggregate_compile_and_execute 334.8% 13.325 44.608; matrix_table_array_arithmetic 133.9% 10.208 13.665; matrix_table_many_aggs_col_wise 131.4% 34.847 45.781; table_aggregate_counter 126.4% 13.136 16.606; per_row_stats_star_star 124.5% 8.474 10.553; matrix_table_filter_entries_unfilter 123.7% 8.581 10.616; ...; shuffle_key_rows_by_mt 82.8% 35.932 29.742; full_combiner_chr22 23.6% 1644.907 388.414; ----------------------; Geometric mean: 100.0%; Median: 99.2%; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650638060
https://github.com/hail-is/hail/pull/8963#issuecomment-650638060:78,Testability,log,logic,78,Phew! I finally got to parity. I'm impressed how well the old method wrapping logic worked. Benchmarks with >20% change:. ```; $ hail-bench compare ./0.2.47-3f8cff262dcf-1.json 0.2.47-63dccdda2a44.json ; Failed benchmarks in run 1:; pc_relate_big; large_range_matrix_table_sum; Failed benchmarks in run 2:; pc_relate_big; large_range_matrix_table_sum; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; table_big_aggregate_compile_and_execute 334.8% 13.325 44.608; matrix_table_array_arithmetic 133.9% 10.208 13.665; matrix_table_many_aggs_col_wise 131.4% 34.847 45.781; table_aggregate_counter 126.4% 13.136 16.606; per_row_stats_star_star 124.5% 8.474 10.553; matrix_table_filter_entries_unfilter 123.7% 8.581 10.616; ...; shuffle_key_rows_by_mt 82.8% 35.932 29.742; full_combiner_chr22 23.6% 1644.907 388.414; ----------------------; Geometric mean: 100.0%; Median: 99.2%; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650638060
https://github.com/hail-is/hail/pull/8963#issuecomment-650638060:92,Testability,Benchmark,Benchmarks,92,Phew! I finally got to parity. I'm impressed how well the old method wrapping logic worked. Benchmarks with >20% change:. ```; $ hail-bench compare ./0.2.47-3f8cff262dcf-1.json 0.2.47-63dccdda2a44.json ; Failed benchmarks in run 1:; pc_relate_big; large_range_matrix_table_sum; Failed benchmarks in run 2:; pc_relate_big; large_range_matrix_table_sum; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; table_big_aggregate_compile_and_execute 334.8% 13.325 44.608; matrix_table_array_arithmetic 133.9% 10.208 13.665; matrix_table_many_aggs_col_wise 131.4% 34.847 45.781; table_aggregate_counter 126.4% 13.136 16.606; per_row_stats_star_star 124.5% 8.474 10.553; matrix_table_filter_entries_unfilter 123.7% 8.581 10.616; ...; shuffle_key_rows_by_mt 82.8% 35.932 29.742; full_combiner_chr22 23.6% 1644.907 388.414; ----------------------; Geometric mean: 100.0%; Median: 99.2%; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650638060
https://github.com/hail-is/hail/pull/8963#issuecomment-650638060:211,Testability,benchmark,benchmarks,211,Phew! I finally got to parity. I'm impressed how well the old method wrapping logic worked. Benchmarks with >20% change:. ```; $ hail-bench compare ./0.2.47-3f8cff262dcf-1.json 0.2.47-63dccdda2a44.json ; Failed benchmarks in run 1:; pc_relate_big; large_range_matrix_table_sum; Failed benchmarks in run 2:; pc_relate_big; large_range_matrix_table_sum; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; table_big_aggregate_compile_and_execute 334.8% 13.325 44.608; matrix_table_array_arithmetic 133.9% 10.208 13.665; matrix_table_many_aggs_col_wise 131.4% 34.847 45.781; table_aggregate_counter 126.4% 13.136 16.606; per_row_stats_star_star 124.5% 8.474 10.553; matrix_table_filter_entries_unfilter 123.7% 8.581 10.616; ...; shuffle_key_rows_by_mt 82.8% 35.932 29.742; full_combiner_chr22 23.6% 1644.907 388.414; ----------------------; Geometric mean: 100.0%; Median: 99.2%; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650638060
https://github.com/hail-is/hail/pull/8963#issuecomment-650638060:285,Testability,benchmark,benchmarks,285,Phew! I finally got to parity. I'm impressed how well the old method wrapping logic worked. Benchmarks with >20% change:. ```; $ hail-bench compare ./0.2.47-3f8cff262dcf-1.json 0.2.47-63dccdda2a44.json ; Failed benchmarks in run 1:; pc_relate_big; large_range_matrix_table_sum; Failed benchmarks in run 2:; pc_relate_big; large_range_matrix_table_sum; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; table_big_aggregate_compile_and_execute 334.8% 13.325 44.608; matrix_table_array_arithmetic 133.9% 10.208 13.665; matrix_table_many_aggs_col_wise 131.4% 34.847 45.781; table_aggregate_counter 126.4% 13.136 16.606; per_row_stats_star_star 124.5% 8.474 10.553; matrix_table_filter_entries_unfilter 123.7% 8.581 10.616; ...; shuffle_key_rows_by_mt 82.8% 35.932 29.742; full_combiner_chr22 23.6% 1644.907 388.414; ----------------------; Geometric mean: 100.0%; Median: 99.2%; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650638060
https://github.com/hail-is/hail/pull/8963#issuecomment-650638060:352,Testability,Benchmark,Benchmark,352,Phew! I finally got to parity. I'm impressed how well the old method wrapping logic worked. Benchmarks with >20% change:. ```; $ hail-bench compare ./0.2.47-3f8cff262dcf-1.json 0.2.47-63dccdda2a44.json ; Failed benchmarks in run 1:; pc_relate_big; large_range_matrix_table_sum; Failed benchmarks in run 2:; pc_relate_big; large_range_matrix_table_sum; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; table_big_aggregate_compile_and_execute 334.8% 13.325 44.608; matrix_table_array_arithmetic 133.9% 10.208 13.665; matrix_table_many_aggs_col_wise 131.4% 34.847 45.781; table_aggregate_counter 126.4% 13.136 16.606; per_row_stats_star_star 124.5% 8.474 10.553; matrix_table_filter_entries_unfilter 123.7% 8.581 10.616; ...; shuffle_key_rows_by_mt 82.8% 35.932 29.742; full_combiner_chr22 23.6% 1644.907 388.414; ----------------------; Geometric mean: 100.0%; Median: 99.2%; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650638060
https://github.com/hail-is/hail/pull/8963#issuecomment-650837044:195,Availability,down,down,195,"oh, man, this is super exciting. 3x on the combiner? yes please!. We can probably make incremental performance improvements to the LIR method splitting code to bring the compile and execute back down, and that one I consider a little less critical anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650837044
https://github.com/hail-is/hail/pull/8963#issuecomment-650837044:99,Performance,perform,performance,99,"oh, man, this is super exciting. 3x on the combiner? yes please!. We can probably make incremental performance improvements to the LIR method splitting code to bring the compile and execute back down, and that one I consider a little less critical anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650837044
https://github.com/hail-is/hail/pull/8964#issuecomment-644427035:642,Safety,avoid,avoid,642,"Additionally, I think this is actually the correct/intended use of setPixelRatio. Note that if we set linewidth = .5 successfully, which we cannot, devices with devicePixelRatio = 1 would still look twice as thick as those with ratio 2. That is not what I want. Iâ€™ve calibrated this to look a certain way on devicePixelRatio = 2 displays, and I want the appearance to be as close to this as possible in lower and higher density fixed-pixel displays. . Ideally devices with ratio < 2 would have lines that had the appropriate thinness, but maybe due to lineWidth limits do not, so we truncate those. Above 2, we continue to scale as before to avoid seeing lines that are too thick.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-644427035
https://github.com/hail-is/hail/pull/8964#issuecomment-644687247:102,Usability,simpl,simple,102,"There may also be a way of handling the intended effect using alpha blending, but this works and is a simple fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-644687247
https://github.com/hail-is/hail/pull/8964#issuecomment-644692798:186,Usability,clear,clear,186,"Better experiment below, you can see in the right hand side the computed values. Note: I don't see any visual difference between .25 and any value < 1 in safari (therefore I did not see clear evidence that safari affected line blending in a way that called for 1 / devicePixelRatio), so for the sake of not complicating this further, I want to keep .25 unless there is evidence this causes issues. Otherwise I think this issue is solved. All on low dpi device (1920*1080 tv):; ![Screenshot 2020-06-16 06 58 11](https://user-images.githubusercontent.com/5543229/84766569-095d6d00-af9f-11ea-8102-6d79eeed2aba.png); ![Screenshot 2020-06-16 06 58 28](https://user-images.githubusercontent.com/5543229/84766571-09f60380-af9f-11ea-9fe6-2fb9ae4bbe43.png); ![Screenshot 2020-06-16 06 58 45](https://user-images.githubusercontent.com/5543229/84766572-09f60380-af9f-11ea-9789-242bc3693598.png); ![Screenshot 2020-06-16 06 59 03](https://user-images.githubusercontent.com/5543229/84766573-09f60380-af9f-11ea-8a11-21c74cc0409d.png). All on high dpi display (pixel ratio 2):; <img width=""1920"" alt=""Screenshot 2020-06-16 07 04 28"" src=""https://user-images.githubusercontent.com/5543229/84766950-a5877400-af9f-11ea-8e4b-a691a1b5f5b7.png"">; <img width=""1920"" alt=""Screenshot 2020-06-16 07 01 51"" src=""https://user-images.githubusercontent.com/5543229/84766749-55101680-af9f-11ea-9a77-c7bacc79dd16.png"">; <img width=""1920"" alt=""Screenshot 2020-06-16 07 02 03"" src=""https://user-images.githubusercontent.com/5543229/84766751-55a8ad00-af9f-11ea-8e49-3c68f588fb0f.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-644692798
https://github.com/hail-is/hail/pull/8964#issuecomment-645569518:1302,Usability,simpl,simple,1302,"This change achieves the effect you desire. What I additionally need from this PR is a mental model of why this change works. This change specifies a pixel ratio of two when the device's physical:css pixel ratio is one, but uses the device's pixel ratio otherwise. This breaks my conceptual model. It's important for us all to share compatible models of what the code does so that we all are able to manipulate the code in the future. I think there's a few moving pieces and if we can get a handle on them all, we'll all agree on what the right fix is. AFAICT, three js is built on WebGL. The MDN recommends not using a WebGL `lineWidth` other than one because of inconsistent (or lack of) support for line widths other than one. You observe that `pixelRatio` affects the visual behavior, at least in Safari, when the lineWidth is set to 0.25. In particular, when the pixel ratio is set to `2` and the lineWidth is set to `0.25`, the lines appear thinner. If the pixel ratio is higher than the device pixel ratio, something must be interpolating to device pixels. It seems to me that relying on this interpolation behavior will lead to code that is more difficult to understand and manipulate. If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645569518
https://github.com/hail-is/hail/pull/8964#issuecomment-645572011:233,Integrability,depend,dependent,233,"> This change achieves the effect you desire. What I additionally need from this PR is a mental model of why this change works. I think I communicated this above: as far as I understand it, the effective resolution of the graphic is dependent on both the viewport resolution and the pixel ratio. The observed resolution is too low with pixelRatio 1, gives thick lines. . reference: https://github.com/mrdoob/three.js/issues/16747. > You observe that pixelRatio affects the visual behavior, at least in Safari, when the lineWidth is set to 0.25. In particular, when the pixel ratio is set to 2 and the lineWidth is set to 0.25, the lines appear thinner. No, as you see above, regardless of pixel ratio, Safari's minimum line width is somewhere below one. Setting a linewidth below 1 has no drawbacks; it will just cause the browser to use the min. > If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?. Not that I know of. Some people use cylinder geometry to get around line thickness issues. Yes, there may be an alpha-based solution, which I brought up above. This will be more complicated, and further increase the development cycle. > It seems to me that relying on this interpolation behavior will lead to code that is more difficult to understand and manipulate. I don't understand. Is there another fix that seems easier?; ; > If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?. Potentially, but this will obviously be a significantly more complicated fix. The proposed fix is effectively 1 line, and takes care of the observed issue in a wide range of browsers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645572011
https://github.com/hail-is/hail/pull/8964#issuecomment-645572011:957,Usability,simpl,simple,957,"> This change achieves the effect you desire. What I additionally need from this PR is a mental model of why this change works. I think I communicated this above: as far as I understand it, the effective resolution of the graphic is dependent on both the viewport resolution and the pixel ratio. The observed resolution is too low with pixelRatio 1, gives thick lines. . reference: https://github.com/mrdoob/three.js/issues/16747. > You observe that pixelRatio affects the visual behavior, at least in Safari, when the lineWidth is set to 0.25. In particular, when the pixel ratio is set to 2 and the lineWidth is set to 0.25, the lines appear thinner. No, as you see above, regardless of pixel ratio, Safari's minimum line width is somewhere below one. Setting a linewidth below 1 has no drawbacks; it will just cause the browser to use the min. > If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?. Not that I know of. Some people use cylinder geometry to get around line thickness issues. Yes, there may be an alpha-based solution, which I brought up above. This will be more complicated, and further increase the development cycle. > It seems to me that relying on this interpolation behavior will lead to code that is more difficult to understand and manipulate. I don't understand. Is there another fix that seems easier?; ; > If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?. Potentially, but this will obviously be a significantly more complicated fix. The proposed fix is effectively 1 line, and takes care of the observed issue in a wide range of browsers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645572011
https://github.com/hail-is/hail/pull/8964#issuecomment-645572011:1545,Usability,simpl,simple,1545,"> This change achieves the effect you desire. What I additionally need from this PR is a mental model of why this change works. I think I communicated this above: as far as I understand it, the effective resolution of the graphic is dependent on both the viewport resolution and the pixel ratio. The observed resolution is too low with pixelRatio 1, gives thick lines. . reference: https://github.com/mrdoob/three.js/issues/16747. > You observe that pixelRatio affects the visual behavior, at least in Safari, when the lineWidth is set to 0.25. In particular, when the pixel ratio is set to 2 and the lineWidth is set to 0.25, the lines appear thinner. No, as you see above, regardless of pixel ratio, Safari's minimum line width is somewhere below one. Setting a linewidth below 1 has no drawbacks; it will just cause the browser to use the min. > If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?. Not that I know of. Some people use cylinder geometry to get around line thickness issues. Yes, there may be an alpha-based solution, which I brought up above. This will be more complicated, and further increase the development cycle. > It seems to me that relying on this interpolation behavior will lead to code that is more difficult to understand and manipulate. I don't understand. Is there another fix that seems easier?; ; > If the intention is to make the lines less striking, can we apply an alpha filter instead? Is there another simple & consistent-across-platforms solution?. Potentially, but this will obviously be a significantly more complicated fix. The proposed fix is effectively 1 line, and takes care of the observed issue in a wide range of browsers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645572011
https://github.com/hail-is/hail/pull/8964#issuecomment-645574948:223,Usability,clear,clear,223,"If you're actually seeing some inconsistent results in some browser, I agree the solution isn't sufficient. Else, why not get the easy solution in, and do more work when it's needed. I see a potential problem statement, no clear reason why the present solution is problematic, and a desire to move to a different solution. . I looked into the relationship between fractional line widths and alpha a few days ago, when I wrote the comment suggesting that alpha blending could be an alternative solution. Antialiased fractional line width (which is also what you would get if the effective resolution is larger than the viewport resolution and then objects scaled), will act like an alpha-blended 1px line width. reference: https://community.khronos.org/t/how-to-draw-a-line-with-width-less-than-1-0/42022",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645574948
https://github.com/hail-is/hail/pull/8964#issuecomment-645598185:1371,Safety,detect,detect,1371,"I'd also like to mention that linewidth in general seems to have cross-browser issue (https://github.com/mrdoob/three.js/issues/10357). Most of the issues detailed are for line width > 1, which is not an issue for us. The fact that safari requires this (< 1) to normalize its behavior with other browsers is unfortunate, but presumably when fixed, it would just truncate to 1 anyhow (this is what every other browser appears to do). So we have 2 ideas on the table: 1) use the fix proposed, because linewidth is mainly problematic when > 1, and truncating setPixelRatio is uncontroversial (line resolution is too low with 1, and using setPixelRatio allows us to get a viewport-independent correct resolution), 2) use an entirely different geometry that does not rely on linewidth. Option 2 is both more technically sound, and also a bunch of additional work that yield no visual benefit. . If you want option 2, I am happy to close this request, and have someone else PR an entirely new background geometry. I don't have time for that unfortunately, and it seems too much work without a visible problem to address. Alternatively we accept this PR, take the fix, and keep an eye out for visual inconsistencies in the corner case that we are worried about (low resolution displays, and safari, which has something like 2% market share). edit: A third solution is to try to detect safari and implement linewidth < 1 only for it. Unfortunately browser detection can be inconsistent, which is why I didn't use that solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645598185
https://github.com/hail-is/hail/pull/8964#issuecomment-645598185:1448,Safety,detect,detection,1448,"I'd also like to mention that linewidth in general seems to have cross-browser issue (https://github.com/mrdoob/three.js/issues/10357). Most of the issues detailed are for line width > 1, which is not an issue for us. The fact that safari requires this (< 1) to normalize its behavior with other browsers is unfortunate, but presumably when fixed, it would just truncate to 1 anyhow (this is what every other browser appears to do). So we have 2 ideas on the table: 1) use the fix proposed, because linewidth is mainly problematic when > 1, and truncating setPixelRatio is uncontroversial (line resolution is too low with 1, and using setPixelRatio allows us to get a viewport-independent correct resolution), 2) use an entirely different geometry that does not rely on linewidth. Option 2 is both more technically sound, and also a bunch of additional work that yield no visual benefit. . If you want option 2, I am happy to close this request, and have someone else PR an entirely new background geometry. I don't have time for that unfortunately, and it seems too much work without a visible problem to address. Alternatively we accept this PR, take the fix, and keep an eye out for visual inconsistencies in the corner case that we are worried about (low resolution displays, and safari, which has something like 2% market share). edit: A third solution is to try to detect safari and implement linewidth < 1 only for it. Unfortunately browser detection can be inconsistent, which is why I didn't use that solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645598185
https://github.com/hail-is/hail/pull/8964#issuecomment-645699918:681,Deployability,deploy,deployed,681,"We seem to be miscommunicating. My issue is that this change uses a knob (pixel ratio) to control something seemingly unrelated to the knob (opacity). This isn't a browser thing, it's a monitor thing. I have two screens one with device pixel ratio (DPR) 1 and one with DPR 2. Regardless of browser, on the DPR 1 monitor, the lines appear thicker than on the DPR 2 monitor. They must appear thicker! The pixels are bigger and less dense. My suggestion is this: https://github.com/hail-is/hail/compare/master...danking:viz. That change sets line width to 1 and uses opacity to reduce the brightness of the lines. My proposal looks, to me, exactly the same as this PR (which I've dev deployed into my namespace: https://internal.hail.is/dking/site/). I've compared both on my DPR 2 monitor and my DPR 1 monitor. . The reason I prefer the change linked above is that it is self-describing: we want the thinest possible lines that the monitor supports and we want them to be a bit faint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645699918
https://github.com/hail-is/hail/pull/8964#issuecomment-645699918:186,Energy Efficiency,monitor,monitor,186,"We seem to be miscommunicating. My issue is that this change uses a knob (pixel ratio) to control something seemingly unrelated to the knob (opacity). This isn't a browser thing, it's a monitor thing. I have two screens one with device pixel ratio (DPR) 1 and one with DPR 2. Regardless of browser, on the DPR 1 monitor, the lines appear thicker than on the DPR 2 monitor. They must appear thicker! The pixels are bigger and less dense. My suggestion is this: https://github.com/hail-is/hail/compare/master...danking:viz. That change sets line width to 1 and uses opacity to reduce the brightness of the lines. My proposal looks, to me, exactly the same as this PR (which I've dev deployed into my namespace: https://internal.hail.is/dking/site/). I've compared both on my DPR 2 monitor and my DPR 1 monitor. . The reason I prefer the change linked above is that it is self-describing: we want the thinest possible lines that the monitor supports and we want them to be a bit faint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645699918
https://github.com/hail-is/hail/pull/8964#issuecomment-645699918:312,Energy Efficiency,monitor,monitor,312,"We seem to be miscommunicating. My issue is that this change uses a knob (pixel ratio) to control something seemingly unrelated to the knob (opacity). This isn't a browser thing, it's a monitor thing. I have two screens one with device pixel ratio (DPR) 1 and one with DPR 2. Regardless of browser, on the DPR 1 monitor, the lines appear thicker than on the DPR 2 monitor. They must appear thicker! The pixels are bigger and less dense. My suggestion is this: https://github.com/hail-is/hail/compare/master...danking:viz. That change sets line width to 1 and uses opacity to reduce the brightness of the lines. My proposal looks, to me, exactly the same as this PR (which I've dev deployed into my namespace: https://internal.hail.is/dking/site/). I've compared both on my DPR 2 monitor and my DPR 1 monitor. . The reason I prefer the change linked above is that it is self-describing: we want the thinest possible lines that the monitor supports and we want them to be a bit faint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645699918
https://github.com/hail-is/hail/pull/8964#issuecomment-645699918:364,Energy Efficiency,monitor,monitor,364,"We seem to be miscommunicating. My issue is that this change uses a knob (pixel ratio) to control something seemingly unrelated to the knob (opacity). This isn't a browser thing, it's a monitor thing. I have two screens one with device pixel ratio (DPR) 1 and one with DPR 2. Regardless of browser, on the DPR 1 monitor, the lines appear thicker than on the DPR 2 monitor. They must appear thicker! The pixels are bigger and less dense. My suggestion is this: https://github.com/hail-is/hail/compare/master...danking:viz. That change sets line width to 1 and uses opacity to reduce the brightness of the lines. My proposal looks, to me, exactly the same as this PR (which I've dev deployed into my namespace: https://internal.hail.is/dking/site/). I've compared both on my DPR 2 monitor and my DPR 1 monitor. . The reason I prefer the change linked above is that it is self-describing: we want the thinest possible lines that the monitor supports and we want them to be a bit faint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645699918
https://github.com/hail-is/hail/pull/8964#issuecomment-645699918:575,Energy Efficiency,reduce,reduce,575,"We seem to be miscommunicating. My issue is that this change uses a knob (pixel ratio) to control something seemingly unrelated to the knob (opacity). This isn't a browser thing, it's a monitor thing. I have two screens one with device pixel ratio (DPR) 1 and one with DPR 2. Regardless of browser, on the DPR 1 monitor, the lines appear thicker than on the DPR 2 monitor. They must appear thicker! The pixels are bigger and less dense. My suggestion is this: https://github.com/hail-is/hail/compare/master...danking:viz. That change sets line width to 1 and uses opacity to reduce the brightness of the lines. My proposal looks, to me, exactly the same as this PR (which I've dev deployed into my namespace: https://internal.hail.is/dking/site/). I've compared both on my DPR 2 monitor and my DPR 1 monitor. . The reason I prefer the change linked above is that it is self-describing: we want the thinest possible lines that the monitor supports and we want them to be a bit faint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645699918
https://github.com/hail-is/hail/pull/8964#issuecomment-645699918:779,Energy Efficiency,monitor,monitor,779,"We seem to be miscommunicating. My issue is that this change uses a knob (pixel ratio) to control something seemingly unrelated to the knob (opacity). This isn't a browser thing, it's a monitor thing. I have two screens one with device pixel ratio (DPR) 1 and one with DPR 2. Regardless of browser, on the DPR 1 monitor, the lines appear thicker than on the DPR 2 monitor. They must appear thicker! The pixels are bigger and less dense. My suggestion is this: https://github.com/hail-is/hail/compare/master...danking:viz. That change sets line width to 1 and uses opacity to reduce the brightness of the lines. My proposal looks, to me, exactly the same as this PR (which I've dev deployed into my namespace: https://internal.hail.is/dking/site/). I've compared both on my DPR 2 monitor and my DPR 1 monitor. . The reason I prefer the change linked above is that it is self-describing: we want the thinest possible lines that the monitor supports and we want them to be a bit faint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645699918
https://github.com/hail-is/hail/pull/8964#issuecomment-645699918:800,Energy Efficiency,monitor,monitor,800,"We seem to be miscommunicating. My issue is that this change uses a knob (pixel ratio) to control something seemingly unrelated to the knob (opacity). This isn't a browser thing, it's a monitor thing. I have two screens one with device pixel ratio (DPR) 1 and one with DPR 2. Regardless of browser, on the DPR 1 monitor, the lines appear thicker than on the DPR 2 monitor. They must appear thicker! The pixels are bigger and less dense. My suggestion is this: https://github.com/hail-is/hail/compare/master...danking:viz. That change sets line width to 1 and uses opacity to reduce the brightness of the lines. My proposal looks, to me, exactly the same as this PR (which I've dev deployed into my namespace: https://internal.hail.is/dking/site/). I've compared both on my DPR 2 monitor and my DPR 1 monitor. . The reason I prefer the change linked above is that it is self-describing: we want the thinest possible lines that the monitor supports and we want them to be a bit faint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645699918
https://github.com/hail-is/hail/pull/8964#issuecomment-645699918:930,Energy Efficiency,monitor,monitor,930,"We seem to be miscommunicating. My issue is that this change uses a knob (pixel ratio) to control something seemingly unrelated to the knob (opacity). This isn't a browser thing, it's a monitor thing. I have two screens one with device pixel ratio (DPR) 1 and one with DPR 2. Regardless of browser, on the DPR 1 monitor, the lines appear thicker than on the DPR 2 monitor. They must appear thicker! The pixels are bigger and less dense. My suggestion is this: https://github.com/hail-is/hail/compare/master...danking:viz. That change sets line width to 1 and uses opacity to reduce the brightness of the lines. My proposal looks, to me, exactly the same as this PR (which I've dev deployed into my namespace: https://internal.hail.is/dking/site/). I've compared both on my DPR 2 monitor and my DPR 1 monitor. . The reason I prefer the change linked above is that it is self-describing: we want the thinest possible lines that the monitor supports and we want them to be a bit faint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645699918
https://github.com/hail-is/hail/pull/8964#issuecomment-645707340:66,Energy Efficiency,monitor,monitor,66,"> This isn't a browser thing,. It is a browser thing as well as a monitor thing, as far as I can see. I linked to a chromium bug that specifically identified the issue as a browser thing. There is also some difference that I cannot find a bug report for Safari that makes line width unequal between Safari and Chromium browsers, unless I set Safari's line width to <1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645707340
https://github.com/hail-is/hail/pull/8964#issuecomment-646218723:425,Deployability,install,installed,425,"I believe you're referencing [Chrome Bug 675308](https://bugs.chromium.org/p/chromium/issues/detail?id=675308) which suggests that lines thicker than 1 are broken in some versions of Chrome. My change uses line width 1. The screenshots you shared seem to suggest that Brave is making the lines somewhat fainter than Safari. I can't explain that. The visuals look the same to me across Chrome and Safari. I don't have Firefox installed to check a non-WebKit renderer. At this pointI feel that I have said my piece. It feels brittle to fiddle with device pixel ratio and line width to try and simulate line widths smaller than a pixel. If you think the only acceptable solution is the proposed changes in this PR, then let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-646218723
https://github.com/hail-is/hail/pull/8964#issuecomment-646227792:899,Testability,test,test,899,"> I believe you're referencing Chrome Bug 675308 which suggests that lines thicker than 1 are broken in some versions of Chrome. My change uses line width 1. My point was that there are browser inconsistencies. > The screenshots you shared seem to suggest that Brave is making the lines somewhat fainter than Safari. I can't explain that. . I also can't explain it, and haven't found a Safari bug that would suggest why this is the case, and yet it is clear from that screenshot that there is a difference. > It feels brittle to fiddle with device pixel ratio and line width to try and simulate line widths smaller than a pixel. My current mental model is that we're just hitting a resolution issue. Internally threejs I believe sets the webgl buffer to pixelRatio * width/height. With resolution 1, resolution seems too low, and this causes fuzzy lines. I don't know why it doesn't work better. To test this hypothesis, I've tried setting pixelRatio to 1 manually on a hidpi display, and it gave a similar fuzzy/thick result, which of course doesn't make any sense if linewidth actually did what it seems it should, and so I agree that not relying on linewidth would be nicer, but would also be additional work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-646227792
https://github.com/hail-is/hail/pull/8964#issuecomment-646227792:452,Usability,clear,clear,452,"> I believe you're referencing Chrome Bug 675308 which suggests that lines thicker than 1 are broken in some versions of Chrome. My change uses line width 1. My point was that there are browser inconsistencies. > The screenshots you shared seem to suggest that Brave is making the lines somewhat fainter than Safari. I can't explain that. . I also can't explain it, and haven't found a Safari bug that would suggest why this is the case, and yet it is clear from that screenshot that there is a difference. > It feels brittle to fiddle with device pixel ratio and line width to try and simulate line widths smaller than a pixel. My current mental model is that we're just hitting a resolution issue. Internally threejs I believe sets the webgl buffer to pixelRatio * width/height. With resolution 1, resolution seems too low, and this causes fuzzy lines. I don't know why it doesn't work better. To test this hypothesis, I've tried setting pixelRatio to 1 manually on a hidpi display, and it gave a similar fuzzy/thick result, which of course doesn't make any sense if linewidth actually did what it seems it should, and so I agree that not relying on linewidth would be nicer, but would also be additional work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-646227792
https://github.com/hail-is/hail/pull/8968#issuecomment-644398412:113,Performance,perform,performance,113,It looks like pylint/flake8 don't like the import structure. I say we just import them! I don't think it affects performance in any meaningful way.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-644398412
https://github.com/hail-is/hail/pull/8968#issuecomment-644774393:51,Usability,learn,learned,51,"I'll have to think about the circular thing, but I learned that; ```; from __future__ import annotations; ```; let's you use names directly rather than as strings.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-644774393
https://github.com/hail-is/hail/pull/8968#issuecomment-662143859:341,Deployability,install,installing,341,"@jigold this should pass now. I also learned that Mypy checks each command line argument as an independent module. Instead of specifying individual files, we have to tell it to check `batch`. I also added `google_storage.py` in an ill-fated attempt to let batch use that. I think we can only type check modules that depend on one another by installing them first. We'll leave that for future work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-662143859
https://github.com/hail-is/hail/pull/8968#issuecomment-662143859:316,Integrability,depend,depend,316,"@jigold this should pass now. I also learned that Mypy checks each command line argument as an independent module. Instead of specifying individual files, we have to tell it to check `batch`. I also added `google_storage.py` in an ill-fated attempt to let batch use that. I think we can only type check modules that depend on one another by installing them first. We'll leave that for future work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-662143859
https://github.com/hail-is/hail/pull/8968#issuecomment-662143859:37,Usability,learn,learned,37,"@jigold this should pass now. I also learned that Mypy checks each command line argument as an independent module. Instead of specifying individual files, we have to tell it to check `batch`. I also added `google_storage.py` in an ill-fated attempt to let batch use that. I think we can only type check modules that depend on one another by installing them first. We'll leave that for future work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-662143859
https://github.com/hail-is/hail/pull/8968#issuecomment-662453343:24,Availability,error,errors,24,There's some major test errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-662453343
https://github.com/hail-is/hail/pull/8968#issuecomment-662453343:19,Testability,test,test,19,There's some major test errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-662453343
https://github.com/hail-is/hail/pull/8970#issuecomment-645082713:667,Energy Efficiency,allocate,allocate,667,"So I don't think this is quite correct. malloc(0) is implementation defined, it either returns a valid pointer which cannot be dereferenced, or 0: https://stackoverflow.com/a/2022402. memcpy with a 0 source is undefined: https://stackoverflow.com/questions/5243012/is-it-guaranteed-to-be-safe-to-perform-memcpy0-0-0. There is some debate in the comments to the first answer about whether any reasonable memcpy implementation would do anything if the length is 0. However, see the second answer in the link above: gcc improves optimization by assuming the undefined behavior cannot happen. So our options seem to be:. 1. Leave this change, but change Memory.malloc to allocate at least one byte (guaranteeing it never returns 0). 2. Revert this change, and assume memcpy with 0 length is safe and weaken the memcpy check. 3. Revert this change, don't assume memcpy of 0 length is safe, and have Memory.memcpy check the length before calling into the Unsafe memcpy implementation. I'm inclined to do 3 in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8970#issuecomment-645082713
https://github.com/hail-is/hail/pull/8970#issuecomment-645082713:296,Performance,perform,perform-,296,"So I don't think this is quite correct. malloc(0) is implementation defined, it either returns a valid pointer which cannot be dereferenced, or 0: https://stackoverflow.com/a/2022402. memcpy with a 0 source is undefined: https://stackoverflow.com/questions/5243012/is-it-guaranteed-to-be-safe-to-perform-memcpy0-0-0. There is some debate in the comments to the first answer about whether any reasonable memcpy implementation would do anything if the length is 0. However, see the second answer in the link above: gcc improves optimization by assuming the undefined behavior cannot happen. So our options seem to be:. 1. Leave this change, but change Memory.malloc to allocate at least one byte (guaranteeing it never returns 0). 2. Revert this change, and assume memcpy with 0 length is safe and weaken the memcpy check. 3. Revert this change, don't assume memcpy of 0 length is safe, and have Memory.memcpy check the length before calling into the Unsafe memcpy implementation. I'm inclined to do 3 in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8970#issuecomment-645082713
https://github.com/hail-is/hail/pull/8970#issuecomment-645082713:526,Performance,optimiz,optimization,526,"So I don't think this is quite correct. malloc(0) is implementation defined, it either returns a valid pointer which cannot be dereferenced, or 0: https://stackoverflow.com/a/2022402. memcpy with a 0 source is undefined: https://stackoverflow.com/questions/5243012/is-it-guaranteed-to-be-safe-to-perform-memcpy0-0-0. There is some debate in the comments to the first answer about whether any reasonable memcpy implementation would do anything if the length is 0. However, see the second answer in the link above: gcc improves optimization by assuming the undefined behavior cannot happen. So our options seem to be:. 1. Leave this change, but change Memory.malloc to allocate at least one byte (guaranteeing it never returns 0). 2. Revert this change, and assume memcpy with 0 length is safe and weaken the memcpy check. 3. Revert this change, don't assume memcpy of 0 length is safe, and have Memory.memcpy check the length before calling into the Unsafe memcpy implementation. I'm inclined to do 3 in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8970#issuecomment-645082713
https://github.com/hail-is/hail/pull/8970#issuecomment-645082713:288,Safety,safe,safe-to-perform-,288,"So I don't think this is quite correct. malloc(0) is implementation defined, it either returns a valid pointer which cannot be dereferenced, or 0: https://stackoverflow.com/a/2022402. memcpy with a 0 source is undefined: https://stackoverflow.com/questions/5243012/is-it-guaranteed-to-be-safe-to-perform-memcpy0-0-0. There is some debate in the comments to the first answer about whether any reasonable memcpy implementation would do anything if the length is 0. However, see the second answer in the link above: gcc improves optimization by assuming the undefined behavior cannot happen. So our options seem to be:. 1. Leave this change, but change Memory.malloc to allocate at least one byte (guaranteeing it never returns 0). 2. Revert this change, and assume memcpy with 0 length is safe and weaken the memcpy check. 3. Revert this change, don't assume memcpy of 0 length is safe, and have Memory.memcpy check the length before calling into the Unsafe memcpy implementation. I'm inclined to do 3 in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8970#issuecomment-645082713
https://github.com/hail-is/hail/pull/8970#issuecomment-645082713:787,Safety,safe,safe,787,"So I don't think this is quite correct. malloc(0) is implementation defined, it either returns a valid pointer which cannot be dereferenced, or 0: https://stackoverflow.com/a/2022402. memcpy with a 0 source is undefined: https://stackoverflow.com/questions/5243012/is-it-guaranteed-to-be-safe-to-perform-memcpy0-0-0. There is some debate in the comments to the first answer about whether any reasonable memcpy implementation would do anything if the length is 0. However, see the second answer in the link above: gcc improves optimization by assuming the undefined behavior cannot happen. So our options seem to be:. 1. Leave this change, but change Memory.malloc to allocate at least one byte (guaranteeing it never returns 0). 2. Revert this change, and assume memcpy with 0 length is safe and weaken the memcpy check. 3. Revert this change, don't assume memcpy of 0 length is safe, and have Memory.memcpy check the length before calling into the Unsafe memcpy implementation. I'm inclined to do 3 in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8970#issuecomment-645082713
https://github.com/hail-is/hail/pull/8970#issuecomment-645082713:879,Safety,safe,safe,879,"So I don't think this is quite correct. malloc(0) is implementation defined, it either returns a valid pointer which cannot be dereferenced, or 0: https://stackoverflow.com/a/2022402. memcpy with a 0 source is undefined: https://stackoverflow.com/questions/5243012/is-it-guaranteed-to-be-safe-to-perform-memcpy0-0-0. There is some debate in the comments to the first answer about whether any reasonable memcpy implementation would do anything if the length is 0. However, see the second answer in the link above: gcc improves optimization by assuming the undefined behavior cannot happen. So our options seem to be:. 1. Leave this change, but change Memory.malloc to allocate at least one byte (guaranteeing it never returns 0). 2. Revert this change, and assume memcpy with 0 length is safe and weaken the memcpy check. 3. Revert this change, don't assume memcpy of 0 length is safe, and have Memory.memcpy check the length before calling into the Unsafe memcpy implementation. I'm inclined to do 3 in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8970#issuecomment-645082713
https://github.com/hail-is/hail/pull/8970#issuecomment-645082713:949,Safety,Unsafe,Unsafe,949,"So I don't think this is quite correct. malloc(0) is implementation defined, it either returns a valid pointer which cannot be dereferenced, or 0: https://stackoverflow.com/a/2022402. memcpy with a 0 source is undefined: https://stackoverflow.com/questions/5243012/is-it-guaranteed-to-be-safe-to-perform-memcpy0-0-0. There is some debate in the comments to the first answer about whether any reasonable memcpy implementation would do anything if the length is 0. However, see the second answer in the link above: gcc improves optimization by assuming the undefined behavior cannot happen. So our options seem to be:. 1. Leave this change, but change Memory.malloc to allocate at least one byte (guaranteeing it never returns 0). 2. Revert this change, and assume memcpy with 0 length is safe and weaken the memcpy check. 3. Revert this change, don't assume memcpy of 0 length is safe, and have Memory.memcpy check the length before calling into the Unsafe memcpy implementation. I'm inclined to do 3 in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8970#issuecomment-645082713
https://github.com/hail-is/hail/pull/8971#issuecomment-644964603:81,Deployability,deploy,deploy,81,fusermount needs root privileges which the container has. I tested this with dev deploy and made sure things were being unmounted properly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8971#issuecomment-644964603
https://github.com/hail-is/hail/pull/8971#issuecomment-644964603:60,Testability,test,tested,60,fusermount needs root privileges which the container has. I tested this with dev deploy and made sure things were being unmounted properly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8971#issuecomment-644964603
https://github.com/hail-is/hail/pull/8997#issuecomment-646921877:46,Deployability,deploy,deploy,46,"The target creates an egg in `hail/hail/build/deploy/dist/`, alongside the wheel.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8997#issuecomment-646921877
https://github.com/hail-is/hail/pull/9011#issuecomment-648947947:13,Testability,benchmark,benchmark,13,Do we have a benchmark or similar to show the improvement here?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9011#issuecomment-648947947
https://github.com/hail-is/hail/pull/9011#issuecomment-706201324:78,Deployability,update,updates,78,Trying to debug a CI issue and this PR's status has exceeded the number of CI updates due to it being rather old. I'll reopen once I've figured out CI's issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9011#issuecomment-706201324
https://github.com/hail-is/hail/pull/9011#issuecomment-706280238:29,Testability,benchmark,benchmarks,29,"Whoops! I've not written any benchmarks for this yet and at this point, it's old enough that I'm just going to close this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9011#issuecomment-706280238
https://github.com/hail-is/hail/pull/9030#issuecomment-651918109:96,Performance,Optimiz,OptimizePass,96,"I removed the assertions about the chain of IRState properties. We could take the parameters in OptimizePass, but that seems a bit hacky/unnecessary. The IRStates are checked at runtime, so I'm confident we'll still be able to debug issues.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9030#issuecomment-651918109
https://github.com/hail-is/hail/pull/9030#issuecomment-651918109:14,Testability,assert,assertions,14,"I removed the assertions about the chain of IRState properties. We could take the parameters in OptimizePass, but that seems a bit hacky/unnecessary. The IRStates are checked at runtime, so I'm confident we'll still be able to debug issues.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9030#issuecomment-651918109
https://github.com/hail-is/hail/pull/9030#issuecomment-651982433:462,Modifiability,refactor,refactor,462,"Oh, I didn't realize we're running pre- and post-condition checks on every pass. I would think the main reason to require the post-condition of one pass to match the pre-condition of the next (besides general hygiene) is to only have to do the check once. Anyways, I agree this seems fine for now. I think the root of the issue is that we're assuming that `Optimize` preserves all possible `IRState`s, which is a pretty bold claim. Eventually we should probably refactor `Optimize` to separate out rules that apply to different `IRState`s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9030#issuecomment-651982433
https://github.com/hail-is/hail/pull/9030#issuecomment-651982433:357,Performance,Optimiz,Optimize,357,"Oh, I didn't realize we're running pre- and post-condition checks on every pass. I would think the main reason to require the post-condition of one pass to match the pre-condition of the next (besides general hygiene) is to only have to do the check once. Anyways, I agree this seems fine for now. I think the root of the issue is that we're assuming that `Optimize` preserves all possible `IRState`s, which is a pretty bold claim. Eventually we should probably refactor `Optimize` to separate out rules that apply to different `IRState`s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9030#issuecomment-651982433
https://github.com/hail-is/hail/pull/9030#issuecomment-651982433:472,Performance,Optimiz,Optimize,472,"Oh, I didn't realize we're running pre- and post-condition checks on every pass. I would think the main reason to require the post-condition of one pass to match the pre-condition of the next (besides general hygiene) is to only have to do the check once. Anyways, I agree this seems fine for now. I think the root of the issue is that we're assuming that `Optimize` preserves all possible `IRState`s, which is a pretty bold claim. Eventually we should probably refactor `Optimize` to separate out rules that apply to different `IRState`s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9030#issuecomment-651982433
https://github.com/hail-is/hail/pull/9035#issuecomment-658331383:20,Availability,failure,failures,20,You still have test failures.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9035#issuecomment-658331383
https://github.com/hail-is/hail/pull/9035#issuecomment-658331383:15,Testability,test,test,15,You still have test failures.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9035#issuecomment-658331383
https://github.com/hail-is/hail/pull/9035#issuecomment-661269154:4,Testability,test,tests,4,The tests will pass now. I had to disable output checking in the doctests because `Batch.run` prints batch ids and urls which would be annoying to maintain.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9035#issuecomment-661269154
https://github.com/hail-is/hail/pull/9044#issuecomment-652536769:287,Performance,load,load,287,This is the start of a series of changes that will culminate in the generation of static ordering methods between PTypes. The next step here is to get reference genomes into static container classes as well. We need this change first because; reference genomes may use the filesystem to load a fasta.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-652536769
https://github.com/hail-is/hail/pull/9044#issuecomment-652673222:294,Security,secur,security,294,"> The first usage of this feature is to move the FS off of the class itself and onto a container class. Is the container class generated globally? That's going to be a problem. The service backend has multiple FSes for multiple, different users. Keeping those FSes separate is **critical** for security. The reference genomes are the same story. Each service query will come with its own, distinct context of reference genomes, and they can't get mixed up. In particular, a reference genome sequence may be sensitive data and can't be exposed to other users. The reference genome global state will only be stored in the Python client. Removing the global reference state from the Scala side is a pending Query service project.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-652673222
https://github.com/hail-is/hail/pull/9044#issuecomment-652673222:535,Security,expose,exposed,535,"> The first usage of this feature is to move the FS off of the class itself and onto a container class. Is the container class generated globally? That's going to be a problem. The service backend has multiple FSes for multiple, different users. Keeping those FSes separate is **critical** for security. The reference genomes are the same story. Each service query will come with its own, distinct context of reference genomes, and they can't get mixed up. In particular, a reference genome sequence may be sensitive data and can't be exposed to other users. The reference genome global state will only be stored in the Python client. Removing the global reference state from the Scala side is a pending Query service project.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-652673222
https://github.com/hail-is/hail/pull/9044#issuecomment-652737719:39,Deployability,pipeline,pipeline,39,"My assumption has been that each fresh pipeline should get a new `EmitModuleBuilder` and `ExecuteContext`, if this is the case, I believe what I have done here should be alright.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-652737719
https://github.com/hail-is/hail/pull/9044#issuecomment-652761221:41,Deployability,pipeline,pipeline,41,"> My assumption has been that each fresh pipeline should get a new EmitModuleBuilder and ExecuteContext, if this is the case, I believe what I have done here should be alright. OK, great. I jumped the gun. But something to think about in places we retry to share across invocations of Compile. > The first usage of this feature is to move the FS off of the class itself and onto a container class. If the the FS field in the container class is static, how does it get serialized? The point is the C returned by EmitClassBuilder.resultWithIndex captures and serializes the FS. > Furthermore, we can't have static comparison functions without static reference genomes, so a solution will need to be found. This is a step in that direction. The RG can be serialized. That's what EmitClassBuilder.getReferenceGenome does. So you can put the reference genomes in static classes that decode them lazily, and then the comparison functions and main code can call into those static methods. This doesn't work for FS because (currently) it can't be serialised as code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-652761221
https://github.com/hail-is/hail/pull/9044#issuecomment-652767079:846,Integrability,interface,interface,846,"There are code paths where ReferenceGenome's `codeSetup` invokes the filesystem. Those must be handled in a static context in order for this to work at all. https://github.com/tpoterba/hail/blob/e5b3fb3d2fba971d0c226094ae0c7ac66f190bbe/hail/src/main/scala/is/hail/variant/ReferenceGenome.scala#L493-L512; ```scala; if (fastaReader != null) {; rg = rg.invoke[String, FS, String, String, Int, Int, ReferenceGenome](; ""addSequenceFromReader"",; localTmpdir,; cb.getFS,; fastaReader.fastaFile,; fastaReader.indexFile,; fastaReader.blockSize,; fastaReader.capacity); }. for ((destRG, lo) <- liftoverMaps) {; rg = rg.invoke[String, FS, String, String, ReferenceGenome](; ""addLiftoverFromFS"",; localTmpdir,; cb.getFS,; lo.chainFile,; destRG); }; rg; ```. The static isn't final or anything, the setter is still currently called from the `FunctionWithFS` interface's `addFS` method, so the `resultWithIndex` closure is still what captures and saves the FS. Like I said, needs some work to make it work always.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-652767079
https://github.com/hail-is/hail/pull/9044#issuecomment-653121962:136,Performance,load,load,136,"I figured it out, `FS` is already searialazable, so we can just serialize the data into the class at compile time and deserialize it at load time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-653121962
https://github.com/hail-is/hail/pull/9049#issuecomment-653485568:136,Testability,test,tested,136,"This is very nice. There are three while loops in BgenRDDPartitions.scala that could easily be translated to for loops, so that it gets tested",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9049#issuecomment-653485568
https://github.com/hail-is/hail/pull/9061#issuecomment-655652327:53,Testability,test,tests,53,"Looks like there were some more `hl._nd`'s around in tests. Sorry about that, I can fix if you'd like.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9061#issuecomment-655652327
https://github.com/hail-is/hail/pull/9063#issuecomment-661191810:45,Integrability,interface,interface,45,"@patrick-schultz I only looked at the Python interface, so I would much appreciate your thoughts on the Scala stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9063#issuecomment-661191810
https://github.com/hail-is/hail/pull/9071#issuecomment-656719287:44,Testability,test,tests,44,"That looks like it should work (though some tests are failing), but rekeying by the global index might be more expensive than necessary. What about using `multi_way_zip_join`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9071#issuecomment-656719287
https://github.com/hail-is/hail/pull/9074#issuecomment-658273654:28,Deployability,deploy,deploy,28,Closing for now while I dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9074#issuecomment-658273654
https://github.com/hail-is/hail/pull/9076#issuecomment-657239708:224,Availability,error,error,224,"This is working except I can't figure out how to delete entries from /etc/projects and /etc/projid when the job finishes. You can't copy or move a file to a bind-mounted file in Docker or you get a ""resource or device busy"" error. This will all work without deleting the entries, but then every time you try and add a new project, you get a directory not found error for previously deleted paths, but the operation will continue successfully ignoring the errors. Any ideas on how to get around this? `sed -i` won't work as well as `mv` and `cp`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-657239708
https://github.com/hail-is/hail/pull/9076#issuecomment-657239708:361,Availability,error,error,361,"This is working except I can't figure out how to delete entries from /etc/projects and /etc/projid when the job finishes. You can't copy or move a file to a bind-mounted file in Docker or you get a ""resource or device busy"" error. This will all work without deleting the entries, but then every time you try and add a new project, you get a directory not found error for previously deleted paths, but the operation will continue successfully ignoring the errors. Any ideas on how to get around this? `sed -i` won't work as well as `mv` and `cp`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-657239708
https://github.com/hail-is/hail/pull/9076#issuecomment-657239708:455,Availability,error,errors,455,"This is working except I can't figure out how to delete entries from /etc/projects and /etc/projid when the job finishes. You can't copy or move a file to a bind-mounted file in Docker or you get a ""resource or device busy"" error. This will all work without deleting the entries, but then every time you try and add a new project, you get a directory not found error for previously deleted paths, but the operation will continue successfully ignoring the errors. Any ideas on how to get around this? `sed -i` won't work as well as `mv` and `cp`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-657239708
https://github.com/hail-is/hail/pull/9076#issuecomment-658827248:24,Testability,test,tests,24,This is good to go now (tests are passing and the xfsquota stuff looks right when I looked at a worker by hand).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-658827248
https://github.com/hail-is/hail/pull/9076#issuecomment-662462004:141,Performance,cache,cache,141,"Got it! You need to add the loop device. Now how to parse the correct thing from the xfs_info output is another story... ```; jigold@jg-file-cache:~$ xfs_info /mnt/test_xfs; meta-data=/dev/loop2 isize=512 agcount=4, agsize=65536 blks; = sectsz=512 attr=2, projid32bit=1; = crc=1 finobt=1 spinodes=0 rmapbt=0; = reflink=1; data = bsize=4096 blocks=262144, imaxpct=25; = sunit=0 swidth=0 blks; naming =version 2 bsize=4096 ascii-ci=0 ftype=1; log =internal bsize=4096 blocks=2560, version=2; = sectsz=512 sunit=0 blks, lazy-count=1; realtime =none extsz=4096 blocks=0, rtextents=0; ```. ```; jigold@jg-file-cache:~$ sudo docker run --rm --mount type=bind,source=/mnt/test_xfs,target=/host --cap-add SYS_ADMIN --security-opt apparmor:unconfined --device ""/dev/loop2:/dev/loop2:rwm"" test-xfs /bin/bash -c 'xfs_quota -x -c ""report -h"" /host'; Project quota on /host (/dev/loop2); Blocks; Project ID Used Soft Hard Warn/Grace; ---------- ---------------------------------; #0 4K 0 0 00 [------]; #200 0 0 0 00 [------]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-662462004
https://github.com/hail-is/hail/pull/9076#issuecomment-662462004:605,Performance,cache,cache,605,"Got it! You need to add the loop device. Now how to parse the correct thing from the xfs_info output is another story... ```; jigold@jg-file-cache:~$ xfs_info /mnt/test_xfs; meta-data=/dev/loop2 isize=512 agcount=4, agsize=65536 blks; = sectsz=512 attr=2, projid32bit=1; = crc=1 finobt=1 spinodes=0 rmapbt=0; = reflink=1; data = bsize=4096 blocks=262144, imaxpct=25; = sunit=0 swidth=0 blks; naming =version 2 bsize=4096 ascii-ci=0 ftype=1; log =internal bsize=4096 blocks=2560, version=2; = sectsz=512 sunit=0 blks, lazy-count=1; realtime =none extsz=4096 blocks=0, rtextents=0; ```. ```; jigold@jg-file-cache:~$ sudo docker run --rm --mount type=bind,source=/mnt/test_xfs,target=/host --cap-add SYS_ADMIN --security-opt apparmor:unconfined --device ""/dev/loop2:/dev/loop2:rwm"" test-xfs /bin/bash -c 'xfs_quota -x -c ""report -h"" /host'; Project quota on /host (/dev/loop2); Blocks; Project ID Used Soft Hard Warn/Grace; ---------- ---------------------------------; #0 4K 0 0 00 [------]; #200 0 0 0 00 [------]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-662462004
https://github.com/hail-is/hail/pull/9076#issuecomment-662462004:709,Security,secur,security-opt,709,"Got it! You need to add the loop device. Now how to parse the correct thing from the xfs_info output is another story... ```; jigold@jg-file-cache:~$ xfs_info /mnt/test_xfs; meta-data=/dev/loop2 isize=512 agcount=4, agsize=65536 blks; = sectsz=512 attr=2, projid32bit=1; = crc=1 finobt=1 spinodes=0 rmapbt=0; = reflink=1; data = bsize=4096 blocks=262144, imaxpct=25; = sunit=0 swidth=0 blks; naming =version 2 bsize=4096 ascii-ci=0 ftype=1; log =internal bsize=4096 blocks=2560, version=2; = sectsz=512 sunit=0 blks, lazy-count=1; realtime =none extsz=4096 blocks=0, rtextents=0; ```. ```; jigold@jg-file-cache:~$ sudo docker run --rm --mount type=bind,source=/mnt/test_xfs,target=/host --cap-add SYS_ADMIN --security-opt apparmor:unconfined --device ""/dev/loop2:/dev/loop2:rwm"" test-xfs /bin/bash -c 'xfs_quota -x -c ""report -h"" /host'; Project quota on /host (/dev/loop2); Blocks; Project ID Used Soft Hard Warn/Grace; ---------- ---------------------------------; #0 4K 0 0 00 [------]; #200 0 0 0 00 [------]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-662462004
https://github.com/hail-is/hail/pull/9076#issuecomment-662462004:441,Testability,log,log,441,"Got it! You need to add the loop device. Now how to parse the correct thing from the xfs_info output is another story... ```; jigold@jg-file-cache:~$ xfs_info /mnt/test_xfs; meta-data=/dev/loop2 isize=512 agcount=4, agsize=65536 blks; = sectsz=512 attr=2, projid32bit=1; = crc=1 finobt=1 spinodes=0 rmapbt=0; = reflink=1; data = bsize=4096 blocks=262144, imaxpct=25; = sunit=0 swidth=0 blks; naming =version 2 bsize=4096 ascii-ci=0 ftype=1; log =internal bsize=4096 blocks=2560, version=2; = sectsz=512 sunit=0 blks, lazy-count=1; realtime =none extsz=4096 blocks=0, rtextents=0; ```. ```; jigold@jg-file-cache:~$ sudo docker run --rm --mount type=bind,source=/mnt/test_xfs,target=/host --cap-add SYS_ADMIN --security-opt apparmor:unconfined --device ""/dev/loop2:/dev/loop2:rwm"" test-xfs /bin/bash -c 'xfs_quota -x -c ""report -h"" /host'; Project quota on /host (/dev/loop2); Blocks; Project ID Used Soft Hard Warn/Grace; ---------- ---------------------------------; #0 4K 0 0 00 [------]; #200 0 0 0 00 [------]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-662462004
https://github.com/hail-is/hail/pull/9076#issuecomment-662462004:779,Testability,test,test-xfs,779,"Got it! You need to add the loop device. Now how to parse the correct thing from the xfs_info output is another story... ```; jigold@jg-file-cache:~$ xfs_info /mnt/test_xfs; meta-data=/dev/loop2 isize=512 agcount=4, agsize=65536 blks; = sectsz=512 attr=2, projid32bit=1; = crc=1 finobt=1 spinodes=0 rmapbt=0; = reflink=1; data = bsize=4096 blocks=262144, imaxpct=25; = sunit=0 swidth=0 blks; naming =version 2 bsize=4096 ascii-ci=0 ftype=1; log =internal bsize=4096 blocks=2560, version=2; = sectsz=512 sunit=0 blks, lazy-count=1; realtime =none extsz=4096 blocks=0, rtextents=0; ```. ```; jigold@jg-file-cache:~$ sudo docker run --rm --mount type=bind,source=/mnt/test_xfs,target=/host --cap-add SYS_ADMIN --security-opt apparmor:unconfined --device ""/dev/loop2:/dev/loop2:rwm"" test-xfs /bin/bash -c 'xfs_quota -x -c ""report -h"" /host'; Project quota on /host (/dev/loop2); Blocks; Project ID Used Soft Hard Warn/Grace; ---------- ---------------------------------; #0 4K 0 0 00 [------]; #200 0 0 0 00 [------]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-662462004
https://github.com/hail-is/hail/pull/9076#issuecomment-662526023:82,Availability,down,down,82,"haha, you are an Awk Queen! This looks great to me, I'm glad we were able to nail down the security.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-662526023
https://github.com/hail-is/hail/pull/9076#issuecomment-662526023:91,Security,secur,security,91,"haha, you are an Awk Queen! This looks great to me, I'm glad we were able to nail down the security.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-662526023
https://github.com/hail-is/hail/pull/9076#issuecomment-664619127:31,Testability,benchmark,benchmark,31,@danking I need to address the benchmark storage requests to not be 100 Gi.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-664619127
https://github.com/hail-is/hail/pull/9076#issuecomment-664654683:50,Testability,Benchmark,Benchmarks,50,I don't think we should hold up this PR for that. Benchmarks are run rarely and they should be setting their limits using PYSPARK_SUBMIT_ARGS anyway.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-664654683
https://github.com/hail-is/hail/pull/9085#issuecomment-658826248:58,Availability,down,down,58,"Note that we never see trisomy 22, but do see trisomy 21 (down)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9085#issuecomment-658826248
https://github.com/hail-is/hail/pull/9089#issuecomment-666769626:17,Testability,benchmark,benchmark,17,"Curious, did you benchmark this change, in particular the local/field business?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9089#issuecomment-666769626
https://github.com/hail-is/hail/pull/9089#issuecomment-667081619:91,Testability,benchmark,benchmark,91,"if there's an effect, I expect we'll see it when we do whole-stage codegen, which we can't benchmark now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9089#issuecomment-667081619
https://github.com/hail-is/hail/pull/9094#issuecomment-660299878:135,Usability,simpl,simpler,135,"> I also think this should just be defined inline in the makeNDArray emitter, we shouldn't need to change this file. Good point that's simpler",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9094#issuecomment-660299878
https://github.com/hail-is/hail/pull/9095#issuecomment-660178082:169,Availability,down,downloading,169,It looks like my cache change is passing tests now. I'd like for you to take a look before I confirm one last time that the cache is actually working by submitting jobs downloading a 512 MB file and making sure the timings of the non-first job is a couple of seconds. It looks like the tests got a bit slower. I'm not sure if that's because of the docker image having gsutil in it. I don't see how the extra copying infrastructure would make a huge difference.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-660178082
https://github.com/hail-is/hail/pull/9095#issuecomment-660178082:17,Performance,cache,cache,17,It looks like my cache change is passing tests now. I'd like for you to take a look before I confirm one last time that the cache is actually working by submitting jobs downloading a 512 MB file and making sure the timings of the non-first job is a couple of seconds. It looks like the tests got a bit slower. I'm not sure if that's because of the docker image having gsutil in it. I don't see how the extra copying infrastructure would make a huge difference.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-660178082
https://github.com/hail-is/hail/pull/9095#issuecomment-660178082:124,Performance,cache,cache,124,It looks like my cache change is passing tests now. I'd like for you to take a look before I confirm one last time that the cache is actually working by submitting jobs downloading a 512 MB file and making sure the timings of the non-first job is a couple of seconds. It looks like the tests got a bit slower. I'm not sure if that's because of the docker image having gsutil in it. I don't see how the extra copying infrastructure would make a huge difference.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-660178082
https://github.com/hail-is/hail/pull/9095#issuecomment-660178082:41,Testability,test,tests,41,It looks like my cache change is passing tests now. I'd like for you to take a look before I confirm one last time that the cache is actually working by submitting jobs downloading a 512 MB file and making sure the timings of the non-first job is a couple of seconds. It looks like the tests got a bit slower. I'm not sure if that's because of the docker image having gsutil in it. I don't see how the extra copying infrastructure would make a huge difference.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-660178082
https://github.com/hail-is/hail/pull/9095#issuecomment-660178082:286,Testability,test,tests,286,It looks like my cache change is passing tests now. I'd like for you to take a look before I confirm one last time that the cache is actually working by submitting jobs downloading a 512 MB file and making sure the timings of the non-first job is a couple of seconds. It looks like the tests got a bit slower. I'm not sure if that's because of the docker image having gsutil in it. I don't see how the extra copying infrastructure would make a huge difference.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-660178082
https://github.com/hail-is/hail/pull/9095#issuecomment-665855101:370,Deployability,deploy,deploy,370,"@danking I think I addressed most of your comments. Everything is passing. Can you look if you like the interface for `sync_check_call` etc.? If so, then can I translate your TLS commands in lists into a single string? Is there ever a reason to want to pass the list explicitly in this case?. Once you're happy with the changes, then I'll do another test round with dev deploy to make sure the cache is actually doing its thing and the garbage collection loop is working.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-665855101
https://github.com/hail-is/hail/pull/9095#issuecomment-665855101:104,Integrability,interface,interface,104,"@danking I think I addressed most of your comments. Everything is passing. Can you look if you like the interface for `sync_check_call` etc.? If so, then can I translate your TLS commands in lists into a single string? Is there ever a reason to want to pass the list explicitly in this case?. Once you're happy with the changes, then I'll do another test round with dev deploy to make sure the cache is actually doing its thing and the garbage collection loop is working.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-665855101
https://github.com/hail-is/hail/pull/9095#issuecomment-665855101:394,Performance,cache,cache,394,"@danking I think I addressed most of your comments. Everything is passing. Can you look if you like the interface for `sync_check_call` etc.? If so, then can I translate your TLS commands in lists into a single string? Is there ever a reason to want to pass the list explicitly in this case?. Once you're happy with the changes, then I'll do another test round with dev deploy to make sure the cache is actually doing its thing and the garbage collection loop is working.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-665855101
https://github.com/hail-is/hail/pull/9095#issuecomment-665855101:350,Testability,test,test,350,"@danking I think I addressed most of your comments. Everything is passing. Can you look if you like the interface for `sync_check_call` etc.? If so, then can I translate your TLS commands in lists into a single string? Is there ever a reason to want to pass the list explicitly in this case?. Once you're happy with the changes, then I'll do another test round with dev deploy to make sure the cache is actually doing its thing and the garbage collection loop is working.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-665855101
https://github.com/hail-is/hail/pull/9095#issuecomment-672126602:20,Deployability,deploy,deploy,20,Closing while I dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-672126602
https://github.com/hail-is/hail/pull/9096#issuecomment-660163759:18,Testability,test,test,18,I also made a new test bucket specifically for the tests -- gs://hail-services-requester-pays/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-660163759
https://github.com/hail-is/hail/pull/9096#issuecomment-660163759:51,Testability,test,tests,51,I also made a new test bucket specifically for the tests -- gs://hail-services-requester-pays/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-660163759
https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:26,Availability,error,error,26,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952
https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:625,Security,Access,AccessDeniedException,625,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952
https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:734,Security,access,access,734,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952
https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:948,Security,Access,AccessDeniedException,948,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952
https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:1057,Security,access,access,1057,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952
https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:1271,Security,Access,AccessDeniedException,1271,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952
https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:1380,Security,access,access,1380,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952
https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:93,Testability,test,test,93,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952
https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:319,Testability,test,test-,319,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952
https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:652,Testability,test,test-,652,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952
https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:975,Testability,test,test-,975,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952
https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:1298,Testability,test,test-,1298,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952
https://github.com/hail-is/hail/pull/9096#issuecomment-662119484:44,Testability,test,test,44,Seems like the correct privilege to get the test account so that it can use requestor pays.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662119484
https://github.com/hail-is/hail/pull/9105#issuecomment-661129687:290,Modifiability,extend,extend,290,"Rather than linking out to Numpy, I added an identity function. In a followup PR I will add the ""k"" argument to `eye` (I have no need for it now, feel free to PR if you want). Until then. `eye` differs from `identity` in that it can generate arbitrary rectangular matrices. Adding ""k"" will extend that to enable non-main diagonals",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9105#issuecomment-661129687
https://github.com/hail-is/hail/pull/9106#issuecomment-661867074:70,Performance,perform,performance,70,"high level comment -- this is an infrastructure change that will hurt performance. We need to know by how much, and probably add benchmarks that target the specific cases where the added work will hurt most. If things get noticeably slower, we're probably going to need to rework streams so that per-row stream code doesn't incur memory management overhead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661867074
https://github.com/hail-is/hail/pull/9106#issuecomment-661867074:129,Testability,benchmark,benchmarks,129,"high level comment -- this is an infrastructure change that will hurt performance. We need to know by how much, and probably add benchmarks that target the specific cases where the added work will hurt most. If things get noticeably slower, we're probably going to need to rework streams so that per-row stream code doesn't incur memory management overhead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661867074
https://github.com/hail-is/hail/pull/9106#issuecomment-661877761:608,Availability,down,down,608,"We definitely need a mechanism to force a stream pipeline (or sub-pipeline) to put all allocations in a single region, and avoid any region management overhead. Then, for example, in table lowering we can set a flag on any single-row stream processing to use a single region, preserving the existing behavior. I have some thoughts on how to do that. We can just pass an ""allocator"" to EmitStream, which is a Region factory, that stream nodes must use to create new regions. An allocator that creates new regions gives the ""free between rows"" behavior. To implement the ""within one row"" behavior, we can pass down an allocator that returns regions backed by a single fixed RegionMemory, with no-op freeing. Maybe that needs to be put in place before this can merge?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661877761
https://github.com/hail-is/hail/pull/9106#issuecomment-661877761:49,Deployability,pipeline,pipeline,49,"We definitely need a mechanism to force a stream pipeline (or sub-pipeline) to put all allocations in a single region, and avoid any region management overhead. Then, for example, in table lowering we can set a flag on any single-row stream processing to use a single region, preserving the existing behavior. I have some thoughts on how to do that. We can just pass an ""allocator"" to EmitStream, which is a Region factory, that stream nodes must use to create new regions. An allocator that creates new regions gives the ""free between rows"" behavior. To implement the ""within one row"" behavior, we can pass down an allocator that returns regions backed by a single fixed RegionMemory, with no-op freeing. Maybe that needs to be put in place before this can merge?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661877761
https://github.com/hail-is/hail/pull/9106#issuecomment-661877761:66,Deployability,pipeline,pipeline,66,"We definitely need a mechanism to force a stream pipeline (or sub-pipeline) to put all allocations in a single region, and avoid any region management overhead. Then, for example, in table lowering we can set a flag on any single-row stream processing to use a single region, preserving the existing behavior. I have some thoughts on how to do that. We can just pass an ""allocator"" to EmitStream, which is a Region factory, that stream nodes must use to create new regions. An allocator that creates new regions gives the ""free between rows"" behavior. To implement the ""within one row"" behavior, we can pass down an allocator that returns regions backed by a single fixed RegionMemory, with no-op freeing. Maybe that needs to be put in place before this can merge?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661877761
https://github.com/hail-is/hail/pull/9106#issuecomment-661877761:123,Safety,avoid,avoid,123,"We definitely need a mechanism to force a stream pipeline (or sub-pipeline) to put all allocations in a single region, and avoid any region management overhead. Then, for example, in table lowering we can set a flag on any single-row stream processing to use a single region, preserving the existing behavior. I have some thoughts on how to do that. We can just pass an ""allocator"" to EmitStream, which is a Region factory, that stream nodes must use to create new regions. An allocator that creates new regions gives the ""free between rows"" behavior. To implement the ""within one row"" behavior, we can pass down an allocator that returns regions backed by a single fixed RegionMemory, with no-op freeing. Maybe that needs to be put in place before this can merge?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661877761
https://github.com/hail-is/hail/pull/9106#issuecomment-661888188:11,Availability,down,down,11,"> can pass down an allocator that returns regions backed by a single fixed RegionMemory, with no-op freeing. Even this may be expensive when we're doing something like ToArray(StreamRange). Let's start by benchmarking and seeing where we're at with the current benchmarks?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661888188
https://github.com/hail-is/hail/pull/9106#issuecomment-661888188:205,Testability,benchmark,benchmarking,205,"> can pass down an allocator that returns regions backed by a single fixed RegionMemory, with no-op freeing. Even this may be expensive when we're doing something like ToArray(StreamRange). Let's start by benchmarking and seeing where we're at with the current benchmarks?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661888188
https://github.com/hail-is/hail/pull/9106#issuecomment-661888188:261,Testability,benchmark,benchmarks,261,"> can pass down an allocator that returns regions backed by a single fixed RegionMemory, with no-op freeing. Even this may be expensive when we're doing something like ToArray(StreamRange). Let's start by benchmarking and seeing where we're at with the current benchmarks?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661888188
https://github.com/hail-is/hail/pull/9106#issuecomment-661890102:95,Energy Efficiency,allocate,allocate,95,"> Even this may be expensive. This would all be staged. The generated code would just directly allocate into the provided region, no overhead. But I agree we should benchmark first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661890102
https://github.com/hail-is/hail/pull/9106#issuecomment-661890102:165,Testability,benchmark,benchmark,165,"> Even this may be expensive. This would all be staged. The generated code would just directly allocate into the provided region, no overhead. But I agree we should benchmark first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661890102
https://github.com/hail-is/hail/pull/9106#issuecomment-664591165:5,Deployability,update,updated,5,"I've updated this to allow stream consumers to choose an allocation strategy independently of the producer: the producer is written to allocate temporary regions that it owns, as needed, but the consumer can override that behavior to actually put all temporary allocations in a single region. This is implemented via the two abstract interfaces `StagedRegion` and `StagedOwnedRegion`, which have two implementations, one of which can create temporary regions, and one which cannot. The producer is written using the interface, and the consumer passes in one of the concrete implementations, determining the allocation strategy. Currently, only the non-allocating instance is ever used, which generates the same code we did before, so this should have zero run-time effect, even if we continue to update more stream nodes to use smarter region management. We can figure out separately how to introduce the other strategy (use less memory, but with more allocator overhead) in a controlled way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664591165
https://github.com/hail-is/hail/pull/9106#issuecomment-664591165:796,Deployability,update,update,796,"I've updated this to allow stream consumers to choose an allocation strategy independently of the producer: the producer is written to allocate temporary regions that it owns, as needed, but the consumer can override that behavior to actually put all temporary allocations in a single region. This is implemented via the two abstract interfaces `StagedRegion` and `StagedOwnedRegion`, which have two implementations, one of which can create temporary regions, and one which cannot. The producer is written using the interface, and the consumer passes in one of the concrete implementations, determining the allocation strategy. Currently, only the non-allocating instance is ever used, which generates the same code we did before, so this should have zero run-time effect, even if we continue to update more stream nodes to use smarter region management. We can figure out separately how to introduce the other strategy (use less memory, but with more allocator overhead) in a controlled way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664591165
https://github.com/hail-is/hail/pull/9106#issuecomment-664591165:135,Energy Efficiency,allocate,allocate,135,"I've updated this to allow stream consumers to choose an allocation strategy independently of the producer: the producer is written to allocate temporary regions that it owns, as needed, but the consumer can override that behavior to actually put all temporary allocations in a single region. This is implemented via the two abstract interfaces `StagedRegion` and `StagedOwnedRegion`, which have two implementations, one of which can create temporary regions, and one which cannot. The producer is written using the interface, and the consumer passes in one of the concrete implementations, determining the allocation strategy. Currently, only the non-allocating instance is ever used, which generates the same code we did before, so this should have zero run-time effect, even if we continue to update more stream nodes to use smarter region management. We can figure out separately how to introduce the other strategy (use less memory, but with more allocator overhead) in a controlled way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664591165
https://github.com/hail-is/hail/pull/9106#issuecomment-664591165:334,Integrability,interface,interfaces,334,"I've updated this to allow stream consumers to choose an allocation strategy independently of the producer: the producer is written to allocate temporary regions that it owns, as needed, but the consumer can override that behavior to actually put all temporary allocations in a single region. This is implemented via the two abstract interfaces `StagedRegion` and `StagedOwnedRegion`, which have two implementations, one of which can create temporary regions, and one which cannot. The producer is written using the interface, and the consumer passes in one of the concrete implementations, determining the allocation strategy. Currently, only the non-allocating instance is ever used, which generates the same code we did before, so this should have zero run-time effect, even if we continue to update more stream nodes to use smarter region management. We can figure out separately how to introduce the other strategy (use less memory, but with more allocator overhead) in a controlled way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664591165
https://github.com/hail-is/hail/pull/9106#issuecomment-664591165:516,Integrability,interface,interface,516,"I've updated this to allow stream consumers to choose an allocation strategy independently of the producer: the producer is written to allocate temporary regions that it owns, as needed, but the consumer can override that behavior to actually put all temporary allocations in a single region. This is implemented via the two abstract interfaces `StagedRegion` and `StagedOwnedRegion`, which have two implementations, one of which can create temporary regions, and one which cannot. The producer is written using the interface, and the consumer passes in one of the concrete implementations, determining the allocation strategy. Currently, only the non-allocating instance is ever used, which generates the same code we did before, so this should have zero run-time effect, even if we continue to update more stream nodes to use smarter region management. We can figure out separately how to introduce the other strategy (use less memory, but with more allocator overhead) in a controlled way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664591165
https://github.com/hail-is/hail/pull/9106#issuecomment-664632027:45,Availability,failure,failure,45,"Weird, I can't replicate the `testShuffleIR` failure any more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664632027
https://github.com/hail-is/hail/pull/9106#issuecomment-664632027:30,Testability,test,testShuffleIR,30,"Weird, I can't replicate the `testShuffleIR` failure any more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664632027
https://github.com/hail-is/hail/pull/9106#issuecomment-664632674:34,Availability,failure,failure,34,"yeah, there's a non-deterministic failure, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664632674
https://github.com/hail-is/hail/pull/9106#issuecomment-665034873:1211,Modifiability,refactor,refactors,1211,"The diff in `deforestNDArray` is terrible. I just changed; ```; // in Emit[C].emit(); def emitDeforestedNDArray(ir: IR): EmitCode =; deforestNDArray(ir, mb, region, env).emit(mb, coerce[PNDArray](ir.pType)); ; // in Emit[C]; def deforestNDArray(x: IR, mb: EmitMethodBuilder[C], region: Value[Region], env: E): NDArrayEmitter[C] = {; def deforest(nd: IR): NDArrayEmitter[C] = deforestNDArray(nd, mb, region, env); x match { ... }; }; ; // in NDArrayEmitter[C]; def emit(mb: EmitMethodBuilder[C], targetType: PNDArray): EmitCode; ```; to; ```; // in Emit[C].emit(); def emitDeforestedNDArray(ir: IR): EmitCode =; deforestNDArray(ir, mb, region, env); ; // in Emit[C]; def deforestNDArray(x0: IR, mb: EmitMethodBuilder[C], region: StagedRegion, env: E): EmitCode = {; def deforest(x: IR): NDArrayEmitter[C] = {; x match { ... }; }; deforest(x0).emit(mb, coerce[PNDArray](x0.pType), region.code); }; ; // in NDArrayEmitter[C]; def emit(mb: EmitMethodBuilder[C], targetType: PNDArray, region: Value[Region]); ```; Before, `NDArrayEmitter.emit` was always building the NDArray in the region argument to `mb`, no matter what region was used in `deforestNDArray`. This passes the region into `NDArrayEmitter.emit`, and refactors things to better enforce that the same region is used in both parts.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-665034873
https://github.com/hail-is/hail/pull/9116#issuecomment-662792307:113,Integrability,message,message,113,"A reminder for next time, due to weird GitHub behavior, if you have a one commit PR, then the commit's title and message become the commit in master. As reviewers, we should enforce that one commit PRs have meaningful titles and messages in said commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9116#issuecomment-662792307
https://github.com/hail-is/hail/pull/9116#issuecomment-662792307:229,Integrability,message,messages,229,"A reminder for next time, due to weird GitHub behavior, if you have a one commit PR, then the commit's title and message become the commit in master. As reviewers, we should enforce that one commit PRs have meaningful titles and messages in said commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9116#issuecomment-662792307
https://github.com/hail-is/hail/pull/9117#issuecomment-662625763:56,Testability,test,tests,56,"@akotlar, I just realized now you've been putting these tests in `test_linalg`. We have a file called `test_nd` where all the ndarray tests are. We should probably be putting them there instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9117#issuecomment-662625763
https://github.com/hail-is/hail/pull/9117#issuecomment-662625763:134,Testability,test,tests,134,"@akotlar, I just realized now you've been putting these tests in `test_linalg`. We have a file called `test_nd` where all the ndarray tests are. We should probably be putting them there instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9117#issuecomment-662625763
https://github.com/hail-is/hail/issues/9121#issuecomment-662693797:983,Energy Efficiency,power,powerful,983,"I think you're right. I tried a number of things, but I need something to key the column by, and a global has no concept a key (which is why it is a global). I found this very confusing. . Let's say mt.C contains phenotypes for samples 1..n. This is, in my mind, a distributed array, with someone fancy (non-integer) indexing support. Great, but I don't care about that, I just want a distributed array. I want to localize_entries, but this creates a hail Table, which drops my phenotypes, because that's now a table and not a matrix table (why! all I wanted was to create a new field in my MT with the result of a column aggregation per row). So the natural thing I reach to is storing my phenotypes elsewhere. I think: ""I want to continue benefitting from Hail query planner), so I try not to materialize the phenotypes in memory. If I say mt.annotate_globals(Y = mt.C) I expect that to just work, because in my mind, I took something that was a a distributed array, but with more powerful indexing support, and converted it to something that is even more array like, that I'm going to need to understand how to index myself (which I'm fine with since I'm moving the thing to globals). Alternatively, I could also expect that globals now contains a reference to a new table, that contains only the column index, and value (phenotype), which seems fine. Neither of these options happens. Instead, I need to realize the array in memory on my master, which seems like a potentially bad idea. The bigger problem though is that I want 1 change (simplify indexing or make a reference to the array), and I seem to need 3 (that + memory + loss of distribution). . In short: I want to be able to choose whether I realize the values in memory, not be forced into it. Let me know if there's something I missed!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121#issuecomment-662693797
https://github.com/hail-is/hail/issues/9121#issuecomment-662693797:1542,Usability,simpl,simplify,1542,"I think you're right. I tried a number of things, but I need something to key the column by, and a global has no concept a key (which is why it is a global). I found this very confusing. . Let's say mt.C contains phenotypes for samples 1..n. This is, in my mind, a distributed array, with someone fancy (non-integer) indexing support. Great, but I don't care about that, I just want a distributed array. I want to localize_entries, but this creates a hail Table, which drops my phenotypes, because that's now a table and not a matrix table (why! all I wanted was to create a new field in my MT with the result of a column aggregation per row). So the natural thing I reach to is storing my phenotypes elsewhere. I think: ""I want to continue benefitting from Hail query planner), so I try not to materialize the phenotypes in memory. If I say mt.annotate_globals(Y = mt.C) I expect that to just work, because in my mind, I took something that was a a distributed array, but with more powerful indexing support, and converted it to something that is even more array like, that I'm going to need to understand how to index myself (which I'm fine with since I'm moving the thing to globals). Alternatively, I could also expect that globals now contains a reference to a new table, that contains only the column index, and value (phenotype), which seems fine. Neither of these options happens. Instead, I need to realize the array in memory on my master, which seems like a potentially bad idea. The bigger problem though is that I want 1 change (simplify indexing or make a reference to the array), and I seem to need 3 (that + memory + loss of distribution). . In short: I want to be able to choose whether I realize the values in memory, not be forced into it. Let me know if there's something I missed!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121#issuecomment-662693797
https://github.com/hail-is/hail/issues/9121#issuecomment-662700219:24,Usability,clear,clear,24,"sorry, I think I wasn't clear -- you can put them in globals when doing `mt.localize_entries` by passing both the `entries_field_name` and `columns_array_field_name` params.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121#issuecomment-662700219
https://github.com/hail-is/hail/issues/9121#issuecomment-662711391:132,Availability,fault,fault,132,"If we feel confident the APIs make sense, then that's a great idea!. I'm a bit worried that localize_entries was an API mistake (my fault :/) that we use because we lack better tools. I'm especially curious to see how its used and whether there's a better API for that kind of work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9121#issuecomment-662711391
https://github.com/hail-is/hail/pull/9123#issuecomment-662709112:13,Testability,test,test,13,"Can we add a test something like:; ```; mt = hl.utils.range_matrix_table(1,1); mt = mt.annotate_entries(x = 1); mt = mt.key_cols_by(col_idx = mt.col_idx + 10); assert str(mt.show()) == '''+---------+-------+; | row_idx | 10.x |; +---------+-------+; | int32 | int32 |; +---------+-------+; | 0 | 1 |; +---------+-------+; '''; ```; ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9123#issuecomment-662709112
https://github.com/hail-is/hail/pull/9123#issuecomment-662709112:160,Testability,assert,assert,160,"Can we add a test something like:; ```; mt = hl.utils.range_matrix_table(1,1); mt = mt.annotate_entries(x = 1); mt = mt.key_cols_by(col_idx = mt.col_idx + 10); assert str(mt.show()) == '''+---------+-------+; | row_idx | 10.x |; +---------+-------+; | int32 | int32 |; +---------+-------+; | 0 | 1 |; +---------+-------+; '''; ```; ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9123#issuecomment-662709112
https://github.com/hail-is/hail/issues/9128#issuecomment-663024852:81,Availability,down,down,81,"I don't recall why that `isPrimitive` was added to be honest. I remember sitting down with you to write `checkedConvertFrom` and we decided we needed it then, but it needs to go away and be dealt with.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128#issuecomment-663024852
https://github.com/hail-is/hail/issues/9128#issuecomment-663025712:41,Availability,robust,robust,41,"I think this needs to be rewritten to be robust to non-primitive elements. We can't just use copyFrom -- We need to loop and use constructAtAddressFromValue, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128#issuecomment-663025712
https://github.com/hail-is/hail/issues/9128#issuecomment-1145286659:42,Availability,error,error,42,"The original code now gives a more useful error. I'm closing as wontfix. ```; TypeError: flatten: parameter 'collection': expected expression of type set<set<any>> or array<array<any>>, found <ArrayExpression of type array<ndarray<int32, 1>>>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9128#issuecomment-1145286659
https://github.com/hail-is/hail/pull/9129#issuecomment-662812817:26,Availability,failure,failure,26,The AddressAndPort pylint failure will be fixed by https://github.com/hail-is/hail/pull/9126,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9129#issuecomment-662812817
https://github.com/hail-is/hail/pull/9139#issuecomment-663248573:143,Testability,test,testng,143,"This more or less follows the same strategy as the python splitting. Instead of each job figuring out which split it owns, I create 5 separate testng.xml files in `build_hail`. Each job reads one of those files. testng.xml has a pretty simple XML format and you can explicitly list the classes of interest. I noticed that hail java tests were up to 15 minutes which was really cramping my PR merging style. With this, some of the splits are a minute or two. I think we suffer a bit from programmatically generated tests since I'm splitting at the granularity of classes rather than methods, or, even better, generated methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9139#issuecomment-663248573
https://github.com/hail-is/hail/pull/9139#issuecomment-663248573:212,Testability,test,testng,212,"This more or less follows the same strategy as the python splitting. Instead of each job figuring out which split it owns, I create 5 separate testng.xml files in `build_hail`. Each job reads one of those files. testng.xml has a pretty simple XML format and you can explicitly list the classes of interest. I noticed that hail java tests were up to 15 minutes which was really cramping my PR merging style. With this, some of the splits are a minute or two. I think we suffer a bit from programmatically generated tests since I'm splitting at the granularity of classes rather than methods, or, even better, generated methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9139#issuecomment-663248573
https://github.com/hail-is/hail/pull/9139#issuecomment-663248573:332,Testability,test,tests,332,"This more or less follows the same strategy as the python splitting. Instead of each job figuring out which split it owns, I create 5 separate testng.xml files in `build_hail`. Each job reads one of those files. testng.xml has a pretty simple XML format and you can explicitly list the classes of interest. I noticed that hail java tests were up to 15 minutes which was really cramping my PR merging style. With this, some of the splits are a minute or two. I think we suffer a bit from programmatically generated tests since I'm splitting at the granularity of classes rather than methods, or, even better, generated methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9139#issuecomment-663248573
https://github.com/hail-is/hail/pull/9139#issuecomment-663248573:514,Testability,test,tests,514,"This more or less follows the same strategy as the python splitting. Instead of each job figuring out which split it owns, I create 5 separate testng.xml files in `build_hail`. Each job reads one of those files. testng.xml has a pretty simple XML format and you can explicitly list the classes of interest. I noticed that hail java tests were up to 15 minutes which was really cramping my PR merging style. With this, some of the splits are a minute or two. I think we suffer a bit from programmatically generated tests since I'm splitting at the granularity of classes rather than methods, or, even better, generated methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9139#issuecomment-663248573
https://github.com/hail-is/hail/pull/9139#issuecomment-663248573:236,Usability,simpl,simple,236,"This more or less follows the same strategy as the python splitting. Instead of each job figuring out which split it owns, I create 5 separate testng.xml files in `build_hail`. Each job reads one of those files. testng.xml has a pretty simple XML format and you can explicitly list the classes of interest. I noticed that hail java tests were up to 15 minutes which was really cramping my PR merging style. With this, some of the splits are a minute or two. I think we suffer a bit from programmatically generated tests since I'm splitting at the granularity of classes rather than methods, or, even better, generated methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9139#issuecomment-663248573
https://github.com/hail-is/hail/pull/9139#issuecomment-663252792:5,Testability,test,testTableJoin,5,"damn testTableJoin is crazy, that one method generates ~500 tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9139#issuecomment-663252792
https://github.com/hail-is/hail/pull/9139#issuecomment-663252792:60,Testability,test,tests,60,"damn testTableJoin is crazy, that one method generates ~500 tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9139#issuecomment-663252792
https://github.com/hail-is/hail/pull/9139#issuecomment-663255065:0,Testability,test,testTableJoin,0,testTableJoin takes five minutes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9139#issuecomment-663255065
https://github.com/hail-is/hail/issues/9144#issuecomment-663267587:66,Deployability,install,installing-editable,66,"Interesting: I tried in a new session (after checking out master, installing-editable), and this time I could break things with out-of-bounds slices, but `hl.eval(a[0:a.shape[0],0:1]) ` and `hl.eval(a[0:a.shape[0],0:2])` worked ok. So what's going on?. ```python; In [22]: hl.eval(a[0:a.shape[0],0:1]) ; Out[22]: ; array([[1],; [2],; [3],; [4],; [5]], dtype=int32). In [23]: hl.eval(a[0:a.shape[0],0:1]) ; Out[23]: ; array([[1],; [2],; [3],; [4],; [5]], dtype=int32). In [24]: a = a.T . In [25]: hl.eval(a) ; Out[25]: ; array([[ 1, 2, 3, 4, 5],; [ 7, 6, 8, 9, 10]], dtype=int32). In [26]: a = a.T . In [27]: hl.eval(a) ; Out[27]: ; array([[ 1, 7],; [ 2, 6],; [ 3, 8],; [ 4, 9],; [ 5, 10]], dtype=int32). In [28]: hl.eval(a[0:a.shape[0],0:1]) ; Out[28]: ; array([[1],; [7],; [0],; [2],; [0]], dtype=int32). In [32]: hl.eval(a[0:a.shape[0],0:1]) ; Out[32]: ; array([[ 1],; [ 7],; [ 0],; [ 4],; [32749]], dtype=int32); ``` . totally broken. Seems like 2 problems: 1) out of bounds checks not being done on inner dimension. 2) strides get scrambled between transposes, or something isn't being cleared after transposition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9144#issuecomment-663267587
https://github.com/hail-is/hail/issues/9144#issuecomment-663267587:1090,Usability,clear,cleared,1090,"Interesting: I tried in a new session (after checking out master, installing-editable), and this time I could break things with out-of-bounds slices, but `hl.eval(a[0:a.shape[0],0:1]) ` and `hl.eval(a[0:a.shape[0],0:2])` worked ok. So what's going on?. ```python; In [22]: hl.eval(a[0:a.shape[0],0:1]) ; Out[22]: ; array([[1],; [2],; [3],; [4],; [5]], dtype=int32). In [23]: hl.eval(a[0:a.shape[0],0:1]) ; Out[23]: ; array([[1],; [2],; [3],; [4],; [5]], dtype=int32). In [24]: a = a.T . In [25]: hl.eval(a) ; Out[25]: ; array([[ 1, 2, 3, 4, 5],; [ 7, 6, 8, 9, 10]], dtype=int32). In [26]: a = a.T . In [27]: hl.eval(a) ; Out[27]: ; array([[ 1, 7],; [ 2, 6],; [ 3, 8],; [ 4, 9],; [ 5, 10]], dtype=int32). In [28]: hl.eval(a[0:a.shape[0],0:1]) ; Out[28]: ; array([[1],; [7],; [0],; [2],; [0]], dtype=int32). In [32]: hl.eval(a[0:a.shape[0],0:1]) ; Out[32]: ; array([[ 1],; [ 7],; [ 0],; [ 4],; [32749]], dtype=int32); ``` . totally broken. Seems like 2 problems: 1) out of bounds checks not being done on inner dimension. 2) strides get scrambled between transposes, or something isn't being cleared after transposition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9144#issuecomment-663267587
https://github.com/hail-is/hail/issues/9144#issuecomment-669592521:0,Usability,Simpl,Simple,0,"Simple reproduction:. ```python; In [1]: import hail as hl . In [2]: t = hl.nd.array([[1,2,3,4], [1,2,3,4]]) . In [3]: hl.eval(t[0:1000,1000]) ; Out[3]: array([32756, 32756], dtype=int32); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9144#issuecomment-669592521
https://github.com/hail-is/hail/pull/9146#issuecomment-663720122:35,Availability,error,error,35,Suddenly getting really surprising error here about low level codegen stuff.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9146#issuecomment-663720122
https://github.com/hail-is/hail/pull/9152#issuecomment-663928283:16,Testability,test,tests,16,"added some more tests, passes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9152#issuecomment-663928283
https://github.com/hail-is/hail/pull/9164#issuecomment-665108669:120,Availability,error,error,120,we should put this on top level expression. Someone could do something like `'NA12878' in mt.s` and that should have an error as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9164#issuecomment-665108669
https://github.com/hail-is/hail/pull/9164#issuecomment-665678053:39,Availability,error,error,39,"Ok, so now there's a generic catch all error on `Expression`, and I override it on `StructExpression` to make sure that one works. `CollectionExpression` and `StringExpression` also get overrides to give appropriate examples.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9164#issuecomment-665678053
https://github.com/hail-is/hail/pull/9184#issuecomment-667107337:248,Modifiability,config,config,248,"@cseed added these print statements. I've never used the output in debugging in production except to quickly realize a job is one of a given user's when I was looking at worker performance. Early on, I might have checked it to make sure the Docker config resource options and volume mounts were correct. But I can add those back when debugging if needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9184#issuecomment-667107337
https://github.com/hail-is/hail/pull/9184#issuecomment-667107337:177,Performance,perform,performance,177,"@cseed added these print statements. I've never used the output in debugging in production except to quickly realize a job is one of a given user's when I was looking at worker performance. Early on, I might have checked it to make sure the Docker config resource options and volume mounts were correct. But I can add those back when debugging if needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9184#issuecomment-667107337
https://github.com/hail-is/hail/pull/9189#issuecomment-667117637:69,Modifiability,plugin,plugin-systemd,69,"For dockerd, we probably need to use this: https://github.com/fluent-plugin-systemd/fluent-plugin-systemd",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189#issuecomment-667117637
https://github.com/hail-is/hail/pull/9189#issuecomment-667117637:91,Modifiability,plugin,plugin-systemd,91,"For dockerd, we probably need to use this: https://github.com/fluent-plugin-systemd/fluent-plugin-systemd",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9189#issuecomment-667117637
https://github.com/hail-is/hail/pull/9191#issuecomment-667226454:423,Integrability,interface,interface,423,"I can add it if you'd like, thought we'd prefer to save keystrokes:. ```python; In [3]: class Backend: ; ...: """""" ; ...: Abstract class for backends. ; ...: """""" ; ...: ; ...: @abc.abstractmethod ; ...: def close(self): ; ...: """""" ; ...: Close a Hail Batch backend. ; ...: """""" ; ...: pass ; ...: ; ...: ; ...: class LocalBackend(Backend): . In [4]: n = LocalBackend() . In [5]: n.close() ; ```. This allows us to declare an interface and a void implementation in one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9191#issuecomment-667226454
https://github.com/hail-is/hail/pull/9191#issuecomment-667227984:346,Usability,intuit,intuition,346,"Ahh, hmm. Pylint agrees with me that this isn't Kosher:; ```; /hailtop/batch/backend.py:44:8: W0107: Unnecessary pass statement (unnecessary-pass); /hailtop/batch/backend.py:47:0: W0223: Method 'close' is abstract in class 'Backend' but is not overridden (abstract-method); ```. We've generally heeded pylint's advice even if it clashes with our intuition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9191#issuecomment-667227984
https://github.com/hail-is/hail/pull/9191#issuecomment-667235187:468,Modifiability,inherit,inheritance,468,"Looking into this further. Python abstract methods are similar to Scala's in that they can have implementation, but less similar than my test had seemed to state. Python doc on abc:. """"""; Note Unlike Java abstract methods, these abstract methods may have an implementation. This implementation can be called via the super() mechanism from the class that overrides it. This could be useful as an end-point for a super-call in a framework that uses cooperative multiple-inheritance.; """""". However unlike Scala, even with an implementation, these need to be implemented by the subclass. I hadn't noticed the issue because our Backend didn't inherit from ABC. Fixed in https://github.com/hail-is/hail/pull/9192. ```python; In [25]: class Backend(abc.ABC): ; ...: """""" ; ...: Abstract class for backends. ; ...: """""" ; ...: @abc.abstractmethod ; ...: def close(self): ; ...: """""" ; ...: Close a Hail Batch backend. ; ...: """""" ; ...: return ""Parent"" ; ...: ; ...: ; ...: class LocalBackend(Backend): ; ...: """""" ; ...: Backend that executes batches on a local computer. ; ...: ; ...: Examples ; ...: -------- ; ...: ; ...: >>> local_backend = LocalBackend(tmp_dir='/tmp/user/') ; ...: >>> b = Batch(backend=local_backend) ; ...: ; ...: Parameters ; ...: ---------- ; ...: tmp_dir: :obj:`str`, optional ; ...: Temporary directory to use. ; ...: gsa_key_file: :obj:`str`, optional ; ...: Mount a file with a gsa key to `/gsa-key/key.json`. Only used if a ; ...: job specifies a docker image. This option will override the value set by ; ...: the environment variable `HAIL_BATCH_GSA_KEY_FILE`. ; ...: extra_docker_run_flags: :obj:`str`, optional ; ...: Additional flags to pass to `docker run`. Only used if a job specifies ; ...: a docker image. This option will override the value set by the environment ; ...: variable `HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS`. ; ...: """""" ; ...: ; ...: . In [26]: n = LocalBackend() ; ---------------------------------------------------------------------------; TypeError Traceback ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9191#issuecomment-667235187
https://github.com/hail-is/hail/pull/9191#issuecomment-667235187:638,Modifiability,inherit,inherit,638,"Looking into this further. Python abstract methods are similar to Scala's in that they can have implementation, but less similar than my test had seemed to state. Python doc on abc:. """"""; Note Unlike Java abstract methods, these abstract methods may have an implementation. This implementation can be called via the super() mechanism from the class that overrides it. This could be useful as an end-point for a super-call in a framework that uses cooperative multiple-inheritance.; """""". However unlike Scala, even with an implementation, these need to be implemented by the subclass. I hadn't noticed the issue because our Backend didn't inherit from ABC. Fixed in https://github.com/hail-is/hail/pull/9192. ```python; In [25]: class Backend(abc.ABC): ; ...: """""" ; ...: Abstract class for backends. ; ...: """""" ; ...: @abc.abstractmethod ; ...: def close(self): ; ...: """""" ; ...: Close a Hail Batch backend. ; ...: """""" ; ...: return ""Parent"" ; ...: ; ...: ; ...: class LocalBackend(Backend): ; ...: """""" ; ...: Backend that executes batches on a local computer. ; ...: ; ...: Examples ; ...: -------- ; ...: ; ...: >>> local_backend = LocalBackend(tmp_dir='/tmp/user/') ; ...: >>> b = Batch(backend=local_backend) ; ...: ; ...: Parameters ; ...: ---------- ; ...: tmp_dir: :obj:`str`, optional ; ...: Temporary directory to use. ; ...: gsa_key_file: :obj:`str`, optional ; ...: Mount a file with a gsa key to `/gsa-key/key.json`. Only used if a ; ...: job specifies a docker image. This option will override the value set by ; ...: the environment variable `HAIL_BATCH_GSA_KEY_FILE`. ; ...: extra_docker_run_flags: :obj:`str`, optional ; ...: Additional flags to pass to `docker run`. Only used if a job specifies ; ...: a docker image. This option will override the value set by the environment ; ...: variable `HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS`. ; ...: """""" ; ...: ; ...: . In [26]: n = LocalBackend() ; ---------------------------------------------------------------------------; TypeError Traceback ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9191#issuecomment-667235187
https://github.com/hail-is/hail/pull/9191#issuecomment-667235187:1546,Modifiability,variab,variable,1546,"Note Unlike Java abstract methods, these abstract methods may have an implementation. This implementation can be called via the super() mechanism from the class that overrides it. This could be useful as an end-point for a super-call in a framework that uses cooperative multiple-inheritance.; """""". However unlike Scala, even with an implementation, these need to be implemented by the subclass. I hadn't noticed the issue because our Backend didn't inherit from ABC. Fixed in https://github.com/hail-is/hail/pull/9192. ```python; In [25]: class Backend(abc.ABC): ; ...: """""" ; ...: Abstract class for backends. ; ...: """""" ; ...: @abc.abstractmethod ; ...: def close(self): ; ...: """""" ; ...: Close a Hail Batch backend. ; ...: """""" ; ...: return ""Parent"" ; ...: ; ...: ; ...: class LocalBackend(Backend): ; ...: """""" ; ...: Backend that executes batches on a local computer. ; ...: ; ...: Examples ; ...: -------- ; ...: ; ...: >>> local_backend = LocalBackend(tmp_dir='/tmp/user/') ; ...: >>> b = Batch(backend=local_backend) ; ...: ; ...: Parameters ; ...: ---------- ; ...: tmp_dir: :obj:`str`, optional ; ...: Temporary directory to use. ; ...: gsa_key_file: :obj:`str`, optional ; ...: Mount a file with a gsa key to `/gsa-key/key.json`. Only used if a ; ...: job specifies a docker image. This option will override the value set by ; ...: the environment variable `HAIL_BATCH_GSA_KEY_FILE`. ; ...: extra_docker_run_flags: :obj:`str`, optional ; ...: Additional flags to pass to `docker run`. Only used if a job specifies ; ...: a docker image. This option will override the value set by the environment ; ...: variable `HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS`. ; ...: """""" ; ...: ; ...: . In [26]: n = LocalBackend() ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-26-6524b19a5d4e> in <module>; ----> 1 n = LocalBackend(); TypeError: Can't instantiate abstract class LocalBackend with abstract methods close; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9191#issuecomment-667235187
https://github.com/hail-is/hail/pull/9191#issuecomment-667235187:1801,Modifiability,variab,variable,1801,"Note Unlike Java abstract methods, these abstract methods may have an implementation. This implementation can be called via the super() mechanism from the class that overrides it. This could be useful as an end-point for a super-call in a framework that uses cooperative multiple-inheritance.; """""". However unlike Scala, even with an implementation, these need to be implemented by the subclass. I hadn't noticed the issue because our Backend didn't inherit from ABC. Fixed in https://github.com/hail-is/hail/pull/9192. ```python; In [25]: class Backend(abc.ABC): ; ...: """""" ; ...: Abstract class for backends. ; ...: """""" ; ...: @abc.abstractmethod ; ...: def close(self): ; ...: """""" ; ...: Close a Hail Batch backend. ; ...: """""" ; ...: return ""Parent"" ; ...: ; ...: ; ...: class LocalBackend(Backend): ; ...: """""" ; ...: Backend that executes batches on a local computer. ; ...: ; ...: Examples ; ...: -------- ; ...: ; ...: >>> local_backend = LocalBackend(tmp_dir='/tmp/user/') ; ...: >>> b = Batch(backend=local_backend) ; ...: ; ...: Parameters ; ...: ---------- ; ...: tmp_dir: :obj:`str`, optional ; ...: Temporary directory to use. ; ...: gsa_key_file: :obj:`str`, optional ; ...: Mount a file with a gsa key to `/gsa-key/key.json`. Only used if a ; ...: job specifies a docker image. This option will override the value set by ; ...: the environment variable `HAIL_BATCH_GSA_KEY_FILE`. ; ...: extra_docker_run_flags: :obj:`str`, optional ; ...: Additional flags to pass to `docker run`. Only used if a job specifies ; ...: a docker image. This option will override the value set by the environment ; ...: variable `HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS`. ; ...: """""" ; ...: ; ...: . In [26]: n = LocalBackend() ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-26-6524b19a5d4e> in <module>; ----> 1 n = LocalBackend(); TypeError: Can't instantiate abstract class LocalBackend with abstract methods close; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9191#issuecomment-667235187
https://github.com/hail-is/hail/pull/9191#issuecomment-667235187:137,Testability,test,test,137,"Looking into this further. Python abstract methods are similar to Scala's in that they can have implementation, but less similar than my test had seemed to state. Python doc on abc:. """"""; Note Unlike Java abstract methods, these abstract methods may have an implementation. This implementation can be called via the super() mechanism from the class that overrides it. This could be useful as an end-point for a super-call in a framework that uses cooperative multiple-inheritance.; """""". However unlike Scala, even with an implementation, these need to be implemented by the subclass. I hadn't noticed the issue because our Backend didn't inherit from ABC. Fixed in https://github.com/hail-is/hail/pull/9192. ```python; In [25]: class Backend(abc.ABC): ; ...: """""" ; ...: Abstract class for backends. ; ...: """""" ; ...: @abc.abstractmethod ; ...: def close(self): ; ...: """""" ; ...: Close a Hail Batch backend. ; ...: """""" ; ...: return ""Parent"" ; ...: ; ...: ; ...: class LocalBackend(Backend): ; ...: """""" ; ...: Backend that executes batches on a local computer. ; ...: ; ...: Examples ; ...: -------- ; ...: ; ...: >>> local_backend = LocalBackend(tmp_dir='/tmp/user/') ; ...: >>> b = Batch(backend=local_backend) ; ...: ; ...: Parameters ; ...: ---------- ; ...: tmp_dir: :obj:`str`, optional ; ...: Temporary directory to use. ; ...: gsa_key_file: :obj:`str`, optional ; ...: Mount a file with a gsa key to `/gsa-key/key.json`. Only used if a ; ...: job specifies a docker image. This option will override the value set by ; ...: the environment variable `HAIL_BATCH_GSA_KEY_FILE`. ; ...: extra_docker_run_flags: :obj:`str`, optional ; ...: Additional flags to pass to `docker run`. Only used if a job specifies ; ...: a docker image. This option will override the value set by the environment ; ...: variable `HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS`. ; ...: """""" ; ...: ; ...: . In [26]: n = LocalBackend() ; ---------------------------------------------------------------------------; TypeError Traceback ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9191#issuecomment-667235187
https://github.com/hail-is/hail/pull/9194#issuecomment-667434082:461,Energy Efficiency,reduce,reduced,461,"> What's the pressing reason for this to join the mainline?. Ben has wanted this for a while, and there's interest from the group. > I'd prefer to stick to the usual process on services team where thorough code review of code and test-code is done for stuff under our purview (as things in hailtop/batch do). Sure. Regarding testing, in my experience regenie (the C++ program) crashes when given improper inputs, rather than writing anything out, so there is a reduced testing need (we should. verify the expected number of non-empty outputs are created I think, and some basic tests that we parse inputs). My plan for getting regenie in is roughly 4 part, of which this is 1. ; 1) Get the most basic / canonical version of regenie up, on the local backend. This version should take all of the arguments demonstrated in the [regenie tutorial / example](https://rgcgithub.github.io/regenie/options/), pass them to step 1 and step 2, and write an output. ; * Here I'm mostly interested in making sure I'm using Batch Resource classes correctly.; 2) Get the same working for the ServiceBackend.; 3) Expand the service offering to run the paper's example (may require some tweaks to the input handling code), and test that we can run this at scale; 4) Introduce a per-phenotype parallelism mode, which loses a bit of per-core efficiency in favor of greater scale out, benchmark that.; 5) Handle all other combinations of inputs.; 6) Optimize performance (local RAID-0 SSDs up to 9TB, larger instances). So, if at all possible, I would like to stay focused on the first task, within the bounds of what you need to accomplish as leader of the services team; I'd really appreciate your help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-667434082
https://github.com/hail-is/hail/pull/9194#issuecomment-667434082:1429,Performance,Optimiz,Optimize,1429,"> What's the pressing reason for this to join the mainline?. Ben has wanted this for a while, and there's interest from the group. > I'd prefer to stick to the usual process on services team where thorough code review of code and test-code is done for stuff under our purview (as things in hailtop/batch do). Sure. Regarding testing, in my experience regenie (the C++ program) crashes when given improper inputs, rather than writing anything out, so there is a reduced testing need (we should. verify the expected number of non-empty outputs are created I think, and some basic tests that we parse inputs). My plan for getting regenie in is roughly 4 part, of which this is 1. ; 1) Get the most basic / canonical version of regenie up, on the local backend. This version should take all of the arguments demonstrated in the [regenie tutorial / example](https://rgcgithub.github.io/regenie/options/), pass them to step 1 and step 2, and write an output. ; * Here I'm mostly interested in making sure I'm using Batch Resource classes correctly.; 2) Get the same working for the ServiceBackend.; 3) Expand the service offering to run the paper's example (may require some tweaks to the input handling code), and test that we can run this at scale; 4) Introduce a per-phenotype parallelism mode, which loses a bit of per-core efficiency in favor of greater scale out, benchmark that.; 5) Handle all other combinations of inputs.; 6) Optimize performance (local RAID-0 SSDs up to 9TB, larger instances). So, if at all possible, I would like to stay focused on the first task, within the bounds of what you need to accomplish as leader of the services team; I'd really appreciate your help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-667434082
https://github.com/hail-is/hail/pull/9194#issuecomment-667434082:1438,Performance,perform,performance,1438,"> What's the pressing reason for this to join the mainline?. Ben has wanted this for a while, and there's interest from the group. > I'd prefer to stick to the usual process on services team where thorough code review of code and test-code is done for stuff under our purview (as things in hailtop/batch do). Sure. Regarding testing, in my experience regenie (the C++ program) crashes when given improper inputs, rather than writing anything out, so there is a reduced testing need (we should. verify the expected number of non-empty outputs are created I think, and some basic tests that we parse inputs). My plan for getting regenie in is roughly 4 part, of which this is 1. ; 1) Get the most basic / canonical version of regenie up, on the local backend. This version should take all of the arguments demonstrated in the [regenie tutorial / example](https://rgcgithub.github.io/regenie/options/), pass them to step 1 and step 2, and write an output. ; * Here I'm mostly interested in making sure I'm using Batch Resource classes correctly.; 2) Get the same working for the ServiceBackend.; 3) Expand the service offering to run the paper's example (may require some tweaks to the input handling code), and test that we can run this at scale; 4) Introduce a per-phenotype parallelism mode, which loses a bit of per-core efficiency in favor of greater scale out, benchmark that.; 5) Handle all other combinations of inputs.; 6) Optimize performance (local RAID-0 SSDs up to 9TB, larger instances). So, if at all possible, I would like to stay focused on the first task, within the bounds of what you need to accomplish as leader of the services team; I'd really appreciate your help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-667434082
https://github.com/hail-is/hail/pull/9194#issuecomment-667434082:230,Testability,test,test-code,230,"> What's the pressing reason for this to join the mainline?. Ben has wanted this for a while, and there's interest from the group. > I'd prefer to stick to the usual process on services team where thorough code review of code and test-code is done for stuff under our purview (as things in hailtop/batch do). Sure. Regarding testing, in my experience regenie (the C++ program) crashes when given improper inputs, rather than writing anything out, so there is a reduced testing need (we should. verify the expected number of non-empty outputs are created I think, and some basic tests that we parse inputs). My plan for getting regenie in is roughly 4 part, of which this is 1. ; 1) Get the most basic / canonical version of regenie up, on the local backend. This version should take all of the arguments demonstrated in the [regenie tutorial / example](https://rgcgithub.github.io/regenie/options/), pass them to step 1 and step 2, and write an output. ; * Here I'm mostly interested in making sure I'm using Batch Resource classes correctly.; 2) Get the same working for the ServiceBackend.; 3) Expand the service offering to run the paper's example (may require some tweaks to the input handling code), and test that we can run this at scale; 4) Introduce a per-phenotype parallelism mode, which loses a bit of per-core efficiency in favor of greater scale out, benchmark that.; 5) Handle all other combinations of inputs.; 6) Optimize performance (local RAID-0 SSDs up to 9TB, larger instances). So, if at all possible, I would like to stay focused on the first task, within the bounds of what you need to accomplish as leader of the services team; I'd really appreciate your help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-667434082
https://github.com/hail-is/hail/pull/9194#issuecomment-667434082:325,Testability,test,testing,325,"> What's the pressing reason for this to join the mainline?. Ben has wanted this for a while, and there's interest from the group. > I'd prefer to stick to the usual process on services team where thorough code review of code and test-code is done for stuff under our purview (as things in hailtop/batch do). Sure. Regarding testing, in my experience regenie (the C++ program) crashes when given improper inputs, rather than writing anything out, so there is a reduced testing need (we should. verify the expected number of non-empty outputs are created I think, and some basic tests that we parse inputs). My plan for getting regenie in is roughly 4 part, of which this is 1. ; 1) Get the most basic / canonical version of regenie up, on the local backend. This version should take all of the arguments demonstrated in the [regenie tutorial / example](https://rgcgithub.github.io/regenie/options/), pass them to step 1 and step 2, and write an output. ; * Here I'm mostly interested in making sure I'm using Batch Resource classes correctly.; 2) Get the same working for the ServiceBackend.; 3) Expand the service offering to run the paper's example (may require some tweaks to the input handling code), and test that we can run this at scale; 4) Introduce a per-phenotype parallelism mode, which loses a bit of per-core efficiency in favor of greater scale out, benchmark that.; 5) Handle all other combinations of inputs.; 6) Optimize performance (local RAID-0 SSDs up to 9TB, larger instances). So, if at all possible, I would like to stay focused on the first task, within the bounds of what you need to accomplish as leader of the services team; I'd really appreciate your help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-667434082
https://github.com/hail-is/hail/pull/9194#issuecomment-667434082:469,Testability,test,testing,469,"> What's the pressing reason for this to join the mainline?. Ben has wanted this for a while, and there's interest from the group. > I'd prefer to stick to the usual process on services team where thorough code review of code and test-code is done for stuff under our purview (as things in hailtop/batch do). Sure. Regarding testing, in my experience regenie (the C++ program) crashes when given improper inputs, rather than writing anything out, so there is a reduced testing need (we should. verify the expected number of non-empty outputs are created I think, and some basic tests that we parse inputs). My plan for getting regenie in is roughly 4 part, of which this is 1. ; 1) Get the most basic / canonical version of regenie up, on the local backend. This version should take all of the arguments demonstrated in the [regenie tutorial / example](https://rgcgithub.github.io/regenie/options/), pass them to step 1 and step 2, and write an output. ; * Here I'm mostly interested in making sure I'm using Batch Resource classes correctly.; 2) Get the same working for the ServiceBackend.; 3) Expand the service offering to run the paper's example (may require some tweaks to the input handling code), and test that we can run this at scale; 4) Introduce a per-phenotype parallelism mode, which loses a bit of per-core efficiency in favor of greater scale out, benchmark that.; 5) Handle all other combinations of inputs.; 6) Optimize performance (local RAID-0 SSDs up to 9TB, larger instances). So, if at all possible, I would like to stay focused on the first task, within the bounds of what you need to accomplish as leader of the services team; I'd really appreciate your help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-667434082
https://github.com/hail-is/hail/pull/9194#issuecomment-667434082:578,Testability,test,tests,578,"> What's the pressing reason for this to join the mainline?. Ben has wanted this for a while, and there's interest from the group. > I'd prefer to stick to the usual process on services team where thorough code review of code and test-code is done for stuff under our purview (as things in hailtop/batch do). Sure. Regarding testing, in my experience regenie (the C++ program) crashes when given improper inputs, rather than writing anything out, so there is a reduced testing need (we should. verify the expected number of non-empty outputs are created I think, and some basic tests that we parse inputs). My plan for getting regenie in is roughly 4 part, of which this is 1. ; 1) Get the most basic / canonical version of regenie up, on the local backend. This version should take all of the arguments demonstrated in the [regenie tutorial / example](https://rgcgithub.github.io/regenie/options/), pass them to step 1 and step 2, and write an output. ; * Here I'm mostly interested in making sure I'm using Batch Resource classes correctly.; 2) Get the same working for the ServiceBackend.; 3) Expand the service offering to run the paper's example (may require some tweaks to the input handling code), and test that we can run this at scale; 4) Introduce a per-phenotype parallelism mode, which loses a bit of per-core efficiency in favor of greater scale out, benchmark that.; 5) Handle all other combinations of inputs.; 6) Optimize performance (local RAID-0 SSDs up to 9TB, larger instances). So, if at all possible, I would like to stay focused on the first task, within the bounds of what you need to accomplish as leader of the services team; I'd really appreciate your help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-667434082
https://github.com/hail-is/hail/pull/9194#issuecomment-667434082:1209,Testability,test,test,1209,"> What's the pressing reason for this to join the mainline?. Ben has wanted this for a while, and there's interest from the group. > I'd prefer to stick to the usual process on services team where thorough code review of code and test-code is done for stuff under our purview (as things in hailtop/batch do). Sure. Regarding testing, in my experience regenie (the C++ program) crashes when given improper inputs, rather than writing anything out, so there is a reduced testing need (we should. verify the expected number of non-empty outputs are created I think, and some basic tests that we parse inputs). My plan for getting regenie in is roughly 4 part, of which this is 1. ; 1) Get the most basic / canonical version of regenie up, on the local backend. This version should take all of the arguments demonstrated in the [regenie tutorial / example](https://rgcgithub.github.io/regenie/options/), pass them to step 1 and step 2, and write an output. ; * Here I'm mostly interested in making sure I'm using Batch Resource classes correctly.; 2) Get the same working for the ServiceBackend.; 3) Expand the service offering to run the paper's example (may require some tweaks to the input handling code), and test that we can run this at scale; 4) Introduce a per-phenotype parallelism mode, which loses a bit of per-core efficiency in favor of greater scale out, benchmark that.; 5) Handle all other combinations of inputs.; 6) Optimize performance (local RAID-0 SSDs up to 9TB, larger instances). So, if at all possible, I would like to stay focused on the first task, within the bounds of what you need to accomplish as leader of the services team; I'd really appreciate your help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-667434082
https://github.com/hail-is/hail/pull/9194#issuecomment-667434082:1364,Testability,benchmark,benchmark,1364,"> What's the pressing reason for this to join the mainline?. Ben has wanted this for a while, and there's interest from the group. > I'd prefer to stick to the usual process on services team where thorough code review of code and test-code is done for stuff under our purview (as things in hailtop/batch do). Sure. Regarding testing, in my experience regenie (the C++ program) crashes when given improper inputs, rather than writing anything out, so there is a reduced testing need (we should. verify the expected number of non-empty outputs are created I think, and some basic tests that we parse inputs). My plan for getting regenie in is roughly 4 part, of which this is 1. ; 1) Get the most basic / canonical version of regenie up, on the local backend. This version should take all of the arguments demonstrated in the [regenie tutorial / example](https://rgcgithub.github.io/regenie/options/), pass them to step 1 and step 2, and write an output. ; * Here I'm mostly interested in making sure I'm using Batch Resource classes correctly.; 2) Get the same working for the ServiceBackend.; 3) Expand the service offering to run the paper's example (may require some tweaks to the input handling code), and test that we can run this at scale; 4) Introduce a per-phenotype parallelism mode, which loses a bit of per-core efficiency in favor of greater scale out, benchmark that.; 5) Handle all other combinations of inputs.; 6) Optimize performance (local RAID-0 SSDs up to 9TB, larger instances). So, if at all possible, I would like to stay focused on the first task, within the bounds of what you need to accomplish as leader of the services team; I'd really appreciate your help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-667434082
https://github.com/hail-is/hail/pull/9194#issuecomment-668357987:1114,Availability,down,down,1114,"ppose, ""applications"" that use Batch. Since we plan to support, maintain, and test the Hail Batch regenie implementation, I think it doesn't belong in a ""contrib"" directory. The hail python package has a `genetics` module for genetics-specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BGEN files for testing in the Hail src/test/resources. I already have an example provided. The",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987
https://github.com/hail-is/hail/pull/9194#issuecomment-668357987:430,Integrability,depend,dependencies,430,"> Thanks for sharing this detailed plan!; > ; > So, I don't currently have any plan for, I suppose, ""applications"" that use Batch. Since we plan to support, maintain, and test the Hail Batch regenie implementation, I think it doesn't belong in a ""contrib"" directory. The hail python package has a `genetics` module for genetics-specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BG",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987
https://github.com/hail-is/hail/pull/9194#issuecomment-668357987:2122,Modifiability,config,config,2122,"specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BGEN files for testing in the Hail src/test/resources. I already have an example provided. The example folder contains the config for that, and the regenie folder contains the example. We need an example that has a known result, and regenie's c++ repo conveniently provides that. This is what the regenie/regenie folder contains.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987
https://github.com/hail-is/hail/pull/9194#issuecomment-668357987:171,Testability,test,test,171,"> Thanks for sharing this detailed plan!; > ; > So, I don't currently have any plan for, I suppose, ""applications"" that use Batch. Since we plan to support, maintain, and test the Hail Batch regenie implementation, I think it doesn't belong in a ""contrib"" directory. The hail python package has a `genetics` module for genetics-specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BG",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987
https://github.com/hail-is/hail/pull/9194#issuecomment-668357987:2014,Testability,test,testing,2014,"specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BGEN files for testing in the Hail src/test/resources. I already have an example provided. The example folder contains the config for that, and the regenie folder contains the example. We need an example that has a known result, and regenie's c++ repo conveniently provides that. This is what the regenie/regenie folder contains.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987
https://github.com/hail-is/hail/pull/9194#issuecomment-668357987:2038,Testability,test,test,2038,"specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BGEN files for testing in the Hail src/test/resources. I already have an example provided. The example folder contains the config for that, and the regenie folder contains the example. We need an example that has a known result, and regenie's c++ repo conveniently provides that. This is what the regenie/regenie folder contains.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987
https://github.com/hail-is/hail/pull/9194#issuecomment-668357987:1323,Usability,clear,clear,1323,"specific Hail Query functionality, let's mirror that structure. Let's move REGENIE and any non-Python dependencies of it into `hailtop/batch/genetics/regenie`. Sure. > How is the Dockerfile meant to be used? As written it doesn't appear that it would work because there doesn't exist any regenie source code to COPY in. It's meant to create a Regenie docker image that we could use. It's a copy of the regenie c++ repo's dockerfile, with the removal of the ENTRYPOINT /usr/local/bin/regenie, so that I could issue a command that included an executable, which is convenient to give me the ability to check that intermediate files are actually created (wc, ls) by batch, and because that seems more idiomatic for batch. I don't think there is a published regenie image, but docker hub is down so can't double check. . You're right, I shouldn't have deleted the bulk of the repo, kept as is. Didn't want to deal with submodules. > I've made some other in-line comments in the python file. It's not clear to me how all those other files are related to the python files and I'm a bit uncomfortable adding a whole directory with a LICENSE file, especially when not all the files in the directory fall under that license (e.g. the regenie py file) and moreover the license makes claims about things linking to BGEN, which none of our code here does. The license is only contained within the folder with the licensed files. In a previous conversation with Nate/Cotton, if we use any open-source software, best to keep those files segregated, alongside their license (license must be kept alongside the code, easier to see the demarcation if in a separate folder). > We have some BGEN files for testing in the Hail src/test/resources. I already have an example provided. The example folder contains the config for that, and the regenie folder contains the example. We need an example that has a known result, and regenie's c++ repo conveniently provides that. This is what the regenie/regenie folder contains.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668357987
https://github.com/hail-is/hail/pull/9194#issuecomment-668904836:81,Integrability,depend,depend,81,"> A few more thoughts.; > ; > ; > ; > I'm apprehensive to make our build process depend on building some third-party software. I think we should assume a REGENIE binary/image exists. It's fine if we produce it by hand and upload it. We should ask the REGENIE folks to start publishing binaries or images. As we discussed, I will ask them to produce it. In the meantime I can include the example folder from the regenie repo alone, as before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-668904836
https://github.com/hail-is/hail/pull/9194#issuecomment-669870559:22,Availability,error,errors,22,Getting some weird CI errors: the hail module isn't found,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-669870559
https://github.com/hail-is/hail/pull/9194#issuecomment-670044799:159,Deployability,patch,patched,159,"It seems the biggest remaining issue is that CI doesn't have access to hail imports when testing hailtop. Could we address this? Do you want a PR?. edit: nvm, patched here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-670044799
https://github.com/hail-is/hail/pull/9194#issuecomment-670044799:61,Security,access,access,61,"It seems the biggest remaining issue is that CI doesn't have access to hail imports when testing hailtop. Could we address this? Do you want a PR?. edit: nvm, patched here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-670044799
https://github.com/hail-is/hail/pull/9194#issuecomment-670044799:89,Testability,test,testing,89,"It seems the biggest remaining issue is that CI doesn't have access to hail imports when testing hailtop. Could we address this? Do you want a PR?. edit: nvm, patched here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-670044799
https://github.com/hail-is/hail/pull/9194#issuecomment-670642110:424,Integrability,rout,route,424,"Dan, I ended up revisiting your output suggestion, after realizing that I could eventually solve the output issue by changing how write_output concatenates identifiers with the prefix. This would be a smaller change than the original thought, which was to allow write_output to treat the output path as a directory and write the identifier unchanged. I still think this would be more what users would expect, but going this route is a bigger deviation so from a project management standpoint less desirable. I think you will now be happy with the code. Should be ready to go modulo the submodule decision. edit: Asana issue made for the output munging issue. If we add a user-definable separator my issue will be solved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-670642110
https://github.com/hail-is/hail/pull/9194#issuecomment-670772749:284,Performance,perform,performed,284,I've realised that hailtop is not the place for contributed methods that use batch if we want to restrict hail imports. Think about how strange it is to exclude our own Batch cookbook example (gwas clumping) from a viable contributed module (in that case no tests on gwas.py could be performed directly).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-670772749
https://github.com/hail-is/hail/pull/9194#issuecomment-670772749:258,Testability,test,tests,258,I've realised that hailtop is not the place for contributed methods that use batch if we want to restrict hail imports. Think about how strange it is to exclude our own Batch cookbook example (gwas clumping) from a viable contributed module (in that case no tests on gwas.py could be performed directly).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-670772749
https://github.com/hail-is/hail/pull/9194#issuecomment-671357400:125,Deployability,install,installed,125,"Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`). I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400
https://github.com/hail-is/hail/pull/9194#issuecomment-671357400:303,Deployability,install,installed,303,"Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`). I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400
https://github.com/hail-is/hail/pull/9194#issuecomment-671357400:402,Integrability,depend,dependent,402,"Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`). I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400
https://github.com/hail-is/hail/pull/9194#issuecomment-671357400:233,Performance,perform,performs,233,"Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`). I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400
https://github.com/hail-is/hail/pull/9194#issuecomment-671357400:16,Usability,clear,clear,16,"Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`). I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:127,Deployability,install,installed,127,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:305,Deployability,install,installed,305,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:1421,Deployability,integrat,integration,1421,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:1634,Deployability,integrat,integration,1634,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:419,Integrability,depend,dependent,419,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:735,Integrability,wrap,wrap,735,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:779,Integrability,interface,interface,779,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:1421,Integrability,integrat,integration,1421,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:1634,Integrability,integrat,integration,1634,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:235,Performance,perform,performs,235,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:666,Testability,test,test,666,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:685,Testability,test,tests,685,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:765,Testability,test,test,765,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:18,Usability,clear,clear,18,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432
https://github.com/hail-is/hail/pull/9194#issuecomment-671516865:145,Integrability,depend,dependence,145,"I don't understand your first sentence. In the case of a hybrid query and batch script, using Hail Query is totally kosher! What I don't want is dependence on a JVM and Hail Query just to check if a file exists in Google Cloud Storage for a script that is otherwise independent of Hail Query.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671516865
https://github.com/hail-is/hail/pull/9194#issuecomment-671553680:223,Integrability,depend,dependence,223,"> I don't understand your first sentence. What didn't you understand? I'd like to clarify if that would be helpful. > In the case of a hybrid query and batch script, using Hail Query is totally kosher! What I don't want is dependence on a JVM and Hail Query just to check if a file exists in Google Cloud Storage for a script that is otherwise independent of Hail Query. So are you ok with keeping the build.yml changes (which would allow hailtop to have something within in that called hail query without being called from batch as a docker image + command line arguments).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671553680
https://github.com/hail-is/hail/pull/9198#issuecomment-673110373:159,Availability,error,error,159,@jigold I added a commit with many changes. Sorry. There were several broken links. I fixed all of them and enabled `nitpicky` which forces Sphinx to throw an error on all broken references. I also removed an unnecessary `rm -rf` from the Makefile.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9198#issuecomment-673110373
https://github.com/hail-is/hail/pull/9209#issuecomment-668610293:838,Integrability,interface,interface,838,"Ok, so Cotton's new thing means emitting separate methods by hand is not a thing we do anymore. But there are two factors hurting the benchmark. . One is that the benchmark is hiding the fact that we are spending ~25 seconds serializing and de-serializing JSON for this ndarray. So the real comparison is more like 55 seconds vs 75 seconds, which is a roughly 25% speed improvement. . The other is that `hl.nd.ones` is just an alias for `hl.nd.array(hl.range(shape_product)).map(lambda x: 1).reshape((n_rows, n_cols))`. This is going to create a bunch of row major data, copy it to column major in a pretty cache inefficient way during the reshape, then do the additions. So that's eating some of the time too. We should probably have a way for all the constant methods to not go through regular array. . Anyway, 25% improvement + better interface is a win for now, we can revisit ways to make this faster in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-668610293
https://github.com/hail-is/hail/pull/9209#issuecomment-668610293:607,Performance,cache,cache,607,"Ok, so Cotton's new thing means emitting separate methods by hand is not a thing we do anymore. But there are two factors hurting the benchmark. . One is that the benchmark is hiding the fact that we are spending ~25 seconds serializing and de-serializing JSON for this ndarray. So the real comparison is more like 55 seconds vs 75 seconds, which is a roughly 25% speed improvement. . The other is that `hl.nd.ones` is just an alias for `hl.nd.array(hl.range(shape_product)).map(lambda x: 1).reshape((n_rows, n_cols))`. This is going to create a bunch of row major data, copy it to column major in a pretty cache inefficient way during the reshape, then do the additions. So that's eating some of the time too. We should probably have a way for all the constant methods to not go through regular array. . Anyway, 25% improvement + better interface is a win for now, we can revisit ways to make this faster in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-668610293
https://github.com/hail-is/hail/pull/9209#issuecomment-668610293:134,Testability,benchmark,benchmark,134,"Ok, so Cotton's new thing means emitting separate methods by hand is not a thing we do anymore. But there are two factors hurting the benchmark. . One is that the benchmark is hiding the fact that we are spending ~25 seconds serializing and de-serializing JSON for this ndarray. So the real comparison is more like 55 seconds vs 75 seconds, which is a roughly 25% speed improvement. . The other is that `hl.nd.ones` is just an alias for `hl.nd.array(hl.range(shape_product)).map(lambda x: 1).reshape((n_rows, n_cols))`. This is going to create a bunch of row major data, copy it to column major in a pretty cache inefficient way during the reshape, then do the additions. So that's eating some of the time too. We should probably have a way for all the constant methods to not go through regular array. . Anyway, 25% improvement + better interface is a win for now, we can revisit ways to make this faster in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-668610293
https://github.com/hail-is/hail/pull/9209#issuecomment-668610293:163,Testability,benchmark,benchmark,163,"Ok, so Cotton's new thing means emitting separate methods by hand is not a thing we do anymore. But there are two factors hurting the benchmark. . One is that the benchmark is hiding the fact that we are spending ~25 seconds serializing and de-serializing JSON for this ndarray. So the real comparison is more like 55 seconds vs 75 seconds, which is a roughly 25% speed improvement. . The other is that `hl.nd.ones` is just an alias for `hl.nd.array(hl.range(shape_product)).map(lambda x: 1).reshape((n_rows, n_cols))`. This is going to create a bunch of row major data, copy it to column major in a pretty cache inefficient way during the reshape, then do the additions. So that's eating some of the time too. We should probably have a way for all the constant methods to not go through regular array. . Anyway, 25% improvement + better interface is a win for now, we can revisit ways to make this faster in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-668610293
https://github.com/hail-is/hail/pull/9209#issuecomment-668622849:219,Performance,optimiz,optimizing,219,I talked to Cotton about it and he said not to. But it's not clear how much of a difference that makes yet anyway. I think this version is pretty good and an improvement. Plus it'll add a benchmark which we can work on optimizing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-668622849
https://github.com/hail-is/hail/pull/9209#issuecomment-668622849:188,Testability,benchmark,benchmark,188,I talked to Cotton about it and he said not to. But it's not clear how much of a difference that makes yet anyway. I think this version is pretty good and an improvement. Plus it'll add a benchmark which we can work on optimizing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-668622849
https://github.com/hail-is/hail/pull/9209#issuecomment-668622849:61,Usability,clear,clear,61,I talked to Cotton about it and he said not to. But it's not clear how much of a difference that makes yet anyway. I think this version is pretty good and an improvement. Plus it'll add a benchmark which we can work on optimizing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-668622849
https://github.com/hail-is/hail/pull/9209#issuecomment-669232370:10,Testability,test,test,10,Added new test that checks summing mix of regular and transposed data to make sure that future changes respect striding.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-669232370
https://github.com/hail-is/hail/pull/9209#issuecomment-669255320:376,Energy Efficiency,allocate,allocated,376,"> Addressed all comments, should be good to go.; > ; > One question I did have is that I have to specify a region size in this aggregator, and I picked `TINY`. Does it matter? `TINY` does seem small compared to the kinds of operations that we'll do on dndarray and block matrix. That specifies the granularity of allocations. Each time the region needs more space than it has allocated, it allocates a new block of size determined by the size parameter. Unless it's trying to make a single allocation larger than the block size, in which case it allocates a block of exactly the desired size. The ideal thing here would be to make a single block with exactly the size needed for the state, but the current Region interface doesn't support that. The closest we can get is using the smallest block size, which is `TINIER`. Then the missing bit and pointer to the data would go in the initial tiny block, and the data would go there too if it's small enough, otherwise it would go in an allocation of exactly the right size.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-669255320
https://github.com/hail-is/hail/pull/9209#issuecomment-669255320:390,Energy Efficiency,allocate,allocates,390,"> Addressed all comments, should be good to go.; > ; > One question I did have is that I have to specify a region size in this aggregator, and I picked `TINY`. Does it matter? `TINY` does seem small compared to the kinds of operations that we'll do on dndarray and block matrix. That specifies the granularity of allocations. Each time the region needs more space than it has allocated, it allocates a new block of size determined by the size parameter. Unless it's trying to make a single allocation larger than the block size, in which case it allocates a block of exactly the desired size. The ideal thing here would be to make a single block with exactly the size needed for the state, but the current Region interface doesn't support that. The closest we can get is using the smallest block size, which is `TINIER`. Then the missing bit and pointer to the data would go in the initial tiny block, and the data would go there too if it's small enough, otherwise it would go in an allocation of exactly the right size.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-669255320
https://github.com/hail-is/hail/pull/9209#issuecomment-669255320:546,Energy Efficiency,allocate,allocates,546,"> Addressed all comments, should be good to go.; > ; > One question I did have is that I have to specify a region size in this aggregator, and I picked `TINY`. Does it matter? `TINY` does seem small compared to the kinds of operations that we'll do on dndarray and block matrix. That specifies the granularity of allocations. Each time the region needs more space than it has allocated, it allocates a new block of size determined by the size parameter. Unless it's trying to make a single allocation larger than the block size, in which case it allocates a block of exactly the desired size. The ideal thing here would be to make a single block with exactly the size needed for the state, but the current Region interface doesn't support that. The closest we can get is using the smallest block size, which is `TINIER`. Then the missing bit and pointer to the data would go in the initial tiny block, and the data would go there too if it's small enough, otherwise it would go in an allocation of exactly the right size.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-669255320
https://github.com/hail-is/hail/pull/9209#issuecomment-669255320:713,Integrability,interface,interface,713,"> Addressed all comments, should be good to go.; > ; > One question I did have is that I have to specify a region size in this aggregator, and I picked `TINY`. Does it matter? `TINY` does seem small compared to the kinds of operations that we'll do on dndarray and block matrix. That specifies the granularity of allocations. Each time the region needs more space than it has allocated, it allocates a new block of size determined by the size parameter. Unless it's trying to make a single allocation larger than the block size, in which case it allocates a block of exactly the desired size. The ideal thing here would be to make a single block with exactly the size needed for the state, but the current Region interface doesn't support that. The closest we can get is using the smallest block size, which is `TINIER`. Then the missing bit and pointer to the data would go in the initial tiny block, and the data would go there too if it's small enough, otherwise it would go in an allocation of exactly the right size.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-669255320
https://github.com/hail-is/hail/pull/9209#issuecomment-669386823:310,Performance,load,loadField,310,"Thanks for pushing me to clean that up with the `consume` infrastructure, much neater / shorter. . However, I can't seem to write a working `_result` function in that style. I tried several different ways, all led to segfaults, not sure why. I ended up just inlining calls to `isFieldDefined` and `PBaseStruct.loadField` for that one, as I didn't want to spend more time digging into it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-669386823
https://github.com/hail-is/hail/pull/9209#issuecomment-669397111:123,Performance,load,loadField,123,"Weird. Did you do something like this?; ```; val statePV = new PCanonicalBaseStructSettable(stateType, state.off); statePV.loadField(ndarrayFieldNumber); .consume(cb, srvb.setMissing(), srvb.addIRIntermediate(_, deepCopy = true)); ```; Anyways, happy to approve if you don't want to mess with it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-669397111
https://github.com/hail-is/hail/pull/9209#issuecomment-669402324:151,Performance,load,loaded,151,"Yup, I did that, and I also tried a version where I did `state.get` to get the emit code, then `toI.consume`ed that, and used that to get the pvalue I loaded the field of, and that also didn't work. I'm going to pick not to mess with it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-669402324
https://github.com/hail-is/hail/pull/9213#issuecomment-668750434:164,Availability,error,error,164,"@catoverdrive Assigned you since it will be good to get you acclimated to the services team code. We have a function called `is_transient_error` which checks if an error is a known-to-be-transient error. An error is transient if perpetually retrying the error-raising-operation will eventually lead to success. We consider most things transient, even 500 Internal Server Error, because we expect our systems work but sometimes fail under heavy load. Eventually load should drop.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9213#issuecomment-668750434
https://github.com/hail-is/hail/pull/9213#issuecomment-668750434:197,Availability,error,error,197,"@catoverdrive Assigned you since it will be good to get you acclimated to the services team code. We have a function called `is_transient_error` which checks if an error is a known-to-be-transient error. An error is transient if perpetually retrying the error-raising-operation will eventually lead to success. We consider most things transient, even 500 Internal Server Error, because we expect our systems work but sometimes fail under heavy load. Eventually load should drop.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9213#issuecomment-668750434
https://github.com/hail-is/hail/pull/9213#issuecomment-668750434:207,Availability,error,error,207,"@catoverdrive Assigned you since it will be good to get you acclimated to the services team code. We have a function called `is_transient_error` which checks if an error is a known-to-be-transient error. An error is transient if perpetually retrying the error-raising-operation will eventually lead to success. We consider most things transient, even 500 Internal Server Error, because we expect our systems work but sometimes fail under heavy load. Eventually load should drop.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9213#issuecomment-668750434
https://github.com/hail-is/hail/pull/9213#issuecomment-668750434:254,Availability,error,error-raising-operation,254,"@catoverdrive Assigned you since it will be good to get you acclimated to the services team code. We have a function called `is_transient_error` which checks if an error is a known-to-be-transient error. An error is transient if perpetually retrying the error-raising-operation will eventually lead to success. We consider most things transient, even 500 Internal Server Error, because we expect our systems work but sometimes fail under heavy load. Eventually load should drop.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9213#issuecomment-668750434
https://github.com/hail-is/hail/pull/9213#issuecomment-668750434:371,Availability,Error,Error,371,"@catoverdrive Assigned you since it will be good to get you acclimated to the services team code. We have a function called `is_transient_error` which checks if an error is a known-to-be-transient error. An error is transient if perpetually retrying the error-raising-operation will eventually lead to success. We consider most things transient, even 500 Internal Server Error, because we expect our systems work but sometimes fail under heavy load. Eventually load should drop.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9213#issuecomment-668750434
https://github.com/hail-is/hail/pull/9213#issuecomment-668750434:444,Performance,load,load,444,"@catoverdrive Assigned you since it will be good to get you acclimated to the services team code. We have a function called `is_transient_error` which checks if an error is a known-to-be-transient error. An error is transient if perpetually retrying the error-raising-operation will eventually lead to success. We consider most things transient, even 500 Internal Server Error, because we expect our systems work but sometimes fail under heavy load. Eventually load should drop.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9213#issuecomment-668750434
https://github.com/hail-is/hail/pull/9213#issuecomment-668750434:461,Performance,load,load,461,"@catoverdrive Assigned you since it will be good to get you acclimated to the services team code. We have a function called `is_transient_error` which checks if an error is a known-to-be-transient error. An error is transient if perpetually retrying the error-raising-operation will eventually lead to success. We consider most things transient, even 500 Internal Server Error, because we expect our systems work but sometimes fail under heavy load. Eventually load should drop.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9213#issuecomment-668750434
https://github.com/hail-is/hail/pull/9217#issuecomment-669202128:49,Testability,test,testing,49,I'll unassign you and assign Chris when I finish testing it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9217#issuecomment-669202128
https://github.com/hail-is/hail/pull/9217#issuecomment-669210874:82,Deployability,install,install-editable,82,"Confirmed this is working. If I introduce a segfault to some code, then run `make install-editable HAIL_DEBUG_MODE=1`, then execute the python code that would cause a segfault, it throws a RuntimeError.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9217#issuecomment-669210874
https://github.com/hail-is/hail/pull/9217#issuecomment-669211553:47,Testability,test,testing,47,@cseed I will make a follow up PR that changes testing to make the checked version run. This at least allows developers to turn on this behavior when debugging locally.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9217#issuecomment-669211553
https://github.com/hail-is/hail/pull/9219#issuecomment-669818238:28,Availability,error,errors,28,"This is generating some 500 errors, having a hard time debugging without hail-vdc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-669818238
https://github.com/hail-is/hail/pull/9219#issuecomment-670288861:173,Security,secur,security,173,"This should now be passing tests. I had to make one LocalBackend test conditional on docker, because we do not have docker in docker (DiD), and there were historically some security considerations that felt out of scope to fully grapple with in this PR (out of scope to create a DiD image and have test_hailtop_batch and test_batch_dcos use this DiD image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670288861
https://github.com/hail-is/hail/pull/9219#issuecomment-670288861:27,Testability,test,tests,27,"This should now be passing tests. I had to make one LocalBackend test conditional on docker, because we do not have docker in docker (DiD), and there were historically some security considerations that felt out of scope to fully grapple with in this PR (out of scope to create a DiD image and have test_hailtop_batch and test_batch_dcos use this DiD image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670288861
https://github.com/hail-is/hail/pull/9219#issuecomment-670288861:65,Testability,test,test,65,"This should now be passing tests. I had to make one LocalBackend test conditional on docker, because we do not have docker in docker (DiD), and there were historically some security considerations that felt out of scope to fully grapple with in this PR (out of scope to create a DiD image and have test_hailtop_batch and test_batch_dcos use this DiD image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670288861
https://github.com/hail-is/hail/pull/9219#issuecomment-670291415:175,Integrability,interface,interface,175,"Long term, I think it would be better to use docker engine for LocalBackend, so that container config/create/start between ServiceBackend and LocalBackend could have a common interface. If you're interested, after regenie is finished, I can PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670291415
https://github.com/hail-is/hail/pull/9219#issuecomment-670291415:95,Modifiability,config,config,95,"Long term, I think it would be better to use docker engine for LocalBackend, so that container config/create/start between ServiceBackend and LocalBackend could have a common interface. If you're interested, after regenie is finished, I can PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670291415
https://github.com/hail-is/hail/pull/9219#issuecomment-670667174:117,Integrability,wrap,wrapper,117,"Just saw this. > adds job.entrypoint method. I'm going to argue against this. The goal is not to make Batch a Docker wrapper, in fact, maybe that's an anti-goal. The goal is to build on docker (and other container tools) to create a natural interface for the problems Batch is trying to solve. The expectation is that the commands to be executed will be specified as part of the Job. In particular, I think if you don't specify the command, the command set is empty and no commands get run. It doesn't default to the docker command by default when no command is specified. So I would solve this by just always invoking the image with `--entrypoint <shell>`. > Long term, I think it would be better to use docker engine for LocalBackend, so that container config/create/start between ServiceBackend and LocalBackend could have a common interface. While code sharing often sounds nice, it also has the effect of entangling otherwise separate pieces of code. That means they cannot evolve separately and makes development more difficult. We have lots of plans to evolve the service backend that won't have analogues in the local backend, and I feel this proposal would make those changes harder and make the code more complex to handle two use cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670667174
https://github.com/hail-is/hail/pull/9219#issuecomment-670667174:241,Integrability,interface,interface,241,"Just saw this. > adds job.entrypoint method. I'm going to argue against this. The goal is not to make Batch a Docker wrapper, in fact, maybe that's an anti-goal. The goal is to build on docker (and other container tools) to create a natural interface for the problems Batch is trying to solve. The expectation is that the commands to be executed will be specified as part of the Job. In particular, I think if you don't specify the command, the command set is empty and no commands get run. It doesn't default to the docker command by default when no command is specified. So I would solve this by just always invoking the image with `--entrypoint <shell>`. > Long term, I think it would be better to use docker engine for LocalBackend, so that container config/create/start between ServiceBackend and LocalBackend could have a common interface. While code sharing often sounds nice, it also has the effect of entangling otherwise separate pieces of code. That means they cannot evolve separately and makes development more difficult. We have lots of plans to evolve the service backend that won't have analogues in the local backend, and I feel this proposal would make those changes harder and make the code more complex to handle two use cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670667174
https://github.com/hail-is/hail/pull/9219#issuecomment-670667174:835,Integrability,interface,interface,835,"Just saw this. > adds job.entrypoint method. I'm going to argue against this. The goal is not to make Batch a Docker wrapper, in fact, maybe that's an anti-goal. The goal is to build on docker (and other container tools) to create a natural interface for the problems Batch is trying to solve. The expectation is that the commands to be executed will be specified as part of the Job. In particular, I think if you don't specify the command, the command set is empty and no commands get run. It doesn't default to the docker command by default when no command is specified. So I would solve this by just always invoking the image with `--entrypoint <shell>`. > Long term, I think it would be better to use docker engine for LocalBackend, so that container config/create/start between ServiceBackend and LocalBackend could have a common interface. While code sharing often sounds nice, it also has the effect of entangling otherwise separate pieces of code. That means they cannot evolve separately and makes development more difficult. We have lots of plans to evolve the service backend that won't have analogues in the local backend, and I feel this proposal would make those changes harder and make the code more complex to handle two use cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670667174
https://github.com/hail-is/hail/pull/9219#issuecomment-670667174:755,Modifiability,config,config,755,"Just saw this. > adds job.entrypoint method. I'm going to argue against this. The goal is not to make Batch a Docker wrapper, in fact, maybe that's an anti-goal. The goal is to build on docker (and other container tools) to create a natural interface for the problems Batch is trying to solve. The expectation is that the commands to be executed will be specified as part of the Job. In particular, I think if you don't specify the command, the command set is empty and no commands get run. It doesn't default to the docker command by default when no command is specified. So I would solve this by just always invoking the image with `--entrypoint <shell>`. > Long term, I think it would be better to use docker engine for LocalBackend, so that container config/create/start between ServiceBackend and LocalBackend could have a common interface. While code sharing often sounds nice, it also has the effect of entangling otherwise separate pieces of code. That means they cannot evolve separately and makes development more difficult. We have lots of plans to evolve the service backend that won't have analogues in the local backend, and I feel this proposal would make those changes harder and make the code more complex to handle two use cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670667174
https://github.com/hail-is/hail/pull/9219#issuecomment-670667174:979,Modifiability,evolve,evolve,979,"Just saw this. > adds job.entrypoint method. I'm going to argue against this. The goal is not to make Batch a Docker wrapper, in fact, maybe that's an anti-goal. The goal is to build on docker (and other container tools) to create a natural interface for the problems Batch is trying to solve. The expectation is that the commands to be executed will be specified as part of the Job. In particular, I think if you don't specify the command, the command set is empty and no commands get run. It doesn't default to the docker command by default when no command is specified. So I would solve this by just always invoking the image with `--entrypoint <shell>`. > Long term, I think it would be better to use docker engine for LocalBackend, so that container config/create/start between ServiceBackend and LocalBackend could have a common interface. While code sharing often sounds nice, it also has the effect of entangling otherwise separate pieces of code. That means they cannot evolve separately and makes development more difficult. We have lots of plans to evolve the service backend that won't have analogues in the local backend, and I feel this proposal would make those changes harder and make the code more complex to handle two use cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670667174
https://github.com/hail-is/hail/pull/9219#issuecomment-670667174:1060,Modifiability,evolve,evolve,1060,"Just saw this. > adds job.entrypoint method. I'm going to argue against this. The goal is not to make Batch a Docker wrapper, in fact, maybe that's an anti-goal. The goal is to build on docker (and other container tools) to create a natural interface for the problems Batch is trying to solve. The expectation is that the commands to be executed will be specified as part of the Job. In particular, I think if you don't specify the command, the command set is empty and no commands get run. It doesn't default to the docker command by default when no command is specified. So I would solve this by just always invoking the image with `--entrypoint <shell>`. > Long term, I think it would be better to use docker engine for LocalBackend, so that container config/create/start between ServiceBackend and LocalBackend could have a common interface. While code sharing often sounds nice, it also has the effect of entangling otherwise separate pieces of code. That means they cannot evolve separately and makes development more difficult. We have lots of plans to evolve the service backend that won't have analogues in the local backend, and I feel this proposal would make those changes harder and make the code more complex to handle two use cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670667174
https://github.com/hail-is/hail/pull/9219#issuecomment-670693135:339,Integrability,interface,interface,339,"> I have an image built from Dockerfile that I cannot modify, and which has an ENTRYPOINT that is not /bin/bash or /bin/sh. How do I get this to run in batch without this PR? I don't understand the proposal. The proposal is to have the backend run `docker run --entrypoint <shell> ...` unconditionally. This involves no change to the user interface. I believe this would implement the intended Batch behavior on any docker image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670693135
https://github.com/hail-is/hail/pull/9219#issuecomment-670707543:316,Modifiability,config,config,316,"I'm sorry, I was unclear when I suggested that we ""fix the ENTRYPOINT bug"". In my view, the bug is that we do not *unset* the entry point when we invoke Docker (locally and in the service backend). Locally this means including the argument `--entrypoint """"`. In the service we need to modify `worker.py` to set the `config`'s `Entrypoint` to, I think, the empty string. I do not want `entrypoint` in the job spec at all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670707543
https://github.com/hail-is/hail/pull/9219#issuecomment-673080598:262,Integrability,depend,dependency,262,"Besides the question to Jackie, I'd still like a bit of clarity on https://github.com/hail-is/hail/pull/9219#discussion_r469015358 (if you want me to add an entrypoint-override image and test). The question there: is the appropriate place a buildImage step (and dependency for test_hailtop_batch), or in Makefile as you stated (I couldn't see how batch/Makefile was being used by CI, may have missed it)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-673080598
https://github.com/hail-is/hail/pull/9219#issuecomment-673080598:187,Testability,test,test,187,"Besides the question to Jackie, I'd still like a bit of clarity on https://github.com/hail-is/hail/pull/9219#discussion_r469015358 (if you want me to add an entrypoint-override image and test). The question there: is the appropriate place a buildImage step (and dependency for test_hailtop_batch), or in Makefile as you stated (I couldn't see how batch/Makefile was being used by CI, may have missed it)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-673080598
https://github.com/hail-is/hail/pull/9220#issuecomment-671943780:110,Testability,test,test,110,"Yeah, I was thinking about that. It could be a parameter on all `ExecStrategy`s, but that's a lot of cases to test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9220#issuecomment-671943780
https://github.com/hail-is/hail/pull/9222#issuecomment-669853205:656,Testability,test,test,656,"You can also checkout main, `git cherry-pick` the commits you need:. `git cherry-pick d9a599b8b3d7b4ecec8775ed9ac3831aa2f01280 b8a1add6a0ce7ccf2ae8f5eac5d5edaac965b098 c0a4768923f1c714f9b5a06ba1a29c059ea5ac03 37b507fa6844af213f3c4eb82da033f5e39 07aa1036e0d2ac2435b357e72721be6352e41a1f 2082f04f17a4282fc6878f4b29239a14bfd3e898 30be897872a71ca22e3338ceb855cbc73307ef16 95ede3f473dddaa67fa7b942193db2cf126447a6 f0055585e596e3c51585f1e0d4359cc19509cbe7 1c3984ba4417dad3e0d8cea2802ba4a6ef4fb14e c0a444cb86bc249d5edfd017a75ba84d9e4e6738 266c7e8223066ef0a1a1f3db4923414f7dd3aaa6`; * here is the result: https://github.com/hail-is/hail/compare/main...akotlar:ann-test?expand=1. I think those are off of the commits, but you may want to check. Again, seems easier to checkout main and copy in the 4 files that were modified",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9222#issuecomment-669853205
https://github.com/hail-is/hail/pull/9223#issuecomment-670037279:42,Availability,error,error,42,"This also matches the corresponding Numpy error message almost precisely, which is nice. Difference is that ""Index"" is lowercase for Numpy. It's too bad that we can't keep the error message in python land, and the stack trace is utterly useless to a user. I'll make an issue for this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9223#issuecomment-670037279
https://github.com/hail-is/hail/pull/9223#issuecomment-670037279:176,Availability,error,error,176,"This also matches the corresponding Numpy error message almost precisely, which is nice. Difference is that ""Index"" is lowercase for Numpy. It's too bad that we can't keep the error message in python land, and the stack trace is utterly useless to a user. I'll make an issue for this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9223#issuecomment-670037279
https://github.com/hail-is/hail/pull/9223#issuecomment-670037279:48,Integrability,message,message,48,"This also matches the corresponding Numpy error message almost precisely, which is nice. Difference is that ""Index"" is lowercase for Numpy. It's too bad that we can't keep the error message in python land, and the stack trace is utterly useless to a user. I'll make an issue for this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9223#issuecomment-670037279
https://github.com/hail-is/hail/pull/9223#issuecomment-670037279:182,Integrability,message,message,182,"This also matches the corresponding Numpy error message almost precisely, which is nice. Difference is that ""Index"" is lowercase for Numpy. It's too bad that we can't keep the error message in python land, and the stack trace is utterly useless to a user. I'll make an issue for this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9223#issuecomment-670037279
https://github.com/hail-is/hail/pull/9223#issuecomment-670168023:30,Availability,failure,failure,30,"needs bump, looks like random failure (shuffler was last test run)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9223#issuecomment-670168023
https://github.com/hail-is/hail/pull/9223#issuecomment-670168023:57,Testability,test,test,57,"needs bump, looks like random failure (shuffler was last test run)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9223#issuecomment-670168023
https://github.com/hail-is/hail/issues/9226#issuecomment-672126952:197,Availability,error,error-messages-idea,197,I think one answer to this would be to just cut out the Java parts and show just the summary. But I also wrote up some further thoughts for how to improve here: https://dev.hail.is/t/better-python-error-messages-idea/201,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226#issuecomment-672126952
https://github.com/hail-is/hail/issues/9226#issuecomment-672126952:203,Integrability,message,messages-idea,203,I think one answer to this would be to just cut out the Java parts and show just the summary. But I also wrote up some further thoughts for how to improve here: https://dev.hail.is/t/better-python-error-messages-idea/201,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226#issuecomment-672126952
https://github.com/hail-is/hail/issues/9226#issuecomment-672876115:124,Availability,error,error,124,"I want to get it to not only omit the Java, but also show the desirable python stack trace that points to the source of the error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226#issuecomment-672876115
https://github.com/hail-is/hail/issues/9226#issuecomment-672877232:141,Usability,usab,usability,141,"This is on the radar, so I vote for closing this issue. We're using issues for bugs, and this seems more in the realm of a feature request / usability improvement.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9226#issuecomment-672877232
https://github.com/hail-is/hail/pull/9228#issuecomment-683803596:71,Availability,error,errors,71,"Ok, merged to resolve conflicts. Going to now add a few commits to fix errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9228#issuecomment-683803596
https://github.com/hail-is/hail/pull/9233#issuecomment-670542937:77,Integrability,message,messages,77,"Alex, can you use the PR naming convention for the first line of your commit messages too? Github doesn't use the PR text as the squashed commit message for single-commit PRs, so the untagged/non-descriptive single commit message went into main history:. `we setlled on a name (#9233)`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9233#issuecomment-670542937
https://github.com/hail-is/hail/pull/9233#issuecomment-670542937:145,Integrability,message,message,145,"Alex, can you use the PR naming convention for the first line of your commit messages too? Github doesn't use the PR text as the squashed commit message for single-commit PRs, so the untagged/non-descriptive single commit message went into main history:. `we setlled on a name (#9233)`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9233#issuecomment-670542937
https://github.com/hail-is/hail/pull/9233#issuecomment-670542937:222,Integrability,message,message,222,"Alex, can you use the PR naming convention for the first line of your commit messages too? Github doesn't use the PR text as the squashed commit message for single-commit PRs, so the untagged/non-descriptive single commit message went into main history:. `we setlled on a name (#9233)`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9233#issuecomment-670542937
https://github.com/hail-is/hail/pull/9241#issuecomment-674186071:64,Availability,failure,failures,64,I put the WIP tag on this. I don't have the energy to debug any failures today. Will merge it on Monday.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9241#issuecomment-674186071
https://github.com/hail-is/hail/pull/9241#issuecomment-674186071:44,Energy Efficiency,energy,energy,44,I put the WIP tag on this. I don't have the energy to debug any failures today. Will merge it on Monday.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9241#issuecomment-674186071
https://github.com/hail-is/hail/pull/9241#issuecomment-677664900:182,Deployability,deploy,deploy,182,"There's something wrong with this PR. The database migration step says it's successful, but the new database is never actually created. I think this is the same thing I saw with dev deploy and attributed it to the wrong cause. ```; +------------------------------------+; | Database |; +------------------------------------+; ...; | pr-9241-auth-zdyt4a4geys7 |; | pr-9241-batch-y0qvw1vpniad |; | pr-9241-ci-nkuua31y7nxn |; | pr-9241-test-instance-zxxeu6gotctw |; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9241#issuecomment-677664900
https://github.com/hail-is/hail/pull/9241#issuecomment-677664900:433,Testability,test,test-instance-,433,"There's something wrong with this PR. The database migration step says it's successful, but the new database is never actually created. I think this is the same thing I saw with dev deploy and attributed it to the wrong cause. ```; +------------------------------------+; | Database |; +------------------------------------+; ...; | pr-9241-auth-zdyt4a4geys7 |; | pr-9241-batch-y0qvw1vpniad |; | pr-9241-ci-nkuua31y7nxn |; | pr-9241-test-instance-zxxeu6gotctw |; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9241#issuecomment-677664900
https://github.com/hail-is/hail/pull/9243#issuecomment-672397590:71,Testability,test,tested,71,@danking could you help me get this merged? this isn't being picked up/tested by CI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9243#issuecomment-672397590
https://github.com/hail-is/hail/pull/9246#issuecomment-671463155:16,Security,encrypt,encrypt,16,I will run lets encrypt now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9246#issuecomment-671463155
https://github.com/hail-is/hail/pull/9250#issuecomment-673731776:29,Integrability,protocol,protocol,29,Thanks. Didn't know what the protocol was and didn't want to be pushy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9250#issuecomment-673731776
https://github.com/hail-is/hail/pull/9253#issuecomment-673068628:0,Testability,Benchmark,Benchmarks,0,Benchmarks look identical:; ```; Harmonic mean: 98.8%; Geometric mean: 99.1%; Arithmetic mean: 99.3%; Median: 99.4%; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9253#issuecomment-673068628
https://github.com/hail-is/hail/pull/9262#issuecomment-673163784:267,Deployability,release,release-notes,267,"Hey @danking. I've also got a PR open for this issue (#9250) which includes a check for the minimum required version of `gcloud`. Also, including the `--secondary-worker-type` flag increases the minimum `gcloud` version to [291.0.0](https://cloud.google.com/sdk/docs/release-notes#28500_2020-03-17) (released in May).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9262#issuecomment-673163784
https://github.com/hail-is/hail/pull/9262#issuecomment-673163784:300,Deployability,release,released,300,"Hey @danking. I've also got a PR open for this issue (#9250) which includes a check for the minimum required version of `gcloud`. Also, including the `--secondary-worker-type` flag increases the minimum `gcloud` version to [291.0.0](https://cloud.google.com/sdk/docs/release-notes#28500_2020-03-17) (released in May).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9262#issuecomment-673163784
https://github.com/hail-is/hail/pull/9283#issuecomment-674927789:158,Testability,test,test,158,"ugh, ok. On Mon, Aug 17, 2020 at 10:41 AM Tim Poterba <notifications@github.com>; wrote:. > *@tpoterba* requested changes on this pull request.; >; > needs a test; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/9283#pullrequestreview-468525706>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADZNYW26LMF4NJHR4TDHL23SBE6SNANCNFSM4QBW7UJA>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9283#issuecomment-674927789
https://github.com/hail-is/hail/pull/9285#issuecomment-674977850:57,Deployability,configurat,configuration,57,"In addition to the main change:; - I made all the global configuration options editable now that we have flexible billing,; - We now check the worker cores and standing worker cores against the worker type to make sure they are valid (there is no highmem/highcpu-1).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674977850
https://github.com/hail-is/hail/pull/9285#issuecomment-674977850:57,Modifiability,config,configuration,57,"In addition to the main change:; - I made all the global configuration options editable now that we have flexible billing,; - We now check the worker cores and standing worker cores against the worker type to make sure they are valid (there is no highmem/highcpu-1).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674977850
https://github.com/hail-is/hail/pull/9285#issuecomment-674977850:105,Modifiability,flexible,flexible,105,"In addition to the main change:; - I made all the global configuration options editable now that we have flexible billing,; - We now check the worker cores and standing worker cores against the worker type to make sure they are valid (there is no highmem/highcpu-1).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674977850
https://github.com/hail-is/hail/pull/9285#issuecomment-674986845:95,Energy Efficiency,schedul,scheduled,95,The reason we didn't expose the other parameters was what if we had 16 core jobs waiting to be scheduled and then we changed the worker pool size to 4 cores.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674986845
https://github.com/hail-is/hail/pull/9285#issuecomment-674986845:21,Security,expose,expose,21,The reason we didn't expose the other parameters was what if we had 16 core jobs waiting to be scheduled and then we changed the worker pool size to 4 cores.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674986845
https://github.com/hail-is/hail/pull/9285#issuecomment-674998109:196,Availability,error,error,196,"> The reason we didn't expose the other parameters was what if we had 16 core jobs waiting to be scheduled and then we changed the worker pool size to 4 cores. Yep. Let's call that admin operator error and let's not do that. The other reason was we had hardcoded the billing computation in the code, but that's fixed now. But it is hardcoded in the documentation, so we still shouldn't really be changing any of these settings (I see this mainly for the second instance at this point). Separately, we should decouple the billing from the details of the implementation so we get a bit more flexibility on the backend in the main instance, as we've discussed. I'm OK with this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674998109
https://github.com/hail-is/hail/pull/9285#issuecomment-674998109:97,Energy Efficiency,schedul,scheduled,97,"> The reason we didn't expose the other parameters was what if we had 16 core jobs waiting to be scheduled and then we changed the worker pool size to 4 cores. Yep. Let's call that admin operator error and let's not do that. The other reason was we had hardcoded the billing computation in the code, but that's fixed now. But it is hardcoded in the documentation, so we still shouldn't really be changing any of these settings (I see this mainly for the second instance at this point). Separately, we should decouple the billing from the details of the implementation so we get a bit more flexibility on the backend in the main instance, as we've discussed. I'm OK with this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674998109
https://github.com/hail-is/hail/pull/9285#issuecomment-674998109:23,Security,expose,expose,23,"> The reason we didn't expose the other parameters was what if we had 16 core jobs waiting to be scheduled and then we changed the worker pool size to 4 cores. Yep. Let's call that admin operator error and let's not do that. The other reason was we had hardcoded the billing computation in the code, but that's fixed now. But it is hardcoded in the documentation, so we still shouldn't really be changing any of these settings (I see this mainly for the second instance at this point). Separately, we should decouple the billing from the details of the implementation so we get a bit more flexibility on the backend in the main instance, as we've discussed. I'm OK with this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674998109
https://github.com/hail-is/hail/pull/9288#issuecomment-675090452:20,Testability,test,tests,20,This is passing the tests. I didn't double check the billing numbers in the database look correct. Can do so if you think it's prudent.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9288#issuecomment-675090452
https://github.com/hail-is/hail/pull/9292#issuecomment-675510814:418,Safety,avoid,avoids,418,"> Using the automatic Json deserializer kind of makes us write the wrong code here, because we really want one LZ4CodecSpec which takes the appropriate parameters, and then deserializer can construct it with the proper arguments based on the specific case being derserialized (a legacy format, or the current one with parameters). Yeah, I agree. I think it would be a couple days of work to write ser/deser stuff that avoids the json4s case class extraction, but I'm not sure how high-priority that is right now since this stuff is somewhat stable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9292#issuecomment-675510814
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:4103,Availability,avail,available,4103,"e(mt_ld_fn); print(mt.count()); print(""running omit filter""); mt=mt.annotate_cols(pheno = table[mt.s]); mt = mt.filter_cols(mt.pheno.omit == 0); print(mt.count()); print(""running pc_relate""); pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); pairs = pc_rel.filter(pc_rel['kin'] >= 0.0883); print(""finding Max ind set""); related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,keep=False); print(""writing related samples to remove""); related_samples_to_remove.export(""file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/related_samples_""+pop+"".tsv""); result = mt.filter_cols(; hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False); print(""final unrelated count""); print(result.count); eigenvalues, scores, loadings = hl.hwe_normalized_pca(result.GT, k=10); scores.export('file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/unrelated_pcs_'+pop+'.tsv'). ```. ```; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to logs/adgc_pc_relate.autosome_all.log; 2020-08-20 10:14:00 Hail: INFO: Reading table to impute column types; [Stage 0:===========================================================(1 + 0) / 1]2020-08-20 10:14:07 Hail: INFO: Loading 76 fields. Counts by type:; 66 fields: user-specified int32; 6 fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:========================================",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:7161,Availability,Error,Error,7161,", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:7410,Availability,error,error,7410,"ail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:9585,Availability,failure,failure,9585,"eContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 103.0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:9645,Availability,failure,failure,9645,"ExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 103.0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:19932,Availability,Error,Error,19932,eOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.46-6ef64c08b000; Error summary: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:5462,Deployability,install,install,5462,"10:14:07 Hail: INFO: Loading 76 fields. Counts by type:; 66 fields: user-specified int32; 6 fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:5620,Deployability,install,install,5620,"eld: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/inst",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:5819,Deployability,install,install,5819,"-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/sp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:5977,Deployability,install,install,5977,"2796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6237,Deployability,install,install,6237,"py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.util",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6395,Deployability,install,install,6395,"[Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeExcep",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6625,Deployability,install,install,6625,"ython3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(Lowe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6777,Deployability,install,install,6777,"r; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.fo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6957,Deployability,install,install,6957,"/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:7074,Deployability,install,install,7074,"ter, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:11566,Energy Efficiency,schedul,scheduler,11566,ava:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.m,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:11637,Energy Efficiency,schedul,scheduler,11637,eSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12155,Energy Efficiency,schedul,scheduler,12155,6); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12195,Energy Efficiency,schedul,scheduler,12195,kMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12293,Energy Efficiency,schedul,scheduler,12293,trix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12390,Energy Efficiency,schedul,scheduler,12390,pute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12641,Energy Efficiency,schedul,scheduler,12641,.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12721,Energy Efficiency,schedul,scheduler,12721,nfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12826,Energy Efficiency,schedul,scheduler,12826,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RD,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12974,Energy Efficiency,schedul,scheduler,12974,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:13062,Energy Efficiency,schedul,scheduler,13062,617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:13159,Energy Efficiency,schedul,scheduler,13159,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.expr.ir.functions.MatrixWriteBlockMatrix.execute(MatrixWriteBlockMatrix.scal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:13254,Energy Efficiency,schedul,scheduler,13254,.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.expr.ir.functions.MatrixWriteBlockMatrix.execute(MatrixWriteBlockMatrix.scala:47); at is.hail.expr.ir.functions.WrappedMatrixToValueFunction.execute(RelationalFunctions.sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:13417,Energy Efficiency,schedul,scheduler,13417,abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.expr.ir.functions.MatrixWriteBlockMatrix.execute(MatrixWriteBlockMatrix.scala:47); at is.hail.expr.ir.functions.WrappedMatrixToValueFunction.execute(RelationalFunctions.scala:88); at is.hail.expr.ir.Interpret$.run(Interpret.scala:735); at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); at is.hail.expr.ir.InterpretNonC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:19348,Energy Efficiency,schedul,scheduler,19348,eOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.46-6ef64c08b000; Error summary: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:19419,Energy Efficiency,schedul,scheduler,19419,eOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.46-6ef64c08b000; Error summary: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:5537,Integrability,wrap,wrapper,5537,"fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:5894,Integrability,wrap,wrapper,5894,"==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6312,Integrability,wrap,wrapper,6312,"nship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.cr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:7771,Integrability,Wrap,WrappedArray,7771,"install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hail.utils.package$.using(package.scala:601); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.S",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:7792,Integrability,Wrap,WrappedArray,7792,"n3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hail.utils.package$.using(package.scala:601); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.execu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:14200,Integrability,Wrap,WrappedMatrixToValueFunction,14200,eduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.expr.ir.functions.MatrixWriteBlockMatrix.execute(MatrixWriteBlockMatrix.scala:47); at is.hail.expr.ir.functions.WrappedMatrixToValueFunction.execute(RelationalFunctions.scala:88); at is.hail.expr.ir.Interpret$.run(Interpret.scala:735); at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:15756,Integrability,Wrap,WrappedArray,15756,m(LoweringPass.scala:50); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hail.utils.package$.using(package.scala:601); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.S,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:15777,Integrability,Wrap,WrappedArray,15777,ala:50); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304); at is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:303); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20); at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18); at is.hail.utils.package$.using(package.scala:601); at is.hail.annotations.Region$.scoped(Region.scala:18); at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.execu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:14571,Modifiability,rewrite,rewrite,14571,ext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.expr.ir.functions.MatrixWriteBlockMatrix.execute(MatrixWriteBlockMatrix.scala:47); at is.hail.expr.ir.functions.WrappedMatrixToValueFunction.execute(RelationalFunctions.scala:88); at is.hail.expr.ir.Interpret$.run(Interpret.scala:735); at is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53); at is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16); at is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53); at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:50); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69); at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); at is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:45); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:20); at is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:3887,Performance,load,loadings,3887,",; 'pc1': hl.tfloat,; 'pc2': hl.tfloat,; 'pc3': hl.tfloat,; 'pc4': hl.tfloat,; 'pc5': hl.tfloat,; 'pc6': hl.tfloat,; 'pc7': hl.tfloat,; 'pc8': hl.tfloat,; 'pc9': hl.tfloat,; 'pc10': hl.tfloat}).key_by('vcfID')). mt=hl.read_matrix_table(mt_ld_fn); print(mt.count()); print(""running omit filter""); mt=mt.annotate_cols(pheno = table[mt.s]); mt = mt.filter_cols(mt.pheno.omit == 0); print(mt.count()); print(""running pc_relate""); pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); pairs = pc_rel.filter(pc_rel['kin'] >= 0.0883); print(""finding Max ind set""); related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,keep=False); print(""writing related samples to remove""); related_samples_to_remove.export(""file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/related_samples_""+pop+"".tsv""); result = mt.filter_cols(; hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False); print(""final unrelated count""); print(result.count); eigenvalues, scores, loadings = hl.hwe_normalized_pca(result.GT, k=10); scores.export('file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/unrelated_pcs_'+pop+'.tsv'). ```. ```; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to logs/adgc_pc_relate.autosome_all.log; 2020-08-20 10:14:00 Hail: INFO: Reading table to impute column types; [Stage 0:===========================================================(1 + 0) / 1]2020-08-20 10:14:07 Hail: INFO: Loading 76 fields. Counts by type:; 66 fields: user-specified int32; 6 fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: runni",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:4491,Performance,Load,Loading,4491,"pc_rel['kin'] >= 0.0883); print(""finding Max ind set""); related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,keep=False); print(""writing related samples to remove""); related_samples_to_remove.export(""file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/related_samples_""+pop+"".tsv""); result = mt.filter_cols(; hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False); print(""final unrelated count""); print(result.count); eigenvalues, scores, loadings = hl.hwe_normalized_pca(result.GT, k=10); scores.export('file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/unrelated_pcs_'+pop+'.tsv'). ```. ```; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to logs/adgc_pc_relate.autosome_all.log; 2020-08-20 10:14:00 Hail: INFO: Reading table to impute column types; [Stage 0:===========================================================(1 + 0) / 1]2020-08-20 10:14:07 Hail: INFO: Loading 76 fields. Counts by type:; 66 fields: user-specified int32; 6 fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Sta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6881,Performance,load,loads,6881,"614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.sca",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:11918,Performance,concurren,concurrent,11918,ava:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Opti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12002,Performance,concurren,concurrent,12002,.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:19700,Performance,concurren,concurrent,19700,eOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.46-6ef64c08b000; Error summary: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:19784,Performance,concurren,concurrent,19784,eOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.46-6ef64c08b000; Error summary: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:9564,Safety,abort,aborted,9564,"eContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 103.0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12325,Safety,abort,abortStage,12325,cala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12422,Safety,abort,abortStage,12422,:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12664,Safety,abort,abortStage,12664,org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:10502,Security,Checksum,ChecksumFileSystem,10502,"5). org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 103.0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:10521,Security,Checksum,ChecksumFSOutputSummer,10521,"5). org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 103.0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:10551,Security,Checksum,ChecksumFileSystem,10551,"n: Job aborted due to stage failure: Task 12 in stage 103.0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:10605,Security,Checksum,ChecksumFileSystem,10605,".0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:10631,Security,Checksum,ChecksumFileSystem,10631," recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:10685,Security,Checksum,ChecksumFileSystem,10685,"4, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.ex",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:10711,Security,Checksum,ChecksumFileSystem,10711,executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:18284,Security,Checksum,ChecksumFileSystem,18284,ommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.it,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:18303,Security,Checksum,ChecksumFSOutputSummer,18303,ommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.it,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:18333,Security,Checksum,ChecksumFileSystem,18333,d.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:18387,Security,Checksum,ChecksumFileSystem,18387,mand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:18413,Security,Checksum,ChecksumFileSystem,18413,GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:18467,Security,Checksum,ChecksumFileSystem,18467,a.lang.Thread.run(Thread.java:745). java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:18493,Security,Checksum,ChecksumFileSystem,18493,d.java:745). java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:231,Testability,log,log,231,"@tpoterba Another hail script ran into this too many files open issue running pc_relate. ```; pc_relate_pop2.py; import pandas as pd; import numpy as np; from scipy.special import chdtri; from numpy import median; from math import log,isnan; import matplotlib as mpl; import matplotlib.pyplot as plt; import seaborn as sns; import sys; import hail as hl; from bokeh.io import output_notebook, show, export_png, output_file, save; from pprint import pprint. pop=sys.argv[1]; maf_cutoff=0.10. # input files; #. # pfn=""file:///restricted/projectnb/adgc/imp.topmed_adsp5k/analysis/adgc.aa.pheno.txt""; # apoe_fn=""file:///restricted/projectnb/ukbiobank/ad/apoe/ukbb_hg38_imputed_phased.tsv""; mt_fn='/project/adgc/topmed2.2020_07/pop/adgc.'+pop+'.autosome.dose.mt'; mt_ld_fn='/project/adgc/topmed2.2020_07/pop/adgc.'+pop+'.autosome.dose.ld_pruned.mt'; pfn=""file:///restricted/projectnb/adgc/topmed.r2.analysis/pheno/ADGC_ea_aa_eas_hisp_graace_pheno.qced.txt""; #; # Initialize Hail for GRCh38. # hl.init(default_reference='GRCh38',log=""results/adgc_pc_relate.autosome.log"",tmp_dir=""file:///restricted/projectnb/ukbiobank/ad/analysis/ad.v1/tmp""). hl.init(default_reference='GRCh38',log=""logs/adgc_pc_relate.autosome_""+pop+"".log""); table = (hl.import_table(pfn,impute=True, missing=['','.','NA'],types={; 'FID': hl.tstr,; 'IID': hl.tstr,; 'vcfID': hl.tstr,; 'status': hl.tfloat,; 'age': hl.tfloat,; 'sex': hl.tfloat,; 'apoe': hl.tstr,; 'cohort': hl.tstr,; 'pop':hl.tstr,; 'omit':hl.tint,; 'AD': hl.tint,; 'MCI': hl.tint,; 'e2': hl.tint,; 'e3': hl.tint,; 'e4': hl.tint,; 'e44': hl.tint,; 'e34': hl.tint,; 'e33': hl.tint,; 'e24': hl.tint,; 'e23': hl.tint,; 'e22': hl.tint,; 'ACT_aa': hl.tint,; 'ACT1_ea': hl.tint,; 'ACT2_ea': hl.tint,; 'ADC1_ea': hl.tint,; 'ADC10_aa': hl.tint,; 'ADC10_eas': hl.tint,; 'ADC10_ea': hl.tint,; 'ADC10_hisp': hl.tint,; 'ADC12_aa': hl.tint,; 'ADC2_ea': hl.tint,; 'ADC3_aa': hl.tint,; 'ADC3_ea': hl.tint,; 'ADC4_ea': hl.tint,; 'ADC5_ea': hl.tint,; 'ADC6_ea': hl.tint,; 'ADC7_ea': hl.tin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:1023,Testability,log,log,1023,"l script ran into this too many files open issue running pc_relate. ```; pc_relate_pop2.py; import pandas as pd; import numpy as np; from scipy.special import chdtri; from numpy import median; from math import log,isnan; import matplotlib as mpl; import matplotlib.pyplot as plt; import seaborn as sns; import sys; import hail as hl; from bokeh.io import output_notebook, show, export_png, output_file, save; from pprint import pprint. pop=sys.argv[1]; maf_cutoff=0.10. # input files; #. # pfn=""file:///restricted/projectnb/adgc/imp.topmed_adsp5k/analysis/adgc.aa.pheno.txt""; # apoe_fn=""file:///restricted/projectnb/ukbiobank/ad/apoe/ukbb_hg38_imputed_phased.tsv""; mt_fn='/project/adgc/topmed2.2020_07/pop/adgc.'+pop+'.autosome.dose.mt'; mt_ld_fn='/project/adgc/topmed2.2020_07/pop/adgc.'+pop+'.autosome.dose.ld_pruned.mt'; pfn=""file:///restricted/projectnb/adgc/topmed.r2.analysis/pheno/ADGC_ea_aa_eas_hisp_graace_pheno.qced.txt""; #; # Initialize Hail for GRCh38. # hl.init(default_reference='GRCh38',log=""results/adgc_pc_relate.autosome.log"",tmp_dir=""file:///restricted/projectnb/ukbiobank/ad/analysis/ad.v1/tmp""). hl.init(default_reference='GRCh38',log=""logs/adgc_pc_relate.autosome_""+pop+"".log""); table = (hl.import_table(pfn,impute=True, missing=['','.','NA'],types={; 'FID': hl.tstr,; 'IID': hl.tstr,; 'vcfID': hl.tstr,; 'status': hl.tfloat,; 'age': hl.tfloat,; 'sex': hl.tfloat,; 'apoe': hl.tstr,; 'cohort': hl.tstr,; 'pop':hl.tstr,; 'omit':hl.tint,; 'AD': hl.tint,; 'MCI': hl.tint,; 'e2': hl.tint,; 'e3': hl.tint,; 'e4': hl.tint,; 'e44': hl.tint,; 'e34': hl.tint,; 'e33': hl.tint,; 'e24': hl.tint,; 'e23': hl.tint,; 'e22': hl.tint,; 'ACT_aa': hl.tint,; 'ACT1_ea': hl.tint,; 'ACT2_ea': hl.tint,; 'ADC1_ea': hl.tint,; 'ADC10_aa': hl.tint,; 'ADC10_eas': hl.tint,; 'ADC10_ea': hl.tint,; 'ADC10_hisp': hl.tint,; 'ADC12_aa': hl.tint,; 'ADC2_ea': hl.tint,; 'ADC3_aa': hl.tint,; 'ADC3_ea': hl.tint,; 'ADC4_ea': hl.tint,; 'ADC5_ea': hl.tint,; 'ADC6_ea': hl.tint,; 'ADC7_ea': hl.tint,; 'ADC8_aa': hl.ti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:1060,Testability,log,log,1060,"pc_relate_pop2.py; import pandas as pd; import numpy as np; from scipy.special import chdtri; from numpy import median; from math import log,isnan; import matplotlib as mpl; import matplotlib.pyplot as plt; import seaborn as sns; import sys; import hail as hl; from bokeh.io import output_notebook, show, export_png, output_file, save; from pprint import pprint. pop=sys.argv[1]; maf_cutoff=0.10. # input files; #. # pfn=""file:///restricted/projectnb/adgc/imp.topmed_adsp5k/analysis/adgc.aa.pheno.txt""; # apoe_fn=""file:///restricted/projectnb/ukbiobank/ad/apoe/ukbb_hg38_imputed_phased.tsv""; mt_fn='/project/adgc/topmed2.2020_07/pop/adgc.'+pop+'.autosome.dose.mt'; mt_ld_fn='/project/adgc/topmed2.2020_07/pop/adgc.'+pop+'.autosome.dose.ld_pruned.mt'; pfn=""file:///restricted/projectnb/adgc/topmed.r2.analysis/pheno/ADGC_ea_aa_eas_hisp_graace_pheno.qced.txt""; #; # Initialize Hail for GRCh38. # hl.init(default_reference='GRCh38',log=""results/adgc_pc_relate.autosome.log"",tmp_dir=""file:///restricted/projectnb/ukbiobank/ad/analysis/ad.v1/tmp""). hl.init(default_reference='GRCh38',log=""logs/adgc_pc_relate.autosome_""+pop+"".log""); table = (hl.import_table(pfn,impute=True, missing=['','.','NA'],types={; 'FID': hl.tstr,; 'IID': hl.tstr,; 'vcfID': hl.tstr,; 'status': hl.tfloat,; 'age': hl.tfloat,; 'sex': hl.tfloat,; 'apoe': hl.tstr,; 'cohort': hl.tstr,; 'pop':hl.tstr,; 'omit':hl.tint,; 'AD': hl.tint,; 'MCI': hl.tint,; 'e2': hl.tint,; 'e3': hl.tint,; 'e4': hl.tint,; 'e44': hl.tint,; 'e34': hl.tint,; 'e33': hl.tint,; 'e24': hl.tint,; 'e23': hl.tint,; 'e22': hl.tint,; 'ACT_aa': hl.tint,; 'ACT1_ea': hl.tint,; 'ACT2_ea': hl.tint,; 'ADC1_ea': hl.tint,; 'ADC10_aa': hl.tint,; 'ADC10_eas': hl.tint,; 'ADC10_ea': hl.tint,; 'ADC10_hisp': hl.tint,; 'ADC12_aa': hl.tint,; 'ADC2_ea': hl.tint,; 'ADC3_aa': hl.tint,; 'ADC3_ea': hl.tint,; 'ADC4_ea': hl.tint,; 'ADC5_ea': hl.tint,; 'ADC6_ea': hl.tint,; 'ADC7_ea': hl.tint,; 'ADC8_aa': hl.tint,; 'ADC8_ea': hl.tint,; 'ADC8_hisp': hl.tint,; 'ADC9_aa': hl.tint,; 'AD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:1173,Testability,log,log,1173,"ial import chdtri; from numpy import median; from math import log,isnan; import matplotlib as mpl; import matplotlib.pyplot as plt; import seaborn as sns; import sys; import hail as hl; from bokeh.io import output_notebook, show, export_png, output_file, save; from pprint import pprint. pop=sys.argv[1]; maf_cutoff=0.10. # input files; #. # pfn=""file:///restricted/projectnb/adgc/imp.topmed_adsp5k/analysis/adgc.aa.pheno.txt""; # apoe_fn=""file:///restricted/projectnb/ukbiobank/ad/apoe/ukbb_hg38_imputed_phased.tsv""; mt_fn='/project/adgc/topmed2.2020_07/pop/adgc.'+pop+'.autosome.dose.mt'; mt_ld_fn='/project/adgc/topmed2.2020_07/pop/adgc.'+pop+'.autosome.dose.ld_pruned.mt'; pfn=""file:///restricted/projectnb/adgc/topmed.r2.analysis/pheno/ADGC_ea_aa_eas_hisp_graace_pheno.qced.txt""; #; # Initialize Hail for GRCh38. # hl.init(default_reference='GRCh38',log=""results/adgc_pc_relate.autosome.log"",tmp_dir=""file:///restricted/projectnb/ukbiobank/ad/analysis/ad.v1/tmp""). hl.init(default_reference='GRCh38',log=""logs/adgc_pc_relate.autosome_""+pop+"".log""); table = (hl.import_table(pfn,impute=True, missing=['','.','NA'],types={; 'FID': hl.tstr,; 'IID': hl.tstr,; 'vcfID': hl.tstr,; 'status': hl.tfloat,; 'age': hl.tfloat,; 'sex': hl.tfloat,; 'apoe': hl.tstr,; 'cohort': hl.tstr,; 'pop':hl.tstr,; 'omit':hl.tint,; 'AD': hl.tint,; 'MCI': hl.tint,; 'e2': hl.tint,; 'e3': hl.tint,; 'e4': hl.tint,; 'e44': hl.tint,; 'e34': hl.tint,; 'e33': hl.tint,; 'e24': hl.tint,; 'e23': hl.tint,; 'e22': hl.tint,; 'ACT_aa': hl.tint,; 'ACT1_ea': hl.tint,; 'ACT2_ea': hl.tint,; 'ADC1_ea': hl.tint,; 'ADC10_aa': hl.tint,; 'ADC10_eas': hl.tint,; 'ADC10_ea': hl.tint,; 'ADC10_hisp': hl.tint,; 'ADC12_aa': hl.tint,; 'ADC2_ea': hl.tint,; 'ADC3_aa': hl.tint,; 'ADC3_ea': hl.tint,; 'ADC4_ea': hl.tint,; 'ADC5_ea': hl.tint,; 'ADC6_ea': hl.tint,; 'ADC7_ea': hl.tint,; 'ADC8_aa': hl.tint,; 'ADC8_ea': hl.tint,; 'ADC8_hisp': hl.tint,; 'ADC9_aa': hl.tint,; 'ADC9_ea': hl.tint,; 'ADNI_ea': hl.tint,; 'BIOCARD_ea': hl.tint,; 'CHAP_aa': hl",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:1178,Testability,log,logs,1178,"ial import chdtri; from numpy import median; from math import log,isnan; import matplotlib as mpl; import matplotlib.pyplot as plt; import seaborn as sns; import sys; import hail as hl; from bokeh.io import output_notebook, show, export_png, output_file, save; from pprint import pprint. pop=sys.argv[1]; maf_cutoff=0.10. # input files; #. # pfn=""file:///restricted/projectnb/adgc/imp.topmed_adsp5k/analysis/adgc.aa.pheno.txt""; # apoe_fn=""file:///restricted/projectnb/ukbiobank/ad/apoe/ukbb_hg38_imputed_phased.tsv""; mt_fn='/project/adgc/topmed2.2020_07/pop/adgc.'+pop+'.autosome.dose.mt'; mt_ld_fn='/project/adgc/topmed2.2020_07/pop/adgc.'+pop+'.autosome.dose.ld_pruned.mt'; pfn=""file:///restricted/projectnb/adgc/topmed.r2.analysis/pheno/ADGC_ea_aa_eas_hisp_graace_pheno.qced.txt""; #; # Initialize Hail for GRCh38. # hl.init(default_reference='GRCh38',log=""results/adgc_pc_relate.autosome.log"",tmp_dir=""file:///restricted/projectnb/ukbiobank/ad/analysis/ad.v1/tmp""). hl.init(default_reference='GRCh38',log=""logs/adgc_pc_relate.autosome_""+pop+"".log""); table = (hl.import_table(pfn,impute=True, missing=['','.','NA'],types={; 'FID': hl.tstr,; 'IID': hl.tstr,; 'vcfID': hl.tstr,; 'status': hl.tfloat,; 'age': hl.tfloat,; 'sex': hl.tfloat,; 'apoe': hl.tstr,; 'cohort': hl.tstr,; 'pop':hl.tstr,; 'omit':hl.tint,; 'AD': hl.tint,; 'MCI': hl.tint,; 'e2': hl.tint,; 'e3': hl.tint,; 'e4': hl.tint,; 'e44': hl.tint,; 'e34': hl.tint,; 'e33': hl.tint,; 'e24': hl.tint,; 'e23': hl.tint,; 'e22': hl.tint,; 'ACT_aa': hl.tint,; 'ACT1_ea': hl.tint,; 'ACT2_ea': hl.tint,; 'ADC1_ea': hl.tint,; 'ADC10_aa': hl.tint,; 'ADC10_eas': hl.tint,; 'ADC10_ea': hl.tint,; 'ADC10_hisp': hl.tint,; 'ADC12_aa': hl.tint,; 'ADC2_ea': hl.tint,; 'ADC3_aa': hl.tint,; 'ADC3_ea': hl.tint,; 'ADC4_ea': hl.tint,; 'ADC5_ea': hl.tint,; 'ADC6_ea': hl.tint,; 'ADC7_ea': hl.tint,; 'ADC8_aa': hl.tint,; 'ADC8_ea': hl.tint,; 'ADC8_hisp': hl.tint,; 'ADC9_aa': hl.tint,; 'ADC9_ea': hl.tint,; 'ADNI_ea': hl.tint,; 'BIOCARD_ea': hl.tint,; 'CHAP_aa': hl",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:1215,Testability,log,log,1215,"import log,isnan; import matplotlib as mpl; import matplotlib.pyplot as plt; import seaborn as sns; import sys; import hail as hl; from bokeh.io import output_notebook, show, export_png, output_file, save; from pprint import pprint. pop=sys.argv[1]; maf_cutoff=0.10. # input files; #. # pfn=""file:///restricted/projectnb/adgc/imp.topmed_adsp5k/analysis/adgc.aa.pheno.txt""; # apoe_fn=""file:///restricted/projectnb/ukbiobank/ad/apoe/ukbb_hg38_imputed_phased.tsv""; mt_fn='/project/adgc/topmed2.2020_07/pop/adgc.'+pop+'.autosome.dose.mt'; mt_ld_fn='/project/adgc/topmed2.2020_07/pop/adgc.'+pop+'.autosome.dose.ld_pruned.mt'; pfn=""file:///restricted/projectnb/adgc/topmed.r2.analysis/pheno/ADGC_ea_aa_eas_hisp_graace_pheno.qced.txt""; #; # Initialize Hail for GRCh38. # hl.init(default_reference='GRCh38',log=""results/adgc_pc_relate.autosome.log"",tmp_dir=""file:///restricted/projectnb/ukbiobank/ad/analysis/ad.v1/tmp""). hl.init(default_reference='GRCh38',log=""logs/adgc_pc_relate.autosome_""+pop+"".log""); table = (hl.import_table(pfn,impute=True, missing=['','.','NA'],types={; 'FID': hl.tstr,; 'IID': hl.tstr,; 'vcfID': hl.tstr,; 'status': hl.tfloat,; 'age': hl.tfloat,; 'sex': hl.tfloat,; 'apoe': hl.tstr,; 'cohort': hl.tstr,; 'pop':hl.tstr,; 'omit':hl.tint,; 'AD': hl.tint,; 'MCI': hl.tint,; 'e2': hl.tint,; 'e3': hl.tint,; 'e4': hl.tint,; 'e44': hl.tint,; 'e34': hl.tint,; 'e33': hl.tint,; 'e24': hl.tint,; 'e23': hl.tint,; 'e22': hl.tint,; 'ACT_aa': hl.tint,; 'ACT1_ea': hl.tint,; 'ACT2_ea': hl.tint,; 'ADC1_ea': hl.tint,; 'ADC10_aa': hl.tint,; 'ADC10_eas': hl.tint,; 'ADC10_ea': hl.tint,; 'ADC10_hisp': hl.tint,; 'ADC12_aa': hl.tint,; 'ADC2_ea': hl.tint,; 'ADC3_aa': hl.tint,; 'ADC3_ea': hl.tint,; 'ADC4_ea': hl.tint,; 'ADC5_ea': hl.tint,; 'ADC6_ea': hl.tint,; 'ADC7_ea': hl.tint,; 'ADC8_aa': hl.tint,; 'ADC8_ea': hl.tint,; 'ADC8_hisp': hl.tint,; 'ADC9_aa': hl.tint,; 'ADC9_ea': hl.tint,; 'ADNI_ea': hl.tint,; 'BIOCARD_ea': hl.tint,; 'CHAP_aa': hl.tint,; 'CHAP2_ea': hl.tint,; 'CHOP_aa': hl.tint,; 'EAS",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:4251,Testability,LOG,LOGGING,4251,"t(mt.count()); print(""running pc_relate""); pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); pairs = pc_rel.filter(pc_rel['kin'] >= 0.0883); print(""finding Max ind set""); related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,keep=False); print(""writing related samples to remove""); related_samples_to_remove.export(""file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/related_samples_""+pop+"".tsv""); result = mt.filter_cols(; hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False); print(""final unrelated count""); print(result.count); eigenvalues, scores, loadings = hl.hwe_normalized_pca(result.GT, k=10); scores.export('file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/unrelated_pcs_'+pop+'.tsv'). ```. ```; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to logs/adgc_pc_relate.autosome_all.log; 2020-08-20 10:14:00 Hail: INFO: Reading table to impute column types; [Stage 0:===========================================================(1 + 0) / 1]2020-08-20 10:14:07 Hail: INFO: Loading 76 fields. Counts by type:; 66 fields: user-specified int32; 6 fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:4271,Testability,log,logs,4271,"t(mt.count()); print(""running pc_relate""); pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); pairs = pc_rel.filter(pc_rel['kin'] >= 0.0883); print(""finding Max ind set""); related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,keep=False); print(""writing related samples to remove""); related_samples_to_remove.export(""file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/related_samples_""+pop+"".tsv""); result = mt.filter_cols(; hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False); print(""final unrelated count""); print(result.count); eigenvalues, scores, loadings = hl.hwe_normalized_pca(result.GT, k=10); scores.export('file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/unrelated_pcs_'+pop+'.tsv'). ```. ```; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to logs/adgc_pc_relate.autosome_all.log; 2020-08-20 10:14:00 Hail: INFO: Reading table to impute column types; [Stage 0:===========================================================(1 + 0) / 1]2020-08-20 10:14:07 Hail: INFO: Loading 76 fields. Counts by type:; 66 fields: user-specified int32; 6 fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:4304,Testability,log,log,4304,"pc_rel['kin'] >= 0.0883); print(""finding Max ind set""); related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,keep=False); print(""writing related samples to remove""); related_samples_to_remove.export(""file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/related_samples_""+pop+"".tsv""); result = mt.filter_cols(; hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False); print(""final unrelated count""); print(result.count); eigenvalues, scores, loadings = hl.hwe_normalized_pca(result.GT, k=10); scores.export('file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/unrelated_pcs_'+pop+'.tsv'). ```. ```; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to logs/adgc_pc_relate.autosome_all.log; 2020-08-20 10:14:00 Hail: INFO: Reading table to impute column types; [Stage 0:===========================================================(1 + 0) / 1]2020-08-20 10:14:07 Hail: INFO: Loading 76 fields. Counts by type:; 66 fields: user-specified int32; 6 fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Sta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403
https://github.com/hail-is/hail/issues/9293#issuecomment-683888038:3,Deployability,update,update,3,"No update yet, sorry. We've just found a pipeline that may replicate it more easily, so hoping for a fix soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-683888038
https://github.com/hail-is/hail/issues/9293#issuecomment-683888038:41,Deployability,pipeline,pipeline,41,"No update yet, sorry. We've just found a pipeline that may replicate it more easily, so hoping for a fix soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-683888038
https://github.com/hail-is/hail/issues/9293#issuecomment-688857271:46,Deployability,release,release,46,"We believe that #9408 may have fixed this for release 0.2.57. We weren't able to replicate locally, though -- I know you're unblocked now, but if it's easy for you, could you try rerunning the failing script to see if that's the fix to your particular issue? Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-688857271
https://github.com/hail-is/hail/issues/9293#issuecomment-689714135:30,Deployability,install,installs,30,"As soon as Research Computing installs the new Hail version, I will rerun that script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-689714135
https://github.com/hail-is/hail/issues/9293#issuecomment-698963629:53,Availability,error,errors,53,I have rerun one of scripts. It ran fine without any errors. Thanks for the fix!!!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-698963629
https://github.com/hail-is/hail/pull/9295#issuecomment-675544269:93,Modifiability,extend,extends,93,"This doesn't look like it can hurt, but why is this necessary? `SplitCompressionInputStream` extends `CompressionInputStream` which has a `close` method already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9295#issuecomment-675544269
https://github.com/hail-is/hail/pull/9297#issuecomment-675587772:102,Testability,benchmark,benchmarks,102,doesn't the ServiceBackend exit(0) even when the batch fails? That's my vague recollection of running benchmarks,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9297#issuecomment-675587772
https://github.com/hail-is/hail/pull/9297#issuecomment-675599575:39,Availability,failure,failure,39,"Yes, it will exit(0), but upon any job failure `state` will be ""failure"", and the STDOUT message will be ""batch {bc_batch.id} complete: failure"" instead of ""Batch completed successfully"" as in the LocalBackend case currently. We rely on exception handling to catch job errors for LocalBackend.run, and the current bug occurs because of an improperly formed ternary expression (we can tell this because `verbose` should not enable `-e`). cc @jigold, @danking, what are your thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9297#issuecomment-675599575
https://github.com/hail-is/hail/pull/9297#issuecomment-675599575:64,Availability,failure,failure,64,"Yes, it will exit(0), but upon any job failure `state` will be ""failure"", and the STDOUT message will be ""batch {bc_batch.id} complete: failure"" instead of ""Batch completed successfully"" as in the LocalBackend case currently. We rely on exception handling to catch job errors for LocalBackend.run, and the current bug occurs because of an improperly formed ternary expression (we can tell this because `verbose` should not enable `-e`). cc @jigold, @danking, what are your thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9297#issuecomment-675599575
https://github.com/hail-is/hail/pull/9297#issuecomment-675599575:136,Availability,failure,failure,136,"Yes, it will exit(0), but upon any job failure `state` will be ""failure"", and the STDOUT message will be ""batch {bc_batch.id} complete: failure"" instead of ""Batch completed successfully"" as in the LocalBackend case currently. We rely on exception handling to catch job errors for LocalBackend.run, and the current bug occurs because of an improperly formed ternary expression (we can tell this because `verbose` should not enable `-e`). cc @jigold, @danking, what are your thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9297#issuecomment-675599575
https://github.com/hail-is/hail/pull/9297#issuecomment-675599575:269,Availability,error,errors,269,"Yes, it will exit(0), but upon any job failure `state` will be ""failure"", and the STDOUT message will be ""batch {bc_batch.id} complete: failure"" instead of ""Batch completed successfully"" as in the LocalBackend case currently. We rely on exception handling to catch job errors for LocalBackend.run, and the current bug occurs because of an improperly formed ternary expression (we can tell this because `verbose` should not enable `-e`). cc @jigold, @danking, what are your thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9297#issuecomment-675599575
https://github.com/hail-is/hail/pull/9297#issuecomment-675599575:89,Integrability,message,message,89,"Yes, it will exit(0), but upon any job failure `state` will be ""failure"", and the STDOUT message will be ""batch {bc_batch.id} complete: failure"" instead of ""Batch completed successfully"" as in the LocalBackend case currently. We rely on exception handling to catch job errors for LocalBackend.run, and the current bug occurs because of an improperly formed ternary expression (we can tell this because `verbose` should not enable `-e`). cc @jigold, @danking, what are your thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9297#issuecomment-675599575
https://github.com/hail-is/hail/pull/9298#issuecomment-675614222:91,Performance,perform,performance,91,"I don't think this is the right solution. We should fix the control flow instead -- a ~40% performance degradation is pretty horrific, even as a stop-gap.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9298#issuecomment-675614222
https://github.com/hail-is/hail/pull/9299#issuecomment-677696154:10,Testability,test,tests,10,Up to 555 tests passing against local backend with this.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9299#issuecomment-677696154
https://github.com/hail-is/hail/pull/9303#issuecomment-677708620:322,Security,expose,expose,322,"> As far as I can tell, there's never a reason to use dgesvd. It appears dgesvd uses less memory and is more accurate, particularly on very small singular values, but is potentially several times slower. I agree dgesdd is the right default, but it's possible (though I'm guessing unlikely) we'll eventually have reason to expose dgesvd as an option. Here's a thread of Julia devs discussing which to use: https://groups.google.com/u/1/g/julia-dev/c/mmgO65i6-fA?pli=1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9303#issuecomment-677708620
https://github.com/hail-is/hail/pull/9304#issuecomment-675747090:60,Testability,benchmark,benchmark,60,"Better attempt at fixing this issue compared to #9298, will benchmark.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-675747090
https://github.com/hail-is/hail/pull/9304#issuecomment-675767918:51,Integrability,message,message,51,"I do think the solution you've described in the PR message seems better, though, if we can make this work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-675767918
https://github.com/hail-is/hail/pull/9304#issuecomment-676303474:43,Performance,cache,cache,43,"Ok, I think this is better. Every read, we cache both file pointers that can refer to the end of the current block. If we would use the smaller one, use the bigger one instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676303474
https://github.com/hail-is/hail/pull/9304#issuecomment-676497772:34,Testability,test,testing,34,Cotton has a vcf he generated for testing BGZipCodec: see BGZipCodecSuite. This has a lot of the edge cases.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676497772
https://github.com/hail-is/hail/pull/9304#issuecomment-676510464:36,Testability,test,testing,36,> Cotton has a vcf he generated for testing BGZipCodec: see BGZipCodecSuite. This has a lot of the edge cases. I'll tabix it and see what happens.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676510464
https://github.com/hail-is/hail/pull/9304#issuecomment-676511001:127,Usability,simpl,simplification,127,"I just realized, `refreshBuffer` is generally always accompanied by a (possibly implicit), `bufferCursor = start`, so that's a simplification that I'm going to try.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676511001
https://github.com/hail-is/hail/pull/9304#issuecomment-676615090:114,Availability,error,error,114,"We end up getting exceptions like this, happens every time we get to the end of the file:; ```; RuntimeException: error reading tabix-indexed file src/test/resources/gvcfs/recoding/HG00187.hg38.g.vcf.gz: i=0, curOff=2469855232, expected=2468020224; ```; These virtual offsets correspond to these pairs of absolute|decompressed offset; ```; 2469855232 => 37687|0; 2468020224 => 37659|0; ```; Looking at these, they differ by 28, which is the size of the empty bgzip block at the end. And so when we refresh: we go too far.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676615090
https://github.com/hail-is/hail/pull/9304#issuecomment-676615090:151,Testability,test,test,151,"We end up getting exceptions like this, happens every time we get to the end of the file:; ```; RuntimeException: error reading tabix-indexed file src/test/resources/gvcfs/recoding/HG00187.hg38.g.vcf.gz: i=0, curOff=2469855232, expected=2468020224; ```; These virtual offsets correspond to these pairs of absolute|decompressed offset; ```; 2469855232 => 37687|0; 2468020224 => 37659|0; ```; Looking at these, they differ by 28, which is the size of the empty bgzip block at the end. And so when we refresh: we go too far.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676615090
https://github.com/hail-is/hail/pull/9304#issuecomment-676650099:53,Testability,test,tests,53,"#9309 up to solve these issues. If I can get all the tests to pass, I'll feel pretty good about stacking this on top of that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676650099
https://github.com/hail-is/hail/pull/9304#issuecomment-676650821:55,Testability,test,tests,55,"> #9309 up to solve these issues. If I can get all the tests to pass, I'll feel pretty good about stacking this on top of that. Yes, agreed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676650821
https://github.com/hail-is/hail/pull/9304#issuecomment-676735526:133,Integrability,message,messages,133,Okay. Hopefully this is good. I also pushed an extra branch up with a small debugging class I wrote to decompose a virtual offset in messages. That work is in c323fa335. Let me know if you want to add it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676735526
https://github.com/hail-is/hail/pull/9304#issuecomment-676837945:64,Testability,test,tests,64,The exact issue here is double counting some lines in the BGzip tests in particular.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676837945
https://github.com/hail-is/hail/pull/9304#issuecomment-677864463:15,Testability,assert,assertion,15,"Can't fail the assertion if it's not there to check. We still need the other parts of this change, because otherwise, we were reading too much.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-677864463
https://github.com/hail-is/hail/pull/9309#issuecomment-676736575:29,Integrability,depend,dependency,29,This and #9304 have a mutual dependency (ugh). Close this in favor of the other.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9309#issuecomment-676736575
https://github.com/hail-is/hail/pull/9328#issuecomment-679425628:13,Testability,benchmark,benchmark,13,I ran a full benchmark comparison and didn't see any significant changes. I realize now there are no benchmarks for `to_matrix_table_row_major`. I'm adding such a benchmark and running it now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9328#issuecomment-679425628
https://github.com/hail-is/hail/pull/9328#issuecomment-679425628:101,Testability,benchmark,benchmarks,101,I ran a full benchmark comparison and didn't see any significant changes. I realize now there are no benchmarks for `to_matrix_table_row_major`. I'm adding such a benchmark and running it now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9328#issuecomment-679425628
https://github.com/hail-is/hail/pull/9328#issuecomment-679425628:163,Testability,benchmark,benchmark,163,I ran a full benchmark comparison and didn't see any significant changes. I realize now there are no benchmarks for `to_matrix_table_row_major`. I'm adding such a benchmark and running it now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9328#issuecomment-679425628
https://github.com/hail-is/hail/pull/9328#issuecomment-679451744:48,Testability,Benchmark,Benchmark,48,heh. It timed out on main (3149211fb79b):. ```; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; to_matrix_table_row_major 716.3% 251.300 1800.000; ----------------------; Harmonic mean: 716.3%; Geometric mean: 716.3%; Arithmetic mean: 716.3%; Median: 716.3%; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9328#issuecomment-679451744
https://github.com/hail-is/hail/pull/9330#issuecomment-680269070:80,Integrability,message,message,80,"oh crap, sorry, didn't see your question. Will definitely try to improve commit message in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9330#issuecomment-680269070
https://github.com/hail-is/hail/pull/9339#issuecomment-679385861:53,Deployability,install,install,53,"Oops, quite right. I pulled this diff from the other install, I just fixed the pieces I deployed there. I didn't intend this to be a common include for all Makefiles (should it be?), but just pull together the relevant defines used in services Makefiles. I grep'ed to make sure the variables defined config.mk are now only defined there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9339#issuecomment-679385861
https://github.com/hail-is/hail/pull/9339#issuecomment-679385861:88,Deployability,deploy,deployed,88,"Oops, quite right. I pulled this diff from the other install, I just fixed the pieces I deployed there. I didn't intend this to be a common include for all Makefiles (should it be?), but just pull together the relevant defines used in services Makefiles. I grep'ed to make sure the variables defined config.mk are now only defined there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9339#issuecomment-679385861
https://github.com/hail-is/hail/pull/9339#issuecomment-679385861:282,Modifiability,variab,variables,282,"Oops, quite right. I pulled this diff from the other install, I just fixed the pieces I deployed there. I didn't intend this to be a common include for all Makefiles (should it be?), but just pull together the relevant defines used in services Makefiles. I grep'ed to make sure the variables defined config.mk are now only defined there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9339#issuecomment-679385861
https://github.com/hail-is/hail/pull/9339#issuecomment-679385861:300,Modifiability,config,config,300,"Oops, quite right. I pulled this diff from the other install, I just fixed the pieces I deployed there. I didn't intend this to be a common include for all Makefiles (should it be?), but just pull together the relevant defines used in services Makefiles. I grep'ed to make sure the variables defined config.mk are now only defined there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9339#issuecomment-679385861
https://github.com/hail-is/hail/pull/9354#issuecomment-705066148:121,Deployability,deploy,deployment,121,I need to rethink the test-tiny-limit and test-zero-limit. This is going to fail every time after the first merge on the deployment.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9354#issuecomment-705066148
https://github.com/hail-is/hail/pull/9354#issuecomment-705066148:22,Testability,test,test-tiny-limit,22,I need to rethink the test-tiny-limit and test-zero-limit. This is going to fail every time after the first merge on the deployment.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9354#issuecomment-705066148
https://github.com/hail-is/hail/pull/9354#issuecomment-705066148:42,Testability,test,test-zero-limit,42,I need to rethink the test-tiny-limit and test-zero-limit. This is going to fail every time after the first merge on the deployment.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9354#issuecomment-705066148
https://github.com/hail-is/hail/pull/9354#issuecomment-705102869:238,Integrability,depend,depend,238,"Ok. The limits for the tests will need to be fixed with a REST API for editing a billing limit and a new build step that is `setup_test_batch`. I'll work on that now. But I think this can go in while I'm working on that, but #9355 should depend on the new PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9354#issuecomment-705102869
https://github.com/hail-is/hail/pull/9354#issuecomment-705102869:23,Testability,test,tests,23,"Ok. The limits for the tests will need to be fixed with a REST API for editing a billing limit and a new build step that is `setup_test_batch`. I'll work on that now. But I think this can go in while I'm working on that, but #9355 should depend on the new PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9354#issuecomment-705102869
https://github.com/hail-is/hail/pull/9354#issuecomment-705120445:17,Testability,test,tests,17,Nevermind. Added tests for editing the limits to this PR. Will figure out the CLI after @catoverdrive has had a chance to do what they wanted to do with that for billing projects.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9354#issuecomment-705120445
https://github.com/hail-is/hail/pull/9354#issuecomment-705127061:71,Testability,test,tests,71,There's no way to fix this problem other than not run those particular tests on the production server. I think that's okay.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9354#issuecomment-705127061
https://github.com/hail-is/hail/pull/9354#issuecomment-705156989:108,Testability,test,test,108,New plan. @catoverdrive already figured out how to have multiple users with different clients to be able to test the dev only endpoints such as editing a billing limit. I'm going to wait for their PR to go in and then reevaluate (#9553).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9354#issuecomment-705156989
https://github.com/hail-is/hail/pull/9354#issuecomment-715557309:20,Deployability,deploy,deploy,20,Closing while I dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9354#issuecomment-715557309
https://github.com/hail-is/hail/pull/9371#issuecomment-684054912:14,Testability,test,tests,14,"@tpoterba The tests now all pass. I'd appreciate your input on 32965c8 , the last commit in this sequence. It shortens the names of methods, possibly truncating them more than we already do, but I had found that something was blowing memory related to large classes/constants and once the types are much larger than a few KB, they're not really that useful. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9371#issuecomment-684054912
https://github.com/hail-is/hail/pull/9379#issuecomment-685130327:45,Testability,test,tests,45,"ready for review, of course it's failing the tests right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9379#issuecomment-685130327
https://github.com/hail-is/hail/pull/9379#issuecomment-685808895:0,Testability,Test,Tests,0,Tests pass. Probably messed up my translation the first time.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9379#issuecomment-685808895
https://github.com/hail-is/hail/pull/9380#issuecomment-684063947:95,Security,password,passwords,95,"AFAIK, none of our CI image builds should need to contact the k8s cluster. That and the create passwords step are the only steps of CI that are not on the private network. Everything else uses k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9380#issuecomment-684063947
https://github.com/hail-is/hail/pull/9380#issuecomment-686655669:42,Security,access,access,42,"AFAIK, building images should not require access to our private network. Neither should creating passwords. I put both of those on the private network. Eventually, I'd prefer that CI jobs explicitly opt-in to the private network, but for now I put them all on the private network.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9380#issuecomment-686655669
https://github.com/hail-is/hail/pull/9380#issuecomment-686655669:97,Security,password,passwords,97,"AFAIK, building images should not require access to our private network. Neither should creating passwords. I put both of those on the private network. Eventually, I'd prefer that CI jobs explicitly opt-in to the private network, but for now I put them all on the private network.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9380#issuecomment-686655669
https://github.com/hail-is/hail/pull/9381#issuecomment-684901927:0,Testability,Test,Tests,0,Tests are failing,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9381#issuecomment-684901927
https://github.com/hail-is/hail/pull/9382#issuecomment-684007827:35,Testability,log,log-b,35,"I spot checked, and noticed that [`log-b.png`](https://github.com/hail-is/hail/blob/aa3cd5c4a64247a550e9fd61e79c0b7de7713559/graphics/32x32/log-b.png) looks like just blue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9382#issuecomment-684007827
https://github.com/hail-is/hail/pull/9382#issuecomment-684007827:140,Testability,log,log-b,140,"I spot checked, and noticed that [`log-b.png`](https://github.com/hail-is/hail/blob/aa3cd5c4a64247a550e9fd61e79c0b7de7713559/graphics/32x32/log-b.png) looks like just blue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9382#issuecomment-684007827
https://github.com/hail-is/hail/pull/9382#issuecomment-684992756:127,Usability,simpl,simplified,127,"Bah, fixed. There was some weird grouping going on in the svgs that was causing the PNG conversion to do something weird, so I simplified all the svgs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9382#issuecomment-684992756
https://github.com/hail-is/hail/issues/9383#issuecomment-684034614:146,Usability,clear,clearly,146,"Looking at our documentation, we document `n` as `maximum number of splits`. That makes this seem like a bug to me, especially in the 0 case. But clearly people use this function and this change is breaking to anyone who uses it. My vote would be to do the deprecation thing I suggested above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9383#issuecomment-684034614
https://github.com/hail-is/hail/issues/9383#issuecomment-1775409513:357,Performance,perform,perform,357,"Unearthing this old issue. I agree with John's assessment. We should:. 1. Fix the docs for `n` to indicate it is the maximum number of returned strings, not the maximum number of splits.; 2. Deprecate the `n` parameter. Do this with a `warnings.warn` when `n` is not `None`.; 3. Introduce a new parameter `maxsplit` which is the maximum number of splits to perform. Document that `maxsplit` is equal to the old `n` parameters minus one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9383#issuecomment-1775409513
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:576,Availability,error,error,576,"I added the cli. I kind of winged it looking at how `hailctl dev deploy` was done. It seems to work though:. ```; (base) wmecc-475:hail jigold$ hailctl batch billing; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession o",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1368,Availability,error,error,1368,"particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-lim",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:65,Deployability,deploy,deploy,65,"I added the cli. I kind of winged it looking at how `hailctl dev deploy` was done. It seems to work though:. ```; (base) wmecc-475:hail jigold$ hailctl batch billing; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession o",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:427,Integrability,message,message,427,"I added the cli. I kind of winged it looking at how `hailctl dev deploy` was done. It seems to work though:. ```; (base) wmecc-475:hail jigold$ hailctl batch billing; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession o",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:2969,Integrability,message,message,2969,"ccrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get Get a particular batch's info; cancel Cancel a batch; delete Delete a batch; log Get log for a job; job Get the status and specification for a job; wait Wait for a batch to complete, then print JSON status. optional arguments:; -h, --help show this help message and exit; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:924,Testability,test,test,924,"I added the cli. I kind of winged it looking at how `hailctl dev deploy` was done. It seems to work though:. ```; (base) wmecc-475:hail jigold$ hailctl batch billing; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession o",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:980,Testability,test,test,980,"I added the cli. I kind of winged it looking at how `hailctl dev deploy` was done. It seems to work though:. ```; (base) wmecc-475:hail jigold$ hailctl batch billing; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession o",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1042,Testability,test,test-tiny-limit,1042," looking at how `hailctl dev deploy` was done. It seems to work though:. ```; (base) wmecc-475:hail jigold$ hailctl batch billing; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed con",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1111,Testability,test,test,1111,"ase) wmecc-475:hail jigold$ hailctl batch billing; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d06",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1156,Testability,test,test-zero-limit,1156,"; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.conne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1205,Testability,test,test,1205,"list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1581,Testability,test,test-tiny-limit,1581,".; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches runnin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1651,Testability,test,test-tiny-limit,1651,"lient session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,canc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1720,Testability,test,test,1720," 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get G",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1783,Testability,test,test-tiny-limit,1783,"ch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get Get a particular batch's info; cancel Cancel a batch; de",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1853,Testability,test,test-tiny-limit,1853,": null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get Get a particular batch's info; cancel Cancel a batch; delete Delete a batch; log Get log for a job; job Get the status and specifica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1922,Testability,test,test,1922,"oject: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get Get a particular batch's info; cancel Cancel a batch; delete Delete a batch; log Get log for a job; job Get the status and specification for a job; wait Wait for a batch to complete, then print JSON stat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:2263,Testability,test,test-tiny-limit,2263,"ccrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get Get a particular batch's info; cancel Cancel a batch; delete Delete a batch; log Get log for a job; job Get the status and specification for a job; wait Wait for a batch to complete, then print JSON status. optional arguments:; -h, --help show this help message and exit; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:2333,Testability,test,test-tiny-limit,2333,"ccrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get Get a particular batch's info; cancel Cancel a batch; delete Delete a batch; log Get log for a job; job Get the status and specification for a job; wait Wait for a batch to complete, then print JSON status. optional arguments:; -h, --help show this help message and exit; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:2402,Testability,test,test,2402,"ccrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get Get a particular batch's info; cancel Cancel a batch; delete Delete a batch; log Get log for a job; job Get the status and specification for a job; wait Wait for a batch to complete, then print JSON status. optional arguments:; -h, --help show this help message and exit; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:2512,Testability,log,log,2512,"ccrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get Get a particular batch's info; cancel Cancel a batch; delete Delete a batch; log Get log for a job; job Get the status and specification for a job; wait Wait for a batch to complete, then print JSON status. optional arguments:; -h, --help show this help message and exit; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:2655,Testability,log,log,2655,"ccrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get Get a particular batch's info; cancel Cancel a batch; delete Delete a batch; log Get log for a job; job Get the status and specification for a job; wait Wait for a batch to complete, then print JSON status. optional arguments:; -h, --help show this help message and exit; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:2792,Testability,log,log,2792,"ccrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get Get a particular batch's info; cancel Cancel a batch; delete Delete a batch; log Get log for a job; job Get the status and specification for a job; wait Wait for a batch to complete, then print JSON status. optional arguments:; -h, --help show this help message and exit; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:2800,Testability,log,log,2800,"ccrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get Get a particular batch's info; cancel Cancel a batch; delete Delete a batch; log Get log for a job; job Get the status and specification for a job; wait Wait for a batch to complete, then print JSON status. optional arguments:; -h, --help show this help message and exit; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006
https://github.com/hail-is/hail/pull/9385#issuecomment-684971261:30,Availability,error,errors,30,I fixed the client not closed errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684971261
https://github.com/hail-is/hail/pull/9389#issuecomment-684910606:107,Testability,test,tests,107,Force merging since both this and https://github.com/hail-is/hail/pull/9390 are both needed for successful tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9389#issuecomment-684910606
https://github.com/hail-is/hail/pull/9391#issuecomment-685116503:13,Deployability,deploy,deployed,13,"It's already deployed, so, I guess that's how I tested it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9391#issuecomment-685116503
https://github.com/hail-is/hail/pull/9391#issuecomment-685116503:48,Testability,test,tested,48,"It's already deployed, so, I guess that's how I tested it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9391#issuecomment-685116503
https://github.com/hail-is/hail/pull/9394#issuecomment-685253481:277,Deployability,install,installs,277,"There are now three check steps:; - check_hail (in the sense of $HAIL_HOME/hail directory), which checks the hail and hailtop packages, using the base image, from the source via `make check-hail`; - check_services, using the services image, except the benchmark service, which installs addition dependencies, from the source via `make -k check-services`, `-k` forces make to check all packages, even if one fails; - check_benchmark_service, which checks benchmark as installed in that image",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685253481
https://github.com/hail-is/hail/pull/9394#issuecomment-685253481:467,Deployability,install,installed,467,"There are now three check steps:; - check_hail (in the sense of $HAIL_HOME/hail directory), which checks the hail and hailtop packages, using the base image, from the source via `make check-hail`; - check_services, using the services image, except the benchmark service, which installs addition dependencies, from the source via `make -k check-services`, `-k` forces make to check all packages, even if one fails; - check_benchmark_service, which checks benchmark as installed in that image",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685253481
https://github.com/hail-is/hail/pull/9394#issuecomment-685253481:295,Integrability,depend,dependencies,295,"There are now three check steps:; - check_hail (in the sense of $HAIL_HOME/hail directory), which checks the hail and hailtop packages, using the base image, from the source via `make check-hail`; - check_services, using the services image, except the benchmark service, which installs addition dependencies, from the source via `make -k check-services`, `-k` forces make to check all packages, even if one fails; - check_benchmark_service, which checks benchmark as installed in that image",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685253481
https://github.com/hail-is/hail/pull/9394#issuecomment-685253481:252,Testability,benchmark,benchmark,252,"There are now three check steps:; - check_hail (in the sense of $HAIL_HOME/hail directory), which checks the hail and hailtop packages, using the base image, from the source via `make check-hail`; - check_services, using the services image, except the benchmark service, which installs addition dependencies, from the source via `make -k check-services`, `-k` forces make to check all packages, even if one fails; - check_benchmark_service, which checks benchmark as installed in that image",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685253481
https://github.com/hail-is/hail/pull/9394#issuecomment-685253481:454,Testability,benchmark,benchmark,454,"There are now three check steps:; - check_hail (in the sense of $HAIL_HOME/hail directory), which checks the hail and hailtop packages, using the base image, from the source via `make check-hail`; - check_services, using the services image, except the benchmark service, which installs addition dependencies, from the source via `make -k check-services`, `-k` forces make to check all packages, even if one fails; - check_benchmark_service, which checks benchmark as installed in that image",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685253481
https://github.com/hail-is/hail/pull/9394#issuecomment-685254706:172,Availability,avail,available,172,"FYI @danking There was also a Makefile bug: make creates make variables from the environment variables by default. Also, when make runs a rule, it makes the make variables available in the environment. That means PYTHONPATH in those Makefiles was getting set to some literal ${...} string, not the right value. I solved this by not assigning PYTHONPATH in the Makefile. https://www.gnu.org/software/make/manual/html_node/Environment.html. ""Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. However, an explicit assignment in the makefile, or with a command argument, overrides the environment."". ""When make runs a recipe, variables defined in the makefile are placed into the environment of each shell.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706
https://github.com/hail-is/hail/pull/9394#issuecomment-685254706:62,Modifiability,variab,variables,62,"FYI @danking There was also a Makefile bug: make creates make variables from the environment variables by default. Also, when make runs a rule, it makes the make variables available in the environment. That means PYTHONPATH in those Makefiles was getting set to some literal ${...} string, not the right value. I solved this by not assigning PYTHONPATH in the Makefile. https://www.gnu.org/software/make/manual/html_node/Environment.html. ""Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. However, an explicit assignment in the makefile, or with a command argument, overrides the environment."". ""When make runs a recipe, variables defined in the makefile are placed into the environment of each shell.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706
https://github.com/hail-is/hail/pull/9394#issuecomment-685254706:93,Modifiability,variab,variables,93,"FYI @danking There was also a Makefile bug: make creates make variables from the environment variables by default. Also, when make runs a rule, it makes the make variables available in the environment. That means PYTHONPATH in those Makefiles was getting set to some literal ${...} string, not the right value. I solved this by not assigning PYTHONPATH in the Makefile. https://www.gnu.org/software/make/manual/html_node/Environment.html. ""Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. However, an explicit assignment in the makefile, or with a command argument, overrides the environment."". ""When make runs a recipe, variables defined in the makefile are placed into the environment of each shell.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706
https://github.com/hail-is/hail/pull/9394#issuecomment-685254706:162,Modifiability,variab,variables,162,"FYI @danking There was also a Makefile bug: make creates make variables from the environment variables by default. Also, when make runs a rule, it makes the make variables available in the environment. That means PYTHONPATH in those Makefiles was getting set to some literal ${...} string, not the right value. I solved this by not assigning PYTHONPATH in the Makefile. https://www.gnu.org/software/make/manual/html_node/Environment.html. ""Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. However, an explicit assignment in the makefile, or with a command argument, overrides the environment."". ""When make runs a recipe, variables defined in the makefile are placed into the environment of each shell.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706
https://github.com/hail-is/hail/pull/9394#issuecomment-685254706:458,Modifiability,variab,variable,458,"FYI @danking There was also a Makefile bug: make creates make variables from the environment variables by default. Also, when make runs a rule, it makes the make variables available in the environment. That means PYTHONPATH in those Makefiles was getting set to some literal ${...} string, not the right value. I solved this by not assigning PYTHONPATH in the Makefile. https://www.gnu.org/software/make/manual/html_node/Environment.html. ""Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. However, an explicit assignment in the makefile, or with a command argument, overrides the environment."". ""When make runs a recipe, variables defined in the makefile are placed into the environment of each shell.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706
https://github.com/hail-is/hail/pull/9394#issuecomment-685254706:527,Modifiability,variab,variable,527,"FYI @danking There was also a Makefile bug: make creates make variables from the environment variables by default. Also, when make runs a rule, it makes the make variables available in the environment. That means PYTHONPATH in those Makefiles was getting set to some literal ${...} string, not the right value. I solved this by not assigning PYTHONPATH in the Makefile. https://www.gnu.org/software/make/manual/html_node/Environment.html. ""Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. However, an explicit assignment in the makefile, or with a command argument, overrides the environment."". ""When make runs a recipe, variables defined in the makefile are placed into the environment of each shell.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706
https://github.com/hail-is/hail/pull/9394#issuecomment-685254706:698,Modifiability,variab,variables,698,"FYI @danking There was also a Makefile bug: make creates make variables from the environment variables by default. Also, when make runs a rule, it makes the make variables available in the environment. That means PYTHONPATH in those Makefiles was getting set to some literal ${...} string, not the right value. I solved this by not assigning PYTHONPATH in the Makefile. https://www.gnu.org/software/make/manual/html_node/Environment.html. ""Every environment variable that make sees when it starts up is transformed into a make variable with the same name and value. However, an explicit assignment in the makefile, or with a command argument, overrides the environment."". ""When make runs a recipe, variables defined in the makefile are placed into the environment of each shell.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685254706
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:128,Modifiability,config,config,128,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:214,Modifiability,config,config,214,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:115,Testability,test,test,115,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:158,Testability,Test,Test,158,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:201,Testability,test,test,201,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:244,Testability,Test,Test,244,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:283,Testability,test,test,283,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:368,Testability,test,test,368,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:455,Testability,test,test,455,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:540,Testability,test,test,540,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:630,Testability,test,test,630,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:699,Testability,test,test,699,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:768,Testability,test,test,768,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:888,Testability,test,test,888,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:1022,Testability,test,test-cluster-m,1022,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:1164,Testability,test,test-cluster,1164,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:1190,Testability,test,test,1190,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:1314,Testability,test,test-cluster-m,1314,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685814393:1456,Testability,test,test-cluster,1456,"These are run in `test_hail_python_i`, e.g. [`test_hail_python_2`](https://ci.hail.is/batches/93958/jobs/65); ```; test/hailtop/config/test_deploy_config.py::Test::test_deploy_external_default PASSED; test/hailtop/config/test_deploy_config.py::Test::test_deploy_k8s_default SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_met SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_required_gcloud_version_unmet SKIPPED; test/hailtop/hailctl/dataproc/test_cli.py::test_unable_to_determine_version SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_cluster_and_service_required SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_dry_run SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_connect SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-ui-18080/?showIncomplete=true] SKIPPED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[ui-18080/?showIncomplete=true] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; test/hailtop/hailctl/dataproc/test_connect.py::test_service_port_and_path[spark-history-18080] gcloud command:; compute ssh test-cluster-m --zone=us-central1-b \; '--ssh-flag=-D 10000' \; '--ssh-flag=-N' \; '--ssh-flag=-f' \; '--ssh-flag=-n'; Connecting to cluster 'test-cluster'...; PASSED; ...; ```. The skipped ones are run in other splits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685814393
https://github.com/hail-is/hail/pull/9396#issuecomment-685904525:37,Modifiability,config,config,37,"Ah, thanks! This is just the gear => config rename now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9396#issuecomment-685904525
https://github.com/hail-is/hail/pull/9398#issuecomment-685798692:66,Availability,error,error,66,One other question: Is it a breaking change to change the type of error we throw?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9398#issuecomment-685798692
https://github.com/hail-is/hail/pull/9401#issuecomment-689831499:15,Testability,test,tests,15,Running python tests in local mode gets some violations of the `eltRegion <= outerRegion` assertion. Adding the WIP label until I fix that.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9401#issuecomment-689831499
https://github.com/hail-is/hail/pull/9401#issuecomment-689831499:90,Testability,assert,assertion,90,Running python tests in local mode gets some violations of the `eltRegion <= outerRegion` assertion. Adding the WIP label until I fix that.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9401#issuecomment-689831499
https://github.com/hail-is/hail/pull/9401#issuecomment-689833234:24,Testability,test,tests,24,"yeah, the local backend tests have way more coverage than the java.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9401#issuecomment-689833234
https://github.com/hail-is/hail/pull/9401#issuecomment-690716921:61,Availability,failure,failures,61,"Okay, python tests in local mode now have the same number of failures as on main. I just needed to be more careful in preserving the information that determines the subregion relation",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9401#issuecomment-690716921
https://github.com/hail-is/hail/pull/9401#issuecomment-690716921:13,Testability,test,tests,13,"Okay, python tests in local mode now have the same number of failures as on main. I just needed to be more careful in preserving the information that determines the subregion relation",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9401#issuecomment-690716921
https://github.com/hail-is/hail/pull/9401#issuecomment-692124775:0,Testability,benchmark,benchmarks,0,benchmarks: https://gist.github.com/tpoterba/7b681ff2a6e5cba8304a19f32f5208b5,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9401#issuecomment-692124775
https://github.com/hail-is/hail/pull/9403#issuecomment-685996666:14781,Performance,cache,cache,14781,w on D's values; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.TupleExpression.count:: WARNING: py:class reference target not found: integer -- return number of occurrences of value; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.TupleExpression.index:: WARNING: py:class reference target not found: integer -- return first index of value.; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.annotate_cols:9: WARNING: py:class reference target not found: hail.Table; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.annotate_rows:11: WARNING: py:class reference target not found: TVariant; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.cache:11: WARNING: py:func reference target not found: hail.MatrixTable.persist; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:15: WARNING: py:data reference target not found: int64; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:17: WARNING: py:data reference target not found: int64; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:19: WARNING: py:data reference target not found: float32; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.index_cols:17: WARNING: py:meth reference target not found: index_cols(exprs); /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.index_rows:17: WARNING: py:meth reference target not found: index_rows(exprs); /Users/dking/projects/hail/hail/pyt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-685996666
https://github.com/hail-is/hail/pull/9403#issuecomment-685996666:16032,Performance,cache,cache,16032,jects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:17: WARNING: py:data reference target not found: int64; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:19: WARNING: py:data reference target not found: float32; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.index_cols:17: WARNING: py:meth reference target not found: index_cols(exprs); /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.index_rows:17: WARNING: py:meth reference target not found: index_rows(exprs); /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.make_table:25: WARNING: py:func reference target not found: make_table; /Users/dking/projects/hail/hail/python/hail/table.py:docstring of hail.table.Table.cache:11: WARNING: py:func reference target not found: hail.Table.persist; /Users/dking/projects/hail/hail/python/hail/table.py:docstring of hail.table.Table.order_by:37: WARNING: py:class reference target not found: Ascending; /Users/dking/projects/hail/hail/python/hail/table.py:docstring of hail.table.Table.order_by:37: WARNING: py:class reference target not found: Descending; /Users/dking/projects/hail/hail/python/hail/table.py:docstring of hail.table.Table.parallelize:14: WARNING: py:class reference target not found: HailType:; /Users/dking/projects/hail/hail/python/hail/docs/linalg/hail.linalg.BlockMatrix.rst:49:<autosummary>:1: WARNING: py:obj reference target not found: hail.linalg.BlockMatrix.element_type; /Users/dking/projects/hail/hail/python/hail/linalg/blockmatrix.py:docstring of hail.linalg.BlockMatrix.export:133: WARNING: py:class reference target not found: str) -- Describes which entries to export. One of:; `; /Users/dking/projects/hail/hail/python/hail/linalg/blockmatrix.py:docstring of hail.linalg,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-685996666
https://github.com/hail-is/hail/pull/9403#issuecomment-685996666:11578,Usability,guid,guides,11578,r.functions.qnorm:1: WARNING: py:meth reference target not found: pnorm; /Users/dking/projects/hail/hail/python/hail/expr/functions.py:docstring of hail.expr.functions.qpois:1: WARNING: py:meth reference target not found: ppois; /Users/dking/projects/hail/hail/python/hail/expr/functions.py:docstring of hail.expr.functions.qpois:15: WARNING: py:meth reference target not found: ppois; /Users/dking/projects/hail/hail/python/hail/genetics/pedigree.py:docstring of hail.genetics.pedigree.Pedigree.filter_to:: WARNING: py:class reference target not found: list of str; /Users/dking/projects/hail/hail/python/hail/genetics/pedigree.py:docstring of hail.genetics.pedigree.Pedigree.write:14: WARNING: py:meth reference target not found: hail.KeyTable.import_fam; /Users/dking/projects/hail/hail/python/hail/genetics/reference_genome.py:docstring of hail.genetics.reference_genome.ReferenceGenome.write:10: WARNING: py:meth reference target not found: hail.ReferenceGenome.read; /Users/dking/projects/hail/hail/python/hail/docs/guides/api.rst:19: WARNING: py:func reference target not found: Table.show; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.CallExpression.one_hot_alleles:25: WARNING: py:obj reference target not found: tint32; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.IntervalExpression.overlaps:11: WARNING: py:data reference target not found: tinterval; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.NDArrayExpression.T:5: WARNING: py:func reference target not found: transpose; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.NDArrayNumericExpression.T:5: WARNING: py:func reference target not found: transpose; /Users/dking/projects/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-685996666
https://github.com/hail-is/hail/pull/9403#issuecomment-688855000:152,Deployability,patch,patching,152,"Most of the changes here are totally fine, but yes, I think we should hold off on changing paths. I have a to-do item to look into configuring / monkey patching the Sphinx function that's trying to generate links for the signatures. Dan is also OOO this week so not high prio.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-688855000
https://github.com/hail-is/hail/pull/9403#issuecomment-688855000:131,Modifiability,config,configuring,131,"Most of the changes here are totally fine, but yes, I think we should hold off on changing paths. I have a to-do item to look into configuring / monkey patching the Sphinx function that's trying to generate links for the signatures. Dan is also OOO this week so not high prio.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-688855000
https://github.com/hail-is/hail/pull/9403#issuecomment-703776111:1842,Usability,simpl,simple,1842,"cause is that newer versions of autodoc include, [as an experimental addition](https://www.sphinx-doc.org/en/2.0/usage/extensions/autodoc.html#generating-documents-from-type-annotations), sphinx-autodoc-typehints. This addition (which we use in batch) only works when a class is documented using its true name (i.e. where it is defined, not re-exported). A quick fix is to [disable this functionality](https://www.sphinx-doc.org/en/2.0/usage/extensions/autodoc.html#confval-autodoc_typehints):; ```; autodoc_typehints = 'none'; ```. - autodoc issue about this https://github.com/agronholm/sphinx-autodoc-typehints/issues/38; - another autodoc issue with a fix for the particular use case https://github.com/agronholm/sphinx-autodoc-typehints/issues/124; - root sphinx issue wrt fully qualified names versus the documented name: https://github.com/sphinx-doc/sphinx/issues/4826. The autodoc-typehints maintainer seems to have gotten stuck when trying to fix this. It appears that someone went and figured out enough of Sphinx to [fix this](https://git-cral.univ-lyon1.fr/MUSE/mpdaf/blob/23d52ba059fe76df5e1655542b17a28a7137cf20/doc/ext/smart_resolver.py). When a doc string is processed, they record a mapping from the documented name to the fully-resolved name. The code that catches missing references and fixes them is kinda big and complicated. I'm uncomfortable dropping it into our project. There's some good documentation about how autodoc_typehints works at [scanpydocs' docs](https://icb-scanpydoc.readthedocs-hosted.com/en/latest/scanpydoc.elegant_typehints.html). This [flying sheep](https://github.com/flying-sheep) seems pretty competent. I think they fixed it for scanpydocs [here](https://github.com/theislab/scanpydoc/pull/19/files) but it's a rather complex looking solution. It's frankly pretty shocking that such a simple operation (have a mapping from all the names of an object to its documented name) results in a two years of back and forth that still hasn't reached a solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-703776111
https://github.com/hail-is/hail/pull/9403#issuecomment-704552873:0,Deployability,install,install-dev-deps,0,"install-dev-deps doubles the ""no change"" build time from 4s to 8s on my machine. I already thing 4s is too long. I'd rather not add that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-704552873
https://github.com/hail-is/hail/pull/9406#issuecomment-686086418:22,Deployability,hotfix,hotfix,22,Overriding review for hotfix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9406#issuecomment-686086418
https://github.com/hail-is/hail/pull/9407#issuecomment-686630822:22,Availability,error,error,22,"@tpoterba Compilation error, I think maybe you can only mark things that are `val` or `var` transient",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9407#issuecomment-686630822
https://github.com/hail-is/hail/issues/9419#issuecomment-688740276:93,Deployability,release,releases,93,"I replaced the maven source ; maven { url ""https://repo.hortonworks.com/content/repositories/releases/"" }. by; maven { url ""https://nexus-private.hortonworks.com/nexus/repository/maven-central/"" }; maven { url ""https://nexus-private.hortonworks.com/nexus/service/rest/repository/browse/maven-central/"" }. then I can compile again:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9419#issuecomment-688740276
https://github.com/hail-is/hail/pull/9421#issuecomment-691300692:108,Availability,error,errors,108,"@patrick-schultz I'm sending this back to you because I made some pretty drastic changes trying to fix some errors. The biggest non-refactoring change that the original this introduces is that we can't parse IR for a persisted block matrix reader if the persisted block matrix doesn't exist. (This makes some amount of sense if you consider that we also can't parse the IR for a native block matrix reader if the file doesn't exist.). This led me down a rabbit hole of test failures since we're parsing IR/types a fair number of times, through the execution and after we get the result. After fiddling with it for a little bit, I removed UnpersistBlockMatrix. I'm not sure what I was thinking when I added it. I re-added an ""unpersist"" function to the backend to handle unpersisting BlockMatrices. It differs from the current Table/MatrixTable unpersist functions in that we only pass the id of the thing we want to unpersist, not the entire IR, since that's all we need.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9421#issuecomment-691300692
https://github.com/hail-is/hail/pull/9421#issuecomment-691300692:447,Availability,down,down,447,"@patrick-schultz I'm sending this back to you because I made some pretty drastic changes trying to fix some errors. The biggest non-refactoring change that the original this introduces is that we can't parse IR for a persisted block matrix reader if the persisted block matrix doesn't exist. (This makes some amount of sense if you consider that we also can't parse the IR for a native block matrix reader if the file doesn't exist.). This led me down a rabbit hole of test failures since we're parsing IR/types a fair number of times, through the execution and after we get the result. After fiddling with it for a little bit, I removed UnpersistBlockMatrix. I'm not sure what I was thinking when I added it. I re-added an ""unpersist"" function to the backend to handle unpersisting BlockMatrices. It differs from the current Table/MatrixTable unpersist functions in that we only pass the id of the thing we want to unpersist, not the entire IR, since that's all we need.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9421#issuecomment-691300692
https://github.com/hail-is/hail/pull/9421#issuecomment-691300692:474,Availability,failure,failures,474,"@patrick-schultz I'm sending this back to you because I made some pretty drastic changes trying to fix some errors. The biggest non-refactoring change that the original this introduces is that we can't parse IR for a persisted block matrix reader if the persisted block matrix doesn't exist. (This makes some amount of sense if you consider that we also can't parse the IR for a native block matrix reader if the file doesn't exist.). This led me down a rabbit hole of test failures since we're parsing IR/types a fair number of times, through the execution and after we get the result. After fiddling with it for a little bit, I removed UnpersistBlockMatrix. I'm not sure what I was thinking when I added it. I re-added an ""unpersist"" function to the backend to handle unpersisting BlockMatrices. It differs from the current Table/MatrixTable unpersist functions in that we only pass the id of the thing we want to unpersist, not the entire IR, since that's all we need.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9421#issuecomment-691300692
https://github.com/hail-is/hail/pull/9421#issuecomment-691300692:132,Modifiability,refactor,refactoring,132,"@patrick-schultz I'm sending this back to you because I made some pretty drastic changes trying to fix some errors. The biggest non-refactoring change that the original this introduces is that we can't parse IR for a persisted block matrix reader if the persisted block matrix doesn't exist. (This makes some amount of sense if you consider that we also can't parse the IR for a native block matrix reader if the file doesn't exist.). This led me down a rabbit hole of test failures since we're parsing IR/types a fair number of times, through the execution and after we get the result. After fiddling with it for a little bit, I removed UnpersistBlockMatrix. I'm not sure what I was thinking when I added it. I re-added an ""unpersist"" function to the backend to handle unpersisting BlockMatrices. It differs from the current Table/MatrixTable unpersist functions in that we only pass the id of the thing we want to unpersist, not the entire IR, since that's all we need.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9421#issuecomment-691300692
https://github.com/hail-is/hail/pull/9421#issuecomment-691300692:469,Testability,test,test,469,"@patrick-schultz I'm sending this back to you because I made some pretty drastic changes trying to fix some errors. The biggest non-refactoring change that the original this introduces is that we can't parse IR for a persisted block matrix reader if the persisted block matrix doesn't exist. (This makes some amount of sense if you consider that we also can't parse the IR for a native block matrix reader if the file doesn't exist.). This led me down a rabbit hole of test failures since we're parsing IR/types a fair number of times, through the execution and after we get the result. After fiddling with it for a little bit, I removed UnpersistBlockMatrix. I'm not sure what I was thinking when I added it. I re-added an ""unpersist"" function to the backend to handle unpersisting BlockMatrices. It differs from the current Table/MatrixTable unpersist functions in that we only pass the id of the thing we want to unpersist, not the entire IR, since that's all we need.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9421#issuecomment-691300692
https://github.com/hail-is/hail/pull/9423#issuecomment-689115183:23,Energy Efficiency,schedul,scheduled,23,Will this actually get scheduled? I thought the max on our 2 core nodes was 1.8 or so.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9423#issuecomment-689115183
https://github.com/hail-is/hail/pull/9425#issuecomment-689544523:57,Testability,test,tests,57,"Something is wrong here, you have tons of failing python tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9425#issuecomment-689544523
https://github.com/hail-is/hail/pull/9425#issuecomment-689597263:286,Availability,error,error,286,"> Could you explain why you think the boundary is invalid?. Boundary can only be used by consumers who generate an iterator with a new context. Here we were just inserting a clear before each next(), no matter who was consuming the iterator. The particular pipeline that triggered this error was a TableMapPartitions with a ToArray(Ref rows) after an IR with a repartitionedOrderedRDD2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9425#issuecomment-689597263
https://github.com/hail-is/hail/pull/9425#issuecomment-689597263:257,Deployability,pipeline,pipeline,257,"> Could you explain why you think the boundary is invalid?. Boundary can only be used by consumers who generate an iterator with a new context. Here we were just inserting a clear before each next(), no matter who was consuming the iterator. The particular pipeline that triggered this error was a TableMapPartitions with a ToArray(Ref rows) after an IR with a repartitionedOrderedRDD2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9425#issuecomment-689597263
https://github.com/hail-is/hail/pull/9425#issuecomment-689597263:174,Usability,clear,clear,174,"> Could you explain why you think the boundary is invalid?. Boundary can only be used by consumers who generate an iterator with a new context. Here we were just inserting a clear before each next(), no matter who was consuming the iterator. The particular pipeline that triggered this error was a TableMapPartitions with a ToArray(Ref rows) after an IR with a repartitionedOrderedRDD2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9425#issuecomment-689597263
https://github.com/hail-is/hail/pull/9425#issuecomment-689815711:25,Availability,down,down,25,Thanks for tracking this down Tim,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9425#issuecomment-689815711
https://github.com/hail-is/hail/pull/9426#issuecomment-689316416:56,Integrability,interface,interface,56,And I'd eventually like a (synchronous) version of this interface to replace the hadoop_* functions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9426#issuecomment-689316416
https://github.com/hail-is/hail/pull/9429#issuecomment-690420227:1063,Testability,benchmark,benchmark,1063,"So, something weird is going on that I don't understand, and we should probably hold off on merging this for now. When I emit the stream code directly, bypassing the compiler, the new reducible version is clearly faster: running; ```scala; val f = compile1[Int, Unit] { (mb, n) =>; val outer = Stream.range(mb, 0, 1, n); val flatMap = outer.flatMap(i => Stream.range(mb, 0, 1, i)); flatMap.forEach(mb, i => Code._empty); }; val n = 50000; var t = System.nanoTime(); f(n); println(s""first run: ${(System.nanoTime() - t) / 1000000} ms""); for (i <- 1 to 10) { f(n) }; t = System.nanoTime(); for (i <- 1 to 50) { f(n) }; println(s""warmed up mean: ${(System.nanoTime() - t) / (1000000 * 50)} ms""); ```; on main prints; ```; first run: 2088 ms; warmed up mean: 1972 ms; ```; and on this PR; ```; first run: 867 ms; warmed up mean: 937 ms; ```; (As an aside, the lack of burn in is interesting. I think it means either the function is never getting jit compiled, or OSR kicks in on the first run and is as effective as full compilation.). On the other hand, running the benchmark; ```scala; ht = hl.utils.range_table(30); ht = ht.annotate(sum=hl.sum(hl.range(5_000).flatmap(lambda x: hl.range(x)))); ht._force_count(); ```; I get; ```; > hail-bench compare main-bench.json branch-bench.json; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; table_annotate_flatMap 371.8% 0.795 2.958; ```. I'm currently at a loss for theories to explain this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9429#issuecomment-690420227
https://github.com/hail-is/hail/pull/9429#issuecomment-690420227:1285,Testability,Benchmark,Benchmark,1285,"So, something weird is going on that I don't understand, and we should probably hold off on merging this for now. When I emit the stream code directly, bypassing the compiler, the new reducible version is clearly faster: running; ```scala; val f = compile1[Int, Unit] { (mb, n) =>; val outer = Stream.range(mb, 0, 1, n); val flatMap = outer.flatMap(i => Stream.range(mb, 0, 1, i)); flatMap.forEach(mb, i => Code._empty); }; val n = 50000; var t = System.nanoTime(); f(n); println(s""first run: ${(System.nanoTime() - t) / 1000000} ms""); for (i <- 1 to 10) { f(n) }; t = System.nanoTime(); for (i <- 1 to 50) { f(n) }; println(s""warmed up mean: ${(System.nanoTime() - t) / (1000000 * 50)} ms""); ```; on main prints; ```; first run: 2088 ms; warmed up mean: 1972 ms; ```; and on this PR; ```; first run: 867 ms; warmed up mean: 937 ms; ```; (As an aside, the lack of burn in is interesting. I think it means either the function is never getting jit compiled, or OSR kicks in on the first run and is as effective as full compilation.). On the other hand, running the benchmark; ```scala; ht = hl.utils.range_table(30); ht = ht.annotate(sum=hl.sum(hl.range(5_000).flatmap(lambda x: hl.range(x)))); ht._force_count(); ```; I get; ```; > hail-bench compare main-bench.json branch-bench.json; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; table_annotate_flatMap 371.8% 0.795 2.958; ```. I'm currently at a loss for theories to explain this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9429#issuecomment-690420227
https://github.com/hail-is/hail/pull/9429#issuecomment-690420227:205,Usability,clear,clearly,205,"So, something weird is going on that I don't understand, and we should probably hold off on merging this for now. When I emit the stream code directly, bypassing the compiler, the new reducible version is clearly faster: running; ```scala; val f = compile1[Int, Unit] { (mb, n) =>; val outer = Stream.range(mb, 0, 1, n); val flatMap = outer.flatMap(i => Stream.range(mb, 0, 1, i)); flatMap.forEach(mb, i => Code._empty); }; val n = 50000; var t = System.nanoTime(); f(n); println(s""first run: ${(System.nanoTime() - t) / 1000000} ms""); for (i <- 1 to 10) { f(n) }; t = System.nanoTime(); for (i <- 1 to 50) { f(n) }; println(s""warmed up mean: ${(System.nanoTime() - t) / (1000000 * 50)} ms""); ```; on main prints; ```; first run: 2088 ms; warmed up mean: 1972 ms; ```; and on this PR; ```; first run: 867 ms; warmed up mean: 937 ms; ```; (As an aside, the lack of burn in is interesting. I think it means either the function is never getting jit compiled, or OSR kicks in on the first run and is as effective as full compilation.). On the other hand, running the benchmark; ```scala; ht = hl.utils.range_table(30); ht = ht.annotate(sum=hl.sum(hl.range(5_000).flatmap(lambda x: hl.range(x)))); ht._force_count(); ```; I get; ```; > hail-bench compare main-bench.json branch-bench.json; Benchmark Name Ratio Time 1 Time 2; -------------- ----- ------ ------; table_annotate_flatMap 371.8% 0.795 2.958; ```. I'm currently at a loss for theories to explain this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9429#issuecomment-690420227
https://github.com/hail-is/hail/pull/9432#issuecomment-690693915:0,Testability,Test,Tests,0,"Tests are failing, seems like the PruneDeadFields rule can change the type in a way that removes the keys? Investigating.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9432#issuecomment-690693915
https://github.com/hail-is/hail/pull/9437#issuecomment-691234229:150,Modifiability,config,config,150,"Ok, I finally figured out why the test wasn't picking up the tokens. This is going to be fairly useless to users unless/until we also bundle in a dev config, although it definitely should suffice for what I need to do. Thoughts on doing that? (either here or in a separate PR?) I imagine the config would just be like {location: gce, namespace: $default_ns} but I'm not sure if that would leads to issues?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691234229
https://github.com/hail-is/hail/pull/9437#issuecomment-691234229:292,Modifiability,config,config,292,"Ok, I finally figured out why the test wasn't picking up the tokens. This is going to be fairly useless to users unless/until we also bundle in a dev config, although it definitely should suffice for what I need to do. Thoughts on doing that? (either here or in a separate PR?) I imagine the config would just be like {location: gce, namespace: $default_ns} but I'm not sure if that would leads to issues?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691234229
https://github.com/hail-is/hail/pull/9437#issuecomment-691234229:34,Testability,test,test,34,"Ok, I finally figured out why the test wasn't picking up the tokens. This is going to be fairly useless to users unless/until we also bundle in a dev config, although it definitely should suffice for what I need to do. Thoughts on doing that? (either here or in a separate PR?) I imagine the config would just be like {location: gce, namespace: $default_ns} but I'm not sure if that would leads to issues?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691234229
https://github.com/hail-is/hail/pull/9437#issuecomment-691238047:72,Safety,safe,safe,72,"Sounds good! I'll PR it separately. Mostly a question of whether it's a safe/desirable thing to do, I think?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691238047
https://github.com/hail-is/hail/pull/9437#issuecomment-691241075:150,Deployability,deploy,deploy,150,I think it's safe. I was more worried about what the namespace should be and whether for developers it should have the service_namespace. I think the deploy config only tells you how to get the correct URL. There's nothing special about it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691241075
https://github.com/hail-is/hail/pull/9437#issuecomment-691241075:157,Modifiability,config,config,157,I think it's safe. I was more worried about what the namespace should be and whether for developers it should have the service_namespace. I think the deploy config only tells you how to get the correct URL. There's nothing special about it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691241075
https://github.com/hail-is/hail/pull/9437#issuecomment-691241075:13,Safety,safe,safe,13,I think it's safe. I was more worried about what the namespace should be and whether for developers it should have the service_namespace. I think the deploy config only tells you how to get the correct URL. There's nothing special about it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691241075
https://github.com/hail-is/hail/pull/9437#issuecomment-691245476:24,Deployability,update,updated,24,"Yeah, that's true. I've updated this PR to include the default gce deploy config for the default namespace, which I think is generally what you'd want for both testing and in production; I'm not sure under what circumstances you'd want separate service namespaces?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691245476
https://github.com/hail-is/hail/pull/9437#issuecomment-691245476:67,Deployability,deploy,deploy,67,"Yeah, that's true. I've updated this PR to include the default gce deploy config for the default namespace, which I think is generally what you'd want for both testing and in production; I'm not sure under what circumstances you'd want separate service namespaces?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691245476
https://github.com/hail-is/hail/pull/9437#issuecomment-691245476:74,Modifiability,config,config,74,"Yeah, that's true. I've updated this PR to include the default gce deploy config for the default namespace, which I think is generally what you'd want for both testing and in production; I'm not sure under what circumstances you'd want separate service namespaces?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691245476
https://github.com/hail-is/hail/pull/9437#issuecomment-691245476:160,Testability,test,testing,160,"Yeah, that's true. I've updated this PR to include the default gce deploy config for the default namespace, which I think is generally what you'd want for both testing and in production; I'm not sure under what circumstances you'd want separate service namespaces?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691245476
https://github.com/hail-is/hail/pull/9441#issuecomment-691229759:206,Availability,outage,outage,206,"Arcturus -- I'm assigning this to you, but please don't take off the WIP tag as merging this will cause Batch to shutdown (database migration). If we're not prepared for it, then it could cause an extended outage.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9441#issuecomment-691229759
https://github.com/hail-is/hail/pull/9441#issuecomment-691229759:197,Modifiability,extend,extended,197,"Arcturus -- I'm assigning this to you, but please don't take off the WIP tag as merging this will cause Batch to shutdown (database migration). If we're not prepared for it, then it could cause an extended outage.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9441#issuecomment-691229759
https://github.com/hail-is/hail/pull/9441#issuecomment-691312517:76,Deployability,deploy,deployment,76,"This basically limits the number of nodes in the worker pool for a test/dev deployment of batch to (default 3, max 4), right? This looks fine to me; should I approve this and let you take off the WIP tag when it's good to merge?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9441#issuecomment-691312517
https://github.com/hail-is/hail/pull/9441#issuecomment-691312517:67,Testability,test,test,67,"This basically limits the number of nodes in the worker pool for a test/dev deployment of batch to (default 3, max 4), right? This looks fine to me; should I approve this and let you take off the WIP tag when it's good to merge?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9441#issuecomment-691312517
https://github.com/hail-is/hail/pull/9462#issuecomment-694395067:25,Availability,mask,mask,25,"I think fixing this will mask an issue where a 1 CPU job blocks the scheduling of smaller jobs. I'm gonna debug that first, then remove WIP.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9462#issuecomment-694395067
https://github.com/hail-is/hail/pull/9462#issuecomment-694395067:68,Energy Efficiency,schedul,scheduling,68,"I think fixing this will mask an issue where a 1 CPU job blocks the scheduling of smaller jobs. I'm gonna debug that first, then remove WIP.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9462#issuecomment-694395067
https://github.com/hail-is/hail/pull/9469#issuecomment-694486478:28,Testability,benchmark,benchmark,28,"`linear_regression_rows_nd` benchmark on my laptop went from 125 seconds to 62 seconds. So this made a 2x difference. The regular linear regression benchmark is still like 25 seconds though, have to figure out next steps to close that gap.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9469#issuecomment-694486478
https://github.com/hail-is/hail/pull/9469#issuecomment-694486478:148,Testability,benchmark,benchmark,148,"`linear_regression_rows_nd` benchmark on my laptop went from 125 seconds to 62 seconds. So this made a 2x difference. The regular linear regression benchmark is still like 25 seconds though, have to figure out next steps to close that gap.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9469#issuecomment-694486478
https://github.com/hail-is/hail/pull/9474#issuecomment-694826372:57,Integrability,interface,interface,57,"Good point @johnc1231, it would be nice to have the same interface for `head`/`tail` on tables and arrays. Instead of adding `tail`, should we add `first`/`last` and deprecate `head`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9474#issuecomment-694826372
https://github.com/hail-is/hail/pull/9474#issuecomment-694910861:13,Deployability,Update,Updated,13,Sounds good. Updated this PR to add `first` and `last` methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9474#issuecomment-694910861
https://github.com/hail-is/hail/pull/9474#issuecomment-694929657:29,Safety,avoid,avoid,29,"yeah, we don't have a way to avoid constructing the full array right now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9474#issuecomment-694929657
https://github.com/hail-is/hail/pull/9474#issuecomment-694955390:20,Testability,test,test,20,"> You need to add a test to ensure `last` works. You'll find a method called `test_array_head` in `test_expr.py`. I'd change that to `test_array_first`, call `first` instead. Then I'd add `test_array_last` and make sure there's a test for empty and nonempty arrays. Thanks. Tests were included in e2fcfb61a5df5ab497269d32c12a5d5fb652ddd4. Are there more checks that should be added to those?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9474#issuecomment-694955390
https://github.com/hail-is/hail/pull/9474#issuecomment-694955390:230,Testability,test,test,230,"> You need to add a test to ensure `last` works. You'll find a method called `test_array_head` in `test_expr.py`. I'd change that to `test_array_first`, call `first` instead. Then I'd add `test_array_last` and make sure there's a test for empty and nonempty arrays. Thanks. Tests were included in e2fcfb61a5df5ab497269d32c12a5d5fb652ddd4. Are there more checks that should be added to those?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9474#issuecomment-694955390
https://github.com/hail-is/hail/pull/9474#issuecomment-694955390:274,Testability,Test,Tests,274,"> You need to add a test to ensure `last` works. You'll find a method called `test_array_head` in `test_expr.py`. I'd change that to `test_array_first`, call `first` instead. Then I'd add `test_array_last` and make sure there's a test for empty and nonempty arrays. Thanks. Tests were included in e2fcfb61a5df5ab497269d32c12a5d5fb652ddd4. Are there more checks that should be added to those?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9474#issuecomment-694955390
https://github.com/hail-is/hail/pull/9474#issuecomment-694955960:15,Testability,test,tests,15,"You're good on tests, I was looking at the wrong thing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9474#issuecomment-694955960
https://github.com/hail-is/hail/pull/9481#issuecomment-700750562:39,Availability,down,down,39,"Yeah, I'm working on smaller images in down time between other meetings",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9481#issuecomment-700750562
https://github.com/hail-is/hail/pull/9484#issuecomment-794121319:124,Performance,cache,cached,124,I'm planning to remove flags in the service because they have bad idempotency properties. Is there a reason to not make the cached FS the default one?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9484#issuecomment-794121319
https://github.com/hail-is/hail/pull/9485#issuecomment-696247576:79,Integrability,depend,dependent,79,"Some thoughts:. - This should probably be the default; - Export table is spark dependent. It shouldn't be.; - For the `CONCATENATED` case, we're probably not going to do better than this.; - For the parallel cases, even if we don't handle indexing while the files are being written (which would be the best option), It would be nice to have a `parallelize` kind of functionality that doesn't rely on spark.; - This is less important though since we still use RDD for `export_table` and `export_vcf`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9485#issuecomment-696247576
https://github.com/hail-is/hail/pull/9485#issuecomment-699127443:33,Integrability,interface,interface,33,"After sleeping on how I want the interface to be in general, I think this is ready for review. Thanks for your patience.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9485#issuecomment-699127443
https://github.com/hail-is/hail/pull/9487#issuecomment-697554339:60,Availability,failure,failure,60,I believe I've addressed all of your comments now. The test failure is some spurious batch thing. I'll rerun when it's approved.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9487#issuecomment-697554339
https://github.com/hail-is/hail/pull/9487#issuecomment-697554339:55,Testability,test,test,55,I believe I've addressed all of your comments now. The test failure is some spurious batch thing. I'll rerun when it's approved.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9487#issuecomment-697554339
https://github.com/hail-is/hail/pull/9487#issuecomment-701618938:10,Testability,test,test,10,"Ok, added test and rules. Back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9487#issuecomment-701618938
https://github.com/hail-is/hail/pull/9493#issuecomment-697063777:25,Modifiability,config,config,25,Tested locally. Renames `config.yaml` to `config.ini` and then otherwise operates as normal for set/get/unset.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9493#issuecomment-697063777
https://github.com/hail-is/hail/pull/9493#issuecomment-697063777:42,Modifiability,config,config,42,Tested locally. Renames `config.yaml` to `config.ini` and then otherwise operates as normal for set/get/unset.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9493#issuecomment-697063777
https://github.com/hail-is/hail/pull/9493#issuecomment-697063777:0,Testability,Test,Tested,0,Tested locally. Renames `config.yaml` to `config.ini` and then otherwise operates as normal for set/get/unset.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9493#issuecomment-697063777
https://github.com/hail-is/hail/pull/9498#issuecomment-697999484:29,Testability,benchmark,benchmark,29,What's the difference in the benchmark?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9498#issuecomment-697999484
https://github.com/hail-is/hail/pull/9498#issuecomment-698315046:23,Modifiability,config,config,23,"Before change:. ```; {""config"": {""cores"": 1, ""version"": ""0.2.57-e20c00f05c78"", ""timestamp"": ""2020-09-24 08:27:24.863298"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""hwe_normalized_pca_blanczos_small_data_10_iterations"", ""failed"": false, ""timed_out"": false, ""times"": [54.736854666000006, 46.213391341000005, 52.75462794499998]}]}; ```. After change: . ```; {""config"": {""cores"": 1, ""version"": ""0.2.57-c013f70fe868"", ""timestamp"": ""2020-09-24 08:32:23.991129"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""hwe_normalized_pca_blanczos_small_data_10_iterations"", ""failed"": false, ""timed_out"": false, ""times"": [28.998368115000005, 40.65512770199999, 28.816323178000005]}]}; ```. Obvious improvement, nearly 2x.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9498#issuecomment-698315046
https://github.com/hail-is/hail/pull/9498#issuecomment-698315046:361,Modifiability,config,config,361,"Before change:. ```; {""config"": {""cores"": 1, ""version"": ""0.2.57-e20c00f05c78"", ""timestamp"": ""2020-09-24 08:27:24.863298"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""hwe_normalized_pca_blanczos_small_data_10_iterations"", ""failed"": false, ""timed_out"": false, ""times"": [54.736854666000006, 46.213391341000005, 52.75462794499998]}]}; ```. After change: . ```; {""config"": {""cores"": 1, ""version"": ""0.2.57-c013f70fe868"", ""timestamp"": ""2020-09-24 08:32:23.991129"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""hwe_normalized_pca_blanczos_small_data_10_iterations"", ""failed"": false, ""timed_out"": false, ""times"": [28.998368115000005, 40.65512770199999, 28.816323178000005]}]}; ```. Obvious improvement, nearly 2x.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9498#issuecomment-698315046
https://github.com/hail-is/hail/pull/9498#issuecomment-698315046:144,Testability,benchmark,benchmarks,144,"Before change:. ```; {""config"": {""cores"": 1, ""version"": ""0.2.57-e20c00f05c78"", ""timestamp"": ""2020-09-24 08:27:24.863298"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""hwe_normalized_pca_blanczos_small_data_10_iterations"", ""failed"": false, ""timed_out"": false, ""times"": [54.736854666000006, 46.213391341000005, 52.75462794499998]}]}; ```. After change: . ```; {""config"": {""cores"": 1, ""version"": ""0.2.57-c013f70fe868"", ""timestamp"": ""2020-09-24 08:32:23.991129"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""hwe_normalized_pca_blanczos_small_data_10_iterations"", ""failed"": false, ""timed_out"": false, ""times"": [28.998368115000005, 40.65512770199999, 28.816323178000005]}]}; ```. Obvious improvement, nearly 2x.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9498#issuecomment-698315046
https://github.com/hail-is/hail/pull/9498#issuecomment-698315046:482,Testability,benchmark,benchmarks,482,"Before change:. ```; {""config"": {""cores"": 1, ""version"": ""0.2.57-e20c00f05c78"", ""timestamp"": ""2020-09-24 08:27:24.863298"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""hwe_normalized_pca_blanczos_small_data_10_iterations"", ""failed"": false, ""timed_out"": false, ""times"": [54.736854666000006, 46.213391341000005, 52.75462794499998]}]}; ```. After change: . ```; {""config"": {""cores"": 1, ""version"": ""0.2.57-c013f70fe868"", ""timestamp"": ""2020-09-24 08:32:23.991129"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""hwe_normalized_pca_blanczos_small_data_10_iterations"", ""failed"": false, ""timed_out"": false, ""times"": [28.998368115000005, 40.65512770199999, 28.816323178000005]}]}; ```. Obvious improvement, nearly 2x.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9498#issuecomment-698315046
https://github.com/hail-is/hail/pull/9501#issuecomment-698590626:5,Testability,test,tests,5,KING tests are failing,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9501#issuecomment-698590626
https://github.com/hail-is/hail/pull/9502#issuecomment-698497283:38,Deployability,install,install,38,"No, but its the same string I used to install hail dependencies for Terra's notebooks. I can run a test now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9502#issuecomment-698497283
https://github.com/hail-is/hail/pull/9502#issuecomment-698497283:51,Integrability,depend,dependencies,51,"No, but its the same string I used to install hail dependencies for Terra's notebooks. I can run a test now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9502#issuecomment-698497283
https://github.com/hail-is/hail/pull/9502#issuecomment-698497283:99,Testability,test,test,99,"No, but its the same string I used to install hail dependencies for Terra's notebooks. I can run a test now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9502#issuecomment-698497283
https://github.com/hail-is/hail/pull/9502#issuecomment-698584747:35,Testability,test,tests,35,This is also failing the `hailctl` tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9502#issuecomment-698584747
https://github.com/hail-is/hail/pull/9502#issuecomment-698604882:0,Testability,test,tested,0,"tested, it works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9502#issuecomment-698604882
https://github.com/hail-is/hail/pull/9502#issuecomment-698632639:24,Testability,test,tests,24,Still failing `hailctl` tests though.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9502#issuecomment-698632639
https://github.com/hail-is/hail/pull/9503#issuecomment-699076394:41,Testability,test,tested,41,"Did you confirm that the behaviors being tested in the deleted `checkedConvertFrom` tests are tested elsewhere for `copyFromType`? Otherwise, looks good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9503#issuecomment-699076394
https://github.com/hail-is/hail/pull/9503#issuecomment-699076394:84,Testability,test,tests,84,"Did you confirm that the behaviors being tested in the deleted `checkedConvertFrom` tests are tested elsewhere for `copyFromType`? Otherwise, looks good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9503#issuecomment-699076394
https://github.com/hail-is/hail/pull/9503#issuecomment-699076394:94,Testability,test,tested,94,"Did you confirm that the behaviors being tested in the deleted `checkedConvertFrom` tests are tested elsewhere for `copyFromType`? Otherwise, looks good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9503#issuecomment-699076394
https://github.com/hail-is/hail/pull/9510#issuecomment-703726796:385,Performance,cache,cached,385,"After discussion with @danking, I redesigned this. The PR was failing due to timeouts since we weren't refreshing statuses in `wait` causing everything to loop forever. I split the API up into ""always hit the endpoint"", and ""if you use this function, hit the endpoint at most one time"". I think it's more explicit. We'll want to audit uses of `status()` to ensure that we're using the cached one if possible in several circumstances especially in `hailctl batch` itself, but that can be a future change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9510#issuecomment-703726796
https://github.com/hail-is/hail/pull/9510#issuecomment-703726796:77,Safety,timeout,timeouts,77,"After discussion with @danking, I redesigned this. The PR was failing due to timeouts since we weren't refreshing statuses in `wait` causing everything to loop forever. I split the API up into ""always hit the endpoint"", and ""if you use this function, hit the endpoint at most one time"". I think it's more explicit. We'll want to audit uses of `status()` to ensure that we're using the cached one if possible in several circumstances especially in `hailctl batch` itself, but that can be a future change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9510#issuecomment-703726796
https://github.com/hail-is/hail/pull/9510#issuecomment-703726796:329,Security,audit,audit,329,"After discussion with @danking, I redesigned this. The PR was failing due to timeouts since we weren't refreshing statuses in `wait` causing everything to loop forever. I split the API up into ""always hit the endpoint"", and ""if you use this function, hit the endpoint at most one time"". I think it's more explicit. We'll want to audit uses of `status()` to ensure that we're using the cached one if possible in several circumstances especially in `hailctl batch` itself, but that can be a future change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9510#issuecomment-703726796
https://github.com/hail-is/hail/pull/9523#issuecomment-701544404:64,Usability,simpl,simpler,64,"I can't remember why I did it the other way, but your way seems simpler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701544404
https://github.com/hail-is/hail/pull/9523#issuecomment-701545652:122,Deployability,a/b,a/b,122,Oh. I remember why now. When we were using rsync I was worried someone would simultaneous try to do this:. cp gs://bucket/a/b/; cp gs://bucket/a/. Then they would clobber each other.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701545652
https://github.com/hail-is/hail/pull/9523#issuecomment-701574241:130,Deployability,a/b,a/b,130,"> Oh. I remember why now. When we were using rsync I was worried someone would simultaneous try to do this:; > ; > cp gs://bucket/a/b/; > cp gs://bucket/a/; > ; > Then they would clobber each other. Can you flesh this out, I'm not sure I understand the sequence of commands here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701574241
https://github.com/hail-is/hail/pull/9523#issuecomment-701580816:55,Deployability,a/b,a/b,55,"Let's say we have the following structure. gs://bucket/a/b/foo.txt; gs://bucket/a/b/bar.txt; gs://bucket/a/baz.txt. In the old design with rsync, we could be doing in parallel the following:. ```; gsutil rsync gs://bucket/a; ```. and. ```; gsutil rsync gs://bucket/a/b/; ```. I was worried we would delete files in the rsync in the middle of one job doing the copying. Basically some kind of race condition. However, with the new design of the cache, this might not be a problem where we're not using gsutil rsync but instead writing our own version of rsync. Cotton's fleshing out the design of that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701580816
https://github.com/hail-is/hail/pull/9523#issuecomment-701580816:80,Deployability,a/b,a/b,80,"Let's say we have the following structure. gs://bucket/a/b/foo.txt; gs://bucket/a/b/bar.txt; gs://bucket/a/baz.txt. In the old design with rsync, we could be doing in parallel the following:. ```; gsutil rsync gs://bucket/a; ```. and. ```; gsutil rsync gs://bucket/a/b/; ```. I was worried we would delete files in the rsync in the middle of one job doing the copying. Basically some kind of race condition. However, with the new design of the cache, this might not be a problem where we're not using gsutil rsync but instead writing our own version of rsync. Cotton's fleshing out the design of that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701580816
https://github.com/hail-is/hail/pull/9523#issuecomment-701580816:105,Deployability,a/b,a/baz,105,"Let's say we have the following structure. gs://bucket/a/b/foo.txt; gs://bucket/a/b/bar.txt; gs://bucket/a/baz.txt. In the old design with rsync, we could be doing in parallel the following:. ```; gsutil rsync gs://bucket/a; ```. and. ```; gsutil rsync gs://bucket/a/b/; ```. I was worried we would delete files in the rsync in the middle of one job doing the copying. Basically some kind of race condition. However, with the new design of the cache, this might not be a problem where we're not using gsutil rsync but instead writing our own version of rsync. Cotton's fleshing out the design of that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701580816
https://github.com/hail-is/hail/pull/9523#issuecomment-701580816:265,Deployability,a/b,a/b,265,"Let's say we have the following structure. gs://bucket/a/b/foo.txt; gs://bucket/a/b/bar.txt; gs://bucket/a/baz.txt. In the old design with rsync, we could be doing in parallel the following:. ```; gsutil rsync gs://bucket/a; ```. and. ```; gsutil rsync gs://bucket/a/b/; ```. I was worried we would delete files in the rsync in the middle of one job doing the copying. Basically some kind of race condition. However, with the new design of the cache, this might not be a problem where we're not using gsutil rsync but instead writing our own version of rsync. Cotton's fleshing out the design of that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701580816
https://github.com/hail-is/hail/pull/9523#issuecomment-701580816:392,Performance,race condition,race condition,392,"Let's say we have the following structure. gs://bucket/a/b/foo.txt; gs://bucket/a/b/bar.txt; gs://bucket/a/baz.txt. In the old design with rsync, we could be doing in parallel the following:. ```; gsutil rsync gs://bucket/a; ```. and. ```; gsutil rsync gs://bucket/a/b/; ```. I was worried we would delete files in the rsync in the middle of one job doing the copying. Basically some kind of race condition. However, with the new design of the cache, this might not be a problem where we're not using gsutil rsync but instead writing our own version of rsync. Cotton's fleshing out the design of that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701580816
https://github.com/hail-is/hail/pull/9523#issuecomment-701580816:444,Performance,cache,cache,444,"Let's say we have the following structure. gs://bucket/a/b/foo.txt; gs://bucket/a/b/bar.txt; gs://bucket/a/baz.txt. In the old design with rsync, we could be doing in parallel the following:. ```; gsutil rsync gs://bucket/a; ```. and. ```; gsutil rsync gs://bucket/a/b/; ```. I was worried we would delete files in the rsync in the middle of one job doing the copying. Basically some kind of race condition. However, with the new design of the cache, this might not be a problem where we're not using gsutil rsync but instead writing our own version of rsync. Cotton's fleshing out the design of that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701580816
https://github.com/hail-is/hail/pull/9523#issuecomment-701598366:20,Deployability,patch,patch,20,"I believe that this patch and the original version both prevent this. They would both lock the parent directory of `path` and thus would prevent concurrent copying. Also, it appears that as long as we don't use the `-d` option, and the filenames are unique, `gsutil rsync` kinda already does what we want. I also feel like the orignal approach was way too aggressive, it seems like it was serializing _all_ locked filesystem operations in the entire filesystem subtree since it would wait for the lock on every parent other than `/` of the requested file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701598366
https://github.com/hail-is/hail/pull/9523#issuecomment-701598366:145,Performance,concurren,concurrent,145,"I believe that this patch and the original version both prevent this. They would both lock the parent directory of `path` and thus would prevent concurrent copying. Also, it appears that as long as we don't use the `-d` option, and the filenames are unique, `gsutil rsync` kinda already does what we want. I also feel like the orignal approach was way too aggressive, it seems like it was serializing _all_ locked filesystem operations in the entire filesystem subtree since it would wait for the lock on every parent other than `/` of the requested file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701598366
https://github.com/hail-is/hail/pull/9524#issuecomment-701068245:5,Availability,error,errors,5,"What errors?. I'll run CI on this, but I don't expect the spark 2 stuff to have any problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701068245
https://github.com/hail-is/hail/pull/9524#issuecomment-701074089:89,Testability,test,tests,89,"I don't believe there were any glaring problems on the Spark 2 side, but some the Python tests on Spark 3 are failing, notably in `test/hail/linalg/test_linalg.py`. I can't quite diagnose the issue off the top of my head.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701074089
https://github.com/hail-is/hail/pull/9524#issuecomment-701074089:131,Testability,test,test,131,"I don't believe there were any glaring problems on the Spark 2 side, but some the Python tests on Spark 3 are failing, notably in `test/hail/linalg/test_linalg.py`. I can't quite diagnose the issue off the top of my head.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701074089
https://github.com/hail-is/hail/pull/9524#issuecomment-701358022:64,Availability,failure,failure,64,"Passing CI tests for Spark 2.4. Do you have a stack trace for a failure?. Sriram saw issues related to Breeze in #9199, but I think it was the bug you noted above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701358022
https://github.com/hail-is/hail/pull/9524#issuecomment-701358022:11,Testability,test,tests,11,"Passing CI tests for Spark 2.4. Do you have a stack trace for a failure?. Sriram saw issues related to Breeze in #9199, but I think it was the bug you noted above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701358022
https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:130,Availability,down,downstream,130,"Yep, I bumped into the Breeze bug a while back while trying to upgrade Hail to Spark 3 internally, and realized it'd be a blocker downstream. I've only seen issues on the Python side; for example:; ```; ______________________________________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406
https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:63,Deployability,upgrade,upgrade,63,"Yep, I bumped into the Breeze bug a while back while trying to upgrade Hail to Spark 3 internally, and realized it'd be a blocker downstream. I've only seen issues on the Python side; for example:; ```; ______________________________________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406
https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:282,Testability,Test,Tests,282,"Yep, I bumped into the Breeze bug a while back while trying to upgrade Hail to Spark 3 internally, and realized it'd be a blocker downstream. I've only seen issues on the Python side; for example:; ```; ______________________________________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406
https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:402,Testability,test,test,402,"Yep, I bumped into the Breeze bug a while back while trying to upgrade Hail to Spark 3 internally, and realized it'd be a blocker downstream. I've only seen issues on the Python side; for example:; ```; ______________________________________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406
https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:431,Testability,Test,Tests,431,"Yep, I bumped into the Breeze bug a while back while trying to upgrade Hail to Spark 3 internally, and realized it'd be a blocker downstream. I've only seen issues on the Python side; for example:; ```; ______________________________________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406
https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:437,Testability,test,testMethod,437,"Yep, I bumped into the Breeze bug a while back while trying to upgrade Hail to Spark 3 internally, and realized it'd be a blocker downstream. I've only seen issues on the Python side; for example:; ```; ______________________________________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406
https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:1143,Testability,assert,assertEqual,1143," issues on the Python side; for example:; ```; ______________________________________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=4.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406
https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:1201,Testability,assert,assertEqual,1201,"_________________________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=4.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=4.0)]; R: [Struct(ent",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406
https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:1248,Testability,assert,assertTrue,1248,"____________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=4.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=4.0)]; R: [Struct(entry=0.0)]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406
https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:1290,Testability,Assert,AssertionError,1290,"____________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=4.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=4.0)]; R: [Struct(entry=0.0)]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406
https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:1325,Testability,test,test,1325,"____________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=4.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=4.0)]; R: [Struct(entry=0.0)]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406
https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:1362,Testability,Assert,AssertionError,1362,"____________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=4.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=4.0)]; R: [Struct(entry=0.0)]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406
https://github.com/hail-is/hail/pull/9524#issuecomment-701717243:52,Integrability,depend,dependencies,52,"Turns out I needed to pull in the transitive breeze dependencies; without these, the Python tests failed while the Scala tests passed. I've re-run the tests on Spark 3 now; let me know if there are any other changes you'd like to see. Unfortunately, there's a small bug I saw on the Spark 3 MLLib side; I'll make sure this gets addressed ASAP: https://issues.apache.org/jira/browse/SPARK-33043",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701717243
https://github.com/hail-is/hail/pull/9524#issuecomment-701717243:92,Testability,test,tests,92,"Turns out I needed to pull in the transitive breeze dependencies; without these, the Python tests failed while the Scala tests passed. I've re-run the tests on Spark 3 now; let me know if there are any other changes you'd like to see. Unfortunately, there's a small bug I saw on the Spark 3 MLLib side; I'll make sure this gets addressed ASAP: https://issues.apache.org/jira/browse/SPARK-33043",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701717243
https://github.com/hail-is/hail/pull/9524#issuecomment-701717243:121,Testability,test,tests,121,"Turns out I needed to pull in the transitive breeze dependencies; without these, the Python tests failed while the Scala tests passed. I've re-run the tests on Spark 3 now; let me know if there are any other changes you'd like to see. Unfortunately, there's a small bug I saw on the Spark 3 MLLib side; I'll make sure this gets addressed ASAP: https://issues.apache.org/jira/browse/SPARK-33043",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701717243
https://github.com/hail-is/hail/pull/9524#issuecomment-701717243:151,Testability,test,tests,151,"Turns out I needed to pull in the transitive breeze dependencies; without these, the Python tests failed while the Scala tests passed. I've re-run the tests on Spark 3 now; let me know if there are any other changes you'd like to see. Unfortunately, there's a small bug I saw on the Spark 3 MLLib side; I'll make sure this gets addressed ASAP: https://issues.apache.org/jira/browse/SPARK-33043",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701717243
https://github.com/hail-is/hail/pull/9524#issuecomment-702189599:7,Testability,test,tests,7,"Ran CI tests, looks like everything is passing now. Will leave approval to @tpoterba though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-702189599
https://github.com/hail-is/hail/pull/9524#issuecomment-702190052:72,Deployability,release,releases,72,Thanks for doing this! Should make things easier when dataproc actually releases a Spark 3 version,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-702190052
https://github.com/hail-is/hail/pull/9527#issuecomment-701653528:53,Integrability,depend,depend,53,I believe we removed PLINK because we didn't want to depend on it. It was historically hard to get a stable URL to a fixed version of the PLINK binary.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9527#issuecomment-701653528
https://github.com/hail-is/hail/pull/9549#issuecomment-702888995:24,Testability,test,test,24,Failing a local backend test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9549#issuecomment-702888995
https://github.com/hail-is/hail/pull/9549#issuecomment-703613115:24,Testability,test,test,24,"fixed the local backend test (had to do with the HailUserException not getting extracted properly in lowered execution, which I'm punting on right now). Docs build is now failing, which I think is a problem in main.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9549#issuecomment-703613115
https://github.com/hail-is/hail/pull/9549#issuecomment-704921949:36,Availability,error,error,36,#9569 Fixes the `HailUserException` error you encountered. `LocalBackend` now correctly handles `HailUserException`s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9549#issuecomment-704921949
https://github.com/hail-is/hail/pull/9553#issuecomment-705227991:60,Deployability,deploy,deployed,60,"One of the problems with the new model is that tests on the deployed version are going to leave billing project poop everywhere unless we actually go in and delete them, hmmm....",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9553#issuecomment-705227991
https://github.com/hail-is/hail/pull/9553#issuecomment-705227991:47,Testability,test,tests,47,"One of the problems with the new model is that tests on the deployed version are going to leave billing project poop everywhere unless we actually go in and delete them, hmmm....",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9553#issuecomment-705227991
https://github.com/hail-is/hail/pull/9553#issuecomment-705604419:46,Testability,test,tests,46,"I have a fix for this. We shouldn't run these tests in production. I added the `pytest.mark.test_scope_only`. Then I added this flag to the existing test_batch_* jobs: `-m ""not test_scope_only""`. Finally, I added a new step that just runs those tests in the test and dev scopes. `-m ""test_scope_only""` with. ```; + scopes:; + - dev; + - test; ```. If you're okay with this strategy, I'll make a separate PR so that we can both use this in our respective PRs. Can you also make a separate PR for the `_token_file` change and having the dev and regular client?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9553#issuecomment-705604419
https://github.com/hail-is/hail/pull/9553#issuecomment-705604419:245,Testability,test,tests,245,"I have a fix for this. We shouldn't run these tests in production. I added the `pytest.mark.test_scope_only`. Then I added this flag to the existing test_batch_* jobs: `-m ""not test_scope_only""`. Finally, I added a new step that just runs those tests in the test and dev scopes. `-m ""test_scope_only""` with. ```; + scopes:; + - dev; + - test; ```. If you're okay with this strategy, I'll make a separate PR so that we can both use this in our respective PRs. Can you also make a separate PR for the `_token_file` change and having the dev and regular client?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9553#issuecomment-705604419
https://github.com/hail-is/hail/pull/9553#issuecomment-705604419:258,Testability,test,test,258,"I have a fix for this. We shouldn't run these tests in production. I added the `pytest.mark.test_scope_only`. Then I added this flag to the existing test_batch_* jobs: `-m ""not test_scope_only""`. Finally, I added a new step that just runs those tests in the test and dev scopes. `-m ""test_scope_only""` with. ```; + scopes:; + - dev; + - test; ```. If you're okay with this strategy, I'll make a separate PR so that we can both use this in our respective PRs. Can you also make a separate PR for the `_token_file` change and having the dev and regular client?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9553#issuecomment-705604419
https://github.com/hail-is/hail/pull/9553#issuecomment-705604419:337,Testability,test,test,337,"I have a fix for this. We shouldn't run these tests in production. I added the `pytest.mark.test_scope_only`. Then I added this flag to the existing test_batch_* jobs: `-m ""not test_scope_only""`. Finally, I added a new step that just runs those tests in the test and dev scopes. `-m ""test_scope_only""` with. ```; + scopes:; + - dev; + - test; ```. If you're okay with this strategy, I'll make a separate PR so that we can both use this in our respective PRs. Can you also make a separate PR for the `_token_file` change and having the dev and regular client?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9553#issuecomment-705604419
https://github.com/hail-is/hail/pull/9567#issuecomment-704535423:11,Deployability,install,install-benchmark,11,I only use install-benchmark,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9567#issuecomment-704535423
https://github.com/hail-is/hail/pull/9567#issuecomment-704535423:19,Testability,benchmark,benchmark,19,I only use install-benchmark,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9567#issuecomment-704535423
https://github.com/hail-is/hail/pull/9568#issuecomment-704997544:53,Testability,test,tests,53,I picked ld score regression and two randomish other tests to be the tests that we run with the unchecked allocator. Let me know if there are others you'd like to see added.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9568#issuecomment-704997544
https://github.com/hail-is/hail/pull/9568#issuecomment-704997544:69,Testability,test,tests,69,I picked ld score regression and two randomish other tests to be the tests that we run with the unchecked allocator. Let me know if there are others you'd like to see added.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9568#issuecomment-704997544
https://github.com/hail-is/hail/pull/9570#issuecomment-704990030:8,Testability,test,tests,8,"Lots of tests failing, this isn't quite right yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9570#issuecomment-704990030
https://github.com/hail-is/hail/pull/9571#issuecomment-705055793:226,Modifiability,variab,variables,226,"&& binds tighter than ||, I don't see how the parentheses make any difference? (Also, we should use curlies, which are for precedence as opposed to parens which trigger sub-shells to start and can be confusing wrt environment variables)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9571#issuecomment-705055793
https://github.com/hail-is/hail/pull/9571#issuecomment-705059061:71,Deployability,deploy,deploy,71,"Well, I look forward to your PR fixing this. I tested the fix with dev deploy before and after my change and it fixed it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9571#issuecomment-705059061
https://github.com/hail-is/hail/pull/9571#issuecomment-705059061:47,Testability,test,tested,47,"Well, I look forward to your PR fixing this. I tested the fix with dev deploy before and after my change and it fixed it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9571#issuecomment-705059061
https://github.com/hail-is/hail/pull/9578#issuecomment-708568970:26,Integrability,message,message,26,"EDIT: The problem in this message is still valid, but the fix described here was replaced following the conversation with Tim. Ok, second pruning bug fixed. This commit illustrates the change, probably worth looking at in isolation: https://github.com/hail-is/hail/pull/9578/commits/f4ce01d59a56bef7496490145e66d274f51139e6. Essentially, the problem was that we were rebuilding an `InsertFields(child, newFields, _)` node with a requested type of empty struct. The old rebuilding strategy handled this by recursively rebuilding the `child` node, then filtering `newFields` to include only those in the requested type. However, if the `child` node can't be filtered (say it's a `Ref` of a certain type), and one of the fields being inserted is overwriting a field in the child, you need to make sure you do the overwriting so the typing works out. Happy to explain more if this is unclear. I also added an optimization to just drop the whole `InsertFields` if the requested type is the empty struct, not sure if that's good pruning manners though. . CC @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9578#issuecomment-708568970
https://github.com/hail-is/hail/pull/9578#issuecomment-708568970:905,Performance,optimiz,optimization,905,"EDIT: The problem in this message is still valid, but the fix described here was replaced following the conversation with Tim. Ok, second pruning bug fixed. This commit illustrates the change, probably worth looking at in isolation: https://github.com/hail-is/hail/pull/9578/commits/f4ce01d59a56bef7496490145e66d274f51139e6. Essentially, the problem was that we were rebuilding an `InsertFields(child, newFields, _)` node with a requested type of empty struct. The old rebuilding strategy handled this by recursively rebuilding the `child` node, then filtering `newFields` to include only those in the requested type. However, if the `child` node can't be filtered (say it's a `Ref` of a certain type), and one of the fields being inserted is overwriting a field in the child, you need to make sure you do the overwriting so the typing works out. Happy to explain more if this is unclear. I also added an optimization to just drop the whole `InsertFields` if the requested type is the empty struct, not sure if that's good pruning manners though. . CC @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9578#issuecomment-708568970
https://github.com/hail-is/hail/pull/9578#issuecomment-708647344:100,Testability,test,test,100,"Something about that must still be invalid, as I'm now failing `TypeCheck` on a seemingly unrelated test as a result of pruning",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9578#issuecomment-708647344
https://github.com/hail-is/hail/pull/9578#issuecomment-713770349:55,Testability,test,tests,55,"I'm going to close this and break it up once I see how tests shake out. I think it's fixed now, but half of this PR has become pruner fixes that probably ought to be their own PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9578#issuecomment-713770349
https://github.com/hail-is/hail/pull/9579#issuecomment-705774492:9,Deployability,update,updated,9,"ok, I've updated this PR so that billing projects can be ""open"", ""closed"", or ""deleted"". Allowed flow is `open` <--> `closed` --> `deleted`. Closed billing projects will still show up in the UI and billing reports; deleted projects show up in neither, but batches belonging to deleted billing projects will still show up in the batches table unless they're deleted. There's no endpoint here for deleting closed projects; I'll add an API endpoint in the billing project API PR so that I can run the tests properly there, but currently don't intend to add a button to the UI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9579#issuecomment-705774492
https://github.com/hail-is/hail/pull/9579#issuecomment-705774492:498,Testability,test,tests,498,"ok, I've updated this PR so that billing projects can be ""open"", ""closed"", or ""deleted"". Allowed flow is `open` <--> `closed` --> `deleted`. Closed billing projects will still show up in the UI and billing reports; deleted projects show up in neither, but batches belonging to deleted billing projects will still show up in the batches table unless they're deleted. There's no endpoint here for deleting closed projects; I'll add an API endpoint in the billing project API PR so that I can run the tests properly there, but currently don't intend to add a button to the UI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9579#issuecomment-705774492
https://github.com/hail-is/hail/pull/9581#issuecomment-705649534:26,Testability,test,test,26,"@jigold not sure why this test is being marked as ""failed""---maybe because there are currently no tests in that filter?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9581#issuecomment-705649534
https://github.com/hail-is/hail/pull/9581#issuecomment-705649534:98,Testability,test,tests,98,"@jigold not sure why this test is being marked as ""failed""---maybe because there are currently no tests in that filter?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9581#issuecomment-705649534
https://github.com/hail-is/hail/pull/9583#issuecomment-705641768:218,Testability,test,test-,218,This didn't work:. ```; + retry gcloud -q auth activate-service-account --key-file=/gsa-key/key.json; + gcloud -q auth activate-service-account --key-file=/gsa-key/key.json; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + '{retry' gsutil -m cp -R /io/data1 'gs://hail-test-dmk9z}'; /bin/bash: line 11: {retry: command not found; + '{retry' gsutil -m cp -R /io/data1 'gs://hail-test-dmk9z}'; /bin/bash: line 11: {retry: command not found; + '{retry' gsutil -m cp -R /io/data2 'gs://hail-test-dmk9z}'; /bin/bash: line 11: {retry: command not found; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9583#issuecomment-705641768
https://github.com/hail-is/hail/pull/9583#issuecomment-705641768:310,Testability,test,test-,310,This didn't work:. ```; + retry gcloud -q auth activate-service-account --key-file=/gsa-key/key.json; + gcloud -q auth activate-service-account --key-file=/gsa-key/key.json; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + '{retry' gsutil -m cp -R /io/data1 'gs://hail-test-dmk9z}'; /bin/bash: line 11: {retry: command not found; + '{retry' gsutil -m cp -R /io/data1 'gs://hail-test-dmk9z}'; /bin/bash: line 11: {retry: command not found; + '{retry' gsutil -m cp -R /io/data2 'gs://hail-test-dmk9z}'; /bin/bash: line 11: {retry: command not found; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9583#issuecomment-705641768
https://github.com/hail-is/hail/pull/9583#issuecomment-705641768:419,Testability,test,test-,419,This didn't work:. ```; + retry gcloud -q auth activate-service-account --key-file=/gsa-key/key.json; + gcloud -q auth activate-service-account --key-file=/gsa-key/key.json; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + '{retry' gsutil -m cp -R /io/data1 'gs://hail-test-dmk9z}'; /bin/bash: line 11: {retry: command not found; + '{retry' gsutil -m cp -R /io/data1 'gs://hail-test-dmk9z}'; /bin/bash: line 11: {retry: command not found; + '{retry' gsutil -m cp -R /io/data2 'gs://hail-test-dmk9z}'; /bin/bash: line 11: {retry: command not found; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9583#issuecomment-705641768
https://github.com/hail-is/hail/pull/9583#issuecomment-705641768:528,Testability,test,test-,528,This didn't work:. ```; + retry gcloud -q auth activate-service-account --key-file=/gsa-key/key.json; + gcloud -q auth activate-service-account --key-file=/gsa-key/key.json; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + '{retry' gsutil -m cp -R /io/data1 'gs://hail-test-dmk9z}'; /bin/bash: line 11: {retry: command not found; + '{retry' gsutil -m cp -R /io/data1 'gs://hail-test-dmk9z}'; /bin/bash: line 11: {retry: command not found; + '{retry' gsutil -m cp -R /io/data2 'gs://hail-test-dmk9z}'; /bin/bash: line 11: {retry: command not found; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9583#issuecomment-705641768
https://github.com/hail-is/hail/pull/9593#issuecomment-713905582:111,Performance,load,load,111,"I removed jinja and restored the build command. I can't see your last comment on docker/Makefile, GitHub won't load it for me, can you repost it here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9593#issuecomment-713905582
https://github.com/hail-is/hail/pull/9594#issuecomment-724099162:21,Availability,Ping,Ping,21,This needs a rebase. Ping me when it's ready.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9594#issuecomment-724099162
https://github.com/hail-is/hail/pull/9594#issuecomment-724108391:48,Availability,echo,echo,48,@jigold ready; I also fixed the recently added `echo` image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9594#issuecomment-724108391
https://github.com/hail-is/hail/pull/9594#issuecomment-725023994:43,Integrability,message,messages,43,we can add a check in CI to look at commit messages and reject if the only difference between the branch and main is a single commit that's fewer than X chars or somethign,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9594#issuecomment-725023994
https://github.com/hail-is/hail/pull/9594#issuecomment-725031817:57,Integrability,message,message,57,"Actually, I think the right fix is for CI to grab the PR message and set it as the merge message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9594#issuecomment-725031817
https://github.com/hail-is/hail/pull/9594#issuecomment-725031817:89,Integrability,message,message,89,"Actually, I think the right fix is for CI to grab the PR message and set it as the merge message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9594#issuecomment-725031817
https://github.com/hail-is/hail/pull/9596#issuecomment-707898595:33,Testability,test,testing,33,"> FYI @tpoterba rather than just testing them independently, it might be worthwhile to have write/read interop tests between the various backends. Spark to local is partially tested by the pre-existing (matrix)tables tests, but not the other way. Agreed, this is a good idea.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9596#issuecomment-707898595
https://github.com/hail-is/hail/pull/9596#issuecomment-707898595:111,Testability,test,tests,111,"> FYI @tpoterba rather than just testing them independently, it might be worthwhile to have write/read interop tests between the various backends. Spark to local is partially tested by the pre-existing (matrix)tables tests, but not the other way. Agreed, this is a good idea.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9596#issuecomment-707898595
https://github.com/hail-is/hail/pull/9596#issuecomment-707898595:175,Testability,test,tested,175,"> FYI @tpoterba rather than just testing them independently, it might be worthwhile to have write/read interop tests between the various backends. Spark to local is partially tested by the pre-existing (matrix)tables tests, but not the other way. Agreed, this is a good idea.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9596#issuecomment-707898595
https://github.com/hail-is/hail/pull/9596#issuecomment-707898595:217,Testability,test,tests,217,"> FYI @tpoterba rather than just testing them independently, it might be worthwhile to have write/read interop tests between the various backends. Spark to local is partially tested by the pre-existing (matrix)tables tests, but not the other way. Agreed, this is a good idea.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9596#issuecomment-707898595
https://github.com/hail-is/hail/pull/9596#issuecomment-707933032:90,Availability,reliab,reliable,90,It is my intention to eventually document it when when local mode is feature complete and reliable. I'm not sure what the standard process is for this. I'd be OK to rename it when that happens.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9596#issuecomment-707933032
https://github.com/hail-is/hail/pull/9598#issuecomment-714488332:95,Integrability,message,message,95,"I thought about the memory thing a bit more. I think we should leave the method, add a warning message that says ""We ignore this method now. It was always confusing. If you need more memory select a concomitant core count.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-714488332
https://github.com/hail-is/hail/pull/9598#issuecomment-714604941:338,Availability,reliab,reliable,338,"This should be good minus tests and double/triple checking the billing is all correct with units etc.. Ideally, there'd be two situations to test:. 1. A storage size that exceeds the unreserved space (i.e. 375Gi); 2. Multiple smaller jobs requesting unreserved space and we need to spin up a disk for one of them. I don't think there's a reliable way to test 2 other than doing it by hand in my dev namespace. Test 1 should be sufficient for checking the new disk and /io works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-714604941
https://github.com/hail-is/hail/pull/9598#issuecomment-714604941:26,Testability,test,tests,26,"This should be good minus tests and double/triple checking the billing is all correct with units etc.. Ideally, there'd be two situations to test:. 1. A storage size that exceeds the unreserved space (i.e. 375Gi); 2. Multiple smaller jobs requesting unreserved space and we need to spin up a disk for one of them. I don't think there's a reliable way to test 2 other than doing it by hand in my dev namespace. Test 1 should be sufficient for checking the new disk and /io works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-714604941
https://github.com/hail-is/hail/pull/9598#issuecomment-714604941:141,Testability,test,test,141,"This should be good minus tests and double/triple checking the billing is all correct with units etc.. Ideally, there'd be two situations to test:. 1. A storage size that exceeds the unreserved space (i.e. 375Gi); 2. Multiple smaller jobs requesting unreserved space and we need to spin up a disk for one of them. I don't think there's a reliable way to test 2 other than doing it by hand in my dev namespace. Test 1 should be sufficient for checking the new disk and /io works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-714604941
https://github.com/hail-is/hail/pull/9598#issuecomment-714604941:354,Testability,test,test,354,"This should be good minus tests and double/triple checking the billing is all correct with units etc.. Ideally, there'd be two situations to test:. 1. A storage size that exceeds the unreserved space (i.e. 375Gi); 2. Multiple smaller jobs requesting unreserved space and we need to spin up a disk for one of them. I don't think there's a reliable way to test 2 other than doing it by hand in my dev namespace. Test 1 should be sufficient for checking the new disk and /io works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-714604941
https://github.com/hail-is/hail/pull/9598#issuecomment-714604941:410,Testability,Test,Test,410,"This should be good minus tests and double/triple checking the billing is all correct with units etc.. Ideally, there'd be two situations to test:. 1. A storage size that exceeds the unreserved space (i.e. 375Gi); 2. Multiple smaller jobs requesting unreserved space and we need to spin up a disk for one of them. I don't think there's a reliable way to test 2 other than doing it by hand in my dev namespace. Test 1 should be sufficient for checking the new disk and /io works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-714604941
https://github.com/hail-is/hail/pull/9598#issuecomment-714610813:238,Deployability,configurat,configuration,238,"@cseed I know you didn't approve the final design document. In summary,. 1. We print a warning to the user if they specify the memory in the batch user library that we're ignoring this parameter. Memory is deduced from CPU and the worker configuration in the front end ignoring the resource request.; 2. There's a concept of reserved vs. unreserved space. Each job gets 5 Gi per core requested in the reserved space. We try and reserve unreserved space if needed for storage needs that are bigger than the reserved space with a semaphore. If we can't get enough unreserved space, then we still give the user the reserved space at 5Gi per core for their container on the worker data disk in addition to the extra disk at /io that is the full storage request. Example:. Storage request is 375Gi and 1 CPU.; User gets 5 Gi for their container.; We spin up a 375Gi disk. For a local ssd with 16 cores, there's 16*5 Gi or 80 Gi in the reserved space. The reserved space for us is 20 Gi (I can set this back to 25 Gi, but I thought 100 Gi being the minimum disk size for persistent SSD data disks was a nice number). This means the unreserved space that is first come first serve is 275 Gi. If the data disk is a 100Gi persistent SSD, then the unreserved space is 0 Gi and any job that requests more storage than 5 Gi per core will have to spin up a new disk.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-714610813
https://github.com/hail-is/hail/pull/9598#issuecomment-714610813:238,Modifiability,config,configuration,238,"@cseed I know you didn't approve the final design document. In summary,. 1. We print a warning to the user if they specify the memory in the batch user library that we're ignoring this parameter. Memory is deduced from CPU and the worker configuration in the front end ignoring the resource request.; 2. There's a concept of reserved vs. unreserved space. Each job gets 5 Gi per core requested in the reserved space. We try and reserve unreserved space if needed for storage needs that are bigger than the reserved space with a semaphore. If we can't get enough unreserved space, then we still give the user the reserved space at 5Gi per core for their container on the worker data disk in addition to the extra disk at /io that is the full storage request. Example:. Storage request is 375Gi and 1 CPU.; User gets 5 Gi for their container.; We spin up a 375Gi disk. For a local ssd with 16 cores, there's 16*5 Gi or 80 Gi in the reserved space. The reserved space for us is 20 Gi (I can set this back to 25 Gi, but I thought 100 Gi being the minimum disk size for persistent SSD data disks was a nice number). This means the unreserved space that is first come first serve is 275 Gi. If the data disk is a 100Gi persistent SSD, then the unreserved space is 0 Gi and any job that requests more storage than 5 Gi per core will have to spin up a new disk.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-714610813
https://github.com/hail-is/hail/pull/9598#issuecomment-718269756:59,Deployability,release,release,59,I need to split this up and think about how we're going to release the change.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-718269756
https://github.com/hail-is/hail/pull/9602#issuecomment-709392527:624,Integrability,depend,depend,624,"OK, here's the high level design:. This diff touches a lot of files, but it is conceptually a small; list of connected changes:. 1. TableStageToRVD. This is infrastructure that lets us take a; TableStage and generate a RVD and BroadcastRow (globals). This; is used in...; 2. SparkBackend shuffle lowering. This now uses TableStageToRVD to; create an RVD, uses RVD methods to shuffle, then RVDToTableStage; to produce a TableStage that reads the shuffled RDD.; 3. TableStageDependencies. As I mentioned in our design meeting this; past week, the RDD we generate in the SparkBackend's parallelizeAndComputeWithIndex; needs to depend on the RDDs consumed by RVDToTableStage. In order to; support this, I have added a notion of a `TableStageDependency` to TableStage,; CollectDistributedArray, and BackendUtils.collectDArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9602#issuecomment-709392527
https://github.com/hail-is/hail/pull/9604#issuecomment-713144112:51,Availability,error,errors,51,"Unfortunately, I can't figure out the verification errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9604#issuecomment-713144112
https://github.com/hail-is/hail/pull/9604#issuecomment-713144645:53,Testability,test,test,53,I can take a look tomorrow - can you point me to the test that's failing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9604#issuecomment-713144645
https://github.com/hail-is/hail/pull/9604#issuecomment-713603033:113,Testability,test,testCollectAsSet,113,"I mean, anything that's touching the BTree is broken, so the following works:. is.hail.expr.ir.Aggregators2Suite.testCollectAsSet",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9604#issuecomment-713603033
https://github.com/hail-is/hail/pull/9604#issuecomment-713805849:317,Availability,error,errors,317,"@chrisvittal I can't figure out how to comment on unedited code, but here:. https://github.com/hail-is/hail/pull/9604/files#diff-689808a42e9fe9329e347edede9bc12419e29a1634afc5d8c899a41c9d550659R211. you probably want to switch to passing in an EmitCode as well. Not sure if that's the source of the code verification errors, but I think the order I was originally passing in the tuples (m, v) is swapped from how emitCodeParams represent them (v, m), so that might help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9604#issuecomment-713805849
https://github.com/hail-is/hail/issues/9607#issuecomment-1145283347:58,Safety,avoid,avoid,58,This is a limitation of the Hadoop library that we cannot avoid.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9607#issuecomment-1145283347
https://github.com/hail-is/hail/pull/9614#issuecomment-714575225:70,Testability,test,test,70,"So I reread the title of your PR. This is fine if you just submit the test batch. However, I think you should come up with a plan to eventually get to the full implementation in stages. Whatever works best for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9614#issuecomment-714575225
https://github.com/hail-is/hail/pull/9614#issuecomment-714575525:43,Integrability,rout,routes,43,"Sorry, fine meaning just having the submit routes and the test. Not fine with the polling of GitHub with the start time stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9614#issuecomment-714575525
https://github.com/hail-is/hail/pull/9614#issuecomment-714575525:58,Testability,test,test,58,"Sorry, fine meaning just having the submit routes and the test. Not fine with the polling of GitHub with the start time stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9614#issuecomment-714575525
https://github.com/hail-is/hail/pull/9618#issuecomment-713077513:10,Availability,failure,failure,10,"ugh, test failure in local backend. Will debug in a bit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9618#issuecomment-713077513
https://github.com/hail-is/hail/pull/9618#issuecomment-713077513:5,Testability,test,test,5,"ugh, test failure in local backend. Will debug in a bit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9618#issuecomment-713077513
https://github.com/hail-is/hail/pull/9618#issuecomment-777080408:183,Availability,failure,failure,183,"I did debug this, though. The failing test (which succeeds in Spark, but fails in local backend) collects a table and asserts that the result is equal to a list of expected rows. The failure is caused by an ordering issue - the rows are the same, but the order is slightly different between the Spark backend (which produces the expected output) and the local backend. However, I think that actually *both* orders are valid under Hail's guarantees. I'll bring this to our next team meeting for group discussion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9618#issuecomment-777080408
https://github.com/hail-is/hail/pull/9618#issuecomment-777080408:38,Testability,test,test,38,"I did debug this, though. The failing test (which succeeds in Spark, but fails in local backend) collects a table and asserts that the result is equal to a list of expected rows. The failure is caused by an ordering issue - the rows are the same, but the order is slightly different between the Spark backend (which produces the expected output) and the local backend. However, I think that actually *both* orders are valid under Hail's guarantees. I'll bring this to our next team meeting for group discussion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9618#issuecomment-777080408
https://github.com/hail-is/hail/pull/9618#issuecomment-777080408:118,Testability,assert,asserts,118,"I did debug this, though. The failing test (which succeeds in Spark, but fails in local backend) collects a table and asserts that the result is equal to a list of expected rows. The failure is caused by an ordering issue - the rows are the same, but the order is slightly different between the Spark backend (which produces the expected output) and the local backend. However, I think that actually *both* orders are valid under Hail's guarantees. I'll bring this to our next team meeting for group discussion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9618#issuecomment-777080408
https://github.com/hail-is/hail/pull/9625#issuecomment-715578188:95,Testability,assert,assertions,95,"I'm also surprised we didn't hit earlier. In doing my linear regression debugging, I had added assertions in every call to memoize and rebuild that we are maintaining the expected supertype relationships, which is how I found most of these. I originally thought all of the problems I found were uncovered by IR changes from #9633, but I now suspect that actually we just have/had cases in `PruneDeadFields` where we break the supertype rules temporarily, but it gets cancelled out by some higher IR and doesn't end up mattering. Would be nice if when debugging / testing we could run those asserts to make sure these things don't slip through in the future, as finding all of these bugs was a big headache.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9625#issuecomment-715578188
https://github.com/hail-is/hail/pull/9625#issuecomment-715578188:563,Testability,test,testing,563,"I'm also surprised we didn't hit earlier. In doing my linear regression debugging, I had added assertions in every call to memoize and rebuild that we are maintaining the expected supertype relationships, which is how I found most of these. I originally thought all of the problems I found were uncovered by IR changes from #9633, but I now suspect that actually we just have/had cases in `PruneDeadFields` where we break the supertype rules temporarily, but it gets cancelled out by some higher IR and doesn't end up mattering. Would be nice if when debugging / testing we could run those asserts to make sure these things don't slip through in the future, as finding all of these bugs was a big headache.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9625#issuecomment-715578188
https://github.com/hail-is/hail/pull/9625#issuecomment-715578188:590,Testability,assert,asserts,590,"I'm also surprised we didn't hit earlier. In doing my linear regression debugging, I had added assertions in every call to memoize and rebuild that we are maintaining the expected supertype relationships, which is how I found most of these. I originally thought all of the problems I found were uncovered by IR changes from #9633, but I now suspect that actually we just have/had cases in `PruneDeadFields` where we break the supertype rules temporarily, but it gets cancelled out by some higher IR and doesn't end up mattering. Would be nice if when debugging / testing we could run those asserts to make sure these things don't slip through in the future, as finding all of these bugs was a big headache.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9625#issuecomment-715578188
https://github.com/hail-is/hail/pull/9627#issuecomment-713899121:27,Testability,test,test,27,I've added a note to add a test for this later. Thanks John!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9627#issuecomment-713899121
https://github.com/hail-is/hail/issues/9628#issuecomment-714496429:51,Deployability,deploy,deployed,51,"I think that's the fix, actually. Just hasn't been deployed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9628#issuecomment-714496429
https://github.com/hail-is/hail/pull/9631#issuecomment-714582672:13,Testability,test,test,13,Can we add a test that triggers it? I think Nick's example from Zulip should do it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9631#issuecomment-714582672
https://github.com/hail-is/hail/pull/9634#issuecomment-718124825:31,Performance,tune,tuned,31,"@patrick-schultz, you may have tuned out the long thread about all the issues with the first Python Chained Linear Regression and pruning, but this one is now clear of all of that and is purely a Python implementation of Scala's `LinearRegressionChained`: https://github.com/hail-is/hail/blob/main/hail/src/main/scala/is/hail/methods/LinearRegression.scala#L175",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9634#issuecomment-718124825
https://github.com/hail-is/hail/pull/9634#issuecomment-718124825:159,Usability,clear,clear,159,"@patrick-schultz, you may have tuned out the long thread about all the issues with the first Python Chained Linear Regression and pruning, but this one is now clear of all of that and is purely a Python implementation of Scala's `LinearRegressionChained`: https://github.com/hail-is/hail/blob/main/hail/src/main/scala/is/hail/methods/LinearRegression.scala#L175",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9634#issuecomment-718124825
https://github.com/hail-is/hail/pull/9634#issuecomment-719583786:82,Testability,benchmark,benchmark,82,This PR seems to have made the non-chained case substantially slower somehow (the benchmark is running 3x slower). Either need to figure out what's wrong with the code we are generating and/or split out different chained and unchained methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9634#issuecomment-719583786
https://github.com/hail-is/hail/pull/9636#issuecomment-715604047:96,Availability,heartbeat,heartbeat,96,For some reason this is timing out after a minute in my dev deploy even though I've removed the heartbeat and it's working on a local server I have running. Need to investigate further.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9636#issuecomment-715604047
https://github.com/hail-is/hail/pull/9636#issuecomment-715604047:60,Deployability,deploy,deploy,60,For some reason this is timing out after a minute in my dev deploy even though I've removed the heartbeat and it's working on a local server I have running. Need to investigate further.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9636#issuecomment-715604047
https://github.com/hail-is/hail/pull/9636#issuecomment-717406840:135,Availability,heartbeat,heartbeat,135,"@danking I ended up rewriting this a bit to make it work with the nginx timeout (instead of getting rid of the timeout, since having a heartbeat seems like a pretty reasonable thing); updated the PR description to match.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9636#issuecomment-717406840
https://github.com/hail-is/hail/pull/9636#issuecomment-717406840:184,Deployability,update,updated,184,"@danking I ended up rewriting this a bit to make it work with the nginx timeout (instead of getting rid of the timeout, since having a heartbeat seems like a pretty reasonable thing); updated the PR description to match.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9636#issuecomment-717406840
https://github.com/hail-is/hail/pull/9636#issuecomment-717406840:72,Safety,timeout,timeout,72,"@danking I ended up rewriting this a bit to make it work with the nginx timeout (instead of getting rid of the timeout, since having a heartbeat seems like a pretty reasonable thing); updated the PR description to match.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9636#issuecomment-717406840
https://github.com/hail-is/hail/pull/9636#issuecomment-717406840:111,Safety,timeout,timeout,111,"@danking I ended up rewriting this a bit to make it work with the nginx timeout (instead of getting rid of the timeout, since having a heartbeat seems like a pretty reasonable thing); updated the PR description to match.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9636#issuecomment-717406840
https://github.com/hail-is/hail/pull/9637#issuecomment-717239923:25,Availability,error,error,25,"Oof, method verification error on one of the lowering tests",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9637#issuecomment-717239923
https://github.com/hail-is/hail/pull/9637#issuecomment-717239923:54,Testability,test,tests,54,"Oof, method verification error on one of the lowering tests",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9637#issuecomment-717239923
https://github.com/hail-is/hail/issues/9645#issuecomment-717257900:117,Safety,avoid,avoid,117,Sorry about that! We changed how input files are stored such that we copy the file to `inputs/{token}/{filename}` to avoid duplicates while not needing to specify extensions. We can't do the same thing for JobResourceFile so it's still there. I'll fix the tutorial.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9645#issuecomment-717257900
https://github.com/hail-is/hail/pull/9646#issuecomment-728896170:35,Performance,load,loadFrom,35,"oops, still need to fix the `null` loadFrom stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9646#issuecomment-728896170
https://github.com/hail-is/hail/pull/9648#issuecomment-717421895:12,Availability,error,errors,12,Compilation errors,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9648#issuecomment-717421895
https://github.com/hail-is/hail/pull/9658#issuecomment-720219339:13,Modifiability,refactor,refactored,13,I completely refactored this. You'll probably want to review the auth.py code from scratch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9658#issuecomment-720219339
https://github.com/hail-is/hail/pull/9659#issuecomment-726104857:5,Availability,failure,failures,5,test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9659#issuecomment-726104857
https://github.com/hail-is/hail/pull/9659#issuecomment-726104857:0,Testability,test,test,0,test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9659#issuecomment-726104857
https://github.com/hail-is/hail/pull/9660#issuecomment-719690473:243,Deployability,deploy,deployed,243,"This was a huge pain. I think I got everything, but it would be great if you can double check. I added two new images since earlier in case you already ran the script. The things I omitted to fix didn't have Makefiles with build steps and are deployed infrequently.; - blog; - notebook/worker/Dockerfile; - notebook/images/Dockerfile; - docker/python-dill. Memory already has the jinja templating in the Makefile.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9660#issuecomment-719690473
https://github.com/hail-is/hail/pull/9660#issuecomment-719759217:91,Performance,cache,cached-images,91,Maybe this is a better solution?. https://cloud.google.com/container-registry/docs/pulling-cached-images,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9660#issuecomment-719759217
https://github.com/hail-is/hail/pull/9661#issuecomment-720515903:31,Deployability,pipeline,pipeline,31,"Yeah I meant what point in the pipeline. One could check on `j.image` (or `hb.Batch(..., default_image=...)`), but maybe `b.run` is best - is it only triggered once per batch, or once per offending job, or once per offending unique image?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9661#issuecomment-720515903
https://github.com/hail-is/hail/pull/9666#issuecomment-724070179:244,Safety,avoid,avoid,244,"So after chatting with @tpoterba last week, I think the conclusion we came to is that `per_y_list` was probably being recomputed for each row of output, which is why changing it to a python list comprehension helped. I tried changing things to avoid that by adding an `rbind` to the old version:. ```; def build_row(per_y_list, row_idx):; # For every field we care about, map across all y's, getting the row_idxth one from each.; idxth_keys = {field_name: block[field_name][row_idx] for field_name in key_field_names}; computed_row_field_names = ['n', 'sum_x', 'y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; computed_row_fields = {; #field_name: [one_y[field_name][row_idx] for one_y in per_y_list] for field_name in computed_row_field_names; field_name: per_y_list.map(lambda one_y: one_y[field_name][row_idx]) for field_name in computed_row_field_names; }; pass_through_rows = {; field_name: block[field_name][row_idx] for field_name in row_field_names; }. if not is_chained:; computed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.rbind(per_y_list, lambda l_per_y_list: hl.range(rows_in_block).map(lambda inner_i: build_row(l_per_y_list, inner_i))); ```. and had no luck, still incredibly slow. Doesn't seem to change the IR size at all if I just add an `rbind` to main branch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9666#issuecomment-724070179
https://github.com/hail-is/hail/pull/9667#issuecomment-722646751:39,Integrability,depend,dependencies,39,The shadow jar file was newer than its dependencies so Make assumed it was good to go. The Make-Way TM would be to make the output of gradle an intermediate step which is then copied to a new file by the jar modification.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9667#issuecomment-722646751
https://github.com/hail-is/hail/pull/9668#issuecomment-721304697:52,Integrability,depend,depends,52,"Also @Dania-Abuhijleh, make sure `deploy_benchmark` depends on this new build step.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9668#issuecomment-721304697
https://github.com/hail-is/hail/pull/9668#issuecomment-722676096:21,Testability,test,test,21,I double checked the test database.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9668#issuecomment-722676096
https://github.com/hail-is/hail/pull/9675#issuecomment-730488453:16,Availability,failure,failures,16,"Seeing sporadic failures, random each run of ci, looking into it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9675#issuecomment-730488453
https://github.com/hail-is/hail/pull/9675#issuecomment-738330547:349,Integrability,depend,depends,349,"Tests are passing now. I am going to mark WIP because I want to run benchmarks on Monday before this merges. You can look whenever though. . Some things to look at:. 1. Does `ServiceTaskContext` have a notion of stages or retries, or should those also always be 0? ; 2. Some of the logic with combOps getting region pools is a little different, now depends on where combOp is running . For the most part though, PR is just refactoring to pass `RegionPool` everywhere",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9675#issuecomment-738330547
https://github.com/hail-is/hail/pull/9675#issuecomment-738330547:423,Modifiability,refactor,refactoring,423,"Tests are passing now. I am going to mark WIP because I want to run benchmarks on Monday before this merges. You can look whenever though. . Some things to look at:. 1. Does `ServiceTaskContext` have a notion of stages or retries, or should those also always be 0? ; 2. Some of the logic with combOps getting region pools is a little different, now depends on where combOp is running . For the most part though, PR is just refactoring to pass `RegionPool` everywhere",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9675#issuecomment-738330547
https://github.com/hail-is/hail/pull/9675#issuecomment-738330547:0,Testability,Test,Tests,0,"Tests are passing now. I am going to mark WIP because I want to run benchmarks on Monday before this merges. You can look whenever though. . Some things to look at:. 1. Does `ServiceTaskContext` have a notion of stages or retries, or should those also always be 0? ; 2. Some of the logic with combOps getting region pools is a little different, now depends on where combOp is running . For the most part though, PR is just refactoring to pass `RegionPool` everywhere",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9675#issuecomment-738330547
https://github.com/hail-is/hail/pull/9675#issuecomment-738330547:68,Testability,benchmark,benchmarks,68,"Tests are passing now. I am going to mark WIP because I want to run benchmarks on Monday before this merges. You can look whenever though. . Some things to look at:. 1. Does `ServiceTaskContext` have a notion of stages or retries, or should those also always be 0? ; 2. Some of the logic with combOps getting region pools is a little different, now depends on where combOp is running . For the most part though, PR is just refactoring to pass `RegionPool` everywhere",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9675#issuecomment-738330547
https://github.com/hail-is/hail/pull/9675#issuecomment-738330547:282,Testability,log,logic,282,"Tests are passing now. I am going to mark WIP because I want to run benchmarks on Monday before this merges. You can look whenever though. . Some things to look at:. 1. Does `ServiceTaskContext` have a notion of stages or retries, or should those also always be 0? ; 2. Some of the logic with combOps getting region pools is a little different, now depends on where combOp is running . For the most part though, PR is just refactoring to pass `RegionPool` everywhere",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9675#issuecomment-738330547
https://github.com/hail-is/hail/pull/9675#issuecomment-741826512:13,Testability,benchmark,benchmarks,13,Here are the benchmarks: https://gist.github.com/johnc1231/3e576bf2e8a39cb73785af0faa451976. @tpoterba should be good for review.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9675#issuecomment-741826512
https://github.com/hail-is/hail/pull/9681#issuecomment-722651702:44,Integrability,rout,router,44,Does this need to have the service with the router like all the other services?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9681#issuecomment-722651702
https://github.com/hail-is/hail/pull/9681#issuecomment-722656282:54,Integrability,rout,router,54,"No, it's not an HTTP service, it's plain old TCP, so `router` (which is an HTTP router) won't work here. The TCP gateway / router will come next.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9681#issuecomment-722656282
https://github.com/hail-is/hail/pull/9681#issuecomment-722656282:80,Integrability,rout,router,80,"No, it's not an HTTP service, it's plain old TCP, so `router` (which is an HTTP router) won't work here. The TCP gateway / router will come next.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9681#issuecomment-722656282
https://github.com/hail-is/hail/pull/9681#issuecomment-722656282:123,Integrability,rout,router,123,"No, it's not an HTTP service, it's plain old TCP, so `router` (which is an HTTP router) won't work here. The TCP gateway / router will come next.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9681#issuecomment-722656282
https://github.com/hail-is/hail/pull/9683#issuecomment-722702611:90,Testability,test,tests,90,These were uncovered while using the TCP Proxy to enter the cluster. Forthcoming Shuffler tests will add tests that would catch this behavior.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9683#issuecomment-722702611
https://github.com/hail-is/hail/pull/9683#issuecomment-722702611:105,Testability,test,tests,105,These were uncovered while using the TCP Proxy to enter the cluster. Forthcoming Shuffler tests will add tests that would catch this behavior.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9683#issuecomment-722702611
https://github.com/hail-is/hail/pull/9684#issuecomment-723127410:24,Deployability,deploy,deploy,24,"I'm testing out the dev deploy now, but realized this is super slow because Dan's PR changing how the base image is done with a new hail_ubuntu image is in now. Just FYI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-723127410
https://github.com/hail-is/hail/pull/9684#issuecomment-723127410:4,Testability,test,testing,4,"I'm testing out the dev deploy now, but realized this is super slow because Dan's PR changing how the base image is done with a new hail_ubuntu image is in now. Just FYI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-723127410
https://github.com/hail-is/hail/pull/9684#issuecomment-723166642:642,Deployability,deploy,deploy,642,"The ATGU intranet service is home for some tools we're going to build help support ATGU operations. After talking to the administrators, we started a simple tool for the admins to curate resources for members of the group. We've also talked about things related to personnel and financial and grant management. Although this will likely be a slightly different resource, we're talking to the PMs about tools we can build to help pulling together large-scale datasets like gnomAD. The complexity of doing this is becoming a blocker for producing such datasets. We're Hail. Whatever we build is part of Hail. Things that we own and operate and deploy together live in our monorepo.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-723166642
https://github.com/hail-is/hail/pull/9684#issuecomment-723166642:150,Usability,simpl,simple,150,"The ATGU intranet service is home for some tools we're going to build help support ATGU operations. After talking to the administrators, we started a simple tool for the admins to curate resources for members of the group. We've also talked about things related to personnel and financial and grant management. Although this will likely be a slightly different resource, we're talking to the PMs about tools we can build to help pulling together large-scale datasets like gnomAD. The complexity of doing this is becoming a blocker for producing such datasets. We're Hail. Whatever we build is part of Hail. Things that we own and operate and deploy together live in our monorepo.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-723166642
https://github.com/hail-is/hail/pull/9684#issuecomment-723167140:69,Safety,avoid,avoid,69,@jigold I just rebased on the current main. Hopefully that will help avoid rebuilding the base image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-723167140
https://github.com/hail-is/hail/pull/9684#issuecomment-726287286:21,Deployability,deploy,deployment,21,"I don't know if your deployment does this, but this is the sequence that I see with the edit page. <img width=""1242"" alt=""Screen Shot 2020-11-12 at 2 21 18 PM"" src=""https://user-images.githubusercontent.com/1693348/98986116-78110900-24f2-11eb-91f1-d9e35b826b16.png"">; <img width=""1263"" alt=""Screen Shot 2020-11-12 at 2 21 29 PM"" src=""https://user-images.githubusercontent.com/1693348/98986126-7b0bf980-24f2-11eb-8ebd-97e1319068df.png"">; <img width=""1262"" alt=""Screen Shot 2020-11-12 at 2 21 37 PM"" src=""https://user-images.githubusercontent.com/1693348/98986135-7d6e5380-24f2-11eb-9f77-9161800e0a59.png"">; <img width=""1256"" alt=""Screen Shot 2020-11-12 at 2 21 44 PM"" src=""https://user-images.githubusercontent.com/1693348/98986141-7fd0ad80-24f2-11eb-8b38-23b9ea5dd32c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-726287286
https://github.com/hail-is/hail/pull/9684#issuecomment-726289411:139,Deployability,update,update,139,"It looks good except for the behavior I was seeing. I can try redeploying my version if you think that's what the issue is (the edit page ""update"" doesn't return a page with the description I just added even though it's there on the resources page).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-726289411
https://github.com/hail-is/hail/pull/9684#issuecomment-726291462:141,Deployability,update,update,141,"> It looks good except for the behavior I was seeing. I can try redeploying my version if you think that's what the issue is (the edit page ""update"" doesn't return a page with the description I just added even though it's there on the resources page). This was intentional, but I see it is confusing when you just edited description. I'll add the description to the resource page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-726291462
https://github.com/hail-is/hail/pull/9686#issuecomment-724956104:9,Deployability,update,update,9,Going to update the elasticsearch version and make a different PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9686#issuecomment-724956104
https://github.com/hail-is/hail/pull/9693#issuecomment-724919692:10,Testability,test,tests,10,"Are there tests for the existence of these files? (I can see how that would be counterproductive if tests fail because the files move, but on the other hand, it's information you may want to know - maybe an ""optional"" test?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9693#issuecomment-724919692
https://github.com/hail-is/hail/pull/9693#issuecomment-724919692:100,Testability,test,tests,100,"Are there tests for the existence of these files? (I can see how that would be counterproductive if tests fail because the files move, but on the other hand, it's information you may want to know - maybe an ""optional"" test?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9693#issuecomment-724919692
https://github.com/hail-is/hail/pull/9693#issuecomment-724919692:218,Testability,test,test,218,"Are there tests for the existence of these files? (I can see how that would be counterproductive if tests fail because the files move, but on the other hand, it's information you may want to know - maybe an ""optional"" test?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9693#issuecomment-724919692
https://github.com/hail-is/hail/pull/9693#issuecomment-724956223:12,Testability,test,tests,12,"> Are there tests for the existence of these files? (I can see how that would be counterproductive if tests fail because the files move, but on the other hand, it's information you may want to know - maybe an ""optional"" test?). Not currently, that may be something worth looking into.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9693#issuecomment-724956223
https://github.com/hail-is/hail/pull/9693#issuecomment-724956223:102,Testability,test,tests,102,"> Are there tests for the existence of these files? (I can see how that would be counterproductive if tests fail because the files move, but on the other hand, it's information you may want to know - maybe an ""optional"" test?). Not currently, that may be something worth looking into.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9693#issuecomment-724956223
https://github.com/hail-is/hail/pull/9693#issuecomment-724956223:220,Testability,test,test,220,"> Are there tests for the existence of these files? (I can see how that would be counterproductive if tests fail because the files move, but on the other hand, it's information you may want to know - maybe an ""optional"" test?). Not currently, that may be something worth looking into.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9693#issuecomment-724956223
https://github.com/hail-is/hail/issues/9704#issuecomment-1145282437:91,Security,access,access,91,This is by design. The square bracket syntax is for joining two distinct matrix tables. To access a specific row and column do this:; ```; mt = mt.filter_rows(mt.row_key = 1); mt = mt.filter_cols(mt.col_key = 1); mt = mt.entry.collect()[0]; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9704#issuecomment-1145282437
https://github.com/hail-is/hail/pull/9713#issuecomment-782270602:104,Testability,test,test,104,"OK, I think I addressed the comments. I will do a dry run through to make everything is working. I will test this again from scratch, but I think it's more important to get this in and focus on building automated tests for it rather than spend time hand testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9713#issuecomment-782270602
https://github.com/hail-is/hail/pull/9713#issuecomment-782270602:213,Testability,test,tests,213,"OK, I think I addressed the comments. I will do a dry run through to make everything is working. I will test this again from scratch, but I think it's more important to get this in and focus on building automated tests for it rather than spend time hand testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9713#issuecomment-782270602
https://github.com/hail-is/hail/pull/9713#issuecomment-782270602:254,Testability,test,testing,254,"OK, I think I addressed the comments. I will do a dry run through to make everything is working. I will test this again from scratch, but I think it's more important to get this in and focus on building automated tests for it rather than spend time hand testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9713#issuecomment-782270602
https://github.com/hail-is/hail/pull/9713#issuecomment-783498605:42,Deployability,deploy,deploy,42,"Thanks, @danking! Looks like I broke auth deploy, investigating.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9713#issuecomment-783498605
https://github.com/hail-is/hail/pull/9718#issuecomment-729993692:214,Availability,failure,failures,214,"I copied all the secrets from batch-pods into default that (1) didn't already exist (by name) in default, and (2) weren't k8s service account tokens (which are batch-pods specific). I also fixed the remaining test failures so this should be ready to go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9718#issuecomment-729993692
https://github.com/hail-is/hail/pull/9718#issuecomment-729993692:209,Testability,test,test,209,"I copied all the secrets from batch-pods into default that (1) didn't already exist (by name) in default, and (2) weren't k8s service account tokens (which are batch-pods specific). I also fixed the remaining test failures so this should be ready to go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9718#issuecomment-729993692
https://github.com/hail-is/hail/pull/9718#issuecomment-730020693:48,Availability,downtime,downtime,48,I hope this breaks so I can use up some of that downtime budget! Totally worth it to get rid of batch-pods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9718#issuecomment-730020693
https://github.com/hail-is/hail/pull/9727#issuecomment-730626936:46,Availability,robust,robust,46,"Actually, comparing singular vectors is not a robust test, even accounting for sign. Suppose `A` has two equal (or nearly equal) singular values. Then there is a 2-dimensional subspace of vectors, all of which are equally good singular vectors for that singular value. If the singular values are sufficiently separated, then comparing singular vectors should be safe, but I don't think it's necessary; the other checks should force that. I think we only need to check (all approximate comparisons),; * we got the right singular values, by comparing with numpy (unless we constructed a test matrix with known singular values); * the singular vectors are orthonormal (i.e. `Ut U = Id` and `Vt V = Id`); * the factorization `A = U Sigma Vt`. Then it follows that for each right singular vector `V_i`, `A v_i = sigma_i u_i` holds approximately, so `v_i` is a good singular vector, i.e. it really does capture `sigma_i` variance, and we checked that `sigma_i` is close to the true singular value.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9727#issuecomment-730626936
https://github.com/hail-is/hail/pull/9727#issuecomment-730626936:362,Safety,safe,safe,362,"Actually, comparing singular vectors is not a robust test, even accounting for sign. Suppose `A` has two equal (or nearly equal) singular values. Then there is a 2-dimensional subspace of vectors, all of which are equally good singular vectors for that singular value. If the singular values are sufficiently separated, then comparing singular vectors should be safe, but I don't think it's necessary; the other checks should force that. I think we only need to check (all approximate comparisons),; * we got the right singular values, by comparing with numpy (unless we constructed a test matrix with known singular values); * the singular vectors are orthonormal (i.e. `Ut U = Id` and `Vt V = Id`); * the factorization `A = U Sigma Vt`. Then it follows that for each right singular vector `V_i`, `A v_i = sigma_i u_i` holds approximately, so `v_i` is a good singular vector, i.e. it really does capture `sigma_i` variance, and we checked that `sigma_i` is close to the true singular value.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9727#issuecomment-730626936
https://github.com/hail-is/hail/pull/9727#issuecomment-730626936:53,Testability,test,test,53,"Actually, comparing singular vectors is not a robust test, even accounting for sign. Suppose `A` has two equal (or nearly equal) singular values. Then there is a 2-dimensional subspace of vectors, all of which are equally good singular vectors for that singular value. If the singular values are sufficiently separated, then comparing singular vectors should be safe, but I don't think it's necessary; the other checks should force that. I think we only need to check (all approximate comparisons),; * we got the right singular values, by comparing with numpy (unless we constructed a test matrix with known singular values); * the singular vectors are orthonormal (i.e. `Ut U = Id` and `Vt V = Id`); * the factorization `A = U Sigma Vt`. Then it follows that for each right singular vector `V_i`, `A v_i = sigma_i u_i` holds approximately, so `v_i` is a good singular vector, i.e. it really does capture `sigma_i` variance, and we checked that `sigma_i` is close to the true singular value.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9727#issuecomment-730626936
https://github.com/hail-is/hail/pull/9727#issuecomment-730626936:585,Testability,test,test,585,"Actually, comparing singular vectors is not a robust test, even accounting for sign. Suppose `A` has two equal (or nearly equal) singular values. Then there is a 2-dimensional subspace of vectors, all of which are equally good singular vectors for that singular value. If the singular values are sufficiently separated, then comparing singular vectors should be safe, but I don't think it's necessary; the other checks should force that. I think we only need to check (all approximate comparisons),; * we got the right singular values, by comparing with numpy (unless we constructed a test matrix with known singular values); * the singular vectors are orthonormal (i.e. `Ut U = Id` and `Vt V = Id`); * the factorization `A = U Sigma Vt`. Then it follows that for each right singular vector `V_i`, `A v_i = sigma_i u_i` holds approximately, so `v_i` is a good singular vector, i.e. it really does capture `sigma_i` variance, and we checked that `sigma_i` is close to the true singular value.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9727#issuecomment-730626936
https://github.com/hail-is/hail/pull/9727#issuecomment-733190588:71,Testability,test,test,71,"I've made the changes you suggested patrick, but now we have a failing test. I'm getting a situation where Vt is clearly orthonormal, and U@S@Vt multiplies back to the input matrix, but U@U.t is not particularly close to the identity matrix. It's not clear why this is. The test is:. ```; np_rank_2_wide_rectangle = np.arange(12).reshape((4, 3)); rank_2_wide_rectangle = hl.nd.array(np_rank_2_wide_rectangle). ......... assert_evals_to_same_svd(rank_2_wide_rectangle, np_rank_2_wide_rectangle, full_matrices=False); ```. (The 4th test in the `test_svd` method)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9727#issuecomment-733190588
https://github.com/hail-is/hail/pull/9727#issuecomment-733190588:274,Testability,test,test,274,"I've made the changes you suggested patrick, but now we have a failing test. I'm getting a situation where Vt is clearly orthonormal, and U@S@Vt multiplies back to the input matrix, but U@U.t is not particularly close to the identity matrix. It's not clear why this is. The test is:. ```; np_rank_2_wide_rectangle = np.arange(12).reshape((4, 3)); rank_2_wide_rectangle = hl.nd.array(np_rank_2_wide_rectangle). ......... assert_evals_to_same_svd(rank_2_wide_rectangle, np_rank_2_wide_rectangle, full_matrices=False); ```. (The 4th test in the `test_svd` method)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9727#issuecomment-733190588
https://github.com/hail-is/hail/pull/9727#issuecomment-733190588:530,Testability,test,test,530,"I've made the changes you suggested patrick, but now we have a failing test. I'm getting a situation where Vt is clearly orthonormal, and U@S@Vt multiplies back to the input matrix, but U@U.t is not particularly close to the identity matrix. It's not clear why this is. The test is:. ```; np_rank_2_wide_rectangle = np.arange(12).reshape((4, 3)); rank_2_wide_rectangle = hl.nd.array(np_rank_2_wide_rectangle). ......... assert_evals_to_same_svd(rank_2_wide_rectangle, np_rank_2_wide_rectangle, full_matrices=False); ```. (The 4th test in the `test_svd` method)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9727#issuecomment-733190588
https://github.com/hail-is/hail/pull/9727#issuecomment-733190588:113,Usability,clear,clearly,113,"I've made the changes you suggested patrick, but now we have a failing test. I'm getting a situation where Vt is clearly orthonormal, and U@S@Vt multiplies back to the input matrix, but U@U.t is not particularly close to the identity matrix. It's not clear why this is. The test is:. ```; np_rank_2_wide_rectangle = np.arange(12).reshape((4, 3)); rank_2_wide_rectangle = hl.nd.array(np_rank_2_wide_rectangle). ......... assert_evals_to_same_svd(rank_2_wide_rectangle, np_rank_2_wide_rectangle, full_matrices=False); ```. (The 4th test in the `test_svd` method)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9727#issuecomment-733190588
https://github.com/hail-is/hail/pull/9727#issuecomment-733190588:251,Usability,clear,clear,251,"I've made the changes you suggested patrick, but now we have a failing test. I'm getting a situation where Vt is clearly orthonormal, and U@S@Vt multiplies back to the input matrix, but U@U.t is not particularly close to the identity matrix. It's not clear why this is. The test is:. ```; np_rank_2_wide_rectangle = np.arange(12).reshape((4, 3)); rank_2_wide_rectangle = hl.nd.array(np_rank_2_wide_rectangle). ......... assert_evals_to_same_svd(rank_2_wide_rectangle, np_rank_2_wide_rectangle, full_matrices=False); ```. (The 4th test in the `test_svd` method)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9727#issuecomment-733190588
https://github.com/hail-is/hail/pull/9728#issuecomment-730648821:5,Deployability,deploy,deploying,5,hand deploying batch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9728#issuecomment-730648821
https://github.com/hail-is/hail/pull/9728#issuecomment-730651886:0,Deployability,Deploy,Deployed,0,"Deployed, branch protections reenabled.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9728#issuecomment-730651886
https://github.com/hail-is/hail/issues/9742#issuecomment-734127073:25,Deployability,install,install,25,Can be overcome by:; pip install wheel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9742#issuecomment-734127073
https://github.com/hail-is/hail/issues/9742#issuecomment-816041422:34,Deployability,install,install,34,I had the same issue - I did `pip install pypandoc` and it worked fine after that :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9742#issuecomment-816041422
https://github.com/hail-is/hail/pull/9749#issuecomment-735921468:278,Deployability,release,release,278,"> The one thing I'd say is just that when we make a user facing change, we try to make one of the commits look like:; >; > CHANGELOG: Added or_error method to SwitchBuilder; >; > This ends up being helpful when I have to go through and generate the change log for a new version release. Good to know, updated the commit message. It would be nice to document conventions like this in the [docs for software developers](https://hail.is/docs/0.2/getting_started_developing.html#contributing) and/or a [contributing file](https://docs.github.com/en/free-pro-team@latest/github/building-a-strong-community/setting-guidelines-for-repository-contributors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9749#issuecomment-735921468
https://github.com/hail-is/hail/pull/9749#issuecomment-735921468:301,Deployability,update,updated,301,"> The one thing I'd say is just that when we make a user facing change, we try to make one of the commits look like:; >; > CHANGELOG: Added or_error method to SwitchBuilder; >; > This ends up being helpful when I have to go through and generate the change log for a new version release. Good to know, updated the commit message. It would be nice to document conventions like this in the [docs for software developers](https://hail.is/docs/0.2/getting_started_developing.html#contributing) and/or a [contributing file](https://docs.github.com/en/free-pro-team@latest/github/building-a-strong-community/setting-guidelines-for-repository-contributors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9749#issuecomment-735921468
https://github.com/hail-is/hail/pull/9749#issuecomment-735921468:320,Integrability,message,message,320,"> The one thing I'd say is just that when we make a user facing change, we try to make one of the commits look like:; >; > CHANGELOG: Added or_error method to SwitchBuilder; >; > This ends up being helpful when I have to go through and generate the change log for a new version release. Good to know, updated the commit message. It would be nice to document conventions like this in the [docs for software developers](https://hail.is/docs/0.2/getting_started_developing.html#contributing) and/or a [contributing file](https://docs.github.com/en/free-pro-team@latest/github/building-a-strong-community/setting-guidelines-for-repository-contributors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9749#issuecomment-735921468
https://github.com/hail-is/hail/pull/9749#issuecomment-735921468:256,Testability,log,log,256,"> The one thing I'd say is just that when we make a user facing change, we try to make one of the commits look like:; >; > CHANGELOG: Added or_error method to SwitchBuilder; >; > This ends up being helpful when I have to go through and generate the change log for a new version release. Good to know, updated the commit message. It would be nice to document conventions like this in the [docs for software developers](https://hail.is/docs/0.2/getting_started_developing.html#contributing) and/or a [contributing file](https://docs.github.com/en/free-pro-team@latest/github/building-a-strong-community/setting-guidelines-for-repository-contributors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9749#issuecomment-735921468
https://github.com/hail-is/hail/pull/9749#issuecomment-735921468:609,Usability,guid,guidelines-for-repository-contributors,609,"> The one thing I'd say is just that when we make a user facing change, we try to make one of the commits look like:; >; > CHANGELOG: Added or_error method to SwitchBuilder; >; > This ends up being helpful when I have to go through and generate the change log for a new version release. Good to know, updated the commit message. It would be nice to document conventions like this in the [docs for software developers](https://hail.is/docs/0.2/getting_started_developing.html#contributing) and/or a [contributing file](https://docs.github.com/en/free-pro-team@latest/github/building-a-strong-community/setting-guidelines-for-repository-contributors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9749#issuecomment-735921468
https://github.com/hail-is/hail/pull/9749#issuecomment-736676863:43,Usability,guid,guidelines,43,"Added the ""CHANGELOG"" note to contribution guidelines in #9752.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9749#issuecomment-736676863
https://github.com/hail-is/hail/pull/9753#issuecomment-740224418:65,Integrability,interface,interface,65,@lgruen Can this change wait until January when we make breaking interface changes?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9753#issuecomment-740224418
https://github.com/hail-is/hail/pull/9753#issuecomment-740252438:67,Integrability,interface,interface,67,"> @lgruen Can this change wait until January when we make breaking interface changes?. Yes, no problem, especially if you want to get rid of `default_image` altogether. I'll close this PR for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9753#issuecomment-740252438
https://github.com/hail-is/hail/pull/9758#issuecomment-736117147:220,Testability,log,logs,220,"example of a finished, failing job:; <img width=""1039"" alt=""Screen Shot 2020-11-30 at 6 28 10 PM"" src=""https://user-images.githubusercontent.com/106194/100677810-ce2adc80-3339-11eb-80c5-1e5c46673d8a.png"">; and after the logs you see the spec:. <img width=""1039"" alt=""Screen Shot 2020-11-30 at 6 28 23 PM"" src=""https://user-images.githubusercontent.com/106194/100677831-db47cb80-3339-11eb-98fd-b023691ce0ad.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9758#issuecomment-736117147
https://github.com/hail-is/hail/pull/9759#issuecomment-737664089:139,Availability,down,down,139,"OK, I reimplemented the sync-er in Python. This works well enough though it would benefit from something that waited for changes to settle down and did one copy-restart. Currently, you can queue up a bunch of changes and it sometimes take as long as 5 seconds for the whole system to settle down enough that you can refresh and get the new page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9759#issuecomment-737664089
https://github.com/hail-is/hail/pull/9759#issuecomment-737664089:291,Availability,down,down,291,"OK, I reimplemented the sync-er in Python. This works well enough though it would benefit from something that waited for changes to settle down and did one copy-restart. Currently, you can queue up a bunch of changes and it sometimes take as long as 5 seconds for the whole system to settle down enough that you can refresh and get the new page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9759#issuecomment-737664089
https://github.com/hail-is/hail/pull/9759#issuecomment-737664089:189,Performance,queue,queue,189,"OK, I reimplemented the sync-er in Python. This works well enough though it would benefit from something that waited for changes to settle down and did one copy-restart. Currently, you can queue up a bunch of changes and it sometimes take as long as 5 seconds for the whole system to settle down enough that you can refresh and get the new page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9759#issuecomment-737664089
https://github.com/hail-is/hail/pull/9760#issuecomment-740258327:105,Deployability,deploy,deployment,105,> I think this change is no longer needed. Are you sure? https://github.com/hail-is/hail/blob/main/query/deployment.yaml#L36 still references `default_ns.name`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9760#issuecomment-740258327
https://github.com/hail-is/hail/pull/9769#issuecomment-738084194:21,Deployability,configurat,configuration,21,I propose saving the configuration question to a different PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9769#issuecomment-738084194
https://github.com/hail-is/hail/pull/9769#issuecomment-738084194:21,Modifiability,config,configuration,21,I propose saving the configuration question to a different PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9769#issuecomment-738084194
https://github.com/hail-is/hail/pull/9770#issuecomment-742027249:256,Availability,down,down,256,"I know I said I thought this was a reasonable approach a while ago, but Iâ€™ve been thinking hard about this change since last week, and I think I want us to explore a larger set of designs before committing to this strategy. The approach in this PR doubles down on the functional Code[T] structure, which is something weâ€™re trying to move away from with CodeBuilder. I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. ```scala; class Emit[C](; val ctx: ExecuteContext,; val cb: EmitClassBuilder[C]) { emitSelf =>. val methods: mutable.Map[(String, Seq[Type], Seq[PType], PType), EmitMethodBuilder[C]] = mutable.Map(). private[ir] def emitVoid(cb: EmitCodeBuilder, ir: IR, mb: EmitMethodBuilder[C], region: StagedRegion, env: E, container: Option[AggContainer], loopEnv: Option[Env[LoopRef]]): Unit = {; cb.startLine(ir.lineNumber); ... implementaiton; cb.endLine(ir.lineNumber); ```. How could we make something like this work? Can we get away without every Code[T] knowing the source line? The JVM represents line numbers as an array of (line start bytecode index, line bytecode length) tuples, and I think it will be possible to produce this more easily. I think part of my concern is that Iâ€™m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers â€” right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration â€” theyâ€™ve been a source of confusion and frustration in the past, and the intentional paucity of implicits in our current codebase reflects that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742027249
https://github.com/hail-is/hail/pull/9770#issuecomment-742027249:395,Integrability,interface,interface,395,"I know I said I thought this was a reasonable approach a while ago, but Iâ€™ve been thinking hard about this change since last week, and I think I want us to explore a larger set of designs before committing to this strategy. The approach in this PR doubles down on the functional Code[T] structure, which is something weâ€™re trying to move away from with CodeBuilder. I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. ```scala; class Emit[C](; val ctx: ExecuteContext,; val cb: EmitClassBuilder[C]) { emitSelf =>. val methods: mutable.Map[(String, Seq[Type], Seq[PType], PType), EmitMethodBuilder[C]] = mutable.Map(). private[ir] def emitVoid(cb: EmitCodeBuilder, ir: IR, mb: EmitMethodBuilder[C], region: StagedRegion, env: E, container: Option[AggContainer], loopEnv: Option[Env[LoopRef]]): Unit = {; cb.startLine(ir.lineNumber); ... implementaiton; cb.endLine(ir.lineNumber); ```. How could we make something like this work? Can we get away without every Code[T] knowing the source line? The JVM represents line numbers as an array of (line start bytecode index, line bytecode length) tuples, and I think it will be possible to produce this more easily. I think part of my concern is that Iâ€™m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers â€” right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration â€” theyâ€™ve been a source of confusion and frustration in the past, and the intentional paucity of implicits in our current codebase reflects that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742027249
https://github.com/hail-is/hail/pull/9770#issuecomment-742027249:409,Integrability,inject,injecting,409,"I know I said I thought this was a reasonable approach a while ago, but Iâ€™ve been thinking hard about this change since last week, and I think I want us to explore a larger set of designs before committing to this strategy. The approach in this PR doubles down on the functional Code[T] structure, which is something weâ€™re trying to move away from with CodeBuilder. I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. ```scala; class Emit[C](; val ctx: ExecuteContext,; val cb: EmitClassBuilder[C]) { emitSelf =>. val methods: mutable.Map[(String, Seq[Type], Seq[PType], PType), EmitMethodBuilder[C]] = mutable.Map(). private[ir] def emitVoid(cb: EmitCodeBuilder, ir: IR, mb: EmitMethodBuilder[C], region: StagedRegion, env: E, container: Option[AggContainer], loopEnv: Option[Env[LoopRef]]): Unit = {; cb.startLine(ir.lineNumber); ... implementaiton; cb.endLine(ir.lineNumber); ```. How could we make something like this work? Can we get away without every Code[T] knowing the source line? The JVM represents line numbers as an array of (line start bytecode index, line bytecode length) tuples, and I think it will be possible to produce this more easily. I think part of my concern is that Iâ€™m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers â€” right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration â€” theyâ€™ve been a source of confusion and frustration in the past, and the intentional paucity of implicits in our current codebase reflects that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742027249
https://github.com/hail-is/hail/pull/9770#issuecomment-742027249:409,Security,inject,injecting,409,"I know I said I thought this was a reasonable approach a while ago, but Iâ€™ve been thinking hard about this change since last week, and I think I want us to explore a larger set of designs before committing to this strategy. The approach in this PR doubles down on the functional Code[T] structure, which is something weâ€™re trying to move away from with CodeBuilder. I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. ```scala; class Emit[C](; val ctx: ExecuteContext,; val cb: EmitClassBuilder[C]) { emitSelf =>. val methods: mutable.Map[(String, Seq[Type], Seq[PType], PType), EmitMethodBuilder[C]] = mutable.Map(). private[ir] def emitVoid(cb: EmitCodeBuilder, ir: IR, mb: EmitMethodBuilder[C], region: StagedRegion, env: E, container: Option[AggContainer], loopEnv: Option[Env[LoopRef]]): Unit = {; cb.startLine(ir.lineNumber); ... implementaiton; cb.endLine(ir.lineNumber); ```. How could we make something like this work? Can we get away without every Code[T] knowing the source line? The JVM represents line numbers as an array of (line start bytecode index, line bytecode length) tuples, and I think it will be possible to produce this more easily. I think part of my concern is that Iâ€™m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers â€” right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration â€” theyâ€™ve been a source of confusion and frustration in the past, and the intentional paucity of implicits in our current codebase reflects that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742027249
https://github.com/hail-is/hail/pull/9770#issuecomment-742042975:34,Availability,down,down,34,"> The approach in this PR doubles down on the functional Code[T] structure. I don't see anything in the design that prevents us from moving away from `Code[T]`, but it does have to support it for now. > I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. This is also the interface I would like to see for CodeBuilder. And you're right, making that change would allow methods taking a `CodeBuilder` to not need a line number argument. I agree that's better. I may have gotten a bit of tunnel vision in the middle of the giant mechanical refactoring :) I will make this change. > I think part of my concern is that Iâ€™m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers â€” right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. I think most of this PR is necessary for debug information with the fully lowered IR line numbers. It doesn't do anything to propagate line numbers through IR lowerings, which is what would be needed to support line numbers at earlier compiler stages. > Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration. My main reason for that approach was to manage the number of changes required in this PR. We could follow up on this with making line number arguments explicit in manageable chunks. But personally this seems like the ideal use case for implicit arguments. And as `Code` goes away, the number of places with implicit line number arguments should go down significantly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975
https://github.com/hail-is/hail/pull/9770#issuecomment-742042975:1629,Availability,down,down,1629,"> The approach in this PR doubles down on the functional Code[T] structure. I don't see anything in the design that prevents us from moving away from `Code[T]`, but it does have to support it for now. > I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. This is also the interface I would like to see for CodeBuilder. And you're right, making that change would allow methods taking a `CodeBuilder` to not need a line number argument. I agree that's better. I may have gotten a bit of tunnel vision in the middle of the giant mechanical refactoring :) I will make this change. > I think part of my concern is that Iâ€™m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers â€” right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. I think most of this PR is necessary for debug information with the fully lowered IR line numbers. It doesn't do anything to propagate line numbers through IR lowerings, which is what would be needed to support line numbers at earlier compiler stages. > Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration. My main reason for that approach was to manage the number of changes required in this PR. We could follow up on this with making line number arguments explicit in manageable chunks. But personally this seems like the ideal use case for implicit arguments. And as `Code` goes away, the number of places with implicit line number arguments should go down significantly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975
https://github.com/hail-is/hail/pull/9770#issuecomment-742042975:232,Integrability,interface,interface,232,"> The approach in this PR doubles down on the functional Code[T] structure. I don't see anything in the design that prevents us from moving away from `Code[T]`, but it does have to support it for now. > I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. This is also the interface I would like to see for CodeBuilder. And you're right, making that change would allow methods taking a `CodeBuilder` to not need a line number argument. I agree that's better. I may have gotten a bit of tunnel vision in the middle of the giant mechanical refactoring :) I will make this change. > I think part of my concern is that Iâ€™m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers â€” right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. I think most of this PR is necessary for debug information with the fully lowered IR line numbers. It doesn't do anything to propagate line numbers through IR lowerings, which is what would be needed to support line numbers at earlier compiler stages. > Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration. My main reason for that approach was to manage the number of changes required in this PR. We could follow up on this with making line number arguments explicit in manageable chunks. But personally this seems like the ideal use case for implicit arguments. And as `Code` goes away, the number of places with implicit line number arguments should go down significantly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975
https://github.com/hail-is/hail/pull/9770#issuecomment-742042975:246,Integrability,inject,injecting,246,"> The approach in this PR doubles down on the functional Code[T] structure. I don't see anything in the design that prevents us from moving away from `Code[T]`, but it does have to support it for now. > I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. This is also the interface I would like to see for CodeBuilder. And you're right, making that change would allow methods taking a `CodeBuilder` to not need a line number argument. I agree that's better. I may have gotten a bit of tunnel vision in the middle of the giant mechanical refactoring :) I will make this change. > I think part of my concern is that Iâ€™m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers â€” right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. I think most of this PR is necessary for debug information with the fully lowered IR line numbers. It doesn't do anything to propagate line numbers through IR lowerings, which is what would be needed to support line numbers at earlier compiler stages. > Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration. My main reason for that approach was to manage the number of changes required in this PR. We could follow up on this with making line number arguments explicit in manageable chunks. But personally this seems like the ideal use case for implicit arguments. And as `Code` goes away, the number of places with implicit line number arguments should go down significantly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975
https://github.com/hail-is/hail/pull/9770#issuecomment-742042975:333,Integrability,interface,interface,333,"> The approach in this PR doubles down on the functional Code[T] structure. I don't see anything in the design that prevents us from moving away from `Code[T]`, but it does have to support it for now. > I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. This is also the interface I would like to see for CodeBuilder. And you're right, making that change would allow methods taking a `CodeBuilder` to not need a line number argument. I agree that's better. I may have gotten a bit of tunnel vision in the middle of the giant mechanical refactoring :) I will make this change. > I think part of my concern is that Iâ€™m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers â€” right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. I think most of this PR is necessary for debug information with the fully lowered IR line numbers. It doesn't do anything to propagate line numbers through IR lowerings, which is what would be needed to support line numbers at earlier compiler stages. > Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration. My main reason for that approach was to manage the number of changes required in this PR. We could follow up on this with making line number arguments explicit in manageable chunks. But personally this seems like the ideal use case for implicit arguments. And as `Code` goes away, the number of places with implicit line number arguments should go down significantly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975
https://github.com/hail-is/hail/pull/9770#issuecomment-742042975:598,Modifiability,refactor,refactoring,598,"> The approach in this PR doubles down on the functional Code[T] structure. I don't see anything in the design that prevents us from moving away from `Code[T]`, but it does have to support it for now. > I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. This is also the interface I would like to see for CodeBuilder. And you're right, making that change would allow methods taking a `CodeBuilder` to not need a line number argument. I agree that's better. I may have gotten a bit of tunnel vision in the middle of the giant mechanical refactoring :) I will make this change. > I think part of my concern is that Iâ€™m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers â€” right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. I think most of this PR is necessary for debug information with the fully lowered IR line numbers. It doesn't do anything to propagate line numbers through IR lowerings, which is what would be needed to support line numbers at earlier compiler stages. > Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration. My main reason for that approach was to manage the number of changes required in this PR. We could follow up on this with making line number arguments explicit in manageable chunks. But personally this seems like the ideal use case for implicit arguments. And as `Code` goes away, the number of places with implicit line number arguments should go down significantly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975
https://github.com/hail-is/hail/pull/9770#issuecomment-742042975:246,Security,inject,injecting,246,"> The approach in this PR doubles down on the functional Code[T] structure. I don't see anything in the design that prevents us from moving away from `Code[T]`, but it does have to support it for now. > I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. This is also the interface I would like to see for CodeBuilder. And you're right, making that change would allow methods taking a `CodeBuilder` to not need a line number argument. I agree that's better. I may have gotten a bit of tunnel vision in the middle of the giant mechanical refactoring :) I will make this change. > I think part of my concern is that Iâ€™m not entirely sold by the need to have a whole stack of IR printouts and associated line numbers â€” right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. I think most of this PR is necessary for debug information with the fully lowered IR line numbers. It doesn't do anything to propagate line numbers through IR lowerings, which is what would be needed to support line numbers at earlier compiler stages. > Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration. My main reason for that approach was to manage the number of changes required in this PR. We could follow up on this with making line number arguments explicit in manageable chunks. But personally this seems like the ideal use case for implicit arguments. And as `Code` goes away, the number of places with implicit line number arguments should go down significantly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975
https://github.com/hail-is/hail/pull/9772#issuecomment-738471689:185,Energy Efficiency,monitor,monitor,185,"I agree with the confusion. However, the reason for this structure is because the JobPrivate Instances can't be in a ""pool"". So we decided when we discussed this two weeks ago that the monitor needed to be centralized and do the tasks we defined: monitor instances and handle events.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738471689
https://github.com/hail-is/hail/pull/9772#issuecomment-738471689:247,Energy Efficiency,monitor,monitor,247,"I agree with the confusion. However, the reason for this structure is because the JobPrivate Instances can't be in a ""pool"". So we decided when we discussed this two weeks ago that the monitor needed to be centralized and do the tasks we defined: monitor instances and handle events.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738471689
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:904,Availability,down,down,904,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:923,Availability,down,down,923,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:1357,Availability,down,downward,1357,"nformation. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very worried about that. I think it's fine and it helps simplify the architecture. It avoids entangling the monitor with the pools and the JPIM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:189,Energy Efficiency,monitor,monitor,189,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:197,Energy Efficiency,monitor,monitors,197,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:225,Energy Efficiency,monitor,monitors,225,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:311,Energy Efficiency,monitor,monitor,311,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:537,Energy Efficiency,monitor,monitor,537,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:793,Energy Efficiency,monitor,monitor,793,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:1169,Energy Efficiency,monitor,monitor,1169,"nformation. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very worried about that. I think it's fine and it helps simplify the architecture. It avoids entangling the monitor with the pools and the JPIM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:1969,Energy Efficiency,monitor,monitoring,1969,"nformation. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very worried about that. I think it's fine and it helps simplify the architecture. It avoids entangling the monitor with the pools and the JPIM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:2103,Energy Efficiency,monitor,monitor,2103,"nformation. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very worried about that. I think it's fine and it helps simplify the architecture. It avoids entangling the monitor with the pools and the JPIM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:556,Integrability,rout,route,556,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:733,Modifiability,layers,layers,733,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:1002,Modifiability,layers,layers,1002,"wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very worried ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:2081,Safety,avoid,avoids,2081,"nformation. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very worried about that. I think it's fine and it helps simplify the architecture. It avoids entangling the monitor with the pools and the JPIM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:20,Usability,clear,clear,20,"The doc wasn't very clear about where information lived. I had imagined that the pools and the JobPrivateInstanceManager (JPIM) owned the information. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very w",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:2051,Usability,simpl,simplify,2051,"nformation. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very worried about that. I think it's fine and it helps simplify the architecture. It avoids entangling the monitor with the pools and the JPIM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358
https://github.com/hail-is/hail/pull/9774#issuecomment-738118480:67,Energy Efficiency,schedul,scheduling,67,"Also, I didn't want N bump loops all bumping the global events for scheduling and cancel events. I also have all schedulers share the same async worker pool.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9774#issuecomment-738118480
https://github.com/hail-is/hail/pull/9774#issuecomment-738118480:113,Energy Efficiency,schedul,schedulers,113,"Also, I didn't want N bump loops all bumping the global events for scheduling and cancel events. I also have all schedulers share the same async worker pool.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9774#issuecomment-738118480
https://github.com/hail-is/hail/pull/9775#issuecomment-738140989:529,Availability,avail,available,529,"Yes. from scala.concurrent.ExecutionContext:. ```; object ExecutionContext {; /**; * The explicit global `ExecutionContext`. Invoke `global` when you want to provide the global; * `ExecutionContext` explicitly.; *; * The default `ExecutionContext` implementation is backed by a work-stealing thread pool.; * It can be configured via the following [[scala.sys.SystemProperties]]:; *; * `scala.concurrent.context.minThreads` = defaults to ""1""; * `scala.concurrent.context.numThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxExtraThreads` = defaults to ""256""; *; * The pool size of threads is then `numThreads` bounded by `minThreads` on the lower end and `maxThreads` on the high end.; *; * The `maxExtraThreads` is the maximum number of extra threads to have at any given time to evade deadlock,; * see [[scala.concurrent.BlockContext]].; *; * @return the global `ExecutionContext`; */; def global: ExecutionContextExecutor = Implicits.global.asInstanceOf[ExecutionContextExecutor]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989
https://github.com/hail-is/hail/pull/9775#issuecomment-738140989:643,Availability,avail,available,643,"Yes. from scala.concurrent.ExecutionContext:. ```; object ExecutionContext {; /**; * The explicit global `ExecutionContext`. Invoke `global` when you want to provide the global; * `ExecutionContext` explicitly.; *; * The default `ExecutionContext` implementation is backed by a work-stealing thread pool.; * It can be configured via the following [[scala.sys.SystemProperties]]:; *; * `scala.concurrent.context.minThreads` = defaults to ""1""; * `scala.concurrent.context.numThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxExtraThreads` = defaults to ""256""; *; * The pool size of threads is then `numThreads` bounded by `minThreads` on the lower end and `maxThreads` on the high end.; *; * The `maxExtraThreads` is the maximum number of extra threads to have at any given time to evade deadlock,; * see [[scala.concurrent.BlockContext]].; *; * @return the global `ExecutionContext`; */; def global: ExecutionContextExecutor = Implicits.global.asInstanceOf[ExecutionContextExecutor]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989
https://github.com/hail-is/hail/pull/9775#issuecomment-738140989:318,Modifiability,config,configured,318,"Yes. from scala.concurrent.ExecutionContext:. ```; object ExecutionContext {; /**; * The explicit global `ExecutionContext`. Invoke `global` when you want to provide the global; * `ExecutionContext` explicitly.; *; * The default `ExecutionContext` implementation is backed by a work-stealing thread pool.; * It can be configured via the following [[scala.sys.SystemProperties]]:; *; * `scala.concurrent.context.minThreads` = defaults to ""1""; * `scala.concurrent.context.numThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxExtraThreads` = defaults to ""256""; *; * The pool size of threads is then `numThreads` bounded by `minThreads` on the lower end and `maxThreads` on the high end.; *; * The `maxExtraThreads` is the maximum number of extra threads to have at any given time to evade deadlock,; * see [[scala.concurrent.BlockContext]].; *; * @return the global `ExecutionContext`; */; def global: ExecutionContextExecutor = Implicits.global.asInstanceOf[ExecutionContextExecutor]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989
https://github.com/hail-is/hail/pull/9775#issuecomment-738140989:16,Performance,concurren,concurrent,16,"Yes. from scala.concurrent.ExecutionContext:. ```; object ExecutionContext {; /**; * The explicit global `ExecutionContext`. Invoke `global` when you want to provide the global; * `ExecutionContext` explicitly.; *; * The default `ExecutionContext` implementation is backed by a work-stealing thread pool.; * It can be configured via the following [[scala.sys.SystemProperties]]:; *; * `scala.concurrent.context.minThreads` = defaults to ""1""; * `scala.concurrent.context.numThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxExtraThreads` = defaults to ""256""; *; * The pool size of threads is then `numThreads` bounded by `minThreads` on the lower end and `maxThreads` on the high end.; *; * The `maxExtraThreads` is the maximum number of extra threads to have at any given time to evade deadlock,; * see [[scala.concurrent.BlockContext]].; *; * @return the global `ExecutionContext`; */; def global: ExecutionContextExecutor = Implicits.global.asInstanceOf[ExecutionContextExecutor]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989
https://github.com/hail-is/hail/pull/9775#issuecomment-738140989:392,Performance,concurren,concurrent,392,"Yes. from scala.concurrent.ExecutionContext:. ```; object ExecutionContext {; /**; * The explicit global `ExecutionContext`. Invoke `global` when you want to provide the global; * `ExecutionContext` explicitly.; *; * The default `ExecutionContext` implementation is backed by a work-stealing thread pool.; * It can be configured via the following [[scala.sys.SystemProperties]]:; *; * `scala.concurrent.context.minThreads` = defaults to ""1""; * `scala.concurrent.context.numThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxExtraThreads` = defaults to ""256""; *; * The pool size of threads is then `numThreads` bounded by `minThreads` on the lower end and `maxThreads` on the high end.; *; * The `maxExtraThreads` is the maximum number of extra threads to have at any given time to evade deadlock,; * see [[scala.concurrent.BlockContext]].; *; * @return the global `ExecutionContext`; */; def global: ExecutionContextExecutor = Implicits.global.asInstanceOf[ExecutionContextExecutor]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989
https://github.com/hail-is/hail/pull/9775#issuecomment-738140989:451,Performance,concurren,concurrent,451,"Yes. from scala.concurrent.ExecutionContext:. ```; object ExecutionContext {; /**; * The explicit global `ExecutionContext`. Invoke `global` when you want to provide the global; * `ExecutionContext` explicitly.; *; * The default `ExecutionContext` implementation is backed by a work-stealing thread pool.; * It can be configured via the following [[scala.sys.SystemProperties]]:; *; * `scala.concurrent.context.minThreads` = defaults to ""1""; * `scala.concurrent.context.numThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxExtraThreads` = defaults to ""256""; *; * The pool size of threads is then `numThreads` bounded by `minThreads` on the lower end and `maxThreads` on the high end.; *; * The `maxExtraThreads` is the maximum number of extra threads to have at any given time to evade deadlock,; * see [[scala.concurrent.BlockContext]].; *; * @return the global `ExecutionContext`; */; def global: ExecutionContextExecutor = Implicits.global.asInstanceOf[ExecutionContextExecutor]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989
https://github.com/hail-is/hail/pull/9775#issuecomment-738140989:565,Performance,concurren,concurrent,565,"Yes. from scala.concurrent.ExecutionContext:. ```; object ExecutionContext {; /**; * The explicit global `ExecutionContext`. Invoke `global` when you want to provide the global; * `ExecutionContext` explicitly.; *; * The default `ExecutionContext` implementation is backed by a work-stealing thread pool.; * It can be configured via the following [[scala.sys.SystemProperties]]:; *; * `scala.concurrent.context.minThreads` = defaults to ""1""; * `scala.concurrent.context.numThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxExtraThreads` = defaults to ""256""; *; * The pool size of threads is then `numThreads` bounded by `minThreads` on the lower end and `maxThreads` on the high end.; *; * The `maxExtraThreads` is the maximum number of extra threads to have at any given time to evade deadlock,; * see [[scala.concurrent.BlockContext]].; *; * @return the global `ExecutionContext`; */; def global: ExecutionContextExecutor = Implicits.global.asInstanceOf[ExecutionContextExecutor]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989
https://github.com/hail-is/hail/pull/9775#issuecomment-738140989:679,Performance,concurren,concurrent,679,"Yes. from scala.concurrent.ExecutionContext:. ```; object ExecutionContext {; /**; * The explicit global `ExecutionContext`. Invoke `global` when you want to provide the global; * `ExecutionContext` explicitly.; *; * The default `ExecutionContext` implementation is backed by a work-stealing thread pool.; * It can be configured via the following [[scala.sys.SystemProperties]]:; *; * `scala.concurrent.context.minThreads` = defaults to ""1""; * `scala.concurrent.context.numThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxExtraThreads` = defaults to ""256""; *; * The pool size of threads is then `numThreads` bounded by `minThreads` on the lower end and `maxThreads` on the high end.; *; * The `maxExtraThreads` is the maximum number of extra threads to have at any given time to evade deadlock,; * see [[scala.concurrent.BlockContext]].; *; * @return the global `ExecutionContext`; */; def global: ExecutionContextExecutor = Implicits.global.asInstanceOf[ExecutionContextExecutor]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989
https://github.com/hail-is/hail/pull/9775#issuecomment-738140989:989,Performance,concurren,concurrent,989,"Yes. from scala.concurrent.ExecutionContext:. ```; object ExecutionContext {; /**; * The explicit global `ExecutionContext`. Invoke `global` when you want to provide the global; * `ExecutionContext` explicitly.; *; * The default `ExecutionContext` implementation is backed by a work-stealing thread pool.; * It can be configured via the following [[scala.sys.SystemProperties]]:; *; * `scala.concurrent.context.minThreads` = defaults to ""1""; * `scala.concurrent.context.numThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxThreads` = defaults to ""x1"" (i.e. the current number of available processors * 1); * `scala.concurrent.context.maxExtraThreads` = defaults to ""256""; *; * The pool size of threads is then `numThreads` bounded by `minThreads` on the lower end and `maxThreads` on the high end.; *; * The `maxExtraThreads` is the maximum number of extra threads to have at any given time to evade deadlock,; * see [[scala.concurrent.BlockContext]].; *; * @return the global `ExecutionContext`; */; def global: ExecutionContextExecutor = Implicits.global.asInstanceOf[ExecutionContextExecutor]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9775#issuecomment-738140989
https://github.com/hail-is/hail/pull/9777#issuecomment-738459457:80,Modifiability,config,config,80,"Ah, yeah, I can't use it as a feature yet: the running CI isn't creating global-config. Fixing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9777#issuecomment-738459457
https://github.com/hail-is/hail/pull/9777#issuecomment-738494376:88,Deployability,update,updated,88,"FYI, https://github.com/hail-is/hail/pull/9786 adds another field, batch_gcp_regions. I updated our cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9777#issuecomment-738494376
https://github.com/hail-is/hail/pull/9778#issuecomment-738244457:78,Safety,safe,safe,78,"And I'll remove the bucket after this goes in (it should be unused, but to be safe)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9778#issuecomment-738244457
https://github.com/hail-is/hail/pull/9797#issuecomment-738988331:0,Deployability,deploy,deploying,0,deploying now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9797#issuecomment-738988331
https://github.com/hail-is/hail/pull/9797#issuecomment-738988432:29,Availability,error,error,29,"Ah, that explains the pylint error. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9797#issuecomment-738988432
https://github.com/hail-is/hail/pull/9804#issuecomment-741921520:11,Deployability,upgrade,upgraders,11,"For future upgraders, there's something about pandas 1.1.5 that causes `pylint` to fail. It seems like underlying ast processing library `astroid` enters infinite recursive loop when pandas 1.1.5 is installed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9804#issuecomment-741921520
https://github.com/hail-is/hail/pull/9804#issuecomment-741921520:199,Deployability,install,installed,199,"For future upgraders, there's something about pandas 1.1.5 that causes `pylint` to fail. It seems like underlying ast processing library `astroid` enters infinite recursive loop when pandas 1.1.5 is installed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9804#issuecomment-741921520
https://github.com/hail-is/hail/pull/9804#issuecomment-742033412:47,Deployability,release,released,47,"I haven't yet Chris, but `astroid` also hasn't released in ~4 months, so I still have to build and check if it's fixed yet. Planning on doing so when I have some time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9804#issuecomment-742033412
https://github.com/hail-is/hail/pull/9809#issuecomment-742845557:91,Testability,test,test,91,"A little more context, this is the number of batch resources per project, per user. Ci and test are an order of magnitude more than Ben W. who is two orders of magnitude above his nearest competitors.; ```; +------------------------+-----------+----------+; | billing_project | user | count(*) |; +------------------------+-----------+----------+; | benchmark | benchmark | 1595 |; | broad-mpg-gnomad | mwilson | 17 |; | ci | ci | 328597 |; | daly-neale-sczmeta | fgulamal | 292 |; | daly-neale-sczmeta | tsingh | 71 |; | hail | dking | 1610 |; | hail | johnc | 5932 |; | test | test | 126717 |; | test | test-dev | 2347 |; | tgg-rare-disease | weisburd | 10534 |; | ukb-pharma-browser-dev | msolomon | 161 |; +------------------------+-----------+----------+; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9809#issuecomment-742845557
https://github.com/hail-is/hail/pull/9809#issuecomment-742845557:350,Testability,benchmark,benchmark,350,"A little more context, this is the number of batch resources per project, per user. Ci and test are an order of magnitude more than Ben W. who is two orders of magnitude above his nearest competitors.; ```; +------------------------+-----------+----------+; | billing_project | user | count(*) |; +------------------------+-----------+----------+; | benchmark | benchmark | 1595 |; | broad-mpg-gnomad | mwilson | 17 |; | ci | ci | 328597 |; | daly-neale-sczmeta | fgulamal | 292 |; | daly-neale-sczmeta | tsingh | 71 |; | hail | dking | 1610 |; | hail | johnc | 5932 |; | test | test | 126717 |; | test | test-dev | 2347 |; | tgg-rare-disease | weisburd | 10534 |; | ukb-pharma-browser-dev | msolomon | 161 |; +------------------------+-----------+----------+; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9809#issuecomment-742845557
https://github.com/hail-is/hail/pull/9809#issuecomment-742845557:362,Testability,benchmark,benchmark,362,"A little more context, this is the number of batch resources per project, per user. Ci and test are an order of magnitude more than Ben W. who is two orders of magnitude above his nearest competitors.; ```; +------------------------+-----------+----------+; | billing_project | user | count(*) |; +------------------------+-----------+----------+; | benchmark | benchmark | 1595 |; | broad-mpg-gnomad | mwilson | 17 |; | ci | ci | 328597 |; | daly-neale-sczmeta | fgulamal | 292 |; | daly-neale-sczmeta | tsingh | 71 |; | hail | dking | 1610 |; | hail | johnc | 5932 |; | test | test | 126717 |; | test | test-dev | 2347 |; | tgg-rare-disease | weisburd | 10534 |; | ukb-pharma-browser-dev | msolomon | 161 |; +------------------------+-----------+----------+; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9809#issuecomment-742845557
https://github.com/hail-is/hail/pull/9809#issuecomment-742845557:572,Testability,test,test,572,"A little more context, this is the number of batch resources per project, per user. Ci and test are an order of magnitude more than Ben W. who is two orders of magnitude above his nearest competitors.; ```; +------------------------+-----------+----------+; | billing_project | user | count(*) |; +------------------------+-----------+----------+; | benchmark | benchmark | 1595 |; | broad-mpg-gnomad | mwilson | 17 |; | ci | ci | 328597 |; | daly-neale-sczmeta | fgulamal | 292 |; | daly-neale-sczmeta | tsingh | 71 |; | hail | dking | 1610 |; | hail | johnc | 5932 |; | test | test | 126717 |; | test | test-dev | 2347 |; | tgg-rare-disease | weisburd | 10534 |; | ukb-pharma-browser-dev | msolomon | 161 |; +------------------------+-----------+----------+; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9809#issuecomment-742845557
https://github.com/hail-is/hail/pull/9809#issuecomment-742845557:579,Testability,test,test,579,"A little more context, this is the number of batch resources per project, per user. Ci and test are an order of magnitude more than Ben W. who is two orders of magnitude above his nearest competitors.; ```; +------------------------+-----------+----------+; | billing_project | user | count(*) |; +------------------------+-----------+----------+; | benchmark | benchmark | 1595 |; | broad-mpg-gnomad | mwilson | 17 |; | ci | ci | 328597 |; | daly-neale-sczmeta | fgulamal | 292 |; | daly-neale-sczmeta | tsingh | 71 |; | hail | dking | 1610 |; | hail | johnc | 5932 |; | test | test | 126717 |; | test | test-dev | 2347 |; | tgg-rare-disease | weisburd | 10534 |; | ukb-pharma-browser-dev | msolomon | 161 |; +------------------------+-----------+----------+; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9809#issuecomment-742845557
https://github.com/hail-is/hail/pull/9809#issuecomment-742845557:598,Testability,test,test,598,"A little more context, this is the number of batch resources per project, per user. Ci and test are an order of magnitude more than Ben W. who is two orders of magnitude above his nearest competitors.; ```; +------------------------+-----------+----------+; | billing_project | user | count(*) |; +------------------------+-----------+----------+; | benchmark | benchmark | 1595 |; | broad-mpg-gnomad | mwilson | 17 |; | ci | ci | 328597 |; | daly-neale-sczmeta | fgulamal | 292 |; | daly-neale-sczmeta | tsingh | 71 |; | hail | dking | 1610 |; | hail | johnc | 5932 |; | test | test | 126717 |; | test | test-dev | 2347 |; | tgg-rare-disease | weisburd | 10534 |; | ukb-pharma-browser-dev | msolomon | 161 |; +------------------------+-----------+----------+; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9809#issuecomment-742845557
https://github.com/hail-is/hail/pull/9809#issuecomment-742845557:605,Testability,test,test-dev,605,"A little more context, this is the number of batch resources per project, per user. Ci and test are an order of magnitude more than Ben W. who is two orders of magnitude above his nearest competitors.; ```; +------------------------+-----------+----------+; | billing_project | user | count(*) |; +------------------------+-----------+----------+; | benchmark | benchmark | 1595 |; | broad-mpg-gnomad | mwilson | 17 |; | ci | ci | 328597 |; | daly-neale-sczmeta | fgulamal | 292 |; | daly-neale-sczmeta | tsingh | 71 |; | hail | dking | 1610 |; | hail | johnc | 5932 |; | test | test | 126717 |; | test | test-dev | 2347 |; | tgg-rare-disease | weisburd | 10534 |; | ukb-pharma-browser-dev | msolomon | 161 |; +------------------------+-----------+----------+; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9809#issuecomment-742845557
https://github.com/hail-is/hail/pull/9819#issuecomment-744092288:106,Availability,fault,fault,106,"You'll note that `check_hail_37` is failing because of an infinite recursion bug. I believe it's pylint's fault through its `astroid` depenency, though I'm not sure what the change in pandas 1.1.5 is that causes this. Mentioned here: https://github.com/hail-is/hail/pull/9804",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9819#issuecomment-744092288
https://github.com/hail-is/hail/pull/9819#issuecomment-744582907:74,Availability,error,error,74,"Remarkable. `hailtop` doesn't even import pandas and yet it triggers this error. I can run pylint on `hail` and it produces many errors, but does not crash.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9819#issuecomment-744582907
https://github.com/hail-is/hail/pull/9819#issuecomment-744582907:129,Availability,error,errors,129,"Remarkable. `hailtop` doesn't even import pandas and yet it triggers this error. I can run pylint on `hail` and it produces many errors, but does not crash.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9819#issuecomment-744582907
https://github.com/hail-is/hail/pull/9822#issuecomment-759790742:59,Testability,test,test,59,I'm still looking at this. I'm having a hard time with the test code. I'll try and spend more time tomorrow so I can either figure out what's going on myself or have specific questions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9822#issuecomment-759790742
https://github.com/hail-is/hail/pull/9822#issuecomment-764847097:196,Availability,error,error,196,"OK, I think I addressed all the comments. I'm going to make the change to not stat the destination if we set treat_dest_as=Transfer.TARGET_FILE. Then we just need to decide if we want to throw an error on file:// for a non-existent Transfer.TARGET_DIR. I say no.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9822#issuecomment-764847097
https://github.com/hail-is/hail/pull/9822#issuecomment-764875037:172,Testability,test,test,172,"OK, I changed _dest_type not to stat the destination if treat_dest_as=Transfer.FILE. I regenerated the copy specs (a few changed) and the test_copy_dest_target_file_is_dir test only fails on the local filesystem. Back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9822#issuecomment-764875037
https://github.com/hail-is/hail/pull/9822#issuecomment-764956189:53,Availability,error,error,53,> Then we just need to decide if we want to throw an error on file:// for a non-existent Transfer.TARGET_DIR. I say no. I agree. I think the user is being explicit what they intend to have happen.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9822#issuecomment-764956189
https://github.com/hail-is/hail/pull/9822#issuecomment-764957523:18,Availability,error,error,18,You have a pylint error with trailing whitespace.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9822#issuecomment-764957523
https://github.com/hail-is/hail/pull/9825#issuecomment-754781031:40,Testability,test,tests,40,I think this builds. Gotta make it pass tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9825#issuecomment-754781031
https://github.com/hail-is/hail/pull/9825#issuecomment-769978628:20,Testability,test,tests,20,"Don't think all the tests pass yet, but its single digit python tests failing now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9825#issuecomment-769978628
https://github.com/hail-is/hail/pull/9825#issuecomment-769978628:64,Testability,test,tests,64,"Don't think all the tests pass yet, but its single digit python tests failing now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9825#issuecomment-769978628
https://github.com/hail-is/hail/pull/9832#issuecomment-747040862:101,Energy Efficiency,schedul,scheduler,101,One thing I wasn't sure about was whether there should be a global async worker pool or whether each scheduler and the canceller can have their own.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9832#issuecomment-747040862
https://github.com/hail-is/hail/pull/9832#issuecomment-758019254:53,Integrability,message,message,53,Can you respond to my concerns in the initial commit message? Specifically some math questions when computing resources and the AsyncWorkerPool usage and the temp variable in the SQL code.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9832#issuecomment-758019254
https://github.com/hail-is/hail/pull/9832#issuecomment-758019254:163,Modifiability,variab,variable,163,Can you respond to my concerns in the initial commit message? Specifically some math questions when computing resources and the AsyncWorkerPool usage and the temp variable in the SQL code.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9832#issuecomment-758019254
https://github.com/hail-is/hail/pull/9832#issuecomment-758162781:104,Usability,simpl,simplest,104,"This is caused by the circularity in the import chain. Imports need to be a directed acyclic graph. The simplest fix seems to be to define `schedule_job` in `pool.py`, the only place it is used.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9832#issuecomment-758162781
https://github.com/hail-is/hail/issues/9833#issuecomment-747571669:0,Deployability,Patch,Patch,0,Patch was merged. Closing issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9833#issuecomment-747571669
https://github.com/hail-is/hail/pull/9836#issuecomment-747653966:24,Availability,fault,fault,24,"So this is partially my fault. I'm using parse_known_args, so we don't know where unknown args are placed, and therefore can't give the right usage (even if I can intervene on the usage being printed by argparse). I really only want to allow unknown args in the relevant subcommands, but don't know how to write that argument given the strange behavior I'm seeing with `nargs='*'` and `nargs=argparse.REMAINDER`. . Hmm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9836#issuecomment-747653966
https://github.com/hail-is/hail/issues/9837#issuecomment-827850982:46,Energy Efficiency,monitor,monitoring,46,"Hi, sorry we missed this -- clearly we're not monitoring issues well. We do support on the forum: https://discuss.hail.is. If this is still an open question, please make a post there!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9837#issuecomment-827850982
https://github.com/hail-is/hail/issues/9837#issuecomment-827850982:28,Usability,clear,clearly,28,"Hi, sorry we missed this -- clearly we're not monitoring issues well. We do support on the forum: https://discuss.hail.is. If this is still an open question, please make a post there!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9837#issuecomment-827850982
https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:138,Availability,avail,available,138,"Hopefully you don't mind this unrequested review, I noticed this PR and had some thoughts. As a developer, I like the new way of defining available parameters and the greater clarity between group and command options. As a user, I like the greater flexibility in providing parameters. A few concerns though:; - For hailctl arguments that are also gcloud arguments (for example, `--project` to `hailctl dataproc start`), what happens if a user provides them in both places (for example, `hailctl dataproc start --project=project-a cluster-name -- --project=project-b`)? One nice attribute of the current parsing method is that it does not allow this, since in most cases the hailctl argument shadows the gcloud argument of the same name.; - It looks like this creates some inconsistency in how the same argument must be provided to different `hailctl dataproc` commands. For example, `--project` can be directly provided to `hailctl dataproc start`, but it would have to go after the `--` for `hailctl dataproc list` or `hailctl dataproc modify`. That seems likely to be surprising/annoying for users. This could be solved by moving such flags (`--project`, `--region`, and `--configuration` are the ones that immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--asyn",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034
https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:1176,Deployability,configurat,configuration,1176,"r, I like the greater flexibility in providing parameters. A few concerns though:; - For hailctl arguments that are also gcloud arguments (for example, `--project` to `hailctl dataproc start`), what happens if a user provides them in both places (for example, `hailctl dataproc start --project=project-a cluster-name -- --project=project-b`)? One nice attribute of the current parsing method is that it does not allow this, since in most cases the hailctl argument shadows the gcloud argument of the same name.; - It looks like this creates some inconsistency in how the same argument must be provided to different `hailctl dataproc` commands. For example, `--project` can be directly provided to `hailctl dataproc start`, but it would have to go after the `--` for `hailctl dataproc list` or `hailctl dataproc modify`. That seems likely to be surprising/annoying for users. This could be solved by moving such flags (`--project`, `--region`, and `--configuration` are the ones that immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034
https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:2799,Deployability,configurat,configuration,2799,"at immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc start` argument, but `--num-secondary-worker-local-ssds` is not. This could cause some confusion/annoyance. This could potentially be reduced by minimizing the number of `gcloud dataproc` arguments that are defined as `hailctl dataproc` arguments. For example, `--num-worker-local-ssds` is just passed through to `gcloud`, so there's no real need for it to be a `hailctl dataproc start` argument. On the other hand, it is nice to have some of those `gcloud` arguments show up in `hailctl dataproc start --help`.; - While making breaking changes, it would be nice if the `--configuration`/`--gcloud-configuration` argument was consistent across `hailctl dataproc` commands. For example, currently `hailctl dataproc start` takes `--configuration` but `hailctl dataproc submit` takes `--gcloud-configuration`. It would also be nice to standardize on kebab case for all arguments. There are some other arguments that use underscores (ex. `--init_timeout` to `hailctl dataproc start`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034
https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:2824,Deployability,configurat,configuration,2824,"at immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc start` argument, but `--num-secondary-worker-local-ssds` is not. This could cause some confusion/annoyance. This could potentially be reduced by minimizing the number of `gcloud dataproc` arguments that are defined as `hailctl dataproc` arguments. For example, `--num-worker-local-ssds` is just passed through to `gcloud`, so there's no real need for it to be a `hailctl dataproc start` argument. On the other hand, it is nice to have some of those `gcloud` arguments show up in `hailctl dataproc start --help`.; - While making breaking changes, it would be nice if the `--configuration`/`--gcloud-configuration` argument was consistent across `hailctl dataproc` commands. For example, currently `hailctl dataproc start` takes `--configuration` but `hailctl dataproc submit` takes `--gcloud-configuration`. It would also be nice to standardize on kebab case for all arguments. There are some other arguments that use underscores (ex. `--init_timeout` to `hailctl dataproc start`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034
https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:2956,Deployability,configurat,configuration,2956,"at immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc start` argument, but `--num-secondary-worker-local-ssds` is not. This could cause some confusion/annoyance. This could potentially be reduced by minimizing the number of `gcloud dataproc` arguments that are defined as `hailctl dataproc` arguments. For example, `--num-worker-local-ssds` is just passed through to `gcloud`, so there's no real need for it to be a `hailctl dataproc start` argument. On the other hand, it is nice to have some of those `gcloud` arguments show up in `hailctl dataproc start --help`.; - While making breaking changes, it would be nice if the `--configuration`/`--gcloud-configuration` argument was consistent across `hailctl dataproc` commands. For example, currently `hailctl dataproc start` takes `--configuration` but `hailctl dataproc submit` takes `--gcloud-configuration`. It would also be nice to standardize on kebab case for all arguments. There are some other arguments that use underscores (ex. `--init_timeout` to `hailctl dataproc start`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034
https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:3017,Deployability,configurat,configuration,3017,"at immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc start` argument, but `--num-secondary-worker-local-ssds` is not. This could cause some confusion/annoyance. This could potentially be reduced by minimizing the number of `gcloud dataproc` arguments that are defined as `hailctl dataproc` arguments. For example, `--num-worker-local-ssds` is just passed through to `gcloud`, so there's no real need for it to be a `hailctl dataproc start` argument. On the other hand, it is nice to have some of those `gcloud` arguments show up in `hailctl dataproc start --help`.; - While making breaking changes, it would be nice if the `--configuration`/`--gcloud-configuration` argument was consistent across `hailctl dataproc` commands. For example, currently `hailctl dataproc start` takes `--configuration` but `hailctl dataproc submit` takes `--gcloud-configuration`. It would also be nice to standardize on kebab case for all arguments. There are some other arguments that use underscores (ex. `--init_timeout` to `hailctl dataproc start`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034
https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:2360,Energy Efficiency,reduce,reduced,2360,"at immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc start` argument, but `--num-secondary-worker-local-ssds` is not. This could cause some confusion/annoyance. This could potentially be reduced by minimizing the number of `gcloud dataproc` arguments that are defined as `hailctl dataproc` arguments. For example, `--num-worker-local-ssds` is just passed through to `gcloud`, so there's no real need for it to be a `hailctl dataproc start` argument. On the other hand, it is nice to have some of those `gcloud` arguments show up in `hailctl dataproc start --help`.; - While making breaking changes, it would be nice if the `--configuration`/`--gcloud-configuration` argument was consistent across `hailctl dataproc` commands. For example, currently `hailctl dataproc start` takes `--configuration` but `hailctl dataproc submit` takes `--gcloud-configuration`. It would also be nice to standardize on kebab case for all arguments. There are some other arguments that use underscores (ex. `--init_timeout` to `hailctl dataproc start`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034
https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:1176,Modifiability,config,configuration,1176,"r, I like the greater flexibility in providing parameters. A few concerns though:; - For hailctl arguments that are also gcloud arguments (for example, `--project` to `hailctl dataproc start`), what happens if a user provides them in both places (for example, `hailctl dataproc start --project=project-a cluster-name -- --project=project-b`)? One nice attribute of the current parsing method is that it does not allow this, since in most cases the hailctl argument shadows the gcloud argument of the same name.; - It looks like this creates some inconsistency in how the same argument must be provided to different `hailctl dataproc` commands. For example, `--project` can be directly provided to `hailctl dataproc start`, but it would have to go after the `--` for `hailctl dataproc list` or `hailctl dataproc modify`. That seems likely to be surprising/annoying for users. This could be solved by moving such flags (`--project`, `--region`, and `--configuration` are the ones that immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034
https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:2799,Modifiability,config,configuration,2799,"at immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc start` argument, but `--num-secondary-worker-local-ssds` is not. This could cause some confusion/annoyance. This could potentially be reduced by minimizing the number of `gcloud dataproc` arguments that are defined as `hailctl dataproc` arguments. For example, `--num-worker-local-ssds` is just passed through to `gcloud`, so there's no real need for it to be a `hailctl dataproc start` argument. On the other hand, it is nice to have some of those `gcloud` arguments show up in `hailctl dataproc start --help`.; - While making breaking changes, it would be nice if the `--configuration`/`--gcloud-configuration` argument was consistent across `hailctl dataproc` commands. For example, currently `hailctl dataproc start` takes `--configuration` but `hailctl dataproc submit` takes `--gcloud-configuration`. It would also be nice to standardize on kebab case for all arguments. There are some other arguments that use underscores (ex. `--init_timeout` to `hailctl dataproc start`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034
https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:2824,Modifiability,config,configuration,2824,"at immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc start` argument, but `--num-secondary-worker-local-ssds` is not. This could cause some confusion/annoyance. This could potentially be reduced by minimizing the number of `gcloud dataproc` arguments that are defined as `hailctl dataproc` arguments. For example, `--num-worker-local-ssds` is just passed through to `gcloud`, so there's no real need for it to be a `hailctl dataproc start` argument. On the other hand, it is nice to have some of those `gcloud` arguments show up in `hailctl dataproc start --help`.; - While making breaking changes, it would be nice if the `--configuration`/`--gcloud-configuration` argument was consistent across `hailctl dataproc` commands. For example, currently `hailctl dataproc start` takes `--configuration` but `hailctl dataproc submit` takes `--gcloud-configuration`. It would also be nice to standardize on kebab case for all arguments. There are some other arguments that use underscores (ex. `--init_timeout` to `hailctl dataproc start`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034
https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:2956,Modifiability,config,configuration,2956,"at immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc start` argument, but `--num-secondary-worker-local-ssds` is not. This could cause some confusion/annoyance. This could potentially be reduced by minimizing the number of `gcloud dataproc` arguments that are defined as `hailctl dataproc` arguments. For example, `--num-worker-local-ssds` is just passed through to `gcloud`, so there's no real need for it to be a `hailctl dataproc start` argument. On the other hand, it is nice to have some of those `gcloud` arguments show up in `hailctl dataproc start --help`.; - While making breaking changes, it would be nice if the `--configuration`/`--gcloud-configuration` argument was consistent across `hailctl dataproc` commands. For example, currently `hailctl dataproc start` takes `--configuration` but `hailctl dataproc submit` takes `--gcloud-configuration`. It would also be nice to standardize on kebab case for all arguments. There are some other arguments that use underscores (ex. `--init_timeout` to `hailctl dataproc start`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034
https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:3017,Modifiability,config,configuration,3017,"at immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc start` argument, but `--num-secondary-worker-local-ssds` is not. This could cause some confusion/annoyance. This could potentially be reduced by minimizing the number of `gcloud dataproc` arguments that are defined as `hailctl dataproc` arguments. For example, `--num-worker-local-ssds` is just passed through to `gcloud`, so there's no real need for it to be a `hailctl dataproc start` argument. On the other hand, it is nice to have some of those `gcloud` arguments show up in `hailctl dataproc start --help`.; - While making breaking changes, it would be nice if the `--configuration`/`--gcloud-configuration` argument was consistent across `hailctl dataproc` commands. For example, currently `hailctl dataproc start` takes `--configuration` but `hailctl dataproc submit` takes `--gcloud-configuration`. It would also be nice to standardize on kebab case for all arguments. There are some other arguments that use underscores (ex. `--init_timeout` to `hailctl dataproc start`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034
https://github.com/hail-is/hail/pull/9842#issuecomment-758080935:703,Deployability,configurat,configuration,703,"Hi @nawatts, your comments are very welcome, and I appreciate your perspective as a hailctl user. OK, mulling over your comments, I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. I think this also addresses the issue `hailctl dataproc submit` not supporting `--`, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: `hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters`. What do you think?. > it would be nice if the --configuration/--gcloud-configuration argument was consistent across hailctl dataproc commands; > It would also be nice to standardize on kebab case for all arguments. Agree on both accounts, will fix. Thanks again!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758080935
https://github.com/hail-is/hail/pull/9842#issuecomment-758080935:726,Deployability,configurat,configuration,726,"Hi @nawatts, your comments are very welcome, and I appreciate your perspective as a hailctl user. OK, mulling over your comments, I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. I think this also addresses the issue `hailctl dataproc submit` not supporting `--`, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: `hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters`. What do you think?. > it would be nice if the --configuration/--gcloud-configuration argument was consistent across hailctl dataproc commands; > It would also be nice to standardize on kebab case for all arguments. Agree on both accounts, will fix. Thanks again!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758080935
https://github.com/hail-is/hail/pull/9842#issuecomment-758080935:703,Modifiability,config,configuration,703,"Hi @nawatts, your comments are very welcome, and I appreciate your perspective as a hailctl user. OK, mulling over your comments, I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. I think this also addresses the issue `hailctl dataproc submit` not supporting `--`, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: `hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters`. What do you think?. > it would be nice if the --configuration/--gcloud-configuration argument was consistent across hailctl dataproc commands; > It would also be nice to standardize on kebab case for all arguments. Agree on both accounts, will fix. Thanks again!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758080935
https://github.com/hail-is/hail/pull/9842#issuecomment-758080935:726,Modifiability,config,configuration,726,"Hi @nawatts, your comments are very welcome, and I appreciate your perspective as a hailctl user. OK, mulling over your comments, I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. I think this also addresses the issue `hailctl dataproc submit` not supporting `--`, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: `hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters`. What do you think?. > it would be nice if the --configuration/--gcloud-configuration argument was consistent across hailctl dataproc commands; > It would also be nice to standardize on kebab case for all arguments. Agree on both accounts, will fix. Thanks again!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758080935
https://github.com/hail-is/hail/pull/9842#issuecomment-758100479:518,Deployability,configurat,configuration,518,"> I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. Do you mean remove all options that are simple pass throughs (such as `--num-worker-local-ssds`) or all options that are also gcloud options (such as `--project`)? The latter could be difficult, since there are some gcloud options that hailctl also needs to read, like `hailctl dataproc start` using `--project` to set requester pays configuration, extending `--initialization-actions` with notebook/VEP init scripts, setting a higher default disk size when `--vep` is specified, etc. or `hailctl dataproc submit` automatically zipping `--py-files`. > I think this also addresses the issue hailctl dataproc submit not supporting --, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters. Nice. That would solve the problem. I would guess submitting script arguments is more common than using gcloud options here, so it would be nice for the gcloud arguments group to be optional, so that `hailctl dataproc submit cluster -- --script-options` would work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758100479
https://github.com/hail-is/hail/pull/9842#issuecomment-758100479:518,Modifiability,config,configuration,518,"> I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. Do you mean remove all options that are simple pass throughs (such as `--num-worker-local-ssds`) or all options that are also gcloud options (such as `--project`)? The latter could be difficult, since there are some gcloud options that hailctl also needs to read, like `hailctl dataproc start` using `--project` to set requester pays configuration, extending `--initialization-actions` with notebook/VEP init scripts, setting a higher default disk size when `--vep` is specified, etc. or `hailctl dataproc submit` automatically zipping `--py-files`. > I think this also addresses the issue hailctl dataproc submit not supporting --, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters. Nice. That would solve the problem. I would guess submitting script arguments is more common than using gcloud options here, so it would be nice for the gcloud arguments group to be optional, so that `hailctl dataproc submit cluster -- --script-options` would work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758100479
https://github.com/hail-is/hail/pull/9842#issuecomment-758100479:533,Modifiability,extend,extending,533,"> I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. Do you mean remove all options that are simple pass throughs (such as `--num-worker-local-ssds`) or all options that are also gcloud options (such as `--project`)? The latter could be difficult, since there are some gcloud options that hailctl also needs to read, like `hailctl dataproc start` using `--project` to set requester pays configuration, extending `--initialization-actions` with notebook/VEP init scripts, setting a higher default disk size when `--vep` is specified, etc. or `hailctl dataproc submit` automatically zipping `--py-files`. > I think this also addresses the issue hailctl dataproc submit not supporting --, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters. Nice. That would solve the problem. I would guess submitting script arguments is more common than using gcloud options here, so it would be nice for the gcloud arguments group to be optional, so that `hailctl dataproc submit cluster -- --script-options` would work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758100479
https://github.com/hail-is/hail/pull/9842#issuecomment-758100479:224,Usability,simpl,simple,224,"> I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. Do you mean remove all options that are simple pass throughs (such as `--num-worker-local-ssds`) or all options that are also gcloud options (such as `--project`)? The latter could be difficult, since there are some gcloud options that hailctl also needs to read, like `hailctl dataproc start` using `--project` to set requester pays configuration, extending `--initialization-actions` with notebook/VEP init scripts, setting a higher default disk size when `--vep` is specified, etc. or `hailctl dataproc submit` automatically zipping `--py-files`. > I think this also addresses the issue hailctl dataproc submit not supporting --, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters. Nice. That would solve the problem. I would guess submitting script arguments is more common than using gcloud options here, so it would be nice for the gcloud arguments group to be optional, so that `hailctl dataproc submit cluster -- --script-options` would work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758100479
https://github.com/hail-is/hail/pull/9842#issuecomment-758128554:1617,Availability,error,error,1617,"> all options that are also gcloud options (such as --project)? [That] could be difficult. Yes, this is what I was thinking. `hailctl` could parse (as much as is needed) the `gcloud` options to find options (like `--project`) and modify others (like `--initialization-actions`). The latter is somewhat surprising since one expects everything after the `--` to pass through unchanged. OK, summarizing our options so far:. - hailctl has no options that are also gcloud options. gcloud options go after the `--`, and get modified as needed by hailctl (with a message).; - hailctl has no gcloud options that are simply pass through. gcloud options that are needed by hailctl commands are hailctl options (like `--project`). When a gcloud option is needed by some hailctl command, all hailctl commands take that option (when it makes sense), even if in some cases that makes them simply pass through. This fixes the inconsistency issues, but the user still needs to keep track of which gcloud options needs to be passed to hailctl and which are passed to gcloud directly. If you specify an option twice, once to hailctl and once to gcloud, we invoke gcloud with the option duplicated. Pros and cons:; - The first option has the most consistent interface.; - The first option modifies options after the --, which is surprising.; - The first option involves replication (some of) the gcloud option parsing semantics, which is annoying.; - The second option requires the user to know which gcloud options need to be passed to hailctl instead (but globally, not per-command).; - With the second option, if we want to warn (or error) on duplicate options, we're back to duplicating the gcloud option parsing logic. I think I'm coming around to the second option. > so that hailctl dataproc submit cluster -- --script-options would work. I see, so if there is only one `--` it refers to script options, and if there are two, the first one corresponds to gcloud options? I think that should be doable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554
https://github.com/hail-is/hail/pull/9842#issuecomment-758128554:556,Integrability,message,message,556,"> all options that are also gcloud options (such as --project)? [That] could be difficult. Yes, this is what I was thinking. `hailctl` could parse (as much as is needed) the `gcloud` options to find options (like `--project`) and modify others (like `--initialization-actions`). The latter is somewhat surprising since one expects everything after the `--` to pass through unchanged. OK, summarizing our options so far:. - hailctl has no options that are also gcloud options. gcloud options go after the `--`, and get modified as needed by hailctl (with a message).; - hailctl has no gcloud options that are simply pass through. gcloud options that are needed by hailctl commands are hailctl options (like `--project`). When a gcloud option is needed by some hailctl command, all hailctl commands take that option (when it makes sense), even if in some cases that makes them simply pass through. This fixes the inconsistency issues, but the user still needs to keep track of which gcloud options needs to be passed to hailctl and which are passed to gcloud directly. If you specify an option twice, once to hailctl and once to gcloud, we invoke gcloud with the option duplicated. Pros and cons:; - The first option has the most consistent interface.; - The first option modifies options after the --, which is surprising.; - The first option involves replication (some of) the gcloud option parsing semantics, which is annoying.; - The second option requires the user to know which gcloud options need to be passed to hailctl instead (but globally, not per-command).; - With the second option, if we want to warn (or error) on duplicate options, we're back to duplicating the gcloud option parsing logic. I think I'm coming around to the second option. > so that hailctl dataproc submit cluster -- --script-options would work. I see, so if there is only one `--` it refers to script options, and if there are two, the first one corresponds to gcloud options? I think that should be doable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554
https://github.com/hail-is/hail/pull/9842#issuecomment-758128554:1239,Integrability,interface,interface,1239,"> all options that are also gcloud options (such as --project)? [That] could be difficult. Yes, this is what I was thinking. `hailctl` could parse (as much as is needed) the `gcloud` options to find options (like `--project`) and modify others (like `--initialization-actions`). The latter is somewhat surprising since one expects everything after the `--` to pass through unchanged. OK, summarizing our options so far:. - hailctl has no options that are also gcloud options. gcloud options go after the `--`, and get modified as needed by hailctl (with a message).; - hailctl has no gcloud options that are simply pass through. gcloud options that are needed by hailctl commands are hailctl options (like `--project`). When a gcloud option is needed by some hailctl command, all hailctl commands take that option (when it makes sense), even if in some cases that makes them simply pass through. This fixes the inconsistency issues, but the user still needs to keep track of which gcloud options needs to be passed to hailctl and which are passed to gcloud directly. If you specify an option twice, once to hailctl and once to gcloud, we invoke gcloud with the option duplicated. Pros and cons:; - The first option has the most consistent interface.; - The first option modifies options after the --, which is surprising.; - The first option involves replication (some of) the gcloud option parsing semantics, which is annoying.; - The second option requires the user to know which gcloud options need to be passed to hailctl instead (but globally, not per-command).; - With the second option, if we want to warn (or error) on duplicate options, we're back to duplicating the gcloud option parsing logic. I think I'm coming around to the second option. > so that hailctl dataproc submit cluster -- --script-options would work. I see, so if there is only one `--` it refers to script options, and if there are two, the first one corresponds to gcloud options? I think that should be doable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554
https://github.com/hail-is/hail/pull/9842#issuecomment-758128554:1698,Testability,log,logic,1698,"> all options that are also gcloud options (such as --project)? [That] could be difficult. Yes, this is what I was thinking. `hailctl` could parse (as much as is needed) the `gcloud` options to find options (like `--project`) and modify others (like `--initialization-actions`). The latter is somewhat surprising since one expects everything after the `--` to pass through unchanged. OK, summarizing our options so far:. - hailctl has no options that are also gcloud options. gcloud options go after the `--`, and get modified as needed by hailctl (with a message).; - hailctl has no gcloud options that are simply pass through. gcloud options that are needed by hailctl commands are hailctl options (like `--project`). When a gcloud option is needed by some hailctl command, all hailctl commands take that option (when it makes sense), even if in some cases that makes them simply pass through. This fixes the inconsistency issues, but the user still needs to keep track of which gcloud options needs to be passed to hailctl and which are passed to gcloud directly. If you specify an option twice, once to hailctl and once to gcloud, we invoke gcloud with the option duplicated. Pros and cons:; - The first option has the most consistent interface.; - The first option modifies options after the --, which is surprising.; - The first option involves replication (some of) the gcloud option parsing semantics, which is annoying.; - The second option requires the user to know which gcloud options need to be passed to hailctl instead (but globally, not per-command).; - With the second option, if we want to warn (or error) on duplicate options, we're back to duplicating the gcloud option parsing logic. I think I'm coming around to the second option. > so that hailctl dataproc submit cluster -- --script-options would work. I see, so if there is only one `--` it refers to script options, and if there are two, the first one corresponds to gcloud options? I think that should be doable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554
https://github.com/hail-is/hail/pull/9842#issuecomment-758128554:608,Usability,simpl,simply,608,"> all options that are also gcloud options (such as --project)? [That] could be difficult. Yes, this is what I was thinking. `hailctl` could parse (as much as is needed) the `gcloud` options to find options (like `--project`) and modify others (like `--initialization-actions`). The latter is somewhat surprising since one expects everything after the `--` to pass through unchanged. OK, summarizing our options so far:. - hailctl has no options that are also gcloud options. gcloud options go after the `--`, and get modified as needed by hailctl (with a message).; - hailctl has no gcloud options that are simply pass through. gcloud options that are needed by hailctl commands are hailctl options (like `--project`). When a gcloud option is needed by some hailctl command, all hailctl commands take that option (when it makes sense), even if in some cases that makes them simply pass through. This fixes the inconsistency issues, but the user still needs to keep track of which gcloud options needs to be passed to hailctl and which are passed to gcloud directly. If you specify an option twice, once to hailctl and once to gcloud, we invoke gcloud with the option duplicated. Pros and cons:; - The first option has the most consistent interface.; - The first option modifies options after the --, which is surprising.; - The first option involves replication (some of) the gcloud option parsing semantics, which is annoying.; - The second option requires the user to know which gcloud options need to be passed to hailctl instead (but globally, not per-command).; - With the second option, if we want to warn (or error) on duplicate options, we're back to duplicating the gcloud option parsing logic. I think I'm coming around to the second option. > so that hailctl dataproc submit cluster -- --script-options would work. I see, so if there is only one `--` it refers to script options, and if there are two, the first one corresponds to gcloud options? I think that should be doable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554
https://github.com/hail-is/hail/pull/9842#issuecomment-758128554:875,Usability,simpl,simply,875,"> all options that are also gcloud options (such as --project)? [That] could be difficult. Yes, this is what I was thinking. `hailctl` could parse (as much as is needed) the `gcloud` options to find options (like `--project`) and modify others (like `--initialization-actions`). The latter is somewhat surprising since one expects everything after the `--` to pass through unchanged. OK, summarizing our options so far:. - hailctl has no options that are also gcloud options. gcloud options go after the `--`, and get modified as needed by hailctl (with a message).; - hailctl has no gcloud options that are simply pass through. gcloud options that are needed by hailctl commands are hailctl options (like `--project`). When a gcloud option is needed by some hailctl command, all hailctl commands take that option (when it makes sense), even if in some cases that makes them simply pass through. This fixes the inconsistency issues, but the user still needs to keep track of which gcloud options needs to be passed to hailctl and which are passed to gcloud directly. If you specify an option twice, once to hailctl and once to gcloud, we invoke gcloud with the option duplicated. Pros and cons:; - The first option has the most consistent interface.; - The first option modifies options after the --, which is surprising.; - The first option involves replication (some of) the gcloud option parsing semantics, which is annoying.; - The second option requires the user to know which gcloud options need to be passed to hailctl instead (but globally, not per-command).; - With the second option, if we want to warn (or error) on duplicate options, we're back to duplicating the gcloud option parsing logic. I think I'm coming around to the second option. > so that hailctl dataproc submit cluster -- --script-options would work. I see, so if there is only one `--` it refers to script options, and if there are two, the first one corresponds to gcloud options? I think that should be doable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758128554
https://github.com/hail-is/hail/pull/9842#issuecomment-758143032:592,Deployability,update,update,592,"I also prefer the second option in https://github.com/hail-is/hail/pull/9842#issuecomment-758128554. The more I think about this, the more problematic the notion of having an opaque list of ""arguments to pass through to gcloud"" seems. For example, `hailctl dataproc start` may run multiple gcloud commands: one to start the cluster and another to apply tags to the master node. In that case, we'd want to pass through extra args to the cluster start command, but not the apply tags command. Or `hailctl dataproc modify`, where we might want to accept extra args for `gcloud dataproc clusters update`. Those args shouldn't be passed through to the `gcloud compute ssh` commands, but options like `--project` or `--configuration` should. It seems like pass through arguments would be best handled on a case by case basis for each hailctl command. That would probably make any approach that required parsing those pass through options more cumbersome to use.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758143032
https://github.com/hail-is/hail/pull/9842#issuecomment-758143032:713,Deployability,configurat,configuration,713,"I also prefer the second option in https://github.com/hail-is/hail/pull/9842#issuecomment-758128554. The more I think about this, the more problematic the notion of having an opaque list of ""arguments to pass through to gcloud"" seems. For example, `hailctl dataproc start` may run multiple gcloud commands: one to start the cluster and another to apply tags to the master node. In that case, we'd want to pass through extra args to the cluster start command, but not the apply tags command. Or `hailctl dataproc modify`, where we might want to accept extra args for `gcloud dataproc clusters update`. Those args shouldn't be passed through to the `gcloud compute ssh` commands, but options like `--project` or `--configuration` should. It seems like pass through arguments would be best handled on a case by case basis for each hailctl command. That would probably make any approach that required parsing those pass through options more cumbersome to use.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758143032
https://github.com/hail-is/hail/pull/9842#issuecomment-758143032:713,Modifiability,config,configuration,713,"I also prefer the second option in https://github.com/hail-is/hail/pull/9842#issuecomment-758128554. The more I think about this, the more problematic the notion of having an opaque list of ""arguments to pass through to gcloud"" seems. For example, `hailctl dataproc start` may run multiple gcloud commands: one to start the cluster and another to apply tags to the master node. In that case, we'd want to pass through extra args to the cluster start command, but not the apply tags command. Or `hailctl dataproc modify`, where we might want to accept extra args for `gcloud dataproc clusters update`. Those args shouldn't be passed through to the `gcloud compute ssh` commands, but options like `--project` or `--configuration` should. It seems like pass through arguments would be best handled on a case by case basis for each hailctl command. That would probably make any approach that required parsing those pass through options more cumbersome to use.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758143032
https://github.com/hail-is/hail/pull/9842#issuecomment-758255396:676,Availability,avail,available,676,"For @tpoterba's benefit (and to get things clear), here's the current proposal:; - Move extra gcloud arguments to `--extra-gcloud-<description>-args=""--arg1 ... --argN""` where there is one such argument for each invocation of gcloud inside a hailctl command. gcloud args no longer go at the end.; - Only `hailctl dataproc submit` supports `--` which is used to separate submit arguments from the script arguments,; - Remove all gcloud arguments that are pass through in all commands, but mention them in the command help so users don't need to look at the gcloud help for commonly used options.; - gcloud options that are needed by some hailctl command should be consistently available among all hailctl commands (where appropriate, and where in some cases they may simply be pass-through).; - The other consistency changes @nawatts highlighted. (I wouldn't be surprised if I other issues come up when I make the changes, but this is a start.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758255396
https://github.com/hail-is/hail/pull/9842#issuecomment-758255396:43,Usability,clear,clear,43,"For @tpoterba's benefit (and to get things clear), here's the current proposal:; - Move extra gcloud arguments to `--extra-gcloud-<description>-args=""--arg1 ... --argN""` where there is one such argument for each invocation of gcloud inside a hailctl command. gcloud args no longer go at the end.; - Only `hailctl dataproc submit` supports `--` which is used to separate submit arguments from the script arguments,; - Remove all gcloud arguments that are pass through in all commands, but mention them in the command help so users don't need to look at the gcloud help for commonly used options.; - gcloud options that are needed by some hailctl command should be consistently available among all hailctl commands (where appropriate, and where in some cases they may simply be pass-through).; - The other consistency changes @nawatts highlighted. (I wouldn't be surprised if I other issues come up when I make the changes, but this is a start.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758255396
https://github.com/hail-is/hail/pull/9842#issuecomment-758255396:766,Usability,simpl,simply,766,"For @tpoterba's benefit (and to get things clear), here's the current proposal:; - Move extra gcloud arguments to `--extra-gcloud-<description>-args=""--arg1 ... --argN""` where there is one such argument for each invocation of gcloud inside a hailctl command. gcloud args no longer go at the end.; - Only `hailctl dataproc submit` supports `--` which is used to separate submit arguments from the script arguments,; - Remove all gcloud arguments that are pass through in all commands, but mention them in the command help so users don't need to look at the gcloud help for commonly used options.; - gcloud options that are needed by some hailctl command should be consistently available among all hailctl commands (where appropriate, and where in some cases they may simply be pass-through).; - The other consistency changes @nawatts highlighted. (I wouldn't be surprised if I other issues come up when I make the changes, but this is a start.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758255396
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:632,Availability,error,error,632,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:831,Availability,error,error,831,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:168,Deployability,configurat,configuration,168,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:1097,Deployability,update,update-args,1097,"e changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:1830,Deployability,update,update,1830," determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2039,Deployability,update,update,2039,"dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the clus",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2088,Deployability,update,update-hail-version,2088,"dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the clus",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2169,Deployability,update,update,2169,"ion name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2218,Deployability,update,update-args,2218,"ion name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2272,Deployability,update,update,2272,"'t run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3527,Deployability,update,update,3527,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3576,Deployability,update,update-hail-version,3576,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3596,Deployability,Update,Update,3596,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3666,Deployability,install,installed,3666,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3708,Deployability,install,installation,3708,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3738,Deployability,update,update-args,3738,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3810,Deployability,update,update,3810,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3824,Deployability,update,update,3824,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3899,Integrability,message,message,3899,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:168,Modifiability,config,configuration,168,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3184,Modifiability,config,configured,3184,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3465,Modifiability,config,configured,3465,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2500,Safety,timeout,timeout,2500,"ugh.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more informat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2644,Safety,Timeout,Timeout,2644,"les`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail inst",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2782,Safety,Timeout,Timeout,2782,"ate. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2869,Safety,timeout,timeout,2869,"ate. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:58,Usability,feedback,feedback,58,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772
https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:501,Availability,error,error,501,"> The hailctl dataproc subcommand now has --beta, --configuration=, --dry-run, --project= and --zone=. These apply to all commands. There is a GcloudRunner object that takes these options, is set to the click context user obj field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with click, the subcommand options must go on the subcommand, so hailctl dataproc stop --dry-run is an error. Nice. It's great that these are handled at the `hailctl dataproc` level instead of having to remember to account for them in every `hailctl dataproc` subcommand. That's going to resolve a lot of inconsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:1008,Availability,error,error,1008,"-beta, --configuration=, --dry-run, --project= and --zone=. These apply to all commands. There is a GcloudRunner object that takes these options, is set to the click context user obj field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with click, the subcommand options must go on the subcommand, so hailctl dataproc stop --dry-run is an error. Nice. It's great that these are handled at the `hailctl dataproc` level instead of having to remember to account for them in every `hailctl dataproc` subcommand. That's going to resolve a lot of inconsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoki",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:52,Deployability,configurat,configuration,52,"> The hailctl dataproc subcommand now has --beta, --configuration=, --dry-run, --project= and --zone=. These apply to all commands. There is a GcloudRunner object that takes these options, is set to the click context user obj field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with click, the subcommand options must go on the subcommand, so hailctl dataproc stop --dry-run is an error. Nice. It's great that these are handled at the `hailctl dataproc` level instead of having to remember to account for them in every `hailctl dataproc` subcommand. That's going to resolve a lot of inconsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:1713,Deployability,configurat,configuration,1713,"onsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the ot",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:2139,Deployability,update,update-args,2139,"`gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the other hand, `--extra-gcloud-dataproc-clusters-update-args` is an awfully long argument name. > I plan to leave the --async option to stop, although it is pass through. > Then there is --files for submit. This is passed through, but --py-files is needed (it is not passed through, but modified). Do I leave --files? I'm currently inclined to. Agreed. I support having the most frequently used parameters as `hailctl` parameters, even if they are only simple pass throughs. My original comment about minimizing ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:2599,Deployability,update,update-args,2599,"eve the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the other hand, `--extra-gcloud-dataproc-clusters-update-args` is an awfully long argument name. > I plan to leave the --async option to stop, although it is pass through. > Then there is --files for submit. This is passed through, but --py-files is needed (it is not passed through, but modified). Do I leave --files? I'm currently inclined to. Agreed. I support having the most frequently used parameters as `hailctl` parameters, even if they are only simple pass throughs. My original comment about minimizing the number of simple pass through parameters was mainly directed toward `hailctl dataproc start`, which has several options than can be specified separately for master node, worker nodes, and secondary worker nodes. I wanted to avoid cases where, for example, `--worker-boot-disk-size` was a `hailctl` option, but `--secondary-worker-boot-disk-size` had to be specified after a `--` or with `--extra-gcloud-start-args",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:2660,Deployability,update,update,2660,"eve the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the other hand, `--extra-gcloud-dataproc-clusters-update-args` is an awfully long argument name. > I plan to leave the --async option to stop, although it is pass through. > Then there is --files for submit. This is passed through, but --py-files is needed (it is not passed through, but modified). Do I leave --files? I'm currently inclined to. Agreed. I support having the most frequently used parameters as `hailctl` parameters, even if they are only simple pass throughs. My original comment about minimizing the number of simple pass through parameters was mainly directed toward `hailctl dataproc start`, which has several options than can be specified separately for master node, worker nodes, and secondary worker nodes. I wanted to avoid cases where, for example, `--worker-boot-disk-size` was a `hailctl` option, but `--secondary-worker-boot-disk-size` had to be specified after a `--` or with `--extra-gcloud-start-args",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:2751,Deployability,update,update-args,2751,"e the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the other hand, `--extra-gcloud-dataproc-clusters-update-args` is an awfully long argument name. > I plan to leave the --async option to stop, although it is pass through. > Then there is --files for submit. This is passed through, but --py-files is needed (it is not passed through, but modified). Do I leave --files? I'm currently inclined to. Agreed. I support having the most frequently used parameters as `hailctl` parameters, even if they are only simple pass throughs. My original comment about minimizing the number of simple pass through parameters was mainly directed toward `hailctl dataproc start`, which has several options than can be specified separately for master node, worker nodes, and secondary worker nodes. I wanted to avoid cases where, for example, `--worker-boot-disk-size` was a `hailctl` option, but `--secondary-worker-boot-disk-size` had to be specified after a `--` or with `--extra-gcloud-start-args`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:52,Modifiability,config,configuration,52,"> The hailctl dataproc subcommand now has --beta, --configuration=, --dry-run, --project= and --zone=. These apply to all commands. There is a GcloudRunner object that takes these options, is set to the click context user obj field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with click, the subcommand options must go on the subcommand, so hailctl dataproc stop --dry-run is an error. Nice. It's great that these are handled at the `hailctl dataproc` level instead of having to remember to account for them in every `hailctl dataproc` subcommand. That's going to resolve a lot of inconsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:1713,Modifiability,config,configuration,1713,"onsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the ot",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:3442,Safety,avoid,avoid,3442,"e the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the other hand, `--extra-gcloud-dataproc-clusters-update-args` is an awfully long argument name. > I plan to leave the --async option to stop, although it is pass through. > Then there is --files for submit. This is passed through, but --py-files is needed (it is not passed through, but modified). Do I leave --files? I'm currently inclined to. Agreed. I support having the most frequently used parameters as `hailctl` parameters, even if they are only simple pass throughs. My original comment about minimizing the number of simple pass through parameters was mainly directed toward `hailctl dataproc start`, which has several options than can be specified separately for master node, worker nodes, and secondary worker nodes. I wanted to avoid cases where, for example, `--worker-boot-disk-size` was a `hailctl` option, but `--secondary-worker-boot-disk-size` had to be specified after a `--` or with `--extra-gcloud-start-args`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:3155,Usability,simpl,simple,3155,"e the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the other hand, `--extra-gcloud-dataproc-clusters-update-args` is an awfully long argument name. > I plan to leave the --async option to stop, although it is pass through. > Then there is --files for submit. This is passed through, but --py-files is needed (it is not passed through, but modified). Do I leave --files? I'm currently inclined to. Agreed. I support having the most frequently used parameters as `hailctl` parameters, even if they are only simple pass throughs. My original comment about minimizing the number of simple pass through parameters was mainly directed toward `hailctl dataproc start`, which has several options than can be specified separately for master node, worker nodes, and secondary worker nodes. I wanted to avoid cases where, for example, `--worker-boot-disk-size` was a `hailctl` option, but `--secondary-worker-boot-disk-size` had to be specified after a `--` or with `--extra-gcloud-start-args`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:3228,Usability,simpl,simple,3228,"e the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the other hand, `--extra-gcloud-dataproc-clusters-update-args` is an awfully long argument name. > I plan to leave the --async option to stop, although it is pass through. > Then there is --files for submit. This is passed through, but --py-files is needed (it is not passed through, but modified). Do I leave --files? I'm currently inclined to. Agreed. I support having the most frequently used parameters as `hailctl` parameters, even if they are only simple pass throughs. My original comment about minimizing the number of simple pass through parameters was mainly directed toward `hailctl dataproc start`, which has several options than can be specified separately for master node, worker nodes, and secondary worker nodes. I wanted to avoid cases where, for example, `--worker-boot-disk-size` was a `hailctl` option, but `--secondary-worker-boot-disk-size` had to be specified after a `--` or with `--extra-gcloud-start-args`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393
https://github.com/hail-is/hail/pull/9842#issuecomment-767171070:136,Deployability,release,released,136,"While we're making breaking changes to the `hailctl` interface... there is a beta feature to start/stop Dataproc clusters. Once this is released, it could create some confusion that `hailctl dataproc start` runs `gcloud dataproc clusters create` instead of `gcloud dataproc clusters start`. Likewise for `hailctl dataproc stop` and `gcloud dataproc clusters delete`. Should we rename `hailctl dataproc` start/stop to create/delete?. https://cloud.google.com/dataproc/docs/guides/dataproc-start-stop",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767171070
https://github.com/hail-is/hail/pull/9842#issuecomment-767171070:53,Integrability,interface,interface,53,"While we're making breaking changes to the `hailctl` interface... there is a beta feature to start/stop Dataproc clusters. Once this is released, it could create some confusion that `hailctl dataproc start` runs `gcloud dataproc clusters create` instead of `gcloud dataproc clusters start`. Likewise for `hailctl dataproc stop` and `gcloud dataproc clusters delete`. Should we rename `hailctl dataproc` start/stop to create/delete?. https://cloud.google.com/dataproc/docs/guides/dataproc-start-stop",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767171070
https://github.com/hail-is/hail/pull/9842#issuecomment-767171070:472,Usability,guid,guides,472,"While we're making breaking changes to the `hailctl` interface... there is a beta feature to start/stop Dataproc clusters. Once this is released, it could create some confusion that `hailctl dataproc start` runs `gcloud dataproc clusters create` instead of `gcloud dataproc clusters start`. Likewise for `hailctl dataproc stop` and `gcloud dataproc clusters delete`. Should we rename `hailctl dataproc` start/stop to create/delete?. https://cloud.google.com/dataproc/docs/guides/dataproc-start-stop",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767171070
https://github.com/hail-is/hail/pull/9842#issuecomment-767171298:459,Deployability,update,update,459,"Hey Nick, wanted to loop you in on an offline discussion I had with Cotton about this. First, thank you for picking up review responsibilities! I'll just do a brief review focusing on interaction of this change with intended directions for hailctl. Here are the conclusions from our discussion:. 1. This is a breaking change to the hailctl interface. We're OK with that.; 2. Although we are OK making breaking changes, we should get Grace's team on board and update their scripts/repos before merging/releasing. For that reason this will sit for a few weeks until their current urgent analysis push is done.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767171298
https://github.com/hail-is/hail/pull/9842#issuecomment-767171298:340,Integrability,interface,interface,340,"Hey Nick, wanted to loop you in on an offline discussion I had with Cotton about this. First, thank you for picking up review responsibilities! I'll just do a brief review focusing on interaction of this change with intended directions for hailctl. Here are the conclusions from our discussion:. 1. This is a breaking change to the hailctl interface. We're OK with that.; 2. Although we are OK making breaking changes, we should get Grace's team on board and update their scripts/repos before merging/releasing. For that reason this will sit for a few weeks until their current urgent analysis push is done.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767171298
https://github.com/hail-is/hail/pull/9842#issuecomment-767181420:94,Deployability,update,update,94,"> Should we rename hailctl dataproc start/stop to create/delete?. I think so. Also, modify to update.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767181420
https://github.com/hail-is/hail/pull/9851#issuecomment-754182658:11,Deployability,rolling,rolling,11,tests pass rolling up,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9851#issuecomment-754182658
https://github.com/hail-is/hail/pull/9851#issuecomment-754182658:0,Testability,test,tests,0,tests pass rolling up,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9851#issuecomment-754182658
https://github.com/hail-is/hail/pull/9852#issuecomment-754163535:40,Testability,test,tests,40,"same as #9851, will rollup if it passes tests",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9852#issuecomment-754163535
https://github.com/hail-is/hail/pull/9852#issuecomment-754182568:13,Deployability,rolling,rolling,13,Tests passed rolling up,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9852#issuecomment-754182568
https://github.com/hail-is/hail/pull/9852#issuecomment-754182568:0,Testability,Test,Tests,0,Tests passed rolling up,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9852#issuecomment-754182568
https://github.com/hail-is/hail/pull/9854#issuecomment-756355967:79,Performance,load,loading,79,I added the `forall` method and methods to look up individual elements without loading whole thing into memory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/9854#issuecomment-756355967
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:1903,Availability,Error,Error,1903," write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementException: key not found: RefEquality(WriteMetadata(Let(__iruid_465,CollectDistributedArray(StreamZip(ArrayBuffer(MakeStream(ArrayBuffer(Literal(struct{start: int32, end: int32},[0,10]), Literal(struct{start: int32, end: int32},[10,20]), Literal(struct{start: int32, end: int32},[20,30]), Literal(struct{start: int32, end: int32},[30,40]), Literal(struct{start: int32, end: int32},[40,50]), Literal(struct{start: int32, end: int32},[50,60]), Literal(struct{start: int32, end: int32},[60,70]), Literal(struct{start: int32, end: int32},[70,80]), Literal(struct{start: int32, end: int32},[80,90]), Literal(struct{start: int32, end: int32},[90,100])),stream<struct{start: int32, end: int32}>,false), MakeStream(ArrayBuffer(Str(""part-00-""), Str(""part-01-""), Str(""part-02-""), Str(""part-03-""), Str(""part-04-""), Str(""part-05-""), Str(""part-06-""), Str(""part-07-""), Str(""part-08-""), Str(""part-09-"")),stream<str>,false)),ArrayBuffe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:1955,Availability,Error,Error,1955," write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementException: key not found: RefEquality(WriteMetadata(Let(__iruid_465,CollectDistributedArray(StreamZip(ArrayBuffer(MakeStream(ArrayBuffer(Literal(struct{start: int32, end: int32},[0,10]), Literal(struct{start: int32, end: int32},[10,20]), Literal(struct{start: int32, end: int32},[20,30]), Literal(struct{start: int32, end: int32},[30,40]), Literal(struct{start: int32, end: int32},[40,50]), Literal(struct{start: int32, end: int32},[50,60]), Literal(struct{start: int32, end: int32},[60,70]), Literal(struct{start: int32, end: int32},[70,80]), Literal(struct{start: int32, end: int32},[80,90]), Literal(struct{start: int32, end: int32},[90,100])),stream<struct{start: int32, end: int32}>,false), MakeStream(ArrayBuffer(Str(""part-00-""), Str(""part-01-""), Str(""part-02-""), Str(""part-03-""), Str(""part-04-""), Str(""part-05-""), Str(""part-06-""), Str(""part-07-""), Str(""part-08-""), Str(""part-09-"")),stream<str>,false)),ArrayBuffe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:789,Integrability,wrap,wrapper,789,"It also fails on this simpler example:; ```; In [1]: import hail as hl ; ...: ; ...: temp = hl.utils.range_table(100) ; ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-c548354b6e81; LOGGING: writing to /Users/dking/projects/hail/hail-20210107-1038-0.2.61-c548354b6e81.log; Traceback (most recent call last):; File ""<ipython-input-1-a2e56feaf799>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1092>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementExce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:3500,Integrability,Wrap,WrappedArray,3500,"entException: key not found: RefEquality(WriteMetadata(Let(__iruid_465,CollectDistributedArray(StreamZip(ArrayBuffer(MakeStream(ArrayBuffer(Literal(struct{start: int32, end: int32},[0,10]), Literal(struct{start: int32, end: int32},[10,20]), Literal(struct{start: int32, end: int32},[20,30]), Literal(struct{start: int32, end: int32},[30,40]), Literal(struct{start: int32, end: int32},[40,50]), Literal(struct{start: int32, end: int32},[50,60]), Literal(struct{start: int32, end: int32},[60,70]), Literal(struct{start: int32, end: int32},[70,80]), Literal(struct{start: int32, end: int32},[80,90]), Literal(struct{start: int32, end: int32},[90,100])),stream<struct{start: int32, end: int32}>,false), MakeStream(ArrayBuffer(Str(""part-00-""), Str(""part-01-""), Str(""part-02-""), Str(""part-03-""), Str(""part-04-""), Str(""part-05-""), Str(""part-06-""), Str(""part-07-""), Str(""part-08-""), Str(""part-09-"")),stream<str>,false)),ArrayBuffer(__iruid_468, __iruid_469),MakeStruct(ArrayBuffer((oldCtx,Ref(__iruid_468,struct{start: int32, end: int32})), (writeCtx,Ref(__iruid_469,str)))),AssertSameLength),Literal(struct{},[]),__iruid_466,__iruid_467,WritePartition(Let(__iruid_470,GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),oldCtx),StreamMap(StreamRange(GetField(Ref(__iruid_470,struct{start: int32, end: int32}),start),GetField(Ref(__iruid_470,struct{start: int32, end: int32}),end),I32(1),false),__iruid_471,MakeStruct(ArrayBuffer((idx,Ref(__iruid_471,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_386)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:4115,Integrability,Wrap,WrappedArray,4115,"}),start),GetField(Ref(__iruid_470,struct{start: int32, end: int32}),end),I32(1),false),__iruid_471,MakeStruct(ArrayBuffer((idx,Ref(__iruid_471,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_386)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/rows/parts/,Some((gs://danking/workshop-test/1kg.mt/index/,+PCStruct{idx:+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(),JArray(List(JObject(List((start,JObject(List())), (end,JObject(List())), (includeStart,JBool(true)), (includeEnd,JBool(true)))))),null,Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_472,GetField(Ref(__iruid_472,struct{fil",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:5007,Integrability,Wrap,WrappedArray,5007,":+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(),JArray(List(JObject(List((start,JObject(List())), (end,JObject(List())), (includeStart,JBool(true)), (includeEnd,JBool(true)))))),null,Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_472,GetField(Ref(__iruid_472,struct{filePath: str, partitionCounts: int64}),filePath))),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/rows,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(idx),JArray(List(JObject(List((start,JObject(List((idx,JInt(0))))), (end,JObject(List((idx,JInt(10))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(10))))), (end,JObject(List((idx,JInt(20))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:5736,Integrability,Wrap,WrappedArray,5736,"ePath: str, partitionCounts: int64}),filePath))),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/rows,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(idx),JArray(List(JObject(List((start,JObject(List((idx,JInt(0))))), (end,JObject(List((idx,JInt(10))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(10))))), (end,JObject(List((idx,JInt(20))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(20))))), (end,JObject(List((idx,JInt(30))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(30))))), (end,JObject(List((idx,JInt(40))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(40))))), (end,JObject(List((idx,JInt(50))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(50))))), (end,JObject(List((idx,JInt(60))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(60))))), (end,JObject(List((idx,JInt(70))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(70))))), (end,JObject(List((idx,JInt(80))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(80))))), (end,JObject(List((idx,JInt(90))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(90))))), (end,JObject(List((idx,JInt(100))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))))),IndexSpec2(../index,{""name"":""TypedCodecSpec"",""_eType"":""EBaseStruct{first_idx:+EInt64,keys:+EArray[+EBaseSt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:10925,Integrability,Wrap,WrappedArray,10925,s.scala:95); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.apply(LoweringPass.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:314); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:308); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:12); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23); 	at is.hail.backend.service.ServiceBackend.userContext(ServiceBackend.scala:122); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11.apply(ServiceBackend.scala:308); 	at is.hail.ba,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:10946,Integrability,Wrap,WrappedArray,10946,is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.apply(LoweringPass.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:314); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:308); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:12); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23); 	at is.hail.backend.service.ServiceBackend.userContext(ServiceBackend.scala:122); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11.apply(ServiceBackend.scala:308); 	at is.hail.backend.service.Ser,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:8940,Security,Hash,HashMap,8940,"vType"":""Struct{children:Array[Struct{index_file_offset:Int64,first_idx:Int64,first_key:Struct{idx:Int32},first_record_offset:Int64,first_annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},struct{idx: int32},struct{},None),Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_473,GetField(Ref(__iruid_473,struct{filePath: str, partitionCounts: int64}),partitionCounts))),TableSpecWriter(gs://danking/workshop-test/1kg.mt,Table{global:Struct{},key:[idx],row:Struct{idx:Int32}},rows,globals,references,true))))),RelationalWriter(gs://danking/workshop-test/1kg.mt,true,Some((references,Set()))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:38); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:37); 	at is.hail.expr.ir.Memo.apply(RefEquality.scala:40); 	at is.hail.expr.ir.Requiredness.lookup(Requiredness.scala:41); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:616); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:615); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.Requiredness.analyzeIR(Requiredness.scala:615); 	at is.hail.expr.ir.Requiredness.analyze(Requiredness.scala:265); 	at is.hail.expr.ir.Requiredness.run(Requiredness.scala:91); 	at is.hail.expr.ir.Requiredness$.apply(Requiredness.scala:16); 	at is.hail.expr.ir.Requiredness$.apply(Requiredness.scala:21); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.transform(LoweringPass.scala:95); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:8954,Security,Hash,HashMap,8954,"uct{children:Array[Struct{index_file_offset:Int64,first_idx:Int64,first_key:Struct{idx:Int32},first_record_offset:Int64,first_annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},struct{idx: int32},struct{},None),Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_473,GetField(Ref(__iruid_473,struct{filePath: str, partitionCounts: int64}),partitionCounts))),TableSpecWriter(gs://danking/workshop-test/1kg.mt,Table{global:Struct{},key:[idx],row:Struct{idx:Int32}},rows,globals,references,true))))),RelationalWriter(gs://danking/workshop-test/1kg.mt,true,Some((references,Set()))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:38); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:37); 	at is.hail.expr.ir.Memo.apply(RefEquality.scala:40); 	at is.hail.expr.ir.Requiredness.lookup(Requiredness.scala:41); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:616); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:615); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.Requiredness.analyzeIR(Requiredness.scala:615); 	at is.hail.expr.ir.Requiredness.analyze(Requiredness.scala:265); 	at is.hail.expr.ir.Requiredness.run(Requiredness.scala:91); 	at is.hail.expr.ir.Requiredness$.apply(Requiredness.scala:16); 	at is.hail.expr.ir.Requiredness$.apply(Requiredness.scala:21); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.transform(LoweringPass.scala:95); 	at is.hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:159,Testability,test,test,159,"It also fails on this simpler example:; ```; In [1]: import hail as hl ; ...: ; ...: temp = hl.utils.range_table(100) ; ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-c548354b6e81; LOGGING: writing to /Users/dking/projects/hail/hail-20210107-1038-0.2.61-c548354b6e81.log; Traceback (most recent call last):; File ""<ipython-input-1-a2e56feaf799>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1092>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementExce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:341,Testability,LOG,LOGGING,341,"It also fails on this simpler example:; ```; In [1]: import hail as hl ; ...: ; ...: temp = hl.utils.range_table(100) ; ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-c548354b6e81; LOGGING: writing to /Users/dking/projects/hail/hail-20210107-1038-0.2.61-c548354b6e81.log; Traceback (most recent call last):; File ""<ipython-input-1-a2e56feaf799>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1092>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementExce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:427,Testability,log,log,427,"It also fails on this simpler example:; ```; In [1]: import hail as hl ; ...: ; ...: temp = hl.utils.range_table(100) ; ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-c548354b6e81; LOGGING: writing to /Users/dking/projects/hail/hail-20210107-1038-0.2.61-c548354b6e81.log; Traceback (most recent call last):; File ""<ipython-input-1-a2e56feaf799>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1092>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementExce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:562,Testability,test,test,562,"It also fails on this simpler example:; ```; In [1]: import hail as hl ; ...: ; ...: temp = hl.utils.range_table(100) ; ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-c548354b6e81; LOGGING: writing to /Users/dking/projects/hail/hail-20210107-1038-0.2.61-c548354b6e81.log; Traceback (most recent call last):; File ""<ipython-input-1-a2e56feaf799>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1092>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementExce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:3061,Testability,Assert,AssertSameLength,3061,"entException: key not found: RefEquality(WriteMetadata(Let(__iruid_465,CollectDistributedArray(StreamZip(ArrayBuffer(MakeStream(ArrayBuffer(Literal(struct{start: int32, end: int32},[0,10]), Literal(struct{start: int32, end: int32},[10,20]), Literal(struct{start: int32, end: int32},[20,30]), Literal(struct{start: int32, end: int32},[30,40]), Literal(struct{start: int32, end: int32},[40,50]), Literal(struct{start: int32, end: int32},[50,60]), Literal(struct{start: int32, end: int32},[60,70]), Literal(struct{start: int32, end: int32},[70,80]), Literal(struct{start: int32, end: int32},[80,90]), Literal(struct{start: int32, end: int32},[90,100])),stream<struct{start: int32, end: int32}>,false), MakeStream(ArrayBuffer(Str(""part-00-""), Str(""part-01-""), Str(""part-02-""), Str(""part-03-""), Str(""part-04-""), Str(""part-05-""), Str(""part-06-""), Str(""part-07-""), Str(""part-08-""), Str(""part-09-"")),stream<str>,false)),ArrayBuffer(__iruid_468, __iruid_469),MakeStruct(ArrayBuffer((oldCtx,Ref(__iruid_468,struct{start: int32, end: int32})), (writeCtx,Ref(__iruid_469,str)))),AssertSameLength),Literal(struct{},[]),__iruid_466,__iruid_467,WritePartition(Let(__iruid_470,GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),oldCtx),StreamMap(StreamRange(GetField(Ref(__iruid_470,struct{start: int32, end: int32}),start),GetField(Ref(__iruid_470,struct{start: int32, end: int32}),end),I32(1),false),__iruid_471,MakeStruct(ArrayBuffer((idx,Ref(__iruid_471,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_386)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:3986,Testability,test,test,3986,"entException: key not found: RefEquality(WriteMetadata(Let(__iruid_465,CollectDistributedArray(StreamZip(ArrayBuffer(MakeStream(ArrayBuffer(Literal(struct{start: int32, end: int32},[0,10]), Literal(struct{start: int32, end: int32},[10,20]), Literal(struct{start: int32, end: int32},[20,30]), Literal(struct{start: int32, end: int32},[30,40]), Literal(struct{start: int32, end: int32},[40,50]), Literal(struct{start: int32, end: int32},[50,60]), Literal(struct{start: int32, end: int32},[60,70]), Literal(struct{start: int32, end: int32},[70,80]), Literal(struct{start: int32, end: int32},[80,90]), Literal(struct{start: int32, end: int32},[90,100])),stream<struct{start: int32, end: int32}>,false), MakeStream(ArrayBuffer(Str(""part-00-""), Str(""part-01-""), Str(""part-02-""), Str(""part-03-""), Str(""part-04-""), Str(""part-05-""), Str(""part-06-""), Str(""part-07-""), Str(""part-08-""), Str(""part-09-"")),stream<str>,false)),ArrayBuffer(__iruid_468, __iruid_469),MakeStruct(ArrayBuffer((oldCtx,Ref(__iruid_468,struct{start: int32, end: int32})), (writeCtx,Ref(__iruid_469,str)))),AssertSameLength),Literal(struct{},[]),__iruid_466,__iruid_467,WritePartition(Let(__iruid_470,GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),oldCtx),StreamMap(StreamRange(GetField(Ref(__iruid_470,struct{start: int32, end: int32}),start),GetField(Ref(__iruid_470,struct{start: int32, end: int32}),end),I32(1),false),__iruid_471,MakeStruct(ArrayBuffer((idx,Ref(__iruid_471,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_386)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:4038,Testability,test,test,4038,"2})), (writeCtx,Ref(__iruid_469,str)))),AssertSameLength),Literal(struct{},[]),__iruid_466,__iruid_467,WritePartition(Let(__iruid_470,GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),oldCtx),StreamMap(StreamRange(GetField(Ref(__iruid_470,struct{start: int32, end: int32}),start),GetField(Ref(__iruid_470,struct{start: int32, end: int32}),end),I32(1),false),__iruid_471,MakeStruct(ArrayBuffer((idx,Ref(__iruid_471,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_386)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/rows/parts/,Some((gs://danking/workshop-test/1kg.mt/index/,+PCStruct{idx:+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray()",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:4607,Testability,test,test,4607,"}),start),GetField(Ref(__iruid_470,struct{start: int32, end: int32}),end),I32(1),false),__iruid_471,MakeStruct(ArrayBuffer((idx,Ref(__iruid_471,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_386)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/rows/parts/,Some((gs://danking/workshop-test/1kg.mt/index/,+PCStruct{idx:+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(),JArray(List(JObject(List((start,JObject(List())), (end,JObject(List())), (includeStart,JBool(true)), (includeEnd,JBool(true)))))),null,Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_472,GetField(Ref(__iruid_472,struct{fil",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:4705,Testability,test,test,4705,"iveWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/rows/parts/,Some((gs://danking/workshop-test/1kg.mt/index/,+PCStruct{idx:+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(),JArray(List(JObject(List((start,JObject(List())), (end,JObject(List())), (includeStart,JBool(true)), (includeEnd,JBool(true)))))),null,Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_472,GetField(Ref(__iruid_472,struct{filePath: str, partitionCounts: int64}),filePath))),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/rows,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBuff",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:5417,Testability,test,test,5417,":+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(),JArray(List(JObject(List((start,JObject(List())), (end,JObject(List())), (includeStart,JBool(true)), (includeEnd,JBool(true)))))),null,Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_472,GetField(Ref(__iruid_472,struct{filePath: str, partitionCounts: int64}),filePath))),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/rows,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(idx),JArray(List(JObject(List((start,JObject(List((idx,JInt(0))))), (end,JObject(List((idx,JInt(10))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(10))))), (end,JObject(List((idx,JInt(20))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:8606,Testability,test,test,8606,")))), JObject(List((start,JObject(List((idx,JInt(80))))), (end,JObject(List((idx,JInt(90))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(90))))), (end,JObject(List((idx,JInt(100))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))))),IndexSpec2(../index,{""name"":""TypedCodecSpec"",""_eType"":""EBaseStruct{first_idx:+EInt64,keys:+EArray[+EBaseStruct{key:+EBaseStruct{idx:+EInt32},offset:+EInt64,annotation:+EBaseStruct{}}]}"",""_vType"":""Struct{first_idx:Int64,keys:Array[Struct{key:Struct{idx:Int32},offset:Int64,annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},{""name"":""TypedCodecSpec"",""_eType"":""EBaseStruct{children:+EArray[+EBaseStruct{index_file_offset:+EInt64,first_idx:+EInt64,first_key:+EBaseStruct{idx:+EInt32},first_record_offset:+EInt64,first_annotation:+EBaseStruct{}}]}"",""_vType"":""Struct{children:Array[Struct{index_file_offset:Int64,first_idx:Int64,first_key:Struct{idx:Int32},first_record_offset:Int64,first_annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},struct{idx: int32},struct{},None),Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_473,GetField(Ref(__iruid_473,struct{filePath: str, partitionCounts: int64}),partitionCounts))),TableSpecWriter(gs://danking/workshop-test/1kg.mt,Table{global:Struct{},key:[idx],row:Struct{idx:Int32}},rows,globals,references,true))))),RelationalWriter(gs://danking/workshop-test/1kg.mt,true,Some((references,Set()))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:8746,Testability,test,test,8746,"""name"":""StreamBlockBufferSpec""}}}}},{""name"":""TypedCodecSpec"",""_eType"":""EBaseStruct{children:+EArray[+EBaseStruct{index_file_offset:+EInt64,first_idx:+EInt64,first_key:+EBaseStruct{idx:+EInt32},first_record_offset:+EInt64,first_annotation:+EBaseStruct{}}]}"",""_vType"":""Struct{children:Array[Struct{index_file_offset:Int64,first_idx:Int64,first_key:Struct{idx:Int32},first_record_offset:Int64,first_annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},struct{idx: int32},struct{},None),Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_473,GetField(Ref(__iruid_473,struct{filePath: str, partitionCounts: int64}),partitionCounts))),TableSpecWriter(gs://danking/workshop-test/1kg.mt,Table{global:Struct{},key:[idx],row:Struct{idx:Int32}},rows,globals,references,true))))),RelationalWriter(gs://danking/workshop-test/1kg.mt,true,Some((references,Set()))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:38); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:37); 	at is.hail.expr.ir.Memo.apply(RefEquality.scala:40); 	at is.hail.expr.ir.Requiredness.lookup(Requiredness.scala:41); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:616); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:615); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.Requiredness.analyzeIR(Requiredness.scala:615); 	at is.hail.expr.ir.Requiredness.analyze(Requiredness.scala:265); 	at is.hail.expr.ir.Re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:12130,Testability,log,logTime,12130,ckend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:314); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:308); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:12); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23); 	at is.hail.backend.service.ServiceBackend.userContext(ServiceBackend.scala:122); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11.apply(ServiceBackend.scala:308); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11.apply(ServiceBackend.scala:307); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1.apply(ServiceBackend.scala:307); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1.apply(ServiceBackend.scala:307); 	at is.hail.backend.service.ServiceBackend.statusForException(ServiceBackend.scala:230); 	at is.hail.backend.service.ServiceBackend.execute(ServiceBackend.scala:306); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.Gate,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:22,Usability,simpl,simpler,22,"It also fails on this simpler example:; ```; In [1]: import hail as hl ; ...: ; ...: temp = hl.utils.range_table(100) ; ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-c548354b6e81; LOGGING: writing to /Users/dking/projects/hail/hail-20210107-1038-0.2.61-c548354b6e81.log; Traceback (most recent call last):; File ""<ipython-input-1-a2e56feaf799>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1092>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 114, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 379, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementExce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693
https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:2183,Availability,Error,Error,2183," write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 116, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 395, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementException: key not found: RefEquality(WriteMetadata(Let(__iruid_369,CollectDistributedArray(StreamZip(ArrayBuffer(MakeStream(ArrayBuffer(Literal(struct{start: int32, end: int32},[0,10]), Literal(struct{start: int32, end: int32},[10,20]), Literal(struct{start: int32, end: int32},[20,30]), Literal(struct{start: int32, end: int32},[30,40]), Literal(struct{start: int32, end: int32},[40,50]), Literal(struct{start: int32, end: int32},[50,60]), Literal(struct{start: int32, end: int32},[60,70]), Literal(struct{start: int32, end: int32},[70,80]), Literal(struct{start: int32, end: int32},[80,90]), Literal(struct{start: int32, end: int32},[90,100])),stream<struct{start: int32, end: int32}>,false), MakeStream(ArrayBuffer(Str(""part-00-""), Str(""part-01-""), Str(""part-02-""), Str(""part-03-""), Str(""part-04-""), Str(""part-05-""), Str(""part-06-""), Str(""part-07-""), Str(""part-08-""), Str(""part-09-"")),stream<str>,false)),ArrayBuffe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601
https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:2235,Availability,Error,Error,2235," write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 116, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 395, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 44, in async_request; raise FatalError(f'Error from server: {result[""value""]}'); FatalError: Error from server: java.util.NoSuchElementException: key not found: RefEquality(WriteMetadata(Let(__iruid_369,CollectDistributedArray(StreamZip(ArrayBuffer(MakeStream(ArrayBuffer(Literal(struct{start: int32, end: int32},[0,10]), Literal(struct{start: int32, end: int32},[10,20]), Literal(struct{start: int32, end: int32},[20,30]), Literal(struct{start: int32, end: int32},[30,40]), Literal(struct{start: int32, end: int32},[40,50]), Literal(struct{start: int32, end: int32},[50,60]), Literal(struct{start: int32, end: int32},[60,70]), Literal(struct{start: int32, end: int32},[70,80]), Literal(struct{start: int32, end: int32},[80,90]), Literal(struct{start: int32, end: int32},[90,100])),stream<struct{start: int32, end: int32}>,false), MakeStream(ArrayBuffer(Str(""part-00-""), Str(""part-01-""), Str(""part-02-""), Str(""part-03-""), Str(""part-04-""), Str(""part-05-""), Str(""part-06-""), Str(""part-07-""), Str(""part-08-""), Str(""part-09-"")),stream<str>,false)),ArrayBuffe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601
https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:1069,Integrability,wrap,wrapper,1069,"I started hail this way:; ```; hailctl config set batch/billing_project hail; hailctl dev config set default_namespace default; HAIL_QUERY_BACKEND=service ipython; ```; ```ipython; In [1]: In [1]: import hail as hl ; ...: ...: ; ...: ...: temp = hl.utils.range_table(100) ; ...: ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; ...: ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-15a45cfb9b0f; LOGGING: writing to /Users/dking/projects/hail/hail-20210202-1642-0.2.61-15a45cfb9b0f.log; Traceback (most recent call last):; File ""<ipython-input-1-92be8dd8c99f>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1094>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 116, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 395, in retry_transient_errors; return await f(*args, **kwargs); File ""/Users/dking/projects/hail/hail/python/hail/backen",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601
https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:3780,Integrability,Wrap,WrappedArray,3780,"entException: key not found: RefEquality(WriteMetadata(Let(__iruid_369,CollectDistributedArray(StreamZip(ArrayBuffer(MakeStream(ArrayBuffer(Literal(struct{start: int32, end: int32},[0,10]), Literal(struct{start: int32, end: int32},[10,20]), Literal(struct{start: int32, end: int32},[20,30]), Literal(struct{start: int32, end: int32},[30,40]), Literal(struct{start: int32, end: int32},[40,50]), Literal(struct{start: int32, end: int32},[50,60]), Literal(struct{start: int32, end: int32},[60,70]), Literal(struct{start: int32, end: int32},[70,80]), Literal(struct{start: int32, end: int32},[80,90]), Literal(struct{start: int32, end: int32},[90,100])),stream<struct{start: int32, end: int32}>,false), MakeStream(ArrayBuffer(Str(""part-00-""), Str(""part-01-""), Str(""part-02-""), Str(""part-03-""), Str(""part-04-""), Str(""part-05-""), Str(""part-06-""), Str(""part-07-""), Str(""part-08-""), Str(""part-09-"")),stream<str>,false)),ArrayBuffer(__iruid_372, __iruid_373),MakeStruct(ArrayBuffer((oldCtx,Ref(__iruid_372,struct{start: int32, end: int32})), (writeCtx,Ref(__iruid_373,str)))),AssertSameLength),Literal(struct{},[]),__iruid_370,__iruid_371,WritePartition(Let(__iruid_374,GetField(Ref(__iruid_370,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),oldCtx),StreamMap(StreamRange(GetField(Ref(__iruid_374,struct{start: int32, end: int32}),start),GetField(Ref(__iruid_374,struct{start: int32, end: int32}),end),I32(1),false),__iruid_375,MakeStruct(ArrayBuffer((idx,Ref(__iruid_375,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_370,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_290)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601
https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:4395,Integrability,Wrap,WrappedArray,4395,"}),start),GetField(Ref(__iruid_374,struct{start: int32, end: int32}),end),I32(1),false),__iruid_375,MakeStruct(ArrayBuffer((idx,Ref(__iruid_375,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_370,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_290)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/rows/parts/,Some((gs://danking/workshop-test/1kg.mt/index/,+PCStruct{idx:+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(),JArray(List(JObject(List((start,JObject(List())), (end,JObject(List())), (includeStart,JBool(true)), (includeEnd,JBool(true)))))),null,Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_369,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_376,GetField(Ref(__iruid_376,struct{fil",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601
https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:5287,Integrability,Wrap,WrappedArray,5287,":+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(),JArray(List(JObject(List((start,JObject(List())), (end,JObject(List())), (includeStart,JBool(true)), (includeEnd,JBool(true)))))),null,Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_369,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_376,GetField(Ref(__iruid_376,struct{filePath: str, partitionCounts: int64}),filePath))),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/rows,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(idx),JArray(List(JObject(List((start,JObject(List((idx,JInt(0))))), (end,JObject(List((idx,JInt(10))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(10))))), (end,JObject(List((idx,JInt(20))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601
https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:6016,Integrability,Wrap,WrappedArray,6016,"ePath: str, partitionCounts: int64}),filePath))),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/rows,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(idx),JArray(List(JObject(List((start,JObject(List((idx,JInt(0))))), (end,JObject(List((idx,JInt(10))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(10))))), (end,JObject(List((idx,JInt(20))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(20))))), (end,JObject(List((idx,JInt(30))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(30))))), (end,JObject(List((idx,JInt(40))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(40))))), (end,JObject(List((idx,JInt(50))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(50))))), (end,JObject(List((idx,JInt(60))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(60))))), (end,JObject(List((idx,JInt(70))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(70))))), (end,JObject(List((idx,JInt(80))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(80))))), (end,JObject(List((idx,JInt(90))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(90))))), (end,JObject(List((idx,JInt(100))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))))),IndexSpec2(../index,{""name"":""TypedCodecSpec"",""_eType"":""EBaseStruct{first_idx:+EInt64,keys:+EArray[+EBaseSt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601
https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:11205,Integrability,Wrap,WrappedArray,11205,s.scala:95); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.apply(LoweringPass.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.backend.service.ServiceBackend.is$hail$backend$service$ServiceBackend$$execute(ServiceBackend.scala:321); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:337); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:334); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23); 	at is.hail.backend.service.ServiceBackend.userContext(ServiceBackend.scala:132); 	at is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601
https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:11226,Integrability,Wrap,WrappedArray,11226,is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15); 	at is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); 	at is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.apply(LoweringPass.scala:89); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14); 	at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:45); 	at is.hail.backend.service.ServiceBackend.is$hail$backend$service$ServiceBackend$$execute(ServiceBackend.scala:321); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:337); 	at is.hail.backend.service.ServiceBackend$$anonfun$execute$1$$anonfun$apply$11$$anonfun$apply$12.apply(ServiceBackend.scala:334); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:25); 	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:23); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:23); 	at is.hail.backend.service.ServiceBackend.userContext(ServiceBackend.scala:132); 	at is.hail.backend.servic,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601
https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:153,Modifiability,config,config,153,"I'm hitting default & my local build is latest hi/main (`15a45cfb9b0f8da01b2d0408993556f8391749e3`), still broke. I started hail this way:; ```; hailctl config set batch/billing_project hail; hailctl dev config set default_namespace default; HAIL_QUERY_BACKEND=service ipython; ```; ```ipython; In [1]: In [1]: import hail as hl ; ...: ...: ; ...: ...: temp = hl.utils.range_table(100) ; ...: ...: temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True) ; ...: ; Initializing Hail with default parameters...; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.61-15a45cfb9b0f; LOGGING: writing to /Users/dking/projects/hail/hail-20210202-1642-0.2.61-15a45cfb9b0f.log; Traceback (most recent call last):; File ""<ipython-input-1-92be8dd8c99f>"", line 4, in <module>; temp.write('gs://danking/workshop-test/1kg.mt', overwrite=True); File ""</Users/dking/miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-1094>"", line 2, in write; File ""/Users/dking/projects/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/Users/dking/projects/hail/hail/python/hail/table.py"", line 1271, in write; Env.backend().execute(ir.TableWrite(self._tir, ir.TableNativeWriter(output, overwrite, stage_locally, _codec_spec))); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 103, in execute; bucket=self._bucket); File ""/Users/dking/projects/hail/hail/python/hail/backend/service_backend.py"", line 48, in request; return async_to_blocking(retry_transient_errors(self.async_request, endpoint, **data)); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 116, in async_to_blocking; return asyncio.get_event_loop().run_until_complete(coro); File ""/Users/dking/miniconda3/lib/python3.7/asyncio/base_events.py"", line 587, in run_until_complete; return future.result(); File ""/Users/dking/projects/hail/hail/python/hailtop/utils/utils.py"", line 395, in ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601
