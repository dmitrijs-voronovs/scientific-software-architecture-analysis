id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:240,Integrability,message,message,240,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:; ```; mat_all = sc.read_loom(filename=""RSV.loom""); sc.pp.pca(mat_all); sc.pp.neighbors(mat_all); sc.tl.umap(mat_all); ```; The error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460
https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1790,Integrability,wrap,wrap,1790,"mple_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; url",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460
https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1311,Performance,Bottleneck,Bottleneck,1311,"--------------; TypeError Traceback (most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWeb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460
https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1345,Performance,cache,cached-property,1345,"(most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460
https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:2512,Usability,learn,learn,2512,"ies'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; urllib3 1.26.8; velocyto 0.17.17; wheel 0.37.1; xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460
https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:2750,Usability,learn,learn,2750,"ies'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; urllib3 1.26.8; velocyto 0.17.17; wheel 0.37.1; xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460
https://github.com/scverse/scanpy/issues/1579#issuecomment-1062411153:5,Deployability,update,updated,5,"Just updated to scanpy 1.8.2, but the problem persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062411153
https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609:39,Deployability,install,install,39,udpating umap-learn work for me . `pip install umap-learn==0.5.3`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609
https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609:14,Usability,learn,learn,14,udpating umap-learn work for me . `pip install umap-learn==0.5.3`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609
https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609:52,Usability,learn,learn,52,udpating umap-learn work for me . `pip install umap-learn==0.5.3`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1663797609
https://github.com/scverse/scanpy/pull/1583#issuecomment-765255875:42,Deployability,update,updates,42,"> * Shouldn't `var_df` should get similar updates to `obs_df`?. I would suggest a different PR to address this. . > * Could we get tests for `get.obs_df`/ `get.var_df` for the issues you addressed here (repeated indices)?. Sure, I added new tests to `get.obs_df` to check duplicated keys.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-765255875
https://github.com/scverse/scanpy/pull/1583#issuecomment-765255875:131,Testability,test,tests,131,"> * Shouldn't `var_df` should get similar updates to `obs_df`?. I would suggest a different PR to address this. . > * Could we get tests for `get.obs_df`/ `get.var_df` for the issues you addressed here (repeated indices)?. Sure, I added new tests to `get.obs_df` to check duplicated keys.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-765255875
https://github.com/scverse/scanpy/pull/1583#issuecomment-765255875:241,Testability,test,tests,241,"> * Shouldn't `var_df` should get similar updates to `obs_df`?. I would suggest a different PR to address this. . > * Could we get tests for `get.obs_df`/ `get.var_df` for the issues you addressed here (repeated indices)?. Sure, I added new tests to `get.obs_df` to check duplicated keys.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-765255875
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:732,Availability,error,error,732,"Fidel, sorry to say, but I've run into some issues. Most of these actually didn't have to do with this PR, but were additional things that broke from #1499. I'll give you a few examples of what I've found, mostly by contrast with the current behaviour of 1.6.1. ```python; import scanpy as sc, pandas as pd, numpy as np. M, N = (5, 3); adata = sc.AnnData(; X=np.zeros((M, N)),; obs=pd.DataFrame(; np.arange(M * 3).reshape((M, 3)),; columns=[""repeated_col"", ""repeated_col"", ""var_id""],; index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),; ),; var=pd.DataFrame(; index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),; ),; ); ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python; M, N = 5, 3; adata = sc.AnnData(; X=np.zeros((M, N)),; obs=pd.DataFrame(; np.arange(M * 2).reshape((M, 2)),; columns=[""repeated_col"", ""repeated_col""],; index=[f""cell_{i}"" for i in range(M)],; ),; var=pd.DataFrame(; index=[f""gene_{i}"" for i in range(N)],; ), ; ); sc.get.obs_df(adata, [""repeated_col""]); ```. ### This pr (gets both columns). ```; repeated_col repeated_col; obs_index ; cell_0 0 1; cell_1 3 4; cell_2 6 7; cell_3 9 10; cell_4 12 13; ```. ### 1.6 (errors). ```pytb; ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim); 140 ; 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):; --> 142 raise ValueError(; 143 f""Wrong number of items passed {len(self.values)}, ""; 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1; ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In thi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:755,Availability,down,downstream,755,"Fidel, sorry to say, but I've run into some issues. Most of these actually didn't have to do with this PR, but were additional things that broke from #1499. I'll give you a few examples of what I've found, mostly by contrast with the current behaviour of 1.6.1. ```python; import scanpy as sc, pandas as pd, numpy as np. M, N = (5, 3); adata = sc.AnnData(; X=np.zeros((M, N)),; obs=pd.DataFrame(; np.arange(M * 3).reshape((M, 3)),; columns=[""repeated_col"", ""repeated_col"", ""var_id""],; index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),; ),; var=pd.DataFrame(; index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),; ),; ); ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python; M, N = 5, 3; adata = sc.AnnData(; X=np.zeros((M, N)),; obs=pd.DataFrame(; np.arange(M * 2).reshape((M, 2)),; columns=[""repeated_col"", ""repeated_col""],; index=[f""cell_{i}"" for i in range(M)],; ),; var=pd.DataFrame(; index=[f""gene_{i}"" for i in range(N)],; ), ; ); sc.get.obs_df(adata, [""repeated_col""]); ```. ### This pr (gets both columns). ```; repeated_col repeated_col; obs_index ; cell_0 0 1; cell_1 3 4; cell_2 6 7; cell_3 9 10; cell_4 12 13; ```. ### 1.6 (errors). ```pytb; ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim); 140 ; 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):; --> 142 raise ValueError(; 143 f""Wrong number of items passed {len(self.values)}, ""; 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1; ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In thi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:1452,Availability,error,errors,1452,""", ""repeated_col"", ""var_id""],; index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),; ),; var=pd.DataFrame(; index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),; ),; ); ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python; M, N = 5, 3; adata = sc.AnnData(; X=np.zeros((M, N)),; obs=pd.DataFrame(; np.arange(M * 2).reshape((M, 2)),; columns=[""repeated_col"", ""repeated_col""],; index=[f""cell_{i}"" for i in range(M)],; ),; var=pd.DataFrame(; index=[f""gene_{i}"" for i in range(N)],; ), ; ); sc.get.obs_df(adata, [""repeated_col""]); ```. ### This pr (gets both columns). ```; repeated_col repeated_col; obs_index ; cell_0 0 1; cell_1 3 4; cell_2 6 7; cell_3 9 10; cell_4 12 13; ```. ### 1.6 (errors). ```pytb; ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim); 140 ; 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):; --> 142 raise ValueError(; 143 f""Wrong number of items passed {len(self.values)}, ""; 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1; ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python; M, N = 5, 3; adata = sc.AnnData(; X=np.zeros((M, N)),; obs=pd.DataFrame(; np.arange(M),; columns=[""var_id""],; index=[f""cell_{i}"" for i in range(M)],; ),; var=pd.DataFrame(; index=[""var_id",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:1909,Availability,error,error,1909,"th repeated column values, but I do think it's reasonable. ```python; M, N = 5, 3; adata = sc.AnnData(; X=np.zeros((M, N)),; obs=pd.DataFrame(; np.arange(M * 2).reshape((M, 2)),; columns=[""repeated_col"", ""repeated_col""],; index=[f""cell_{i}"" for i in range(M)],; ),; var=pd.DataFrame(; index=[f""gene_{i}"" for i in range(N)],; ), ; ); sc.get.obs_df(adata, [""repeated_col""]); ```. ### This pr (gets both columns). ```; repeated_col repeated_col; obs_index ; cell_0 0 1; cell_1 3 4; cell_2 6 7; cell_3 9 10; cell_4 12 13; ```. ### 1.6 (errors). ```pytb; ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim); 140 ; 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):; --> 142 raise ValueError(; 143 f""Wrong number of items passed {len(self.values)}, ""; 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1; ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python; M, N = 5, 3; adata = sc.AnnData(; X=np.zeros((M, N)),; obs=pd.DataFrame(; np.arange(M),; columns=[""var_id""],; index=[f""cell_{i}"" for i in range(M)],; ),; var=pd.DataFrame(; index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],; ), ; ); sc.get.obs_df(adata, [""var_id""]); ```. ### This pr (warns). ```; /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used.; warnings.warn(; Out[58]: ; var_id; obs_index ; cell_0 2; cell_1 5; cell_2 8; cell_3 11; cell_4 14; ```. ### 1.6 (errors). ```pytb; ------------------------------------------------------------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:2116,Availability,error,error,2116,"ns=[""repeated_col"", ""repeated_col""],; index=[f""cell_{i}"" for i in range(M)],; ),; var=pd.DataFrame(; index=[f""gene_{i}"" for i in range(N)],; ), ; ); sc.get.obs_df(adata, [""repeated_col""]); ```. ### This pr (gets both columns). ```; repeated_col repeated_col; obs_index ; cell_0 0 1; cell_1 3 4; cell_2 6 7; cell_3 9 10; cell_4 12 13; ```. ### 1.6 (errors). ```pytb; ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim); 140 ; 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):; --> 142 raise ValueError(; 143 f""Wrong number of items passed {len(self.values)}, ""; 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1; ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python; M, N = 5, 3; adata = sc.AnnData(; X=np.zeros((M, N)),; obs=pd.DataFrame(; np.arange(M),; columns=[""var_id""],; index=[f""cell_{i}"" for i in range(M)],; ),; var=pd.DataFrame(; index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],; ), ; ); sc.get.obs_df(adata, [""var_id""]); ```. ### This pr (warns). ```; /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used.; warnings.warn(; Out[58]: ; var_id; obs_index ; cell_0 2; cell_1 5; cell_2 8; cell_3 11; cell_4 14; ```. ### 1.6 (errors). ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-69be169f6a4f> in <module>; ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:2843,Availability,error,errors,2843,"number of items passed 2, placement implies 1; ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python; M, N = 5, 3; adata = sc.AnnData(; X=np.zeros((M, N)),; obs=pd.DataFrame(; np.arange(M),; columns=[""var_id""],; index=[f""cell_{i}"" for i in range(M)],; ),; var=pd.DataFrame(; index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],; ), ; ); sc.get.obs_df(adata, [""var_id""]); ```. ### This pr (warns). ```; /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used.; warnings.warn(; Out[58]: ; var_id; obs_index ; cell_0 2; cell_1 5; cell_2 8; cell_3 11; cell_4 14; ```. ### 1.6 (errors). ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-69be169f6a4f> in <module>; ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 171 for k, l in zip(keys, lookup_keys):; 172 if not use_raw or k in adata.obs.columns:; --> 173 df[k] = adata.obs_vector(l, layer=layer); 174 else:; 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer); 1362 ); 1363 layer = None; -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer); 1365 ; 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer); 156 ; 157 if (in_col + in_idx) == 2:; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:4238,Availability,error,error,4238," use_raw or k in adata.obs.columns:; --> 173 df[k] = adata.obs_vector(l, layer=layer); 174 else:; 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer); 1362 ); 1363 layer = None; -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer); 1365 ; 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer); 156 ; 157 if (in_col + in_idx) == 2:; --> 158 raise ValueError(; 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns""; 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns; ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:4516,Availability,error,errors,4516," 1363 layer = None; -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer); 1365 ; 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer); 156 ; 157 if (in_col + in_idx) == 2:; --> 158 raise ValueError(; 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns""; 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns; ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:5163,Availability,toler,tolerance,5163,"s that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:5476,Availability,error,error,5476,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:5520,Availability,error,error,5520,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:6011,Deployability,release,release,6011,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:6032,Deployability,patch,patch,6032,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:6071,Deployability,release,release,6071,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:5526,Integrability,message,message,5526,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:6041,Performance,perform,performance,6041,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:5971,Testability,test,tests,5971,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421
https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601:108,Availability,error,error,108,"@ivirshup regarding the three comments:; ; * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found; * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique; ```PYTHON; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); adata[:, ['gene-1']]; ```. ```; --------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-58-8d1a96772653> in <module>; ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index); 1085 def __getitem__(self, index: Index) -> ""AnnData"":; 1086 """"""Returns a sliced view of the object.""""""; -> 1087 oidx, vidx = self._normalize_indices(index); 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index); 1066 ; 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1068 return _normalize_indices(index, self.obs_names, self.var_names); 1069 ; 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1); 33 ax0, ax1 = unpack_index(index); 34 ax0 = _normalize_index(ax0, names0); ---> 35 ax1 = _normalize_index(ax1, names1); 36 return ax0, ax1; 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index); 95 return positions # np.ndarray[int]; 96 else: # indexer should b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601
https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601:293,Availability,error,error,293,"@ivirshup regarding the three comments:; ; * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found; * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique; ```PYTHON; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); adata[:, ['gene-1']]; ```. ```; --------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-58-8d1a96772653> in <module>; ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index); 1085 def __getitem__(self, index: Index) -> ""AnnData"":; 1086 """"""Returns a sliced view of the object.""""""; -> 1087 oidx, vidx = self._normalize_indices(index); 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index); 1066 ; 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1068 return _normalize_indices(index, self.obs_names, self.var_names); 1069 ; 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1); 33 ax0, ax1 = unpack_index(index); 34 ax0 = _normalize_index(ax0, names0); ---> 35 ax1 = _normalize_index(ax1, names1); 36 return ax0, ax1; 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index); 95 return positions # np.ndarray[int]; 96 else: # indexer should b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601
https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601:447,Availability,error,error,447,"@ivirshup regarding the three comments:; ; * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found; * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique; ```PYTHON; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); adata[:, ['gene-1']]; ```. ```; --------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-58-8d1a96772653> in <module>; ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index); 1085 def __getitem__(self, index: Index) -> ""AnnData"":; 1086 """"""Returns a sliced view of the object.""""""; -> 1087 oidx, vidx = self._normalize_indices(index); 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index); 1066 ; 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1068 return _normalize_indices(index, self.obs_names, self.var_names); 1069 ; 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1); 33 ax0, ax1 = unpack_index(index); 34 ax0 = _normalize_index(ax0, names0); ---> 35 ax1 = _normalize_index(ax1, names1); 36 return ax0, ax1; 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index); 95 return positions # np.ndarray[int]; 96 else: # indexer should b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601
https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601:587,Availability,error,error,587,"@ivirshup regarding the three comments:; ; * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found; * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique; ```PYTHON; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); adata[:, ['gene-1']]; ```. ```; --------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-58-8d1a96772653> in <module>; ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index); 1085 def __getitem__(self, index: Index) -> ""AnnData"":; 1086 """"""Returns a sliced view of the object.""""""; -> 1087 oidx, vidx = self._normalize_indices(index); 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index); 1066 ; 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1068 return _normalize_indices(index, self.obs_names, self.var_names); 1069 ; 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1); 33 ax0, ax1 = unpack_index(index); 34 ax0 = _normalize_index(ax0, names0); ---> 35 ax1 = _normalize_index(ax1, names1); 36 return ax0, ax1; 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index); 95 return positions # np.ndarray[int]; 96 else: # indexer should b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601
https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601:2220,Availability,toler,tolerance,2220,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index); 1085 def __getitem__(self, index: Index) -> ""AnnData"":; 1086 """"""Returns a sliced view of the object.""""""; -> 1087 oidx, vidx = self._normalize_indices(index); 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index); 1066 ; 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1068 return _normalize_indices(index, self.obs_names, self.var_names); 1069 ; 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1); 33 ax0, ax1 = unpack_index(index); 34 ax0 = _normalize_index(ax0, names0); ---> 35 ax1 = _normalize_index(ax1, names1); 36 return ax0, ax1; 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index); 95 return positions # np.ndarray[int]; 96 else: # indexer should be string array; ---> 97 positions = index.get_indexer(indexer); 98 if np.any(positions < 0):; 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```; * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601
https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601:2947,Testability,test,tests,2947,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index); 1085 def __getitem__(self, index: Index) -> ""AnnData"":; 1086 """"""Returns a sliced view of the object.""""""; -> 1087 oidx, vidx = self._normalize_indices(index); 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index); 1066 ; 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1068 return _normalize_indices(index, self.obs_names, self.var_names); 1069 ; 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1); 33 ax0, ax1 = unpack_index(index); 34 ax0 = _normalize_index(ax0, names0); ---> 35 ax1 = _normalize_index(ax1, names1); 36 return ax0, ax1; 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index); 95 return positions # np.ndarray[int]; 96 else: # indexer should be string array; ---> 97 positions = index.get_indexer(indexer); 98 if np.any(positions < 0):; 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```; * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601
https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601:2995,Testability,test,test,2995,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index); 1085 def __getitem__(self, index: Index) -> ""AnnData"":; 1086 """"""Returns a sliced view of the object.""""""; -> 1087 oidx, vidx = self._normalize_indices(index); 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index); 1066 ; 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1068 return _normalize_indices(index, self.obs_names, self.var_names); 1069 ; 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1); 33 ax0, ax1 = unpack_index(index); 34 ax0 = _normalize_index(ax0, names0); ---> 35 ax1 = _normalize_index(ax1, names1); 36 return ax0, ax1; 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index); 95 return positions # np.ndarray[int]; 96 else: # indexer should be string array; ---> 97 positions = index.get_indexer(indexer); 98 if np.any(positions < 0):; 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```; * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770217601
https://github.com/scverse/scanpy/pull/1583#issuecomment-770641710:317,Availability,error,error,317,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770641710
https://github.com/scverse/scanpy/pull/1583#issuecomment-770641710:486,Security,access,accessed,486,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770641710
https://github.com/scverse/scanpy/pull/1584#issuecomment-759955470:111,Testability,test,test,111,"Huh, it looks like pandas does not have the option for doing a stable sort here, so maybe we should change the test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584#issuecomment-759955470
https://github.com/scverse/scanpy/issues/1585#issuecomment-763304270:177,Deployability,pipeline,pipelines,177,"That'd be great, thanks!. I was thinking it could replace this step in the test builds:. https://github.com/theislab/scanpy/blob/5fc12f4a918e21f0c57937b787d52040db046f01/.azure-pipelines.yml#L78-L81. And was thinking of using azure for it instead of actions, just to consolidate CI stuff a bit. I was thinking a build and check could just be a separate job? Open to suggestions on this however.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585#issuecomment-763304270
https://github.com/scverse/scanpy/issues/1585#issuecomment-763304270:75,Testability,test,test,75,"That'd be great, thanks!. I was thinking it could replace this step in the test builds:. https://github.com/theislab/scanpy/blob/5fc12f4a918e21f0c57937b787d52040db046f01/.azure-pipelines.yml#L78-L81. And was thinking of using azure for it instead of actions, just to consolidate CI stuff a bit. I was thinking a build and check could just be a separate job? Open to suggestions on this however.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585#issuecomment-763304270
https://github.com/scverse/scanpy/issues/1585#issuecomment-763462643:138,Testability,test,tests,138,"> Well you have to build the wheels to run the check anyways. Oh, what I meant is that we sorta do a check in the same job as running the tests. I was thinking this should be separated out into a job which does ""build and check"" together, rather than including it in the testing job.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585#issuecomment-763462643
https://github.com/scverse/scanpy/issues/1585#issuecomment-763462643:271,Testability,test,testing,271,"> Well you have to build the wheels to run the check anyways. Oh, what I meant is that we sorta do a check in the same job as running the tests. I was thinking this should be separated out into a job which does ""build and check"" together, rather than including it in the testing job.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585#issuecomment-763462643
https://github.com/scverse/scanpy/pull/1587#issuecomment-761748373:404,Deployability,update,update,404,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live; - [x] `.gitignore` update; - [x] Remove failing test (just there as an example); - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587#issuecomment-761748373
https://github.com/scverse/scanpy/pull/1587#issuecomment-761748373:86,Testability,test,test,86,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live; - [x] `.gitignore` update; - [x] Remove failing test (just there as an example); - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587#issuecomment-761748373
https://github.com/scverse/scanpy/pull/1587#issuecomment-761748373:433,Testability,test,test,433,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live; - [x] `.gitignore` update; - [x] Remove failing test (just there as an example); - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587#issuecomment-761748373
https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128:157,Deployability,update,update,157,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 5fc12f4a918e21f0c57937b787d52040db046f01; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1587: Attach failing plots to CI results'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1587-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1587 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128
https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128:874,Deployability,continuous,continuous,874,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 5fc12f4a918e21f0c57937b787d52040db046f01; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1587: Attach failing plots to CI results'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1587-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1587 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128
https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128:885,Deployability,integrat,integration,885,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 5fc12f4a918e21f0c57937b787d52040db046f01; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1587: Attach failing plots to CI results'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1587-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1587 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128
https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128:885,Integrability,integrat,integration,885,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 5fc12f4a918e21f0c57937b787d52040db046f01; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1587: Attach failing plots to CI results'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1587-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1587 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128
https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128:860,Testability,test,tested,860,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 5fc12f4a918e21f0c57937b787d52040db046f01; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1587: Attach failing plots to CI results'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1587-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1587 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128
https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638:168,Deployability,integrat,integration,168,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638
https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638:168,Integrability,integrat,integration,168,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638
https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638:498,Modifiability,variab,variables,498,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638
https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638:150,Testability,benchmark,benchmarking,150,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638
https://github.com/scverse/scanpy/pull/1589#issuecomment-762495449:143,Deployability,release,release,143,"@ivirshup i see you opened the issue about graph initialization, though i haven't checked yet the things that were added. Do we wait for a new release of pynndescent and use the new features or we try to fix ingest with the current version of pynndescent (i have some code already that partially works)?. upd: ah, i see `init_graph` should work without hacks now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589#issuecomment-762495449
https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133:693,Availability,down,downside,693,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python; from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices); from_init._rp_forest = rp_forest; query_indices_init, query_distances_init = from_scratch.query(test); ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133
https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133:27,Deployability,release,release,27,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python; from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices); from_init._rp_forest = rp_forest; query_indices_init, query_distances_init = from_scratch.query(test); ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133
https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133:390,Deployability,release,release,390,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python; from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices); from_init._rp_forest = rp_forest; query_indices_init, query_distances_init = from_scratch.query(test); ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133
https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133:609,Deployability,release,release,609,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python; from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices); from_init._rp_forest = rp_forest; query_indices_init, query_distances_init = from_scratch.query(test); ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133
https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133:371,Testability,test,test,371,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python; from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices); from_init._rp_forest = rp_forest; query_indices_init, query_distances_init = from_scratch.query(test); ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589#issuecomment-762553133
https://github.com/scverse/scanpy/issues/1591#issuecomment-761747050:0,Availability,Ping,Ping,0,Ping @fidelram,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591#issuecomment-761747050
https://github.com/scverse/scanpy/issues/1591#issuecomment-762778317:48,Availability,error,error,48,Thanks for the nice report submission. The code error is caused by the categories being integers when the code expect an 'str'. This is an easy fix. . The mapping of labels to color being different when colors are not in `adata.uns` is because the mapping is not saved (Fig 2 and 3). It is also an easy fix but requires the modification of `adata.uns` to save the colors. This is already done in `sc.pl.embedding` so should be OK to do but I would like to know @ivirshup opinion. For Fig 1. when there are more cells the problem is not there or maybe is simply not visible But I have some idea on how to address it. For Fig 6 I really don't know.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591#issuecomment-762778317
https://github.com/scverse/scanpy/issues/1591#issuecomment-762778317:554,Usability,simpl,simply,554,Thanks for the nice report submission. The code error is caused by the categories being integers when the code expect an 'str'. This is an easy fix. . The mapping of labels to color being different when colors are not in `adata.uns` is because the mapping is not saved (Fig 2 and 3). It is also an easy fix but requires the modification of `adata.uns` to save the colors. This is already done in `sc.pl.embedding` so should be OK to do but I would like to know @ivirshup opinion. For Fig 1. when there are more cells the problem is not there or maybe is simply not visible But I have some idea on how to address it. For Fig 6 I really don't know.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591#issuecomment-762778317
https://github.com/scverse/scanpy/issues/1591#issuecomment-762781618:311,Testability,log,logic,311,> It is also an easy fix but requires the modification of adata.uns to save the colors. This seems reasonable since we already do it. Can you make sure to use the same `_get_palette` function for this? Maybe that should be moved out of `scatterplots.py` to somewhere more general?. We should probably have some logic for not saving the colors if the object is a view. Would have to think about this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591#issuecomment-762781618
https://github.com/scverse/scanpy/issues/1599#issuecomment-762792128:96,Testability,test,testing,96,"Hi @Pawan291,; It looks like your `adata.var_names` do not contain the cell cycle genes you are testing for. Is your dataset human or mouse?. Could you just print `adata.var_names[:10]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-762792128
https://github.com/scverse/scanpy/issues/1599#issuecomment-762881039:205,Availability,error,error,205,"So the format seems to be correct. Do you have the genes that are apparently missing in your object? You can check this for all the values that are mentioned in the `KeyError` output at the bottom of your error report. For example, is this statement `True`?; `'USP1' in [str(i) for i in adata.var_names]`. If not, your `adata` object might just be missing the cell cycle genes you are looking for. How many genes are in your object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-762881039
https://github.com/scverse/scanpy/issues/1599#issuecomment-767782350:344,Availability,error,error,344,"Had same problem. It seems it tries to look for all gene names present in the anndata object,; rather than the cell cycle genes that are requested?. I have previously checked that the s_genes and g2m_genes in call; sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found); are in the data. use_raw=False ; makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```; Gene_names_in_data:; Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',; 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',; ...; 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',; 'TMLHE-AS1', 'PRKY', 'UTY'],; dtype='object', length=3658). calculating cell cycle phase; computing score 'S_score'; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-82-30815e6b6381> in <module>; 20 print(gdata.var.index); 21 ; ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found); 23 ; 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs); 247 ctrl_size = min(len(s_genes), len(g2m_genes)); 248 # add s-score; --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs); 250 # add g2m-score; 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw); 128 _adata = adata.raw if use_raw else adata; 129 ; --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata; 131 if issparse(_adata_subset.X):; 132 obs_avg = pd.Series(. ~/.pyenv/version",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-767782350
https://github.com/scverse/scanpy/issues/1599#issuecomment-767782350:351,Availability,Error,Error,351,"Had same problem. It seems it tries to look for all gene names present in the anndata object,; rather than the cell cycle genes that are requested?. I have previously checked that the s_genes and g2m_genes in call; sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found); are in the data. use_raw=False ; makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```; Gene_names_in_data:; Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',; 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',; ...; 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',; 'TMLHE-AS1', 'PRKY', 'UTY'],; dtype='object', length=3658). calculating cell cycle phase; computing score 'S_score'; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-82-30815e6b6381> in <module>; 20 print(gdata.var.index); 21 ; ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found); 23 ; 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs); 247 ctrl_size = min(len(s_genes), len(g2m_genes)); 248 # add s-score; --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs); 250 # add g2m-score; 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw); 128 _adata = adata.raw if use_raw else adata; 129 ; --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata; 131 if issparse(_adata_subset.X):; 132 obs_avg = pd.Series(. ~/.pyenv/version",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-767782350
https://github.com/scverse/scanpy/issues/1599#issuecomment-767782350:357,Integrability,message,message,357,"Had same problem. It seems it tries to look for all gene names present in the anndata object,; rather than the cell cycle genes that are requested?. I have previously checked that the s_genes and g2m_genes in call; sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found); are in the data. use_raw=False ; makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```; Gene_names_in_data:; Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',; 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',; ...; 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',; 'TMLHE-AS1', 'PRKY', 'UTY'],; dtype='object', length=3658). calculating cell cycle phase; computing score 'S_score'; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-82-30815e6b6381> in <module>; 20 print(gdata.var.index); 21 ; ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found); 23 ; 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs); 247 ctrl_size = min(len(s_genes), len(g2m_genes)); 248 # add s-score; --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs); 250 # add g2m-score; 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw); 128 _adata = adata.raw if use_raw else adata; 129 ; --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata; 131 if issparse(_adata_subset.X):; 132 obs_avg = pd.Series(. ~/.pyenv/version",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-767782350
https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381:673,Performance,perform,performed,673,"Hello,; For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381
https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381:69,Safety,risk,risk,69,"Hello,; For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381
https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381:348,Testability,log,log-transformed,348,"Hello,; For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381
https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381:617,Testability,Log,Log-transformation,617,"Hello,; For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381
https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381:757,Testability,log,logged,757,"Hello,; For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381
https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257:682,Performance,perform,performed,682,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; > ; > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring.""; > ; > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257
https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257:70,Safety,risk,risk,70,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; > ; > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring.""; > ; > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257
https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257:355,Testability,log,log-transformed,355,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; > ; > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring.""; > ; > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257
https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257:626,Testability,Log,Log-transformation,626,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; > ; > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring.""; > ; > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257
https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257:772,Testability,log,logged,772,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; > ; > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring.""; > ; > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257
https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257:1542,Testability,log,logNormalize,1542,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; > ; > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring.""; > ; > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257
https://github.com/scverse/scanpy/issues/1599#issuecomment-1467415727:150,Integrability,message,message,150,You have already replied [here](https://github.com/theislab/single-cell-tutorial/issues/103#issuecomment-1313299316). Many apologies for missing this message.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1467415727
https://github.com/scverse/scanpy/pull/1601#issuecomment-763294926:69,Usability,simpl,simple,69,Would making pynndescent a requirement make the code here a bit more simple? I'm wondering if all the branches around umap versions and whether pynndescent is present could be removed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601#issuecomment-763294926
https://github.com/scverse/scanpy/pull/1601#issuecomment-763606483:173,Testability,test,test,173,"> rp_forest storage is still broken though. Is this something you are planning on fixing in this pr?. Also, what's broken about it? I would have thought we'd have a failing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601#issuecomment-763606483
https://github.com/scverse/scanpy/pull/1601#issuecomment-763734853:171,Safety,avoid,avoid,171,"I am checking, it seems that the format for `rp_forest `changed significantly. No, failing forest conversion won't cause any problems because it was deliberately coded to avoid problems. The code is in neighbors, and neighbors is critical functuionality and can work without `rp_forest`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601#issuecomment-763734853
https://github.com/scverse/scanpy/pull/1601#issuecomment-764398251:86,Security,validat,validate,86,"> I am checking, it seems that the format for rp_forest changed significantly. Can we validate the format? If so, can we just recalculate the forest if the format isn't what we expect?. Also, can you point me to the code where this is handled in `ingest`? I feel like something user visible should happen if there's malformed data in the object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601#issuecomment-764398251
https://github.com/scverse/scanpy/pull/1602#issuecomment-763582191:168,Integrability,depend,dependencies,168,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows; * Test against lower bounds of our requirements; * Test on Mac; * Dev builds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763582191
https://github.com/scverse/scanpy/pull/1602#issuecomment-763582191:621,Testability,Test,Test,621,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows; * Test against lower bounds of our requirements; * Test on Mac; * Dev builds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763582191
https://github.com/scverse/scanpy/pull/1602#issuecomment-763582191:640,Testability,Test,Test,640,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows; * Test against lower bounds of our requirements; * Test on Mac; * Dev builds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763582191
https://github.com/scverse/scanpy/pull/1602#issuecomment-763582191:689,Testability,Test,Test,689,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows; * Test against lower bounds of our requirements; * Test on Mac; * Dev builds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763582191
https://github.com/scverse/scanpy/pull/1602#issuecomment-763583210:63,Testability,test,testing,63,@ivirshup perfectly fine with me.; Removed 3.6 . So we are now testing against 3.7 and 3.8.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763583210
https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:914,Availability,recover,recover,914,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019
https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:550,Deployability,integrat,integrate,550,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019
https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:389,Integrability,Depend,Depends,389,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019
https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:415,Integrability,depend,depends,415,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019
https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:550,Integrability,integrat,integrate,550,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019
https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:914,Safety,recover,recover,914,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019
https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:509,Testability,test,testing,509,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019
https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:1029,Usability,clear,clear,1029,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019
https://github.com/scverse/scanpy/pull/1603#issuecomment-768033082:129,Deployability,release,release,129,"@giovp, that does seem bad, but we can consider that a separate issue, right?. Is the dendrogram bug important for the `squidgy` release?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603#issuecomment-768033082
https://github.com/scverse/scanpy/issues/1604#issuecomment-765363376:692,Availability,robust,robust,692,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public].; The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604#issuecomment-765363376
https://github.com/scverse/scanpy/issues/1604#issuecomment-765363376:362,Deployability,pipeline,pipeline,362,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public].; The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604#issuecomment-765363376
https://github.com/scverse/scanpy/issues/1604#issuecomment-765363376:633,Testability,log,logic,633,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public].; The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604#issuecomment-765363376
https://github.com/scverse/scanpy/issues/1605#issuecomment-766288497:19,Deployability,update,update,19,"Ah, thanks for the update!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1605#issuecomment-766288497
https://github.com/scverse/scanpy/issues/1606#issuecomment-766480303:1894,Integrability,wrap,wrap,1894,"lobs""] = pd.Categorical(blobs.obs[""blobs""]); blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]); # LinAlgError: Singular matrix; ```. <details>; <summary> Full traceback </summary>. ```pytb; ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <ipython-input-13-5685c001369c> in <module>; ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 204 # standardize across genes using a pooled variance estimator; 205 logg.info(""Standardizing Data across genes.\n""); --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key); 207 ; 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key); 102 ; 103 # compute pooled variance estimator; --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T); 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]); 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a); 544 signature = 'D->D' if isComplexType(t) else 'd->d'; 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular); --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj); 547 return wrap(ainv.astype(result_t, copy=False)); 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag); 86 ; 87 def _raise_linalgerror_singular(err, flag):; ---> 88 raise LinAlgError(""Singular matrix""); 89 ; 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix; ```. </details>. Does this occur in your data? You can check with:. ```python; pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606#issuecomment-766480303
https://github.com/scverse/scanpy/issues/1606#issuecomment-766480303:1011,Testability,log,logg,1011,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python; import scanpy as sc; import pandas as pd. blobs = sc.datasets.blobs(); blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]); blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]); # LinAlgError: Singular matrix; ```. <details>; <summary> Full traceback </summary>. ```pytb; ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <ipython-input-13-5685c001369c> in <module>; ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 204 # standardize across genes using a pooled variance estimator; 205 logg.info(""Standardizing Data across genes.\n""); --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key); 207 ; 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key); 102 ; 103 # compute pooled variance estimator; --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T); 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]); 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a); 544 signature = 'D->D' if isComplexType(t) else 'd->d'; 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular); --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj); 547 return wrap(ainv.astype(result_t, copy=False)); 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606#issuecomment-766480303
https://github.com/scverse/scanpy/issues/1606#issuecomment-766498505:34,Availability,Error,Error,34,"Thank you so much for the reply.; Error posted above is happening in two independent datasets possibly due to confounding factors as you mentioned. Though I don't quite comprehend what is a confounding factor in my specific case. . Here is the output of the batches and covariates as requested. ```python; pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]); ```. ```pytb; age_group Old YoungAdult Pediatric Fetal NewBorn; 384plate ; 27YOMP1 0 368 0 0 0; 27YOMP2 0 383 0 0 0; 27YOMP3 0 184 0 0 0; BM01152P1 0 384 0 0 0; BM01152P2 0 384 0 0 0; BM01158P1 0 0 382 0 0; BM01158P2 0 0 384 0 0; BM8182P1 0 55 0 0 0; FBFLP5L2 0 0 0 382 0; FBML1 0 0 0 378 0; FBML2 0 0 0 322 0; FBP5L1 0 0 0 377 0; FLP5L1 0 0 0 381 0; FLP5L3 0 0 0 338 0; FLP6L1 0 0 0 376 0; FLP6L2 0 0 0 383 0; FLP6L3 0 0 0 53 0; UCB250P1 0 0 0 0 382; UCB250P2 0 0 0 0 384; UCB250P3 0 0 0 0 150; UCB259P1 0 0 0 0 373; UCB259P2 0 0 0 0 376; UCB270P1 0 0 0 0 382; UCB270P2 0 0 0 0 128; UCBBMP4 0 41 272 0 63; UCBBMP5 0 0 0 0 325; hHSCP1B1 379 0 0 0 0; hHSCP1B2 359 0 0 0 0; hHSCP1B3 377 0 0 0 0; hHSCP2B1 376 0 0 0 0; hHSCP2B2 350 0 0 0 0; hHSCP3B1 371 0 0 0 0; hHSCP3B2 340 0 0 0 0; hHSCP4B1 290 0 0 0 0; ```. ```python; pd.crosstab(adata.obs[""384plate""], adata.obs[""sex""]); ```. ```pytb; sex F M U; 384plate ; 27YOMP1 0 368 0; 27YOMP2 0 383 0; 27YOMP3 0 184 0; BM01152P1 0 384 0; BM01152P2 0 384 0; BM01158P1 0 382 0; BM01158P2 0 384 0; BM8182P1 0 55 0; FBFLP5L2 0 55 327; FBML1 0 0 378; FBML2 0 0 322; FBP5L1 0 0 377; FLP5L1 0 0 381; FLP5L3 0 338 0; FLP6L1 0 376 0; FLP6L2 0 71 312; FLP6L3 0 0 53; UCB250P1 0 382 0; UCB250P2 0 384 0; UCB250P3 0 150 0; UCB259P1 0 373 0; UCB259P2 0 376 0; UCB270P1 0 382 0; UCB270P2 0 128 0; UCBBMP4 0 376 0; UCBBMP5 0 325 0; hHSCP1B1 0 379 0; hHSCP1B2 173 186 0; hHSCP1B3 216 161 0; hHSCP2B1 0 376 0; hHSCP2B2 350 0 0; hHSCP3B1 0 371 0; hHSCP3B2 158 182 0; hHSCP4B1 0 290 0; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606#issuecomment-766498505
https://github.com/scverse/scanpy/issues/1606#issuecomment-766739616:170,Testability,test,testing,170,"Hi @transcriptomics, ; If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606#issuecomment-766739616
https://github.com/scverse/scanpy/issues/1606#issuecomment-766739616:38,Usability,learn,learn,38,"Hi @transcriptomics, ; If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606#issuecomment-766739616
https://github.com/scverse/scanpy/issues/1606#issuecomment-766739616:106,Usability,guid,guide,106,"Hi @transcriptomics, ; If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606#issuecomment-766739616
https://github.com/scverse/scanpy/issues/1612#issuecomment-768657586:251,Testability,test,testing,251,"Do your input objects have different `dtype` values in `X`? I suspect that is what's causing this. If so, are the results very different? I would expect normalizing 64 bit vs 32 bit values to not be exactly the same (which is what `np.array_equal` is testing), but it's not good if the function is returning very different values. You can check this with `np.all_close`/ `np.isclose` or by looking at the distribution of the differences of the results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612#issuecomment-768657586
https://github.com/scverse/scanpy/issues/1613#issuecomment-768656224:107,Integrability,depend,dependencies,107,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613#issuecomment-768656224
https://github.com/scverse/scanpy/issues/1613#issuecomment-768656224:43,Testability,log,logging,43,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613#issuecomment-768656224
https://github.com/scverse/scanpy/pull/1614#issuecomment-771145250:25,Usability,clear,clear,25,Makes sense! Thanks it's clear now why the corr matrix,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614#issuecomment-771145250
https://github.com/scverse/scanpy/issues/1619#issuecomment-831008286:658,Integrability,depend,dependent,658,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well?. I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619#issuecomment-831008286
https://github.com/scverse/scanpy/issues/1619#issuecomment-831156546:68,Deployability,release,release,68,"Great, I think we may go for an experimental module as soon as next release (though, maybe the one after). If you form any strong opinions or have any cool use cases for the function, I'd definitely be interested in hearing about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619#issuecomment-831156546
https://github.com/scverse/scanpy/issues/1619#issuecomment-1906223696:28,Testability,test,testing,28,"Hi, did any one ever end up testing Densmap for scRNA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619#issuecomment-1906223696
https://github.com/scverse/scanpy/issues/1625#issuecomment-772438164:155,Deployability,release,released,155,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625#issuecomment-772438164
https://github.com/scverse/scanpy/issues/1625#issuecomment-772438164:325,Deployability,update,updated,325,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625#issuecomment-772438164
https://github.com/scverse/scanpy/issues/1625#issuecomment-772439376:84,Deployability,release,release,84,@LuckyMD I checked the commits. Between the Scanpy version on agando and the latest release the `marker_genes_overlap` was not changed. But maybe I am blind.; I'll go for the empirical route and try it out. Will report back!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625#issuecomment-772439376
https://github.com/scverse/scanpy/issues/1625#issuecomment-772439376:185,Integrability,rout,route,185,@LuckyMD I checked the commits. Between the Scanpy version on agando and the latest release the `marker_genes_overlap` was not changed. But maybe I am blind.; I'll go for the empirical route and try it out. Will report back!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625#issuecomment-772439376
https://github.com/scverse/scanpy/issues/1626#issuecomment-773090710:191,Integrability,wrap,wrapped,191,"We are currently using `sklearn.utils.check_random_state` to validate the argument for `random_state` in most places. Sometime's (especially in external) we pass the argument directly to the wrapped tool. In sklearn `0.24.1`, this looks like `np.random.RandomState` if you pass an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626#issuecomment-773090710
https://github.com/scverse/scanpy/issues/1626#issuecomment-773090710:61,Security,validat,validate,61,"We are currently using `sklearn.utils.check_random_state` to validate the argument for `random_state` in most places. Sometime's (especially in external) we pass the argument directly to the wrapped tool. In sklearn `0.24.1`, this looks like `np.random.RandomState` if you pass an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626#issuecomment-773090710
https://github.com/scverse/scanpy/pull/1628#issuecomment-781794583:50,Testability,log,logs,50,Something went wrong ... Please have a look at my logs. It seem that the branch you are trying to backport to does not exists.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1628#issuecomment-781794583
https://github.com/scverse/scanpy/issues/1629#issuecomment-776883494:939,Availability,ping,pinging,939,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test?. > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each?. since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629#issuecomment-776883494
https://github.com/scverse/scanpy/issues/1629#issuecomment-776883494:368,Testability,test,test,368,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test?. > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each?. since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629#issuecomment-776883494
https://github.com/scverse/scanpy/issues/1629#issuecomment-781323134:662,Performance,perform,perform,662,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway.; There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each?. As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629#issuecomment-781323134
https://github.com/scverse/scanpy/issues/1629#issuecomment-781323134:686,Testability,test,test,686,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway.; There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each?. As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629#issuecomment-781323134
https://github.com/scverse/scanpy/issues/1630#issuecomment-775584984:199,Testability,test,tests,199,"Yes, looking at the source code, the `""vst""` method uses the counts slot. https://github.com/satijalab/seurat/blob/69dbc46c82c5aaa5c2fd275a5d7bf28c04351c6a/R/preprocessing.R#L1817-L1825. In the unit tests, we test directly against the output of the Seurat method. The only known difference is how HVGs are aggregated across batches. I could not reproduce their results, as some of the R code was unclear to me. It would be helpful if someone fixed that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630#issuecomment-775584984
https://github.com/scverse/scanpy/issues/1630#issuecomment-775584984:209,Testability,test,test,209,"Yes, looking at the source code, the `""vst""` method uses the counts slot. https://github.com/satijalab/seurat/blob/69dbc46c82c5aaa5c2fd275a5d7bf28c04351c6a/R/preprocessing.R#L1817-L1825. In the unit tests, we test directly against the output of the Seurat method. The only known difference is how HVGs are aggregated across batches. I could not reproduce their results, as some of the R code was unclear to me. It would be helpful if someone fixed that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630#issuecomment-775584984
https://github.com/scverse/scanpy/issues/1630#issuecomment-775835519:25,Usability,clear,clear,25,"Thanks @adamgayoso, it's clear now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630#issuecomment-775835519
https://github.com/scverse/scanpy/pull/1632#issuecomment-775750950:318,Deployability,update,updated,318,"@ivirshup This looks great! Thanks. The issue with the cropping of the labels is quite annoying and indeed `bbox_inches=""tight""` should help. However, I don't think is nice to add that line for each example, but, on the other hand, if we add this to the scanpy code, many figures will be affected and would need to be updated. The same issue also affects the test images which most are cropped.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632#issuecomment-775750950
https://github.com/scverse/scanpy/pull/1632#issuecomment-775750950:359,Testability,test,test,359,"@ivirshup This looks great! Thanks. The issue with the cropping of the labels is quite annoying and indeed `bbox_inches=""tight""` should help. However, I don't think is nice to add that line for each example, but, on the other hand, if we add this to the scanpy code, many figures will be affected and would need to be updated. The same issue also affects the test images which most are cropped.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632#issuecomment-775750950
https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103:254,Availability,error,error,254,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>; <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3; * The error is:. ```pytb; scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric; ```. * The error will still occur as long as this has been added:. ```rst; .. plot::; :context: close-figs. >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> sc.tl.umap(adata); ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103
https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103:457,Availability,error,error,457,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>; <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3; * The error is:. ```pytb; scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric; ```. * The error will still occur as long as this has been added:. ```rst; .. plot::; :context: close-figs. >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> sc.tl.umap(adata); ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103
https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103:883,Availability,error,error,883,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>; <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3; * The error is:. ```pytb; scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric; ```. * The error will still occur as long as this has been added:. ```rst; .. plot::; :context: close-figs. >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> sc.tl.umap(adata); ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103
https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103:0,Deployability,Update,Update,0,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>; <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3; * The error is:. ```pytb; scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric; ```. * The error will still occur as long as this has been added:. ```rst; .. plot::; :context: close-figs. >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> sc.tl.umap(adata); ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103
https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103:84,Deployability,update,updated,84,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>; <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3; * The error is:. ```pytb; scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric; ```. * The error will still occur as long as this has been added:. ```rst; .. plot::; :context: close-figs. >>> import scanpy as sc; >>> adata = sc.datasets.pbmc68k_reduced(); >>> sc.tl.umap(adata); ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632#issuecomment-775780103
https://github.com/scverse/scanpy/pull/1632#issuecomment-775794535:295,Deployability,release,release,295,"For this PR, I'm thinking I'm going to wait on a response from @flying-sheep to figure out what to do about the sklearn intersphinx problem, then clean it up for a merge. A new issue will be opened up with all the functions that need examples, and those can be added through separate PRs as the release progresses.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632#issuecomment-775794535
https://github.com/scverse/scanpy/issues/1633#issuecomment-2283901014:344,Deployability,update,update,344,"Changing the behavior of the `style()` methods to no longer reset everything to the default values means that the parameter defaults of that methods *shouldn’t* mention the instance defaults, because it will no longer have anything to do with them. Specifying defaults in text descriptions practically doesn’t work, because nobody remembers to update the defaults in 2 places. So unless there is a DRY solution, let’s not do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633#issuecomment-2283901014
https://github.com/scverse/scanpy/issues/1636#issuecomment-776540580:125,Availability,error,error,125,"@galamm, thanks for the report! Also thanks for the example, very useful!. I think I see what the issue is here, though your error is unexpected. Are you using the most recent version of scanpy (1.7.0)?. The `gene_symbols` argument is supposed to refer to column in `var` that has more human readable gene names. The idea here is that you might have some unique identifier as `var_names` (like ensembl ids), but would have something more interpretable sorted in `adata.var[gene_symbols]`. On my machine, I get a `KeyError` when I run your example since there is no column `""TEST""` in `adata.var`. This is expected. It's strange to me that you get a `NameError`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636#issuecomment-776540580
https://github.com/scverse/scanpy/issues/1636#issuecomment-776540580:574,Testability,TEST,TEST,574,"@galamm, thanks for the report! Also thanks for the example, very useful!. I think I see what the issue is here, though your error is unexpected. Are you using the most recent version of scanpy (1.7.0)?. The `gene_symbols` argument is supposed to refer to column in `var` that has more human readable gene names. The idea here is that you might have some unique identifier as `var_names` (like ensembl ids), but would have something more interpretable sorted in `adata.var[gene_symbols]`. On my machine, I get a `KeyError` when I run your example since there is no column `""TEST""` in `adata.var`. This is expected. It's strange to me that you get a `NameError`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636#issuecomment-776540580
https://github.com/scverse/scanpy/issues/1636#issuecomment-1284430963:853,Availability,toler,tolerance,853,"Hello! I also have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```; $ adata.var.columns; $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',; 'dispersions', 'dispersions_norm', 'mean', 'std',; 'alternate_gene_symbols'],; dtype='object'). ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance); 3620 try:; -> 3621 return self._engine.get_loc(casted_key); 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'; ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```; ...; KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols'].""; ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636#issuecomment-1284430963
https://github.com/scverse/scanpy/issues/1636#issuecomment-1284430963:1679,Availability,error,error,1679,"so have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```; $ adata.var.columns; $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',; 'dispersions', 'dispersions_norm', 'mean', 'std',; 'alternate_gene_symbols'],; dtype='object'). ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance); 3620 try:; -> 3621 return self._engine.get_loc(casted_key); 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'; ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```; ...; KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols'].""; ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this parameter?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636#issuecomment-1284430963
https://github.com/scverse/scanpy/issues/1636#issuecomment-1284430963:1284,Security,hash,hashtable,1284,"so have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```; $ adata.var.columns; $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',; 'dispersions', 'dispersions_norm', 'mean', 'std',; 'alternate_gene_symbols'],; dtype='object'). ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance); 3620 try:; -> 3621 return self._engine.get_loc(casted_key); 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'; ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```; ...; KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols'].""; ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this parameter?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636#issuecomment-1284430963
https://github.com/scverse/scanpy/issues/1636#issuecomment-1284430963:1391,Security,hash,hashtable,1391,"so have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```; $ adata.var.columns; $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',; 'dispersions', 'dispersions_norm', 'mean', 'std',; 'alternate_gene_symbols'],; dtype='object'). ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance); 3620 try:; -> 3621 return self._engine.get_loc(casted_key); 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'; ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```; ...; KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols'].""; ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this parameter?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636#issuecomment-1284430963
https://github.com/scverse/scanpy/issues/1636#issuecomment-2048223788:1348,Availability,error,error,1348,"ith non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(); ID_dict = {; ""nbis-gene-777"":""MT451954"",; ""nbis-gene-775"":""MT451955"",; ""nbis-gene-3785"":""MT451956"",; ""nbis-gene-3784"":""MT451957"",; ""nbis-gene-23114"":""MT451958"",; ""nbis-gene-25113"":""MT451959"",; ""nbis-gene-3783"":""MT518195""; }; bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe.; And then I tried to plot the dotplot:. dict = {; ""Actin 1"": [""nbis-gene-777""],; ""Actin 2"": [""nbis-gene-775""],; ""Actin 3"": [""nbis-gene-3785""],; ""Actin 4"": [""nbis-gene-3784""],; ""Actin 5"": [""nbis-gene-23114""],; ""Actin 6"": [""nbis-gene-25113""],; ""Actin 7"": [""nbis-gene-3783""]; }; dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key); 3790 try:; -> 3791 return self._engine.get_loc(casted_key); 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636#issuecomment-2048223788
https://github.com/scverse/scanpy/issues/1636#issuecomment-2048223788:465,Security,access,accessions,465,"Hello! I'm running scanpy version 1.9.3 now and it seems that bug is still not fixed since it was found in scanpy 1.6.0. ; The situation is the same as in previous comments.; I created the new column in the adata.var with some names changed to the GenBank ID (I'm working with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(); ID_dict = {; ""nbis-gene-777"":""MT451954"",; ""nbis-gene-775"":""MT451955"",; ""nbis-gene-3785"":""MT451956"",; ""nbis-gene-3784"":""MT451957"",; ""nbis-gene-23114"":""MT451958"",; ""nbis-gene-25113"":""MT451959"",; ""nbis-gene-3783"":""MT518195""; }; bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe.; And then I tried to plot the dotplot:. dict = {; ""Actin 1"": [""nbis-gene-777""],; ""Actin 2"": [""nbis-gene-775""],; ""Actin 3"": [""nbis-gene-3785""],; ""Actin 4"": [""nbis-gene-3784""],; ""Actin 5"": [""nbis-gene-23114""],; ""Actin 6"": [""nbis-gene-25113""],; ""Actin 7"": [""nbis-gene-3783""]; }; dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key); 3790 try:; -> 3791 return self._engine.get_loc(casted_key); 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObje",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636#issuecomment-2048223788
https://github.com/scverse/scanpy/issues/1636#issuecomment-2048223788:1878,Security,hash,hashtable,1878,"ith non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(); ID_dict = {; ""nbis-gene-777"":""MT451954"",; ""nbis-gene-775"":""MT451955"",; ""nbis-gene-3785"":""MT451956"",; ""nbis-gene-3784"":""MT451957"",; ""nbis-gene-23114"":""MT451958"",; ""nbis-gene-25113"":""MT451959"",; ""nbis-gene-3783"":""MT518195""; }; bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe.; And then I tried to plot the dotplot:. dict = {; ""Actin 1"": [""nbis-gene-777""],; ""Actin 2"": [""nbis-gene-775""],; ""Actin 3"": [""nbis-gene-3785""],; ""Actin 4"": [""nbis-gene-3784""],; ""Actin 5"": [""nbis-gene-23114""],; ""Actin 6"": [""nbis-gene-25113""],; ""Actin 7"": [""nbis-gene-3783""]; }; dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key); 3790 try:; -> 3791 return self._engine.get_loc(casted_key); 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636#issuecomment-2048223788
https://github.com/scverse/scanpy/issues/1636#issuecomment-2048223788:1985,Security,hash,hashtable,1985,"ith non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(); ID_dict = {; ""nbis-gene-777"":""MT451954"",; ""nbis-gene-775"":""MT451955"",; ""nbis-gene-3785"":""MT451956"",; ""nbis-gene-3784"":""MT451957"",; ""nbis-gene-23114"":""MT451958"",; ""nbis-gene-25113"":""MT451959"",; ""nbis-gene-3783"":""MT518195""; }; bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe.; And then I tried to plot the dotplot:. dict = {; ""Actin 1"": [""nbis-gene-777""],; ""Actin 2"": [""nbis-gene-775""],; ""Actin 3"": [""nbis-gene-3785""],; ""Actin 4"": [""nbis-gene-3784""],; ""Actin 5"": [""nbis-gene-23114""],; ""Actin 6"": [""nbis-gene-25113""],; ""Actin 7"": [""nbis-gene-3783""]; }; dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key); 3790 try:; -> 3791 return self._engine.get_loc(casted_key); 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636#issuecomment-2048223788
https://github.com/scverse/scanpy/pull/1642#issuecomment-776841793:77,Availability,error,error,77,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642#issuecomment-776841793
https://github.com/scverse/scanpy/pull/1642#issuecomment-776841793:142,Testability,log,log,142,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642#issuecomment-776841793
https://github.com/scverse/scanpy/pull/1642#issuecomment-777188844:80,Availability,error,error,80,"How about a `check_values` argument which defaults to `True`, and replacing the error with a warning like:. ""Your data doesn't appear to be have integer values, but this method expects raw count data. Proceed with caution."". `check_values` is nice and generic, and could be used in other functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642#issuecomment-777188844
https://github.com/scverse/scanpy/pull/1642#issuecomment-777942673:373,Availability,error,error,373,"> Though if check_values is False, it shouldn't even run the check, correct?. Yes. > So the warning would be more like,; >; > ""'seurat_v3' flavor expects raw count data. Proceed with caution."". I was thinking the warning would only occur if `check_values=True` and non integer values were found. ""partial counts"" are common enough I'd agree it's not reasonable for this to error if the results are still meaningful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642#issuecomment-777942673
https://github.com/scverse/scanpy/pull/1642#issuecomment-778078171:90,Testability,test,test,90,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html; but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642#issuecomment-778078171
https://github.com/scverse/scanpy/pull/1642#issuecomment-778078171:213,Testability,test,test,213,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html; but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642#issuecomment-778078171
https://github.com/scverse/scanpy/pull/1642#issuecomment-778078171:223,Testability,log,logging,223,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html; but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642#issuecomment-778078171
https://github.com/scverse/scanpy/issues/1643#issuecomment-777194405:140,Testability,benchmark,benchmarks,140,"Definitely love to have more contributions! I believe @LuckyMD and @giovp are quite keen on having this in the library. Excited to see your benchmarks!. Side note: I'd definitely recommend looking into using `joblib` instead of `multiprocessing` for parallelization. It's a bit more simple to use, is much better about not oversubscribing your resources, and copying less data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-777194405
https://github.com/scverse/scanpy/issues/1643#issuecomment-777194405:283,Usability,simpl,simple,283,"Definitely love to have more contributions! I believe @LuckyMD and @giovp are quite keen on having this in the library. Excited to see your benchmarks!. Side note: I'd definitely recommend looking into using `joblib` instead of `multiprocessing` for parallelization. It's a bit more simple to use, is much better about not oversubscribing your resources, and copying less data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-777194405
https://github.com/scverse/scanpy/issues/1643#issuecomment-777196655:36,Testability,benchmark,benchmarks,36,Gotcha! I'll prioritize getting the benchmarks up and then I'll need some guidance on how to organize it to fit in scanpy's codebase. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-777196655
https://github.com/scverse/scanpy/issues/1643#issuecomment-777196655:74,Usability,guid,guidance,74,Gotcha! I'll prioritize getting the benchmarks up and then I'll need some guidance on how to organize it to fit in scanpy's codebase. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-777196655
https://github.com/scverse/scanpy/issues/1643#issuecomment-778222682:120,Performance,perform,performance,120,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-778222682
https://github.com/scverse/scanpy/issues/1643#issuecomment-778222682:216,Performance,optimiz,optimized,216,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-778222682
https://github.com/scverse/scanpy/issues/1643#issuecomment-778299582:253,Testability,benchmark,benchmarks,253,I looked at a few genes and it looks like I'm getting pretty similar residuals compared to the R implementation. Something weird is going on with the genes in the last couple images so I'm currently trying to figure that before generating more thorough benchmarks. (I clip negative values to zero to preserve sparsity structure). ![image](https://user-images.githubusercontent.com/16548075/107794002-c4dfc800-6d0b-11eb-8c69-eca5963a2cc4.png); ![image](https://user-images.githubusercontent.com/16548075/107794217-02445580-6d0c-11eb-9c12-a1fce473a69a.png); ![image](https://user-images.githubusercontent.com/16548075/107794044-d1642080-6d0b-11eb-9659-bd72e7a9aa99.png); ![image](https://user-images.githubusercontent.com/16548075/107794308-23a54180-6d0c-11eb-8b2e-59bb479b09af.png),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-778299582
https://github.com/scverse/scanpy/issues/1643#issuecomment-778671357:350,Performance,tune,tuned,350,- Here are the fit parameters vs the geometric mean expression applied to the 33k pbmc dataset. Looks pretty similar!. ![image](https://user-images.githubusercontent.com/16548075/107860398-faf87700-6df3-11eb-8c55-5befa70b350a.png). - Thanks for pointing me towards the joblib parallelization. I’ll work on applying it here. - Notebook incoming! Stay tuned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-778671357
https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730:219,Modifiability,variab,variables,219,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks!. few questions before opening the PR:; - do you think it's worth it to allow users to add other variables (beside log_umi) ?; - do you think it would be useful to add other models other than poisson?; - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them?; - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730
https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730:656,Modifiability,variab,variable,656,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks!. few questions before opening the PR:; - do you think it's worth it to allow users to add other variables (beside log_umi) ?; - do you think it would be useful to add other models other than poisson?; - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them?; - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730
https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730:453,Testability,test,testing,453,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks!. few questions before opening the PR:; - do you think it's worth it to allow users to add other variables (beside log_umi) ?; - do you think it would be useful to add other models other than poisson?; - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them?; - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730
https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730:497,Testability,test,tested,497,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks!. few questions before opening the PR:; - do you think it's worth it to allow users to add other variables (beside log_umi) ?; - do you think it would be useful to add other models other than poisson?; - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them?; - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730
https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730:575,Testability,test,test,575,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks!. few questions before opening the PR:; - do you think it's worth it to allow users to add other variables (beside log_umi) ?; - do you think it would be useful to add other models other than poisson?; - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them?; - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730
https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077:866,Availability,down,downstream,866,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame.; - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved.; - I think that would be pretty straightforward. What outputs are you referring to, specifically?; - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077
https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077:133,Modifiability,extend,extend,133,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame.; - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved.; - I think that would be pretty straightforward. What outputs are you referring to, specifically?; - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077
https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077:158,Modifiability,variab,variables,158,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame.; - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved.; - I think that would be pretty straightforward. What outputs are you referring to, specifically?; - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077
https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077:665,Testability,test,testing,665,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame.; - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved.; - I think that would be pretty straightforward. What outputs are you referring to, specifically?; - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077
https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077:374,Usability,simpl,simplest,374,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame.; - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved.; - I think that would be pretty straightforward. What outputs are you referring to, specifically?; - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077
https://github.com/scverse/scanpy/issues/1643#issuecomment-786378869:76,Safety,avoid,avoid,76,"I don't know how ""easy"" the density estimation problem here is, but I would avoid scott's whenever I can (e.g. https://aakinshin.net/posts/kde-bw/).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-786378869
https://github.com/scverse/scanpy/issues/1643#issuecomment-790524087:527,Testability,benchmark,benchmarking,527,"hi @atarashansky ,; sorryf or late reply again. ; I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right?. For the rest, I think the two main points that could be addressed before PR are:; - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking.; - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-790524087
https://github.com/scverse/scanpy/issues/1643#issuecomment-790524087:806,Testability,test,tests,806,"hi @atarashansky ,; sorryf or late reply again. ; I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right?. For the rest, I think the two main points that could be addressed before PR are:; - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking.; - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-790524087
https://github.com/scverse/scanpy/issues/1643#issuecomment-790524087:812,Testability,benchmark,benchmarks,812,"hi @atarashansky ,; sorryf or late reply again. ; I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right?. For the rest, I think the two main points that could be addressed before PR are:; - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking.; - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-790524087
https://github.com/scverse/scanpy/issues/1643#issuecomment-790524087:881,Testability,test,test,881,"hi @atarashansky ,; sorryf or late reply again. ; I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right?. For the rest, I think the two main points that could be addressed before PR are:; - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking.; - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-790524087
https://github.com/scverse/scanpy/issues/1643#issuecomment-790840382:403,Usability,simpl,simplified,403,"Hi @atarashansky and everyone following this interesting discussion!. I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`!. Looking forward to you thoughts on this :); Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-790840382
https://github.com/scverse/scanpy/issues/1643#issuecomment-790840382:1001,Usability,learn,learns,1001,"Hi @atarashansky and everyone following this interesting discussion!. I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`!. Looking forward to you thoughts on this :); Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-790840382
https://github.com/scverse/scanpy/issues/1643#issuecomment-790840382:1441,Usability,learn,learn,1441,"Hi @atarashansky and everyone following this interesting discussion!. I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`!. Looking forward to you thoughts on this :); Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-790840382
https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293:403,Integrability,wrap,wrapper,403,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector?. With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293
https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293:53,Safety,avoid,avoid,53,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector?. With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293
https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293:69,Usability,learn,learn,69,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector?. With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293
https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293:130,Usability,learn,learning,130,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector?. With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293
https://github.com/scverse/scanpy/issues/1643#issuecomment-791998671:102,Integrability,wrap,wrapper,102,"If I remember correctly, the SCTransform `vst` method uses `sqrt(n)` by default but the `SCTransform` wrapper in Seurat uses `sqrt(n/30)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-791998671
https://github.com/scverse/scanpy/issues/1643#issuecomment-792059556:34,Usability,learn,learning,34,"> What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector?. Regarding this -- yes, that's what we thought to do. Do you think it makes sense? However, for computational efficiency, I would threshold the genes by expression and take e.g. 1000 or even 100 genes with the highest average expression (for the purpose of estimating theta). Lowly-expressed genes don't really constrain theta much, it's highly-expressed ones that do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-792059556
https://github.com/scverse/scanpy/issues/1643#issuecomment-853445582:98,Deployability,update,updates,98,"Hi guys, I've been following this thread and it's been quiet recently :) wondering if there's any updates on incorporating ScTransform on Scanpy. Thanks!! 🙏🏼",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-853445582
https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200:333,Modifiability,layers,layers,333,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT.; ```python; import anndata2ri; from rpy2.robjects.packages import importr; from rpy2.robjects import r, pandas2ri; import numpy as np. anndata2ri.activate(); pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):; if layer:; mat = adata.layers[layer]; else:; mat = adata.X. # Set names for the input matrix; cell_names = adata.obs_names; gene_names = adata.var_names; r.assign('mat', mat.T); r.assign('cell_names', cell_names); r.assign('gene_names', gene_names); r('colnames(mat) <- cell_names'); r('rownames(mat) <- gene_names'). seurat = importr('Seurat'); r('seurat_obj <- CreateSeuratObject(mat)'). # Run; for k, v in kwargs.items():; r.assign(k, v); kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]); r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))); adata.layers['SCT_data'] = sct_data.T; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))); adata.layers['SCT_counts'] = sct_data.T; return adata; ```; ```python; adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA; R[write to console]: Place corrected count matrix in counts slot; R[write to console]: Set default assay to SCT. adata; layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```; Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200
https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200:1047,Modifiability,layers,layers,1047,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT.; ```python; import anndata2ri; from rpy2.robjects.packages import importr; from rpy2.robjects import r, pandas2ri; import numpy as np. anndata2ri.activate(); pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):; if layer:; mat = adata.layers[layer]; else:; mat = adata.X. # Set names for the input matrix; cell_names = adata.obs_names; gene_names = adata.var_names; r.assign('mat', mat.T); r.assign('cell_names', cell_names); r.assign('gene_names', gene_names); r('colnames(mat) <- cell_names'); r('rownames(mat) <- gene_names'). seurat = importr('Seurat'); r('seurat_obj <- CreateSeuratObject(mat)'). # Run; for k, v in kwargs.items():; r.assign(k, v); kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]); r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))); adata.layers['SCT_data'] = sct_data.T; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))); adata.layers['SCT_counts'] = sct_data.T; return adata; ```; ```python; adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA; R[write to console]: Place corrected count matrix in counts slot; R[write to console]: Set default assay to SCT. adata; layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```; Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200
https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200:1160,Modifiability,layers,layers,1160,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT.; ```python; import anndata2ri; from rpy2.robjects.packages import importr; from rpy2.robjects import r, pandas2ri; import numpy as np. anndata2ri.activate(); pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):; if layer:; mat = adata.layers[layer]; else:; mat = adata.X. # Set names for the input matrix; cell_names = adata.obs_names; gene_names = adata.var_names; r.assign('mat', mat.T); r.assign('cell_names', cell_names); r.assign('gene_names', gene_names); r('colnames(mat) <- cell_names'); r('rownames(mat) <- gene_names'). seurat = importr('Seurat'); r('seurat_obj <- CreateSeuratObject(mat)'). # Run; for k, v in kwargs.items():; r.assign(k, v); kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]); r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))); adata.layers['SCT_data'] = sct_data.T; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))); adata.layers['SCT_counts'] = sct_data.T; return adata; ```; ```python; adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA; R[write to console]: Place corrected count matrix in counts slot; R[write to console]: Set default assay to SCT. adata; layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```; Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200
https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200:1231,Modifiability,layers,layers,1231,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT.; ```python; import anndata2ri; from rpy2.robjects.packages import importr; from rpy2.robjects import r, pandas2ri; import numpy as np. anndata2ri.activate(); pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):; if layer:; mat = adata.layers[layer]; else:; mat = adata.X. # Set names for the input matrix; cell_names = adata.obs_names; gene_names = adata.var_names; r.assign('mat', mat.T); r.assign('cell_names', cell_names); r.assign('gene_names', gene_names); r('colnames(mat) <- cell_names'); r('rownames(mat) <- gene_names'). seurat = importr('Seurat'); r('seurat_obj <- CreateSeuratObject(mat)'). # Run; for k, v in kwargs.items():; r.assign(k, v); kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]); r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))); adata.layers['SCT_data'] = sct_data.T; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))); adata.layers['SCT_counts'] = sct_data.T; return adata; ```; ```python; adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA; R[write to console]: Place corrected count matrix in counts slot; R[write to console]: Set default assay to SCT. adata; layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```; Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200
https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200:1488,Modifiability,layers,layers,1488,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT.; ```python; import anndata2ri; from rpy2.robjects.packages import importr; from rpy2.robjects import r, pandas2ri; import numpy as np. anndata2ri.activate(); pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):; if layer:; mat = adata.layers[layer]; else:; mat = adata.X. # Set names for the input matrix; cell_names = adata.obs_names; gene_names = adata.var_names; r.assign('mat', mat.T); r.assign('cell_names', cell_names); r.assign('gene_names', gene_names); r('colnames(mat) <- cell_names'); r('rownames(mat) <- gene_names'). seurat = importr('Seurat'); r('seurat_obj <- CreateSeuratObject(mat)'). # Run; for k, v in kwargs.items():; r.assign(k, v); kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]); r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))); adata.layers['SCT_data'] = sct_data.T; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))); adata.layers['SCT_counts'] = sct_data.T; return adata; ```; ```python; adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA; R[write to console]: Place corrected count matrix in counts slot; R[write to console]: Set default assay to SCT. adata; layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```; Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200
https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901:275,Modifiability,variab,variables,275,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python; #[...]; r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition; r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'); r('diffDash <- gsub(""-"", ""_"", diffDash)'); r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'); filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')); filtout_indicator = np.in1d(adata.var_names, filtout_genes); adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object; #[...]; ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901
https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901:393,Modifiability,layers,layers,393,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python; #[...]; r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition; r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'); r('diffDash <- gsub(""-"", ""_"", diffDash)'); r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'); filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')); filtout_indicator = np.in1d(adata.var_names, filtout_genes); adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object; #[...]; ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901
https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901:480,Usability,simpl,simple,480,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python; #[...]; r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition; r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'); r('diffDash <- gsub(""-"", ""_"", diffDash)'); r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'); filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')); filtout_indicator = np.in1d(adata.var_names, filtout_genes); adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object; #[...]; ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901
https://github.com/scverse/scanpy/issues/1644#issuecomment-781238831:143,Testability,log,logic,143,@ivirshup Honestly don't know what I was thinking here- the parameters are clearly not passed through. Perhaps I broke things when rearranging logic in the PR. In in any case I'll submit a fix soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644#issuecomment-781238831
https://github.com/scverse/scanpy/issues/1644#issuecomment-781238831:75,Usability,clear,clearly,75,@ivirshup Honestly don't know what I was thinking here- the parameters are clearly not passed through. Perhaps I broke things when rearranging logic in the PR. In in any case I'll submit a fix soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644#issuecomment-781238831
https://github.com/scverse/scanpy/issues/1645#issuecomment-778224048:9,Availability,error,error,9,"From the error message, you may want to try to convert the dense matrix to sparse matrix format as follows:. ```python; from scipy.sparse impor csr_matrix; adata.X = csr_matrix(adata.X); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645#issuecomment-778224048
https://github.com/scverse/scanpy/issues/1645#issuecomment-778224048:15,Integrability,message,message,15,"From the error message, you may want to try to convert the dense matrix to sparse matrix format as follows:. ```python; from scipy.sparse impor csr_matrix; adata.X = csr_matrix(adata.X); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645#issuecomment-778224048
https://github.com/scverse/scanpy/issues/1645#issuecomment-778326318:118,Availability,error,error,118,"Sure, this works as a workaround. Thing is, if I call doublets like described in the scrublet README, I do not get an error:; This works:; ```python; import scrublet as scr; import scanpy as sc; adata = sc.datasets.paul15(). scrub = scr.Scrublet(X); doublet_scores, predicted_doublets = scrub.scrub_doublets(); ```; So is this upstream?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645#issuecomment-778326318
https://github.com/scverse/scanpy/issues/1645#issuecomment-788832663:291,Integrability,inject,injecting,291,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645#issuecomment-788832663
https://github.com/scverse/scanpy/issues/1645#issuecomment-788832663:291,Security,inject,injecting,291,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645#issuecomment-788832663
https://github.com/scverse/scanpy/issues/1645#issuecomment-788832663:551,Testability,test,test,551,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645#issuecomment-788832663
https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088:58,Availability,error,error,58,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088
https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088:64,Integrability,message,message,64,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088
https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088:24,Modifiability,enhance,enhancement,24,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088
https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088:140,Usability,clear,clear,140,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088
https://github.com/scverse/scanpy/issues/1646#issuecomment-778079358:45,Safety,predict,predicted,45,"I came across this when I wanted to plot the predicted doublets from scrublet. `predicted_doublet` is stored as boolean. So, I would like to have this plotted like a categorical. I realized that plotting actually works when using `pl.scatter`:. ```python; import scanpy as sc; adata = sc.datasets.blobs(); sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.scatter(adata, color='boolean', basis='pca'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-778079358
https://github.com/scverse/scanpy/issues/1646#issuecomment-778331854:268,Availability,error,error,268,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:; ```python; adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'); adata[adata.obs['boolean'].astype(bool)]; ```; This throws a key error:; ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-26-3fef793fe5bd> in <module>; ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index); 1085 def __getitem__(self, index: Index) -> ""AnnData"":; 1086 """"""Returns a sliced view of the object.""""""; -> 1087 oidx, vidx = self._normalize_indices(index); 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index); 1066 ; 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1068 return _normalize_indices(index, self.obs_names, self.var_names); 1069 ; 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1); 32 index = index[0].values, index[1]; 33 ax0, ax1 = unpack_index(index); ---> 34 ax0 = _normalize_index(ax0, names0); 35 ax1 = _normalize_index(ax1, names1); 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index); 99 not_found = indexer[positions < 0]; 100 raise KeyError(; --> 101 f""Values {list(not_found)}, from {list(indexer)}, ""; 102 ""are not valid obs/ var names or indices.""; 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'; ```; while this works:; ```python; adata[adata.obs['boolean'].astype(bool) == True]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-778331854
https://github.com/scverse/scanpy/issues/1646#issuecomment-778338666:29,Safety,avoid,avoid,29,you can make a new column to avoid overwriting the boolean. ```python; adata.obs['boolean_cat'] = adata.obs['boolean'].astype(str).astype('category'); ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-778338666
https://github.com/scverse/scanpy/issues/1646#issuecomment-1156714165:57,Modifiability,variab,variables,57,@ivirshup are you looking for default colors for boolean variables?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-1156714165
https://github.com/scverse/scanpy/pull/1649#issuecomment-802501619:32,Availability,avail,available,32,I believe @fidelram isn't super available for scanpy stuff until next month. I figured we could talk about this then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1649#issuecomment-802501619
https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748:36,Integrability,depend,depends,36,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748
https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748:364,Modifiability,layers,layers,364,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748
https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748:455,Modifiability,layers,layers,455,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748
https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748:130,Safety,avoid,avoid,130,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748
https://github.com/scverse/scanpy/issues/1650#issuecomment-1413256189:93,Modifiability,layers,layers,93,"> For example, before scaling, you can just store a copy of your data by e.g., calling adata.layers['normalized_unscaled] = adata.X. Why is .copy() not necessary here?. ie, adata.layers['normalized_unscaled] = adata.X.copy()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650#issuecomment-1413256189
https://github.com/scverse/scanpy/issues/1650#issuecomment-1413256189:179,Modifiability,layers,layers,179,"> For example, before scaling, you can just store a copy of your data by e.g., calling adata.layers['normalized_unscaled] = adata.X. Why is .copy() not necessary here?. ie, adata.layers['normalized_unscaled] = adata.X.copy()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650#issuecomment-1413256189
https://github.com/scverse/scanpy/issues/1651#issuecomment-779382361:30,Availability,error,error,30,"Sort of separate but also, an error writing such data with `adata.write(""results.h5ad"")`. Traceback:; ```; TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'uns/rank_genes_groups_filtered/names' of <class 'h5py._hl.files.File'> from /.; ```. `del adata.uns[""rank_genes_groups_filtered""]` and the .write() call succeeds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651#issuecomment-779382361
https://github.com/scverse/scanpy/issues/1651#issuecomment-779382361:180,Availability,error,error,180,"Sort of separate but also, an error writing such data with `adata.write(""results.h5ad"")`. Traceback:; ```; TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'uns/rank_genes_groups_filtered/names' of <class 'h5py._hl.files.File'> from /.; ```. `del adata.uns[""rank_genes_groups_filtered""]` and the .write() call succeeds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651#issuecomment-779382361
https://github.com/scverse/scanpy/issues/1652#issuecomment-779686831:118,Availability,avail,available,118,Found the problem. I was running the library using python v3.9 with numba v0.51. Compatibility for python 3.9 is only available in the (currently) latest version of numba v0.53. . This incompatibility generated the crash in the pynndescent library. Why it worked for certain sizes and no other remains a mistery...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-779686831
https://github.com/scverse/scanpy/issues/1652#issuecomment-789672144:31,Deployability,install,install,31,"Hi @mr-september , you have to install the prerelease version of Numba. pip install -pre numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789672144
https://github.com/scverse/scanpy/issues/1652#issuecomment-789672144:76,Deployability,install,install,76,"Hi @mr-september , you have to install the prerelease version of Numba. pip install -pre numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789672144
https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344:44,Deployability,install,install,44,"@gatocor Thanks, for me it worked with `pip install --pre numba`, ; terminal output:; ```; Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2); Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0); Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1); Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0); ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version?. Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344
https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344:655,Deployability,Update,Update,655,"@gatocor Thanks, for me it worked with `pip install --pre numba`, ; terminal output:; ```; Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2); Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0); Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1); Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0); ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version?. Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344
https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344:715,Deployability,install,installed,715,"@gatocor Thanks, for me it worked with `pip install --pre numba`, ; terminal output:; ```; Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2); Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0); Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1); Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0); ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version?. Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344
https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344:628,Performance,load,load,628,"@gatocor Thanks, for me it worked with `pip install --pre numba`, ; terminal output:; ```; Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2); Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0); Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1); Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0); ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version?. Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344
https://github.com/scverse/scanpy/issues/1652#issuecomment-789702914:57,Deployability,install,installed,57,You have to first uninstall the version you have already installed of numba.; pip uninstall numba; pip install --pre numba,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789702914
https://github.com/scverse/scanpy/issues/1652#issuecomment-789702914:103,Deployability,install,install,103,You have to first uninstall the version you have already installed of numba.; pip uninstall numba; pip install --pre numba,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789702914
https://github.com/scverse/scanpy/issues/1652#issuecomment-789707448:99,Deployability,install,installed,99,"@gatocor just tried it, but it says `Successfully uninstalled numba-0.53.0rc2`, then `Successfully installed numba-0.53.0rc2`. So it looks like my system/terminal has the right `numba` version, but I can't get Python Console to also see the new version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789707448
https://github.com/scverse/scanpy/issues/1652#issuecomment-789711337:28,Deployability,install,installing,28,"It is possible that you are installing it in a python pathway different than the one you are using to execute your code. I advise you to create a virtual environment (loot it up somewhere), and execute your single cell projects from it to make sure of the packages you have installed in each place.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789711337
https://github.com/scverse/scanpy/issues/1652#issuecomment-789711337:274,Deployability,install,installed,274,"It is possible that you are installing it in a python pathway different than the one you are using to execute your code. I advise you to create a virtual environment (loot it up somewhere), and execute your single cell projects from it to make sure of the packages you have installed in each place.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789711337
https://github.com/scverse/scanpy/issues/1652#issuecomment-790446699:222,Deployability,update,updated,222,"Yes, I made sure I was in the right venv. I found one workaround - open a new Python Console (and close the old one if you wish), this fresh Console will not have any previously imported modules, but new `import`s will be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-790446699
https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:537,Availability,error,error,537,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. .; > ; > The problem is even more dramatic for method=""umap"".; > ; > ### Minimal code sample (that we can copy&paste without having any data); > ```python; > import scanpy; > import numpy; > ; > matrix = numpy.random.uniform(size=[10000,1000]); > ; > adata = scanpy.AnnData(matrix); > ; > scanpy.pp.pca(adata,n_comps=10); > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10); > ```; > ; > The error I get:; > ---------------------------------------------------------------------------; > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context; yield; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block; self.lower_inst(inst); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst; val = self.lower_assign(ty, inst); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign; return self.lower_expr(ty, value); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr; res = self.lower_call(resty, expr); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call; res = self._lower_call_normal(fnty, expr, signature); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal; res = impl(self.builder, argvals, self.loc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__; res = self._imp(self._context, builder, self._sig, args, loc=loc); File ""C:\Users\RUTBO\AppData\L",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418
https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:721,Availability,error,errors,721,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. .; > ; > The problem is even more dramatic for method=""umap"".; > ; > ### Minimal code sample (that we can copy&paste without having any data); > ```python; > import scanpy; > import numpy; > ; > matrix = numpy.random.uniform(size=[10000,1000]); > ; > adata = scanpy.AnnData(matrix); > ; > scanpy.pp.pca(adata,n_comps=10); > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10); > ```; > ; > The error I get:; > ---------------------------------------------------------------------------; > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context; yield; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block; self.lower_inst(inst); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst; val = self.lower_assign(ty, inst); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign; return self.lower_expr(ty, value); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr; res = self.lower_call(resty, expr); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call; res = self._lower_call_normal(fnty, expr, signature); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal; res = impl(self.builder, argvals, self.loc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__; res = self._imp(self._context, builder, self._sig, args, loc=loc); File ""C:\Users\RUTBO\AppData\L",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418
https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:7539,Availability,error,errors,7539,"te-packages\numba\core\compiler_machinery.py"", line 269, in check; mangled = func(compiler_state); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass; lower.lower(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower; self.lower_normal_function(self.fndesc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function; entry_block_tail = self.lower_function_body(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body; self.lower_block(block); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block; self.lower_inst(inst); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__; self.gen.throw(type, value, traceback); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context; raise newerr.with_traceback(tb); numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418
https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:7627,Availability,error,errors,7627,"te-packages\numba\core\compiler_machinery.py"", line 269, in check; mangled = func(compiler_state); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass; lower.lower(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower; self.lower_normal_function(self.fndesc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function; entry_block_tail = self.lower_function_body(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body; self.lower_block(block); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block; self.lower_inst(inst); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__; self.gen.throw(type, value, traceback); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context; raise newerr.with_traceback(tb); numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418
https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:5002,Deployability,pipeline,pipeline,5002,"sers\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper; disp.compile(sig); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile; cres = self._compiler.compile(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile; status, retval = self._compile_cached(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached; retval = self._compile_core(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core; cres = compiler.compile_extra(self.targetdescr.typing_context,; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra; return pipeline.compile_extra(func); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra; return self._compile_bytecode(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode; return self._compile_core(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core; raise e; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core; pm.run(self.state); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run; raise patched_exception; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run; self._runPass(idx, pass_inst, state); File ""C:\Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418
https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:7673,Deployability,pipeline,pipeline,7673,"te-packages\numba\core\compiler_machinery.py"", line 269, in check; mangled = func(compiler_state); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass; lower.lower(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower; self.lower_normal_function(self.fndesc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function; entry_block_tail = self.lower_function_body(); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body; self.lower_block(block); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block; self.lower_inst(inst); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__; self.gen.throw(type, value, traceback); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context; raise newerr.with_traceback(tb); numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418
https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:2084,Integrability,wrap,wrapper,2084,"n39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign; return self.lower_expr(ty, value); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr; res = self.lower_call(resty, expr); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call; res = self._lower_call_normal(fnty, expr, signature); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal; res = impl(self.builder, argvals, self.loc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__; res = self._imp(self._context, builder, self._sig, args, loc=loc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper; return fn(*args, **kwargs); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl; state.stop = stop; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__; self[self._datamodel.get_field_position(field)] = value; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__; raise TypeError(""Invalid store of {value.type} to ""; TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>; sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418
https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:4094,Integrability,wrap,wrapper,4094,"packages\scanpy\neighbors\__init__.py"", line 139, in neighbors; neighbors.compute_neighbors(; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors; self._distances, self._connectivities = _compute_connectivities_umap(; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap; from umap.umap_ import fuzzy_simplicial_set; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>; from .umap_ import UMAP; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>; from umap.layouts import (; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>; def rdist(x, y):; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper; disp.compile(sig); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile; cres = self._compiler.compile(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile; status, retval = self._compile_cached(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached; retval = self._compile_core(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core; cres = compiler.compile_extra(self.targetdescr.typing_context,; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra; return pipeline.compile_extra(func); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418
https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:809,Deployability,integrat,integrate,809,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618
https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:921,Deployability,integrat,integrate,921,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618
https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:809,Integrability,integrat,integrate,809,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618
https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:921,Integrability,integrat,integrate,921,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618
https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:366,Testability,test,test,366,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618
https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:23,Usability,simpl,simple,23,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618
https://github.com/scverse/scanpy/pull/1654#issuecomment-781473683:13,Testability,test,test,13,I've added a test in https://github.com/theislab/scanpy/pull/1654/commits/189354eb0074140e3f2204d09b02aaa912d13934,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1654#issuecomment-781473683
https://github.com/scverse/scanpy/pull/1656#issuecomment-781413662:46,Testability,test,test,46,"That is a good question, I'm just porting our test suite that we used in 1.4.4.post1, so I assume this is an old default ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656#issuecomment-781413662
https://github.com/scverse/scanpy/pull/1656#issuecomment-781925241:273,Testability,test,test,273,Unfortunately I just know that the files were added in https://github.com/galaxyproject/tools-iuc/commit/8dc1fcd2a14f1548249fbc6570bcf399de83703d. If this shouldn't be happening anymore I suppose I could just change `adata.uns[key]['params']['groupby']` to a string in our test data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656#issuecomment-781925241
https://github.com/scverse/scanpy/pull/1656#issuecomment-800059016:19,Testability,test,test,19,"Yeah, updating our test data solved this, I don't think this is necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1656#issuecomment-800059016
https://github.com/scverse/scanpy/pull/1659#issuecomment-781357674:70,Testability,test,tests,70,"Never mind, just turned off the problem option for the purpose of the tests :-)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659#issuecomment-781357674
https://github.com/scverse/scanpy/pull/1659#issuecomment-782141395:6,Testability,test,test,6,"Okay, test added!. Couldn't test for use_approx_neighbors since we know from the above that one version of that breaks the CI. Also, 'stdev_doublet_rate' rate seems to have no impact, but I'm fairly sure it's passed correctly, so I'm going to blame the Scrublet code itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659#issuecomment-782141395
https://github.com/scverse/scanpy/pull/1659#issuecomment-782141395:28,Testability,test,test,28,"Okay, test added!. Couldn't test for use_approx_neighbors since we know from the above that one version of that breaks the CI. Also, 'stdev_doublet_rate' rate seems to have no impact, but I'm fairly sure it's passed correctly, so I'm going to blame the Scrublet code itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659#issuecomment-782141395
https://github.com/scverse/scanpy/pull/1659#issuecomment-783241825:17,Deployability,update,updates,17,> Thanks for the updates @pinin4fjords! LGTM. Thanks for the education / help. And for Scanpy of course :-),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1659#issuecomment-783241825
https://github.com/scverse/scanpy/pull/1663#issuecomment-781880180:23,Testability,test,tests,23,"Huh, I don't think the tests are checking what I thought they were. . 1. It doesn't look like you can have an AnnData object with a Dask array, what am I doing wrong here?. ```python; from anndata import AnnData; import numpy as np; import dask.array as da; from scipy.sparse import csr_matrix. X_total = [[1, 0], [3, 0], [5, 6]]. adata = AnnData(np.array(X_total), dtype='float32'); print(type(adata.X) # is a numpy matrix, as expected. adata = AnnData(csr_matrix(X_total), dtype='float32'); print(type(adata.X) # is a sparse matrix, as expected. adata = AnnData(da.from_array(X_total), dtype='float32'); print(type(adata.X) # is a numpy array NOT a dask array, not what I expected; ```. 2. The change I made to `_normalize_data()` changed coercion of `counts`, not `X`. When I stepped through this private function, it seemed like things were working the way I'd expected, but there's a lot of other stuff happening before & afterwards in `normalize_total()` which I haven't looked at much. What combinations of inputs to `_normalize_data()` need to be supported?; * numpjy `X`, numpy `counts`; * dask `X`, dask `counts` ; * csr_matrix `X`, csr_matrix `counts` . Combinations?; * numpjy `X`, dask `counts`; * dask `X`, numpy `counts`; * numpjy `X`, csr_matrix `counts`; * csr_matrix `X`, numpy `counts`; * dask `X`, csr_matrix `counts`; * csr_matrix `X`, dask `counts`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663#issuecomment-781880180
https://github.com/scverse/scanpy/pull/1663#issuecomment-782803190:1010,Integrability,wrap,wrapper,1010,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python; a = ad.AnnData(np.ones((1000, 100))); a.X = da.from_array(a.X); type(a.X); # dask.array.core.Array; ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663#issuecomment-782803190
https://github.com/scverse/scanpy/pull/1663#issuecomment-782803190:98,Testability,test,testing,98,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python; a = ad.AnnData(np.ones((1000, 100))); a.X = da.from_array(a.X); type(a.X); # dask.array.core.Array; ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663#issuecomment-782803190
https://github.com/scverse/scanpy/pull/1663#issuecomment-783235345:68,Deployability,integrat,integration,68,"Great to hear from both of you. I'd really love to have better Dask integration with AnnData and am excited to see these progress!. @ryan-williams, it'd great if you could open an issue over on anndata about this! I think that'd be a good place to discuss design considerations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663#issuecomment-783235345
https://github.com/scverse/scanpy/pull/1663#issuecomment-783235345:68,Integrability,integrat,integration,68,"Great to hear from both of you. I'd really love to have better Dask integration with AnnData and am excited to see these progress!. @ryan-williams, it'd great if you could open an issue over on anndata about this! I think that'd be a good place to discuss design considerations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663#issuecomment-783235345
https://github.com/scverse/scanpy/pull/1663#issuecomment-784836308:104,Modifiability,variab,variable,104,"So it seems that in every case, no matter what array type is given to `andata.X`, the `counts_per_cell` variable generated in `normalize_total()` is always being created as a numpy array. So I'm not sure why there was a note next to the line in `_normalize_data()` about not being able to use dask, because the input counts here are always numpy (because they've been created already in `normalize_total()`). Presumably this is not intended?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663#issuecomment-784836308
https://github.com/scverse/scanpy/pull/1669#issuecomment-782814776:174,Testability,test,test,174,"Thanks for all these fixes @mvdbeek!. It looks like the issue here is less about `use_raw` and more about `X` being a two dimensional dense array. I think a more appropriate test would be to check that: if the plots are made using the same data, regardless of whether that data is dense, sparse, or in raw it should look the same. Basically, I'd expand the test introduced in #1548. It might be worth waiting to hear back from @fidelram before that though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-782814776
https://github.com/scverse/scanpy/pull/1669#issuecomment-782814776:357,Testability,test,test,357,"Thanks for all these fixes @mvdbeek!. It looks like the issue here is less about `use_raw` and more about `X` being a two dimensional dense array. I think a more appropriate test would be to check that: if the plots are made using the same data, regardless of whether that data is dense, sparse, or in raw it should look the same. Basically, I'd expand the test introduced in #1548. It might be worth waiting to hear back from @fidelram before that though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-782814776
https://github.com/scverse/scanpy/pull/1669#issuecomment-799447066:74,Deployability,update,update,74,"I think we can work without this particular fix, we probably only need to update the test data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-799447066
https://github.com/scverse/scanpy/pull/1669#issuecomment-799447066:85,Testability,test,test,85,"I think we can work without this particular fix, we probably only need to update the test data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-799447066
https://github.com/scverse/scanpy/pull/1669#issuecomment-800046083:93,Testability,test,tests,93,"Nope, I don't think we can work around this. If the fix is not right, could someone take the tests that are included here and fix them ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-800046083
https://github.com/scverse/scanpy/pull/1669#issuecomment-801174773:752,Availability,mainten,maintenance,752,"At some point we changed the return values of the anndata slicing and that is why I think the check for sparse was needed. My recommendation is to replace this whole block. ```python; for g in _gene_names:; if adata.raw is not None and use_raw:; X_col = adata.raw[:, g].X; if gene_symbols:; g = adata.raw.var[gene_symbols][g]; else:; X_col = adata[:, g].X; if gene_symbols:; g = adata.var[gene_symbols][g]; if issparse(X_col):; X_col = X_col.toarray().flatten(); X_col = X_col.toarray().flatten(); new_gene_names.append(g); df[g] = X_col; ```. by ; ```python; df = sc.get.obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols; new_gene_names = df.columns; ```. `sc.get.obs_df` is a well tested function and using it makes it easier for maintenance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-801174773
https://github.com/scverse/scanpy/pull/1669#issuecomment-801174773:703,Testability,test,tested,703,"At some point we changed the return values of the anndata slicing and that is why I think the check for sparse was needed. My recommendation is to replace this whole block. ```python; for g in _gene_names:; if adata.raw is not None and use_raw:; X_col = adata.raw[:, g].X; if gene_symbols:; g = adata.raw.var[gene_symbols][g]; else:; X_col = adata[:, g].X; if gene_symbols:; g = adata.var[gene_symbols][g]; if issparse(X_col):; X_col = X_col.toarray().flatten(); X_col = X_col.toarray().flatten(); new_gene_names.append(g); df[g] = X_col; ```. by ; ```python; df = sc.get.obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols; new_gene_names = df.columns; ```. `sc.get.obs_df` is a well tested function and using it makes it easier for maintenance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-801174773
https://github.com/scverse/scanpy/pull/1669#issuecomment-804067298:94,Deployability,update,updates,94,"@mvdbeek does the solution from @fidelram solves the issue? if yes, would you be able to push updates to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-804067298
https://github.com/scverse/scanpy/pull/1669#issuecomment-826382269:1520,Testability,log,logging,1520,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1669?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > Merging [#1669](https://codecov.io/gh/theislab/scanpy/pull/1669?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (62eef47) into [master](https://codecov.io/gh/theislab/scanpy/commit/c488909a54e9ab1462186cca35b537426e4630db?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c488909) will **increase** coverage by `0.02%`.; > The diff coverage is `100.00%`. ```diff; @@ Coverage Diff @@; ## master #1669 +/- ##; ==========================================; + Coverage 71.18% 71.21% +0.02% ; ==========================================; Files 92 92 ; Lines 11190 11180 -10 ; ==========================================; - Hits 7966 7962 -4 ; + Misses 3224 3218 -6 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1669?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1669/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.74% <100.00%> (+0.47%)` | :arrow_up: |; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1669/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <0.00%> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-826382269
https://github.com/scverse/scanpy/pull/1669#issuecomment-826764814:129,Testability,test,tests,129,"Ok this should be good to go @ivirshup , I've incorporated the suggestion from @fidelram , thanks @mvdbeek for first attempt and tests!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-826764814
https://github.com/scverse/scanpy/pull/1669#issuecomment-827461688:296,Deployability,release,release,296,"Hi @ivirshup ,. just checked #1529 , that's a more general additions to `rank_genes_groups_matrixplot` and `rank_genes_groups_dotplot`, but does not address this bug of `violinplot` which has to do with sparse `adata.X`. This also adds a test for that case. Thanks for pointing it out.; I'll add release note and merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-827461688
https://github.com/scverse/scanpy/pull/1669#issuecomment-827461688:238,Testability,test,test,238,"Hi @ivirshup ,. just checked #1529 , that's a more general additions to `rank_genes_groups_matrixplot` and `rank_genes_groups_dotplot`, but does not address this bug of `violinplot` which has to do with sparse `adata.X`. This also adds a test for that case. Thanks for pointing it out.; I'll add release note and merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-827461688
https://github.com/scverse/scanpy/pull/1669#issuecomment-831161163:24,Deployability,release,release,24,"thanks @mvdbeek , added release, will go on and merge this as soon as tests pass",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-831161163
https://github.com/scverse/scanpy/pull/1669#issuecomment-831161163:70,Testability,test,tests,70,"thanks @mvdbeek , added release, will go on and merge this as soon as tests pass",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669#issuecomment-831161163
https://github.com/scverse/scanpy/issues/1670#issuecomment-782792403:155,Availability,error,error,155,"I'm having some trouble reproducing this. Can you provide a complete example that reproduces this. I need to be able to recreate the data that causes this error for you locally. On my end, this works:. ```python; import scanpy as sc. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); sc.tl.rank_genes_groups(pbmc, groupby=""louvain"", method=""wilcoxon""); sc.tl.filter_rank_genes_groups(; pbmc,; min_fold_change=1,; min_in_group_fraction=0.25,; max_out_group_fraction=0.5,; use_raw=False,; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-782792403
https://github.com/scverse/scanpy/issues/1670#issuecomment-783074166:203,Availability,error,error,203,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error?; * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783074166
https://github.com/scverse/scanpy/issues/1670#issuecomment-783074166:161,Deployability,upgrade,upgrade,161,"Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:. * If you upgrade scipy, do you still run into this error?; * Could you get the version info from an environment where you've only imported scanpy and run this command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783074166
https://github.com/scverse/scanpy/issues/1670#issuecomment-783075376:211,Availability,error,error,211,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:; > ; > * If you upgrade scipy, do you still run into this error?; > * Could you get the version info from an environment where you've only imported scanpy and run this command?. I will try to update scipy. Here is the output from only import scanpy:; BTW, everything works fine until I updated scanpy to 1.7.0. ```; anndata 0.7.4; scanpy 1.7.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.4; backcall 0.2.0; cairo 1.19.1; cffi 1.14.4; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; h5py 2.10.0; igraph 0.8.2; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; matplotlib 3.3.1; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.8; pandas 1.2.1; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.6; psutil 5.7.2; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.7.0; scipy 1.4.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; storemagic NA; tables 3.6.1; texttable 1.6.2; tornado 6.0.4; traitlets 4.3.3; wcwidth 0.2.5; zmq 19.0.2; zope NA; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]; Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2021-02-21 23:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783075376
https://github.com/scverse/scanpy/issues/1670#issuecomment-783075376:169,Deployability,upgrade,upgrade,169,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:; > ; > * If you upgrade scipy, do you still run into this error?; > * Could you get the version info from an environment where you've only imported scanpy and run this command?. I will try to update scipy. Here is the output from only import scanpy:; BTW, everything works fine until I updated scanpy to 1.7.0. ```; anndata 0.7.4; scanpy 1.7.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.4; backcall 0.2.0; cairo 1.19.1; cffi 1.14.4; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; h5py 2.10.0; igraph 0.8.2; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; matplotlib 3.3.1; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.8; pandas 1.2.1; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.6; psutil 5.7.2; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.7.0; scipy 1.4.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; storemagic NA; tables 3.6.1; texttable 1.6.2; tornado 6.0.4; traitlets 4.3.3; wcwidth 0.2.5; zmq 19.0.2; zope NA; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]; Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2021-02-21 23:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783075376
https://github.com/scverse/scanpy/issues/1670#issuecomment-783075376:345,Deployability,update,update,345,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:; > ; > * If you upgrade scipy, do you still run into this error?; > * Could you get the version info from an environment where you've only imported scanpy and run this command?. I will try to update scipy. Here is the output from only import scanpy:; BTW, everything works fine until I updated scanpy to 1.7.0. ```; anndata 0.7.4; scanpy 1.7.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.4; backcall 0.2.0; cairo 1.19.1; cffi 1.14.4; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; h5py 2.10.0; igraph 0.8.2; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; matplotlib 3.3.1; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.8; pandas 1.2.1; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.6; psutil 5.7.2; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.7.0; scipy 1.4.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; storemagic NA; tables 3.6.1; texttable 1.6.2; tornado 6.0.4; traitlets 4.3.3; wcwidth 0.2.5; zmq 19.0.2; zope NA; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]; Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2021-02-21 23:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783075376
https://github.com/scverse/scanpy/issues/1670#issuecomment-783075376:439,Deployability,update,updated,439,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:; > ; > * If you upgrade scipy, do you still run into this error?; > * Could you get the version info from an environment where you've only imported scanpy and run this command?. I will try to update scipy. Here is the output from only import scanpy:; BTW, everything works fine until I updated scanpy to 1.7.0. ```; anndata 0.7.4; scanpy 1.7.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.4; backcall 0.2.0; cairo 1.19.1; cffi 1.14.4; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; h5py 2.10.0; igraph 0.8.2; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; matplotlib 3.3.1; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.8; pandas 1.2.1; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.6; psutil 5.7.2; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.7.0; scipy 1.4.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; storemagic NA; tables 3.6.1; texttable 1.6.2; tornado 6.0.4; traitlets 4.3.3; wcwidth 0.2.5; zmq 19.0.2; zope NA; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]; Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2021-02-21 23:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783075376
https://github.com/scverse/scanpy/issues/1670#issuecomment-783075376:1650,Deployability,update,updated,1650,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:; > ; > * If you upgrade scipy, do you still run into this error?; > * Could you get the version info from an environment where you've only imported scanpy and run this command?. I will try to update scipy. Here is the output from only import scanpy:; BTW, everything works fine until I updated scanpy to 1.7.0. ```; anndata 0.7.4; scanpy 1.7.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.4; backcall 0.2.0; cairo 1.19.1; cffi 1.14.4; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; h5py 2.10.0; igraph 0.8.2; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; matplotlib 3.3.1; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.8; pandas 1.2.1; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.6; psutil 5.7.2; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.7.0; scipy 1.4.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; storemagic NA; tables 3.6.1; texttable 1.6.2; tornado 6.0.4; traitlets 4.3.3; wcwidth 0.2.5; zmq 19.0.2; zope NA; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]; Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2021-02-21 23:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783075376
https://github.com/scverse/scanpy/issues/1670#issuecomment-783075376:1596,Testability,log,logical,1596,"> Huh. This is really weird, since it looks like it's almost entirely due to scipy sparse indexing. Must have something to do with versions. Two things:; > ; > * If you upgrade scipy, do you still run into this error?; > * Could you get the version info from an environment where you've only imported scanpy and run this command?. I will try to update scipy. Here is the output from only import scanpy:; BTW, everything works fine until I updated scanpy to 1.7.0. ```; anndata 0.7.4; scanpy 1.7.0; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.4; backcall 0.2.0; cairo 1.19.1; cffi 1.14.4; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; h5py 2.10.0; igraph 0.8.2; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; louvain 0.7.0; matplotlib 3.3.1; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; numba 0.51.2; numexpr 2.7.1; numpy 1.19.1; packaging 20.8; pandas 1.2.1; parso 0.7.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.6; psutil 5.7.2; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.7.0; scipy 1.4.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; storemagic NA; tables 3.6.1; texttable 1.6.2; tornado 6.0.4; traitlets 4.3.3; wcwidth 0.2.5; zmq 19.0.2; zope NA; -----; IPython 7.17.0; jupyter_client 6.1.6; jupyter_core 4.6.3; notebook 6.1.3; -----; Python 3.8.2 (default, May 7 2020, 20:00:49) [GCC 7.3.0]; Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2021-02-21 23:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783075376
https://github.com/scverse/scanpy/issues/1670#issuecomment-783799732:2496,Availability,error,error,2496,"matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last); <ipython-input-102-4378df4ffefd> in <module>; ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1844 filename = self.filename; 1845 ; -> 1846 _write_h5ad(; 1847 Path(filename),; 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):; 91 # If adata.isbacked, X should already be up to date; ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs); 93 if ""raw/X"" in as_dense and isinstance(; 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw); 872 '1 positional argument'); 873 ; --> 874 return dispatch(args[0].__class__)(*args, **kw); 875 ; 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 189 except Exception as e:; 190 parent = _get_parent(elem); --> 191 raise type(e)(; 192 f""{e}\n\n""; 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783799732
https://github.com/scverse/scanpy/issues/1670#issuecomment-783799732:2701,Availability,error,error,2701,"matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last); <ipython-input-102-4378df4ffefd> in <module>; ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1844 filename = self.filename; 1845 ; -> 1846 _write_h5ad(; 1847 Path(filename),; 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):; 91 # If adata.isbacked, X should already be up to date; ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs); 93 if ""raw/X"" in as_dense and isinstance(; 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw); 872 '1 positional argument'); 873 ; --> 874 return dispatch(args[0].__class__)(*args, **kw); 875 ; 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 189 except Exception as e:; 190 parent = _get_parent(elem); --> 191 raise type(e)(; 192 f""{e}\n\n""; 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783799732
https://github.com/scverse/scanpy/issues/1670#issuecomment-783799732:1828,Integrability,wrap,wrapper,1828,"matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last); <ipython-input-102-4378df4ffefd> in <module>; ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1844 filename = self.filename; 1845 ; -> 1846 _write_h5ad(; 1847 Path(filename),; 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):; 91 # If adata.isbacked, X should already be up to date; ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs); 93 if ""raw/X"" in as_dense and isinstance(; 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw); 872 '1 positional argument'); 873 ; --> 874 return dispatch(args[0].__class__)(*args, **kw); 875 ; 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 189 except Exception as e:; 190 parent = _get_parent(elem); --> 191 raise type(e)(; 192 f""{e}\n\n""; 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783799732
https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773:282,Availability,avail,available,282,"Sorta!. ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773
https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773:297,Availability,down,downside,297,"Sorta!. ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773
https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773:185,Deployability,configurat,configuration,185,"Sorta!. ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773
https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773:410,Deployability,upgrade,upgrade,410,"Sorta!. ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773
https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773:185,Modifiability,config,configuration,185,"Sorta!. ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773
https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773:455,Security,access,access,455,"Sorta!. ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773
https://github.com/scverse/scanpy/issues/1675#issuecomment-783836263:98,Integrability,depend,dependency,98,"I'd add the line as a `.. note` to `neighbors`, but I'd also be fine with adding pynndescent as a dependency. We'd just need to get rid of the code that works with non-pynndescent search.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1675#issuecomment-783836263
https://github.com/scverse/scanpy/pull/1679#issuecomment-784310828:93,Usability,clear,clearly,93,"> we don't have pre-commit in place, we are discussing it here #1563 . I do check flake8 but clearly didn't do it this time. I thought at one point you guys were checking flake8 with CI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679#issuecomment-784310828
https://github.com/scverse/scanpy/pull/1679#issuecomment-784998888:220,Availability,reliab,reliable,220,"> I thought at one point you guys were checking flake8 with CI. We were, sorta. The CI tool we were using had pretty stochastic reporting (which isn't really what we want in a CI tool). Hopefully it'll be back in a more reliable form soon: https://github.com/theislab/scanpy/issues/1563#issuecomment-784922713",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679#issuecomment-784998888
https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648:157,Deployability,update,update,157,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 f7279f6342f1e4a340bae2a8d345c1c43b2097bb; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1679: enables highly_variable_genes_seurat_v3 to accept pseudocounts'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1679-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1679 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648
https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648:902,Deployability,continuous,continuous,902,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 f7279f6342f1e4a340bae2a8d345c1c43b2097bb; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1679: enables highly_variable_genes_seurat_v3 to accept pseudocounts'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1679-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1679 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648
https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648:913,Deployability,integrat,integration,913,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 f7279f6342f1e4a340bae2a8d345c1c43b2097bb; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1679: enables highly_variable_genes_seurat_v3 to accept pseudocounts'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1679-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1679 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648
https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648:913,Integrability,integrat,integration,913,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 f7279f6342f1e4a340bae2a8d345c1c43b2097bb; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1679: enables highly_variable_genes_seurat_v3 to accept pseudocounts'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1679-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1679 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648
https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648:888,Testability,test,tested,888,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 f7279f6342f1e4a340bae2a8d345c1c43b2097bb; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1679: enables highly_variable_genes_seurat_v3 to accept pseudocounts'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1679-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1679 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648
https://github.com/scverse/scanpy/pull/1680#issuecomment-785709933:26,Deployability,release,release,26,"Oops, forgot to ask for a release note",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680#issuecomment-785709933
https://github.com/scverse/scanpy/pull/1680#issuecomment-787580560:2,Deployability,Release,Release,2,"* Release note, yeah, a new PR.; * Backport, up to you. Docs should be fine to backport. [Instructions here](https://scanpy.readthedocs.io/en/latest/dev/versioning.html), but you basically just need to write a comment with the right format and a backport PR will be opened.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680#issuecomment-787580560
https://github.com/scverse/scanpy/pull/1680#issuecomment-802661441:7,Deployability,release,release,7,adding release note in #1740,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1680#issuecomment-802661441
https://github.com/scverse/scanpy/issues/1688#issuecomment-784969965:55,Testability,test,test,55,"Hi, for `method='wilcoxon'` this is [Wilcoxon rank-sum test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test), and the scores are U_1 from methods in in the link. Higher absolute value of score -> lower p-value (more evidence the levels of expression between groups are different), higher score indicates higher expression, lower score -> lower expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1688#issuecomment-784969965
https://github.com/scverse/scanpy/issues/1688#issuecomment-785029681:57,Testability,test,test,57,"> Hi, for `method='wilcoxon'` this is [Wilcoxon rank-sum test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test), and the scores are U_1 from methods in in the link. Higher absolute value of score -> lower p-value (more evidence the levels of expression between groups are different), higher score indicates higher expression, lower score -> lower expression. Thank you very much !!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1688#issuecomment-785029681
https://github.com/scverse/scanpy/pull/1689#issuecomment-785044073:202,Availability,down,down,202,"`E ImportError: cannot import name 'settings' from partially initialized module 'scanpy' (most likely due to a circular import) (/home/vsts/work/1/s/scanpy/__init__.py); `. Meh, it's hell to track this down now. I assume that autopep8 removed an unused variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785044073
https://github.com/scverse/scanpy/pull/1689#issuecomment-785044073:253,Modifiability,variab,variable,253,"`E ImportError: cannot import name 'settings' from partially initialized module 'scanpy' (most likely due to a circular import) (/home/vsts/work/1/s/scanpy/__init__.py); `. Meh, it's hell to track this down now. I assume that autopep8 removed an unused variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785044073
https://github.com/scverse/scanpy/pull/1689#issuecomment-785089639:209,Availability,error,errors,209,"```; $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff; ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785089639
https://github.com/scverse/scanpy/pull/1689#issuecomment-785089639:262,Availability,error,errors,262,"```; $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff; ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785089639
https://github.com/scverse/scanpy/pull/1689#issuecomment-785089639:24,Testability,test,tests,24,"```; $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff; ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785089639
https://github.com/scverse/scanpy/pull/1689#issuecomment-785089639:359,Testability,test,tests,359,"```; $ python -m scanpy.tests.blackdiff 10. /home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/setuptools_scm/git.py:68: UserWarning: ""/home/travis/build/theislab/scanpy"" is shallow and may cause errors. warnings.warn('""{}"" is shallow and may cause errors'.format(wd.path)). /home/travis/virtualenv/python3.7.1/bin/python: No module named scanpy.tests.blackdiff; ```. Not sure why this happened, but we well we're using black with pre-commit now anyways so w/e. Does this need fixing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785089639
https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068:254,Availability,error,error,254,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068
https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068:216,Modifiability,variab,variable,216,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068
https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068:452,Modifiability,variab,variables,452,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068
https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068:536,Modifiability,variab,variables,536,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068
https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068:352,Testability,test,tests,352,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068
https://github.com/scverse/scanpy/pull/1689#issuecomment-785871670:1529,Availability,error,error,1529,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this.""; >; > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785871670
https://github.com/scverse/scanpy/pull/1689#issuecomment-785871670:1491,Modifiability,variab,variable,1491,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this.""; >; > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785871670
https://github.com/scverse/scanpy/pull/1689#issuecomment-785871670:1631,Testability,test,tests,1631,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this.""; >; > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785871670
https://github.com/scverse/scanpy/pull/1689#issuecomment-785875783:985,Integrability,Depend,Depends,985,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785875783
https://github.com/scverse/scanpy/pull/1689#issuecomment-786522462:127,Integrability,Depend,Depends,127,"@Zethson thanks 💯 ! looks really good! Tbh I was expecting much worse, the changes with flake8 are pretty conservative imho. > Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I totally share this view, I think it's fine to use noqa as flag and come back later. I admit you guys are 1000x more experts in this, but from what I can judge I think it's fine to merge this as first attempt and then in subsequent PRs improve and address flake 8 fails. . Maybe worth to merge #1527 first? otherwise there will be potential conflicts more for phil.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-786522462
https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099:1089,Modifiability,config,config,1089,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added?. In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs; - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)); - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config.; - [x] Add annotation to `noqa`s.; - [x] Also add `TODO` annotation to `except Exception`s that have been added.; - [x] Turn off E731; - [x] Turn off the rule that doesn't allow multiple leading `#` in comments; - [x] Turn off F811 for tests (rule violated by using fixture as test argument); - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this.; - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099
https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099:1333,Testability,test,tests,1333,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added?. In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs; - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)); - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config.; - [x] Add annotation to `noqa`s.; - [x] Also add `TODO` annotation to `except Exception`s that have been added.; - [x] Turn off E731; - [x] Turn off the rule that doesn't allow multiple leading `#` in comments; - [x] Turn off F811 for tests (rule violated by using fixture as test argument); - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this.; - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099
https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099:1374,Testability,test,test,1374,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added?. In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs; - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)); - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config.; - [x] Add annotation to `noqa`s.; - [x] Also add `TODO` annotation to `except Exception`s that have been added.; - [x] Turn off E731; - [x] Turn off the rule that doesn't allow multiple leading `#` in comments; - [x] Turn off F811 for tests (rule violated by using fixture as test argument); - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this.; - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099
https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099:1449,Testability,test,tests,1449,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added?. In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs; - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)); - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config.; - [x] Add annotation to `noqa`s.; - [x] Also add `TODO` annotation to `except Exception`s that have been added.; - [x] Turn off E731; - [x] Turn off the rule that doesn't allow multiple leading `#` in comments; - [x] Turn off F811 for tests (rule violated by using fixture as test argument); - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this.; - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099
https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099:1493,Testability,test,tests,1493,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added?. In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs; - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)); - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config.; - [x] Add annotation to `noqa`s.; - [x] Also add `TODO` annotation to `except Exception`s that have been added.; - [x] Turn off E731; - [x] Turn off the rule that doesn't allow multiple leading `#` in comments; - [x] Turn off F811 for tests (rule violated by using fixture as test argument); - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this.; - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099
https://github.com/scverse/scanpy/pull/1689#issuecomment-787424021:899,Deployability,configurat,configuration,899,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added?. The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787424021
https://github.com/scverse/scanpy/pull/1689#issuecomment-787424021:838,Modifiability,config,config,838,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added?. The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787424021
https://github.com/scverse/scanpy/pull/1689#issuecomment-787424021:899,Modifiability,config,configuration,899,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added?. The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787424021
https://github.com/scverse/scanpy/pull/1689#issuecomment-787424021:929,Modifiability,config,config,929,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added?. The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787424021
https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782:539,Availability,failure,failure,539,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks?. How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest?. ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782
https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782:882,Availability,error,errors,882,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks?. How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest?. ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782
https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782:1180,Deployability,configurat,configuration,1180,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks?. How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest?. ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782
https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782:1289,Deployability,configurat,configuration,1289,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks?. How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest?. ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782
https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782:285,Integrability,message,messages,285,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks?. How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest?. ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782
https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782:1180,Modifiability,config,configuration,1180,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks?. How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest?. ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782
https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782:1210,Modifiability,config,config,1210,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks?. How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest?. ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782
https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782:1289,Modifiability,config,configuration,1289,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks?. How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest?. ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782
https://github.com/scverse/scanpy/pull/1689#issuecomment-787846457:217,Performance,queue,queued,217,"I reverted the setup.py change which hopefully means that an alrady-compliant setup.py from the flit PR will just pass [the check](https://github.com/theislab/scanpy/actions/runs/610117826) (once it runs, seems to be queued for a while now)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787846457
https://github.com/scverse/scanpy/pull/1689#issuecomment-789127611:96,Usability,feedback,feedback,96,I'll fix this PR up in ~10 days. Appreciate your comments @ivirshup and think that some of your feedback could have been mitigated by me not going for short cuts :). Will request (final hopefully) review then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-789127611
https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430:65,Deployability,configurat,configuration,65,"@ivirshup I hope that I caught all of your comments.; The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever.; Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430
https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430:313,Deployability,configurat,configuration,313,"@ivirshup I hope that I caught all of your comments.; The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever.; Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430
https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430:65,Modifiability,config,configuration,65,"@ivirshup I hope that I caught all of your comments.; The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever.; Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430
https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430:313,Modifiability,config,configuration,313,"@ivirshup I hope that I caught all of your comments.; The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever.; Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430
https://github.com/scverse/scanpy/pull/1689#issuecomment-801763494:62,Deployability,release,release,62,"> Looks good, thanks for all the work!; > ; > We should add a release note for this at some point, I'm just not sure where yet, probably a section for dev practices. Could you suggest a line for that?; > ; > I was unsure about the variable naming for PAGA, so I've decided to revert that. I couldn't get flake8 to call it a redefinition. :tada: ; Maybe ""Enabled flake8 (https://flake8.pycqa.org/en/latest/) pre-commit to run code style checks""?; Everything else might just be details that people will uncover anyways since the workflows might complain :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-801763494
https://github.com/scverse/scanpy/pull/1689#issuecomment-801763494:231,Modifiability,variab,variable,231,"> Looks good, thanks for all the work!; > ; > We should add a release note for this at some point, I'm just not sure where yet, probably a section for dev practices. Could you suggest a line for that?; > ; > I was unsure about the variable naming for PAGA, so I've decided to revert that. I couldn't get flake8 to call it a redefinition. :tada: ; Maybe ""Enabled flake8 (https://flake8.pycqa.org/en/latest/) pre-commit to run code style checks""?; Everything else might just be details that people will uncover anyways since the workflows might complain :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-801763494
https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892:228,Availability,error,error-reference,228,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@c943b93`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `71.86%`. [![Impacted file tree graph](https://codecov.io/gh/theislab/scanpy/pull/1693/graphs/tree.svg?width=650&height=150&src=pr&token=UsIEoV0aqg)](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #1693 +/- ##; =========================================; Coverage ? 71.32% ; =========================================; Files ? 89 ; Lines ? 10969 ; Branches ? 0 ; =========================================; Hits ? 7824 ; Misses ? 3145 ; Partials ? 0 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [scanpy/\_\_main\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L19fbWFpbl9fLnB5) | `0.00% <0.00%> (ø)` | |; | [scanpy/plotting/\_dotplot.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19kb3RwbG90LnB5) | `86.79% <ø> (ø)` | |; | [scanpy/plotting/\_matrixplot.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_preprocessing.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19wcmVwcm9jZXNzaW5nLnB5) | `87.75% <ø> (ø)` | |; | [scanpy/plotting/\_qc.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19xYy5weQ==) | `88.23% <ø> (ø)` | |; | [scanpy/plotting/\_rcmod.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19yY21vZC5weQ==) | `100.00% <ø> (ø)` | |; | [scanpy/plotting/\_stacked\_violin.py](https://codecov.io/gh/theislab,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892
https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892:3160,Deployability,update,update,3160,"?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_preprocessing.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19wcmVwcm9jZXNzaW5nLnB5) | `87.75% <ø> (ø)` | |; | [scanpy/plotting/\_qc.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19xYy5weQ==) | `88.23% <ø> (ø)` | |; | [scanpy/plotting/\_rcmod.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19yY21vZC5weQ==) | `100.00% <ø> (ø)` | |; | [scanpy/plotting/\_stacked\_violin.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19zdGFja2VkX3Zpb2xpbi5weQ==) | `83.75% <ø> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.27% <ø> (ø)` | |; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `67.70% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9zY2F0dGVycGxvdHMucHk=) | `86.80% <ø> (ø)` | |; | ... and [58 more](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=footer). Last update [c943b93...1cc4115](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892
https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892:3067,Energy Efficiency,Power,Powered,3067,"?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_preprocessing.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19wcmVwcm9jZXNzaW5nLnB5) | `87.75% <ø> (ø)` | |; | [scanpy/plotting/\_qc.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19xYy5weQ==) | `88.23% <ø> (ø)` | |; | [scanpy/plotting/\_rcmod.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19yY21vZC5weQ==) | `100.00% <ø> (ø)` | |; | [scanpy/plotting/\_stacked\_violin.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19zdGFja2VkX3Zpb2xpbi5weQ==) | `83.75% <ø> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.27% <ø> (ø)` | |; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `67.70% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9zY2F0dGVycGxvdHMucHk=) | `86.80% <ø> (ø)` | |; | ... and [58 more](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=footer). Last update [c943b93...1cc4115](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892
https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892:176,Usability,learn,learn,176,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`master@c943b93`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `71.86%`. [![Impacted file tree graph](https://codecov.io/gh/theislab/scanpy/pull/1693/graphs/tree.svg?width=650&height=150&src=pr&token=UsIEoV0aqg)](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## master #1693 +/- ##; =========================================; Coverage ? 71.32% ; =========================================; Files ? 89 ; Lines ? 10969 ; Branches ? 0 ; =========================================; Hits ? 7824 ; Misses ? 3145 ; Partials ? 0 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=tree) | Coverage Δ | |; |---|---|---|; | [scanpy/\_\_main\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L19fbWFpbl9fLnB5) | `0.00% <0.00%> (ø)` | |; | [scanpy/plotting/\_dotplot.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19kb3RwbG90LnB5) | `86.79% <ø> (ø)` | |; | [scanpy/plotting/\_matrixplot.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_preprocessing.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19wcmVwcm9jZXNzaW5nLnB5) | `87.75% <ø> (ø)` | |; | [scanpy/plotting/\_qc.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19xYy5weQ==) | `88.23% <ø> (ø)` | |; | [scanpy/plotting/\_rcmod.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19yY21vZC5weQ==) | `100.00% <ø> (ø)` | |; | [scanpy/plotting/\_stacked\_violin.py](https://codecov.io/gh/theislab,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892
https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892:2930,Usability,learn,learn,2930,"?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_preprocessing.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19wcmVwcm9jZXNzaW5nLnB5) | `87.75% <ø> (ø)` | |; | [scanpy/plotting/\_qc.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19xYy5weQ==) | `88.23% <ø> (ø)` | |; | [scanpy/plotting/\_rcmod.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19yY21vZC5weQ==) | `100.00% <ø> (ø)` | |; | [scanpy/plotting/\_stacked\_violin.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL19zdGFja2VkX3Zpb2xpbi5weQ==) | `83.75% <ø> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.27% <ø> (ø)` | |; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `67.70% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9zY2F0dGVycGxvdHMucHk=) | `86.80% <ø> (ø)` | |; | ... and [58 more](https://codecov.io/gh/theislab/scanpy/pull/1693/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=footer). Last update [c943b93...1cc4115](https://codecov.io/gh/theislab/scanpy/pull/1693?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1693#issuecomment-785678892
https://github.com/scverse/scanpy/issues/1694#issuecomment-787409697:141,Availability,avail,available,141,"Some ways I like to work with the code:. * Left side of screen is text editor, right side is terminal/ docs/ something else; * 86 characters available if I have a side bar open, 95 without; * Split code browser; * 84 columns with a side bar open, 95 without. 120 is too long for this. Also this is a pretty wide laptop screen (16-inch). I believe Alex uses a MacBook Air with even more limited screen real estate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1694#issuecomment-787409697
https://github.com/scverse/scanpy/issues/1698#issuecomment-787484724:106,Availability,redundant,redundant,106,hi @Hrovatin ; so was it useful for your task? Curious to hear. Couple of questions:; - why having a tool redundant between two packages? ; - what is SEMITONES?; thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787484724
https://github.com/scverse/scanpy/issues/1698#issuecomment-787484724:106,Safety,redund,redundant,106,hi @Hrovatin ; so was it useful for your task? Curious to hear. Couple of questions:; - why having a tool redundant between two packages? ; - what is SEMITONES?; thank you!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787484724
https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:87,Deployability,integrat,integrated,87,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982
https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:176,Deployability,integrat,integrated,176,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982
https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:421,Deployability,integrat,integration,421,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982
https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:1257,Deployability,continuous,continuous,1257,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982
https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:87,Integrability,integrat,integrated,87,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982
https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:176,Integrability,integrat,integrated,176,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982
https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:421,Integrability,integrat,integration,421,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982
https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:644,Integrability,depend,dependency,644,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982
https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:726,Integrability,depend,dependency,726,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982
https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:141,Modifiability,variab,variable,141,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982
https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:882,Modifiability,variab,variable,882,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982
https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:581,Performance,perform,performs,581,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982
https://github.com/scverse/scanpy/issues/1698#issuecomment-805564864:1795,Testability,test,testing,1795,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>; <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>; <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>; <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>; <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-805564864
https://github.com/scverse/scanpy/issues/1698#issuecomment-805564864:687,Usability,clear,clear,687,"I was thinking about these in the context of of feature selection, where you may want a principled cutoff for inclusion. From looking at this in one visium datasets and one single cell dataset. It looks like expected value for any gene with a high morans I were quite low. This was not the case for Geary's C on the umap connectivity with single cell data. Here are some plots around this. Values from permuting the order are in blue, measured values are in black. This only shows the genes which were in the 95th percentile of scores. I inverted the values of gearys C so it was easier to compare with morans I. The x-axis is score between 0 and 1, the y axis is gene rank. It's pretty clear there is much greater dispersion of expected value for Geary's C. <details>; <summary> Morans I UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266866-bac8c600-8cc8-11eb-96bc-922256b7e52e.png). </details>. <details>; <summary> Geary's C UMAP connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266847-b3092180-8cc8-11eb-8e1a-56b26c6bfe23.png). </details>. <details>; <summary> Morans I spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112269342-5b6cb500-8ccc-11eb-8339-b0b9512a5081.png). </details>. <details>; <summary> Geary's C spatial connectivity </summary>. ![image](https://user-images.githubusercontent.com/8238804/112266893-c5835b00-8cc8-11eb-931e-0169ccc0471f.png). </details>. Comparing distribution of scores for the single cell PBMC data:. ![image](https://user-images.githubusercontent.com/8238804/112268036-76d6c080-8cca-11eb-8d0d-a22c1e11ff7c.png). My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-805564864
https://github.com/scverse/scanpy/issues/1698#issuecomment-805574439:79,Integrability,depend,dependency,79,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin.; ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png); And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though).; ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-805574439
https://github.com/scverse/scanpy/issues/1698#issuecomment-805613901:117,Testability,test,testing,117,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...).; I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-805613901
https://github.com/scverse/scanpy/issues/1698#issuecomment-805613901:1019,Testability,test,test,1019,"> My current thinking is that Gearys C is more sensitive to sparse features, and may be more in need of significance testing. I think this is not as visible for visium data since features are less sparse. Really interesting comparison @ivirshup . I think it makes sense that is more sensible to sparsity because the score is not computed ""against a mean"" but against the neighborhood graph. In some sense, Moran's I is more smooth. . > And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though). indeed, they are computed against the null distribution that is computed from permutations. As a personal opinion, I don't think reporting p values for these type of statistics is very useful (as in, I personally wouldn't draw conclusions on significance, but more on effect size, and this holds true for a t-test as well imho...).; I should also say that for squidpy we might want to compute p-values out of completeness... but again I would refrain from looking at them. In that case I also think it's more fair since the graph comes from another source, and is not computed from gexp similarity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-805613901
https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639:423,Modifiability,variab,variability,423,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:; I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode.; <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:; <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639
https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639:192,Usability,clear,clear,192,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:; I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode.; <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:; <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639
https://github.com/scverse/scanpy/issues/1698#issuecomment-826441704:78,Availability,down,down,78,"Ha, no I'm not on the mattermost. ivirshup@gmail.com. Though if you could cut down the dataset to something small and more shareable that would be helpful as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826441704
https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541:548,Modifiability,variab,variables,548,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>; <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes; * This is especially weird since all values are changing if I run the function on the full set of variables; * If I use a smaller interval size (10), I don't get any varying results; * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent?. -----------------. This is so weird. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):; sub = X[idx]; result = np.ones(sub.shape[0], dtype=bool); first = func(g, sub). for i in range(tries):; result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each; samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while; results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({; ""var_idx"": samples.flatten(),; ""consistent"": results.flatten(), ; ""sample"": np.repeat(np.arange(adata.n_vars), 100),; ""order"": np.tile(np.arange(100), adata.n_vars),; }). df.groupby(""order"").mean()[""consistent""].plot(); ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541
https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541:568,Modifiability,variab,variable,568,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>; <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes; * This is especially weird since all values are changing if I run the function on the full set of variables; * If I use a smaller interval size (10), I don't get any varying results; * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent?. -----------------. This is so weird. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):; sub = X[idx]; result = np.ones(sub.shape[0], dtype=bool); first = func(g, sub). for i in range(tries):; result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each; samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while; results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({; ""var_idx"": samples.flatten(),; ""consistent"": results.flatten(), ; ""sample"": np.repeat(np.arange(adata.n_vars), 100),; ""order"": np.tile(np.arange(100), adata.n_vars),; }). df.groupby(""order"").mean()[""consistent""].plot(); ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541
https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541:712,Modifiability,variab,variables,712,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>; <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes; * This is especially weird since all values are changing if I run the function on the full set of variables; * If I use a smaller interval size (10), I don't get any varying results; * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent?. -----------------. This is so weird. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):; sub = X[idx]; result = np.ones(sub.shape[0], dtype=bool); first = func(g, sub). for i in range(tries):; result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each; samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while; results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({; ""var_idx"": samples.flatten(),; ""consistent"": results.flatten(), ; ""sample"": np.repeat(np.arange(adata.n_vars), 100),; ""order"": np.tile(np.arange(100), adata.n_vars),; }). df.groupby(""order"").mean()[""consistent""].plot(); ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541
https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541:942,Modifiability,variab,variables,942,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>; <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes; * This is especially weird since all values are changing if I run the function on the full set of variables; * If I use a smaller interval size (10), I don't get any varying results; * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent?. -----------------. This is so weird. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):; sub = X[idx]; result = np.ones(sub.shape[0], dtype=bool); first = func(g, sub). for i in range(tries):; result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each; samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while; results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({; ""var_idx"": samples.flatten(),; ""consistent"": results.flatten(), ; ""sample"": np.repeat(np.arange(adata.n_vars), 100),; ""order"": np.tile(np.arange(100), adata.n_vars),; }). df.groupby(""order"").mean()[""consistent""].plot(); ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541
https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541:1019,Modifiability,variab,variables,1019,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>; <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes; * This is especially weird since all values are changing if I run the function on the full set of variables; * If I use a smaller interval size (10), I don't get any varying results; * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent?. -----------------. This is so weird. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):; sub = X[idx]; result = np.ones(sub.shape[0], dtype=bool); first = func(g, sub). for i in range(tries):; result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each; samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while; results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({; ""var_idx"": samples.flatten(),; ""consistent"": results.flatten(), ; ""sample"": np.repeat(np.arange(adata.n_vars), 100),; ""order"": np.tile(np.arange(100), adata.n_vars),; }). df.groupby(""order"").mean()[""consistent""].plot(); ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541
https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541:1557,Modifiability,variab,variables,1557,"he 45th variable is the only one who's value changes; * This is especially weird since all values are changing if I run the function on the full set of variables; * If I use a smaller interval size (10), I don't get any varying results; * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent?. -----------------. This is so weird. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):; sub = X[idx]; result = np.ones(sub.shape[0], dtype=bool); first = func(g, sub). for i in range(tries):; result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each; samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while; results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({; ""var_idx"": samples.flatten(),; ""consistent"": results.flatten(), ; ""sample"": np.repeat(np.arange(adata.n_vars), 100),; ""order"": np.tile(np.arange(100), adata.n_vars),; }). df.groupby(""order"").mean()[""consistent""].plot(); ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress!. Minimal reproducer:. ```python; pbmc = sc.datasets.pbmc68k_reduced(); pbmc = pbmc[:411].copy(); sc.pp.neighbors(pbmc); res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]); ```. I can trigger the bug by changing the dataset size. . I think this may be a memory alignment issue. If `X` is a sparse matrix, I don't get any inconsistency in the results (which would ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541
https://github.com/scverse/scanpy/issues/1698#issuecomment-826743830:101,Availability,error,error,101,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python; np.where((adata.X[[0], :] == adata.X).all(axis=0)); ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error?. --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826743830
https://github.com/scverse/scanpy/issues/1698#issuecomment-826743830:407,Availability,error,error,407,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python; np.where((adata.X[[0], :] == adata.X).all(axis=0)); ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error?. --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826743830
https://github.com/scverse/scanpy/issues/1698#issuecomment-826743830:22,Modifiability,variab,variables,22,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python; np.where((adata.X[[0], :] == adata.X).all(axis=0)); ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error?. --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826743830
https://github.com/scverse/scanpy/issues/1698#issuecomment-827271245:254,Availability,avail,available,254,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-827271245
https://github.com/scverse/scanpy/issues/1698#issuecomment-827271245:417,Testability,log,logic,417,"> I think the result in such a case could just contain nans and emit a warning. This sounds reasonable to me. With sparse values, it's consistently giving results, but it's the wrong results. The iteration is being chunked (probably related to number of available cores), and it looks like within each chunk all values after the constant one are filled with zeros. I should look into whether this is also numba, or a logic bug. If it's numba, it's strange that it's zeros and not `inf` or `nan`. If it's on our end, I'm not sure why the later iterations would be skipped. ----------. > p.s.: I really like how you document everything you do so nicely 😃. Thanks! Is mostly so I can remember my reasoning a month later 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-827271245
https://github.com/scverse/scanpy/issues/1698#issuecomment-846745204:275,Safety,detect,detection,275,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-846745204
https://github.com/scverse/scanpy/issues/1698#issuecomment-846745204:410,Testability,test,testing,410,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-846745204
https://github.com/scverse/scanpy/issues/1696#issuecomment-797415965:15,Deployability,update,update,15,can you try to update to `numba=0.52` and see if it's still an issue?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797415965
https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893:231,Availability,down,downgrading,231,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893
https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893:501,Availability,error,errors,501,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893
https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893:658,Deployability,update,update,658,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893
https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893:251,Integrability,depend,dependencies,251,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893
https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893:596,Usability,learn,learn,596,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893
https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:45,Availability,error,error,45,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745
https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:6,Deployability,install,install,6,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745
https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:201,Deployability,install,install,201,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745
https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:253,Deployability,install,install,253,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745
https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:1830,Deployability,update,updated,1830,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745
https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:319,Testability,log,logging,319,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745
https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:1776,Testability,log,logical,1776,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745
https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745:137,Usability,learn,learn,137,"Fresh install in a new env gives me the same error (jupyter kernel crashes):; ```; conda create --name squidpy python=3.8 seaborn scikit-learn statsmodels numba pytables; conda activate squidpy; conda install -c conda-forge leidenalg python-igraph; pip install scanpy squidpy imctools stardist; ```; And here's the `sc.logging.print_versions()`:; ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; asciitree NA; backcall 0.2.0; cairo 1.20.0; cffi 1.14.5; cmocean 2.0; constants NA; cycler 0.10.0; cython_runtime NA; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.2; fasteners NA; get_version 2.1; h5py 2.10.0; highs_wrapper NA; igraph 0.8.3; imagecodecs 2020.12.24; imageio 2.9.0; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; networkx 2.5; numba 0.52.0; numcodecs 0.7.3; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; pywt 1.1.1; scanpy 1.7.1; scipy 1.6.0; seaborn 0.11.1; sinfo 0.3.1; six 1.15.0; skimage 0.18.1; sklearn 0.24.1; squidpy 1.0.0; statsmodels 0.12.2; storemagic NA; tables 3.6.1; texttable 1.6.3; tifffile 2021.3.5; tornado 6.1; traitlets 5.0.5; typing_extensions NA; wcwidth 0.2.5; xarray 0.17.0; yaml 5.4.1; zarr 2.6.1; zmq 22.0.3; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; notebook 6.2.0; -----; Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) [GCC 9.3.0]; Linux-3.10.0-1062.1.2.el7.x86_64-x86_64-with-glibc2.10; 72 logical CPU cores, x86_64; -----; Session information updated at 2021-03-12 11:42; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797629745
https://github.com/scverse/scanpy/issues/1696#issuecomment-797640822:133,Availability,error,errors,133,"> and I'm fairly certain this has to do with the call to NNDescent in umap.umap_.py as if I import that directly, it raises the same errors. sorry just read this, this sounds it could be potentially data specific, have you tried playing around with other nndescent params?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797640822
https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223:180,Integrability,depend,dependency,180,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:; ```{python}; AnnData object with n_obs × n_vars = 68865 × 29; obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'; var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'; uns: 'spatial', 'log1p', 'pca',; obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'; varm: 'PCs'; layers: 'cleaned', 'normed', 'lognormed'; ```. I will probably raise this with `pynndescent` then because; ```; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes; ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223
https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223:710,Modifiability,layers,layers,710,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:; ```{python}; AnnData object with n_obs × n_vars = 68865 × 29; obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'; var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'; uns: 'spatial', 'log1p', 'pca',; obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'; varm: 'PCs'; layers: 'cleaned', 'normed', 'lognormed'; ```. I will probably raise this with `pynndescent` then because; ```; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes; ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223
https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223:740,Testability,log,lognormed,740,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:; ```{python}; AnnData object with n_obs × n_vars = 68865 × 29; obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'; var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'; uns: 'spatial', 'log1p', 'pca',; obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'; varm: 'PCs'; layers: 'cleaned', 'normed', 'lognormed'; ```. I will probably raise this with `pynndescent` then because; ```; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes; ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223
https://github.com/scverse/scanpy/issues/1696#issuecomment-797652809:489,Availability,ping,pinging,489,"> Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the imctools dependency). I have an object that looks like:. thank you for reporting, this is very interesting use case! and thanks for the detailed evaluation. I would also try with different number of PCs to see whether that has an impact. if you open an issue on `pynndescent`, would you mind referencing this issue or pinging me there, would be interested to see what's the proposed solution/bug. @TiongSun let us know about your use case, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797652809
https://github.com/scverse/scanpy/issues/1696#issuecomment-797652809:180,Integrability,depend,dependency,180,"> Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the imctools dependency). I have an object that looks like:. thank you for reporting, this is very interesting use case! and thanks for the detailed evaluation. I would also try with different number of PCs to see whether that has an impact. if you open an issue on `pynndescent`, would you mind referencing this issue or pinging me there, would be interested to see what's the proposed solution/bug. @TiongSun let us know about your use case, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797652809
https://github.com/scverse/scanpy/issues/1696#issuecomment-802328453:281,Availability,redundant,redundant,281,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802328453
https://github.com/scverse/scanpy/issues/1696#issuecomment-802328453:309,Availability,reliab,reliably,309,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802328453
https://github.com/scverse/scanpy/issues/1696#issuecomment-802328453:281,Safety,redund,redundant,281,"@giovp Looking more into the crashing I was getting with my strange use case, it turns out that I had both (a) a pair of completely correlated features, and (b) very strange count distributions. Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features, I can't reliably reproduce this issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802328453
https://github.com/scverse/scanpy/issues/1696#issuecomment-802666323:279,Availability,redundant,redundant,279,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ?. > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802666323
https://github.com/scverse/scanpy/issues/1696#issuecomment-802666323:279,Safety,redund,redundant,279,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ?. > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802666323
https://github.com/scverse/scanpy/issues/1696#issuecomment-802857928:53,Availability,down,down,53,"very interesting, also new to me. I think this boils down to issues in `pynndescent` not being able to handle such edge cases. I wonder if this happens with other metrics as well... @TiongSun can you update us on whether this is a similar issue for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802857928
https://github.com/scverse/scanpy/issues/1696#issuecomment-802857928:200,Deployability,update,update,200,"very interesting, also new to me. I think this boils down to issues in `pynndescent` not being able to handle such edge cases. I wonder if this happens with other metrics as well... @TiongSun can you update us on whether this is a similar issue for you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802857928
https://github.com/scverse/scanpy/issues/1696#issuecomment-802982688:385,Availability,error,error,385,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802982688
https://github.com/scverse/scanpy/issues/1696#issuecomment-802982688:391,Availability,reliab,reliably,391,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802982688
https://github.com/scverse/scanpy/issues/1696#issuecomment-802982688:41,Integrability,depend,depending,41,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802982688
https://github.com/scverse/scanpy/issues/1696#issuecomment-802982688:252,Integrability,rout,routinely,252,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802982688
https://github.com/scverse/scanpy/issues/1696#issuecomment-814555908:83,Availability,error,error,83,Sorry for the late reply. The issue seems to only occur in Win10 but not Linux. No error on linux using the exact same codes.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-814555908
https://github.com/scverse/scanpy/issues/1697#issuecomment-797406947:43,Testability,test,tests,43,random point but in #1715 noticed that 3.6 tests are failing due to missing `typing.Literal` import. We could fix it ad hoc but otherwise other good reason to drop 3.6,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697#issuecomment-797406947
https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473:89,Availability,reliab,reliably,89,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473
https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473:110,Availability,failure,failures,110,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473
https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473:127,Deployability,update,updates,127,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473
https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473:77,Integrability,depend,dependency,77,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473
https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473:105,Testability,test,test,105,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473
https://github.com/scverse/scanpy/pull/1699#issuecomment-787809506:93,Availability,error,errors,93,"Hmm, weird that it didn't fail against master. Also that I forgot to back port the nice plot errors to 1.7.x. TODO: https://github.com/theislab/scanpy/pull/1587#issuecomment-787808128",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1699#issuecomment-787809506
https://github.com/scverse/scanpy/pull/1700#issuecomment-788509083:157,Testability,test,tests,157,"Can we keep this open until the anndata pr has merged? For instance, I'd like to check this all works after merging #1702. I'm a bit concerned about `scanpy.tests` using stuff from `anndata.tests` while those are being ignored from the `sdist`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1700#issuecomment-788509083
https://github.com/scverse/scanpy/pull/1700#issuecomment-788509083:190,Testability,test,tests,190,"Can we keep this open until the anndata pr has merged? For instance, I'd like to check this all works after merging #1702. I'm a bit concerned about `scanpy.tests` using stuff from `anndata.tests` while those are being ignored from the `sdist`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1700#issuecomment-788509083
https://github.com/scverse/scanpy/issues/1701#issuecomment-787864572:157,Integrability,depend,dependencies,157,"Can you show us the values of `combined_bbknn.obs['scNym']`?. Also, if you create a conda environment, does your problems still occur? I'm wondering if some dependencies like `pandas` could be out of date.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787864572
https://github.com/scverse/scanpy/issues/1701#issuecomment-787867150:361,Availability,down,downgraded,361,"Hi, . Thanks for the quick reply!. I'm attaching the output for `combined_bbknn.obs['scNym']`:. ![Screenshot 2021-03-01 at 11 09 39](https://user-images.githubusercontent.com/3297906/109489440-ca187300-7a7e-11eb-943d-270c0273c3fc.png). This is really weird. When I tested it on my macbook I created a new environment and the problem persisted. However, there I downgraded to `scanpy==1.6` as well, the problem persisted, but the `NA`s weren't there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787867150
https://github.com/scverse/scanpy/issues/1701#issuecomment-787867150:265,Testability,test,tested,265,"Hi, . Thanks for the quick reply!. I'm attaching the output for `combined_bbknn.obs['scNym']`:. ![Screenshot 2021-03-01 at 11 09 39](https://user-images.githubusercontent.com/3297906/109489440-ca187300-7a7e-11eb-943d-270c0273c3fc.png). This is really weird. When I tested it on my macbook I created a new environment and the problem persisted. However, there I downgraded to `scanpy==1.6` as well, the problem persisted, but the `NA`s weren't there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787867150
https://github.com/scverse/scanpy/issues/1701#issuecomment-787868527:77,Availability,error,errors,77,"Do you know how that entry could have been filled with `NaN`?. The plots and errors you were showing above are consistent with all the values being ""null"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787868527
https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441:237,Availability,error,errors,237,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441
https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441:539,Deployability,upgrade,upgraded,539,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441
https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441:426,Modifiability,variab,variable,426,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441
https://github.com/scverse/scanpy/issues/1701#issuecomment-787877279:166,Availability,avail,available,166,Could you come up with the minimum amount of commands you'd need to run to reproduce this? It would also be helpful if this could be done using generated or publicly available data. [Something like this](http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) would be great.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787877279
https://github.com/scverse/scanpy/issues/1701#issuecomment-788489841:231,Deployability,update,update,231,"Just checking back on this, it's concerning if you are getting null values unexpectedly, but it's difficult for me to figure out why that could be happening without more information. It would be great if you're able to give us any update on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-788489841
https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:593,Availability,echo,echo,593,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasn’t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure it’s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617
https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:108,Deployability,install,install,108,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasn’t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure it’s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617
https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:136,Deployability,install,install,136,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasn’t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure it’s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617
https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:368,Deployability,install,install,368,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasn’t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure it’s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617
https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:412,Deployability,install,install,412,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasn’t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure it’s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617
https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:712,Deployability,Install,Install,712,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasn’t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure it’s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617
https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:760,Deployability,install,install,760,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasn’t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure it’s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617
https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:805,Deployability,install,install,805,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasn’t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure it’s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617
https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:733,Integrability,depend,depends,733,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasn’t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure it’s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617
https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:629,Testability,test,test,629,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasn’t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure it’s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617
https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:818,Testability,test,test,818,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasn’t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure it’s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617
https://github.com/scverse/scanpy/pull/1703#issuecomment-788515341:127,Deployability,release,releases,127,Hi @ivirshup we were hoping to completely remove scvi from external. Users received notice about it's deprecation in the 1.7.X releases. I suppose this can wait until 1.8,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1703#issuecomment-788515341
https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203:514,Deployability,integrat,integrated,514,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with; * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203
https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203:514,Integrability,integrat,integrated,514,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with; * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203
https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203:363,Usability,simpl,simplexes,363,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with; * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203
https://github.com/scverse/scanpy/issues/1706#issuecomment-788885174:1002,Availability,error,error,1002,"It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:; ```python; import scanpy as sc; adata = sc.datasets.blobs(n_observations=5). for n_neighbors in [1, 2, 3]:; sc.pp.neighbors(adata, n_neighbors=n_neighbors); print(f'n_neighbors = {n_neighbors}:\n', adata.uns['neighbors']['connectivities'].A); ```; Output:; ```; n_neighbors = 1:; [[0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]]; n_neighbors = 2:; [[0. 0. 0. 1. 0.]; [0. 0. 1. 0. 0.]; [0. 1. 0. 0. 0.]; [1. 0. 0. 0. 1.]; [0. 0. 0. 1. 0.]]; n_neighbors = 3:; [[0. 0.5849553 0. 1. 0.5849636 ]; [0.5849553 0. 1. 0.5849678 0. ]; [0. 1. 0. 0.58496827 0. ]; [1. 0.5849678 0.58496827 0. 1. ]; [0.5849636 0. 0. 1. 0. ]]; ```; It is been while that I read about UMAP and can't get my head around why this happens right now. Relying on UMAP seems a good idea to me, maybe the corner case `n_neighbors=1` should just be catched with a more meaningful error message?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788885174
https://github.com/scverse/scanpy/issues/1706#issuecomment-788885174:1008,Integrability,message,message,1008,"It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:; ```python; import scanpy as sc; adata = sc.datasets.blobs(n_observations=5). for n_neighbors in [1, 2, 3]:; sc.pp.neighbors(adata, n_neighbors=n_neighbors); print(f'n_neighbors = {n_neighbors}:\n', adata.uns['neighbors']['connectivities'].A); ```; Output:; ```; n_neighbors = 1:; [[0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]]; n_neighbors = 2:; [[0. 0. 0. 1. 0.]; [0. 0. 1. 0. 0.]; [0. 1. 0. 0. 0.]; [1. 0. 0. 0. 1.]; [0. 0. 0. 1. 0.]]; n_neighbors = 3:; [[0. 0.5849553 0. 1. 0.5849636 ]; [0.5849553 0. 1. 0.5849678 0. ]; [0. 1. 0. 0.58496827 0. ]; [1. 0.5849678 0.58496827 0. 1. ]; [0.5849636 0. 0. 1. 0. ]]; ```; It is been while that I read about UMAP and can't get my head around why this happens right now. Relying on UMAP seems a good idea to me, maybe the corner case `n_neighbors=1` should just be catched with a more meaningful error message?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788885174
https://github.com/scverse/scanpy/issues/1706#issuecomment-789305891:447,Availability,error,error,447,"> It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:. I think the varying number of neighbors is because the connectivity graph is made symmetric. ```python; import scanpy as sc, numpy as np; pbmc = sc.datasets.pbmc68k_reduced(); sc.pp.neighbors(pbmc). np.unique((pbmc.obsp[""distances""] != 0).sum(axis=1).flat); # array([14]); ```. Yeah, n_neighbors=1 should definitely throw an error (I think it does for UMAP). We do document that reasonable values start at 2, but it could also be good to have more reasoning on that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-789305891
https://github.com/scverse/scanpy/issues/1714#issuecomment-790279213:100,Deployability,release,releases,100,"It looks like you've got an outdated version of scanpy (1.5.0), this should be fixed in more recent releases by #1334.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1714#issuecomment-790279213
https://github.com/scverse/scanpy/pull/1715#issuecomment-790649506:219,Usability,feedback,feedback,219,"> thanks @jlause ! Really excited for this!; > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this?; > ; > thank you!. cool, looking forward to your feedback!; I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-790649506
https://github.com/scverse/scanpy/pull/1715#issuecomment-790649506:465,Usability,simpl,simplify,465,"> thanks @jlause ! Really excited for this!; > shall be able to start reviewing during weekend. quick q to @ivirshup , does #1667 somehow potentially conflict with this?; > ; > thank you!. cool, looking forward to your feedback!; I had a brief look at #1667 - since I use the same layer management that is now to be changed in `normalize_total()`, it would make sense if I mirror the change in `normalize_pearson_residuals()`, right? I believe doing that will even simplify the function further. If @ivirshup agrees, I could quickly do that :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-790649506
https://github.com/scverse/scanpy/pull/1715#issuecomment-791140191:117,Usability,simpl,simplify,117,"> it would make sense if I mirror the change in normalize_pearson_residuals(), right? I believe doing that will even simplify the function further. If ivirshup agrees, I could quickly do that :). Sounds good to me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-791140191
https://github.com/scverse/scanpy/pull/1715#issuecomment-791524373:496,Usability,guid,guide,496,"I finished the edits to conform with #1667! Now looking forward to your thoughts :). One note on the coding style checks (which I'm not very experienced with): When I activate pre-commit locally, it finds quite a number of style violations in parts of the code that I did not touch and automatically fixes them. This causes many changes that are unrelated to the code I wrote. That's why I disabled pre-commit again (so you don't have to go over all these changes), and tried to follow the style guide manually as good as possible. Hope that is ok for now.. Do you have any advice how to handle that? Should I just do one ""style"" commit (that fixes all these issues throughout the files I work on here) once you've checked the new parts of the code I wrote? Or should the style be ok in all ""old"" parts of the code, implying that I set up pre-commit wrong? I'm new to it so that could very well be the case as well..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-791524373
https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562:8412,Deployability,update,update,8412," E501 line too long (93 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:656:80: E501 line too long (80 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:693:80: E501 line too long (80 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```; jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status; On branch pearson_residuals_1.7; Changes to be committed:; (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:; (use ""git add <file>..."" to update what will be committed); (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:; (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py ; diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py; index 03b01940..e2851f50 100644; --- a/scanpy/preprocessing/_highly_variable_genes.py; +++ b/scanpy/preprocessing/_highly_variable_genes.py; @@ -15,7 +15,8 @@ from ._utils import _get_mean_var; from ._distributed import materialize_as_ndarray; from ._simple import filter_genes; ; -#testedit; +# testedit; +; ; def _highly_variable_genes_seurat_v3(; adata: AnnData,; @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(; else:; clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape); np.putmask(; - batch_counts, batch_counts > clip_val_broad, clip_val_br",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562
https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562:8658,Modifiability,config,config,8658,"> 79 characters); scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```; jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status; On branch pearson_residuals_1.7; Changes to be committed:; (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:; (use ""git add <file>..."" to update what will be committed); (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:; (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py ; diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py; index 03b01940..e2851f50 100644; --- a/scanpy/preprocessing/_highly_variable_genes.py; +++ b/scanpy/preprocessing/_highly_variable_genes.py; @@ -15,7 +15,8 @@ from ._utils import _get_mean_var; from ._distributed import materialize_as_ndarray; from ._simple import filter_genes; ; -#testedit; +# testedit; +; ; def _highly_variable_genes_seurat_v3(; adata: AnnData,; @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(; else:; clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape); np.putmask(; - batch_counts, batch_counts > clip_val_broad, clip_val_broad,; + batch_counts,; + batch_counts > clip_val_broad,; + clip_val_broad,; ); ; if sp_sparse.issparse(batch_counts):; @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(; df = df.drop(['highly_variable_nbatche",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562
https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562:57,Testability,test,test,57,"Hey @ivirshup,; below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit.; Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code?. hope this helps, let me know if you need anything else. <details>; <summary> </summary>. ```; jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit""; black....................................................................Failed; - hook id: black; - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py; All done! ✨ 🍰 ✨; 1 file reformatted. flake8...................................................................Failed; - hook id: flake8; - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace; scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'; scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'; scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters); scanpy/preprocessing/_highly_va",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562
https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562:484,Testability,test,test,484,"Hey @ivirshup,; below is the output after making a small test edit to `_highly_variable_genes.py` and trying to commit.; Could it be that the file has not been style-checked so far (or not on the branch/version I was forking from?), leading to all these automatic changes/style violations in the old part of the code?. hope this helps, let me know if you need anything else. <details>; <summary> </summary>. ```; jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git commit -m ""test commit""; black....................................................................Failed; - hook id: black; - files were modified by this hook. reformatted scanpy/preprocessing/_highly_variable_genes.py; All done! ✨ 🍰 ✨; 1 file reformatted. flake8...................................................................Failed; - hook id: flake8; - exit code: 1. scanpy/preprocessing/_highly_variable_genes.py:33:80: E501 line too long (84 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:37:80: E501 line too long (81 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:49:80: E501 line too long (102 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:51:80: E501 line too long (89 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:108:80: E501 line too long (82 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:123:80: E501 line too long (83 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:204:27: W291 trailing whitespace; scanpy/preprocessing/_highly_variable_genes.py:219:5: F821 undefined name 'view_to_actual'; scanpy/preprocessing/_highly_variable_genes.py:220:9: F821 undefined name '_get_obs_rep'; scanpy/preprocessing/_highly_variable_genes.py:244:80: E501 line too long (85 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:257:80: E501 line too long (85 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:282:80: E501 line too long (88 > 79 characters); scanpy/preprocessing/_highly_va",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562
https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562:9157,Testability,test,testedit,9157,"son_residuals_1.7; Changes to be committed:; (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:; (use ""git add <file>..."" to update what will be committed); (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:; (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py ; diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py; index 03b01940..e2851f50 100644; --- a/scanpy/preprocessing/_highly_variable_genes.py; +++ b/scanpy/preprocessing/_highly_variable_genes.py; @@ -15,7 +15,8 @@ from ._utils import _get_mean_var; from ._distributed import materialize_as_ndarray; from ._simple import filter_genes; ; -#testedit; +# testedit; +; ; def _highly_variable_genes_seurat_v3(; adata: AnnData,; @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(; else:; clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape); np.putmask(; - batch_counts, batch_counts > clip_val_broad, clip_val_broad,; + batch_counts,; + batch_counts > clip_val_broad,; + clip_val_broad,; ); ; if sp_sparse.issparse(batch_counts):; @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(; df = df.drop(['highly_variable_nbatches'], axis=1); return df; ; +; def _highly_variable_pearson_residuals(; adata: AnnData,; layer: Optional[str] = None,; @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(; clip: Union[Literal['auto', 'none'], float] = 'auto',; chunksize: int = 100,; subset: bool = False,; - inplace: bool = True; + inplace: bool = True,; ) -> Optional[pd.DataFrame]:; """"""\; See `highly_variable_genes`.; @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(; in all batches; """"""; ; - view_to_actual(adata) ; + view_to_actual(adata); X = _get_obs_rep(adata, layer=layer); c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562
https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562:9170,Testability,test,testedit,9170,"son_residuals_1.7; Changes to be committed:; (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:; (use ""git add <file>..."" to update what will be committed); (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:; (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py ; diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py; index 03b01940..e2851f50 100644; --- a/scanpy/preprocessing/_highly_variable_genes.py; +++ b/scanpy/preprocessing/_highly_variable_genes.py; @@ -15,7 +15,8 @@ from ._utils import _get_mean_var; from ._distributed import materialize_as_ndarray; from ._simple import filter_genes; ; -#testedit; +# testedit; +; ; def _highly_variable_genes_seurat_v3(; adata: AnnData,; @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(; else:; clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape); np.putmask(; - batch_counts, batch_counts > clip_val_broad, clip_val_broad,; + batch_counts,; + batch_counts > clip_val_broad,; + clip_val_broad,; ); ; if sp_sparse.issparse(batch_counts):; @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(; df = df.drop(['highly_variable_nbatches'], axis=1); return df; ; +; def _highly_variable_pearson_residuals(; adata: AnnData,; layer: Optional[str] = None,; @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(; clip: Union[Literal['auto', 'none'], float] = 'auto',; chunksize: int = 100,; subset: bool = False,; - inplace: bool = True; + inplace: bool = True,; ) -> Optional[pd.DataFrame]:; """"""\; See `highly_variable_genes`.; @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(; in all batches; """"""; ; - view_to_actual(adata) ; + view_to_actual(adata); X = _get_obs_rep(adata, layer=layer); c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562
https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562:10553,Testability,log,logg,10553," ; if sp_sparse.issparse(batch_counts):; @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(; df = df.drop(['highly_variable_nbatches'], axis=1); return df; ; +; def _highly_variable_pearson_residuals(; adata: AnnData,; layer: Optional[str] = None,; @@ -182,7 +186,7 @@ def _highly_variable_pearson_residuals(; clip: Union[Literal['auto', 'none'], float] = 'auto',; chunksize: int = 100,; subset: bool = False,; - inplace: bool = True; + inplace: bool = True,; ) -> Optional[pd.DataFrame]:; """"""\; See `highly_variable_genes`.; @@ -212,7 +216,7 @@ def _highly_variable_pearson_residuals(; in all batches; """"""; ; - view_to_actual(adata) ; + view_to_actual(adata); X = _get_obs_rep(adata, layer=layer); computed_on = layer if layer else 'adata.X'; ; @@ -328,8 +332,7 @@ def _highly_variable_pearson_residuals(; df = df.loc[adata.var_names]; ; if inplace or subset:; - adata.uns['hvg'] = {'flavor': 'pearson_residuals',; - 'computed_on':computed_on}; + adata.uns['hvg'] = {'flavor': 'pearson_residuals', 'computed_on': computed_on}; logg.hint(; 'added\n'; ' \'highly_variable\', boolean vector (adata.var)\n'. ...skipping 1 line; ['highly_variable_nbatches', 'highly_variable_intersection'], axis=1; ); return df; - ; - ; - ; +; ; def _highly_variable_genes_single_batch(; adata: AnnData,; @@ -479,7 +480,6 @@ def _highly_variable_genes_single_batch(; return df; ; ; -; def highly_variable_genes(; adata: AnnData,; layer: Optional[str] = None,; @@ -493,7 +493,9 @@ def highly_variable_genes(; theta: float = 100,; clip: Union[Literal['auto', 'none'], float] = 'auto',; chunksize: int = 1000,; - flavor: Literal['seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'] = 'seurat',; + flavor: Literal[; + 'seurat', 'cell_ranger', 'seurat_v3', 'pearson_residuals'; + ] = 'seurat',; subset: bool = False,; inplace: bool = True,; batch_key: Optional[str] = None,; @@ -651,21 +653,20 @@ def highly_variable_genes(; if flavor == 'pearson_residuals':; if n_top_genes is None:; raise ValueError(; - ""`pp.hi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562
https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:167,Deployability,install,installed,167,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768
https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:394,Integrability,interface,interface,394,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768
https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:628,Integrability,interface,interface,628,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768
https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:186,Modifiability,config,config,186,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768
https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:847,Testability,log,logic,847,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768
https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:746,Usability,intuit,intuitive,746,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768
https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:169,Deployability,install,installed,169,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189
https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:403,Integrability,interface,interface,403,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189
https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:691,Integrability,interface,interface,691,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189
https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:188,Modifiability,config,config,188,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189
https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:910,Testability,log,logic,910,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189
https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:809,Usability,intuit,intuitive,809,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189
https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:890,Availability,error,errors,890,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681
https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:240,Testability,test,tests,240,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681
https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:536,Testability,test,tests,536,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681
https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:594,Testability,test,tested,594,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681
https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:623,Testability,test,tests,623,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681
https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:688,Testability,test,test,688,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681
https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:711,Testability,test,tests,711,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681
https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:861,Testability,test,tests,861,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681
https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:1029,Testability,test,tested,1029,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681
https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681:561,Usability,guid,guidelines,561,"> thank you @jlause for the PR! This is really exciting and super useful!; > This is a first round of review, most comments are re types, args and function behviour. I think it looks really good overall and maybe it's time to start writing tests ?; > please let me know if anything unclear and also thanks in advance for code explanations!. Hey @giovp ,; thanks a lot for the review, this looks very helpful! I'll address the single points above one-by-one and make the required changes over the next few days! Will also add some first tests - are there formal guidelines what you expect to be tested? After looking at the tests for `highly_variable_genes`, from my naive perspective I'd test the following:. - tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; - tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). Any advice/ideas what else should be tested?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797435681
https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245:179,Availability,error,errors,179,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ).; another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245
https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245:754,Availability,robust,robust,754,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ).; another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245
https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245:897,Deployability,release,release,897,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ).; another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245
https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245:2,Testability,test,tests,2,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ).; another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245
https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245:150,Testability,test,tests,150,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ).; another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245
https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245:396,Testability,test,tested,396,"> tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..). yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough 😄 ).; another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797462245
https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998:276,Availability,error,errors,276,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..); > ; > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ).; > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week.; ; > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998
https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998:863,Availability,robust,robust,863,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..); > ; > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ).; > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week.; ; > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998
https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998:1089,Deployability,release,release,1089,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..); > ; > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ).; > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week.; ; > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998
https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998:95,Testability,test,tests,95,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..); > ; > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ).; > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week.; ; > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998
https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998:247,Testability,test,tests,247,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..); > ; > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ).; > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week.; ; > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998
https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998:499,Testability,test,tested,499,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..); > ; > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ).; > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week.; ; > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998
https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998:1003,Testability,test,tests,1003,"I addressed some of the points in your review already and will finish latest on Monday :). > > tests that check if combinations of input arguments lead to expected output (in terms of returned shapes/columns/...) and don't break the function; > > tests that check if warnings/errors are raised for ""common mistakes"" (inappropriate data, nonsense input argument combinations..); > ; > yes both makes sense, it would also be useful to come up with a dummy example for which the actual output could be tested against. This is done in seurat_v3 for instance, but in that case it's kind of straightforward because the ""expected"" is the output computed with original implementation (and as you catched in #1732 it's still might not be enough smile ).; > another random thing that comes to mind re this specific case is to make sure that indexing etc. is consistent and robust, as you seem to have to sort and resort a fair bit in the hvg implementation. Sounds good, thanks for the input! I will prepare some tests early next week.; ; > on another note, I was thinking if it makes sense to also release a short tutorial together with the PR (that would be on theislab/scanpy_tutorials) ? I think that for a lot of people the term ""pearson residuals"" could be alienating, and so they'd rather stick to `normalize_total` for comfort (but they shouldn't!). So maybe just something easy like pearson res norm + umap and hvg plots ? curious to hear what you and the others @ivirshup @LuckyMD think about it. I think that would be really nice - I'd very happy prepare to some examples if everyone agrees that this would be useful to have :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797689998
https://github.com/scverse/scanpy/pull/1715#issuecomment-797848694:328,Usability,clear,clear,328,"This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint? It feels like something that should more go to external, considering the method itself will undergo the peer-review process. And if it's clear later that this is a foundational scrna method, then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. As another example, why not add GLM-PCA to `sc.tl.glm_pca`? It's supposed to be better. I even think in GLM-PCA they describe a fast approximation using [deviance residuals](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6#Sec18), so why not add that? . My point isn't specifically about GLM-PCA, but since many people will probably generically use these functions, shouldn't we put more weight on the peer-review process here? It's not like scanpy is just adding any method to `sc.{pp, tl}`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-797848694
https://github.com/scverse/scanpy/pull/1715#issuecomment-798444591:1316,Testability,log,logic,1316,"@adamgayoso This is a fair point and thanks for raising this issue. Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. Whether this goes into `external` or into the core, is definitely not our call, so let's see what the Scanpy team says. . --------------------. I just wanted to make some small comments to what you wrote. The main comment is that we don't view this as a ""new method"". We view it basically as ""scTransform done right"". And scTransform is already published and is being used. . > It feels like something that should more go to external, considering the method itself will undergo the peer-review process. . I understand this point, but I guess the distinction here is not only published vs not-yet-published. A lot of stuff gets published but is not included into Scanpy... > As another example, why not add GLM-PCA to sc.tl.glm_pca? It's supposed to be better. See our preprint regarding ""supposed to be better""... . > I even think in GLM-PCA they describe a fast approximation using deviance residuals, so why not add that?. Deviance residuals are very similar to Pearson residuals. We could consider adding deviance residuals as an option to the functions suggested in this PR, it would be all the same logic, just deviance vs. Pearson. Hmm, actually I would need to check what exactly is the expression for deviance residuals for NB model. > shouldn't we put more weight on the peer-review process here?. One option would be to hold this PR until our paper is formally accepted...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-798444591
https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817:1440,Availability,avail,available,1440,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings?. > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817
https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817:782,Deployability,continuous,continuous,782,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings?. > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817
https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817:1456,Performance,perform,performance,1456,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings?. > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817
https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817:1208,Usability,guid,guidelines,1208,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings?. > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817
https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115:482,Integrability,message,messages,482,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):; - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`.; - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint?. I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):; - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes); - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ?. With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't poss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115
https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115:577,Performance,scalab,scalability,577,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):; - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`.; - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint?. I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):; - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes); - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ?. With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't poss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115
https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115:521,Usability,simpl,simplicity,521,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):; - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`.; - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint?. I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):; - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes); - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ?. With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't poss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115
https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693:1006,Testability,log,log,1006,"Again, everyone has raised fair points. Thank you for the responses. > Should this process be somehow formalized, e.g. a common issue title like [new method] My new method ?. Yes!. > I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Peer-review is not the gold standard. As far as I understand, cell ranger has not been peer reviewed and everyone I know uses it. IMO this particular paper could benefit from the process based on the sorts of claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both?. > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with reall",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693
https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693:2550,Testability,log,log,2550," claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both?. > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found.; 2. Should people be using these values for visualization?; 3. What's the workflow if people need both log normalized and pearson residuals?; 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693
https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693:2003,Usability,clear,clear,2003," claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both?. > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found.; 2. Should people be using these values for visualization?; 3. What's the workflow if people need both log normalized and pearson residuals?; 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693
https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693:2381,Usability,clear,clear,2381," claims it makes. > it is not strictly a new method, but has several connections with previous sctransform and glm-pca (also, not sure on what basis you said that ""glm-pca is supposed to be better"", would be genuinely curious to see some evaluations). My point here was to say that historically Scanpy hasn't rushed to add _any_ method that is better than log normalization -> PCA. I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). If deviance residuals give a similar latent space, what do you do then? Add both?. > So, my take is: let's get the pearson residuals from @jlause @dkobak in scanpy, and keep pushing to get the others methods in here as well! at the end, this will ultimately benefit greatly the users. Personally this is how I feel -- the more the better! But historically getting a method in the scanpy core is not so easy (even just seeing the back and forth on the linked issue makes it seem like this is the case for this method). This is why I think it's practically important for Scanpy to be very choosy if it's not going to offer multiple competing workflows with really clear tutorials about which is appropriate and their limitations. Looking into this method further, I even have more questions in the context of Scanpy:. 1. How does this affect the workflow for DE? In scTransform I noticed they create some corrected counts by using the median library size to ""invert"" the regression. This should also be possible here? Though it's not exactly clear in Seurat what to use for DE, based on many issues I found.; 2. Should people be using these values for visualization?; 3. What's the workflow if people need both log normalized and pearson residuals?; 4. Will there be more tutorials covering all these use cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799542693
https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490:444,Deployability,integrat,integrate,444,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward?. In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490
https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490:894,Deployability,integrat,integration,894,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward?. In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490
https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490:444,Integrability,integrat,integrate,444,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward?. In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490
https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490:894,Integrability,integrat,integration,894,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward?. In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490
https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490:778,Modifiability,extend,extend,778,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward?. In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490
https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490:1221,Testability,test,tests,1221,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward?. In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490
https://github.com/scverse/scanpy/pull/1715#issuecomment-828228025:756,Testability,benchmark,benchmark,756,"Regarding Pearson vs. deviance residuals @adamgayoso: we looked into in in detail for the second version of the manuscript (just posted to bioRxiv: https://www.biorxiv.org/content/10.1101/2020.12.01.405886v2.full.pdf). Our conclusion is that deviance residuals don't work here at all because they -- unlike Pearson residuals -- show a very strong mean-variance relationship. Here, see an excerpt from Figure S2:. ![Screenshot from 2021-04-28 09-29-02](https://user-images.githubusercontent.com/8970231/116365322-6f449300-a805-11eb-8458-0bbf2aceb23a.png). I was surprised by that because I fully expected that deviance and Pearson residuals would be very similar and we'd see no qualitative difference between them. But this wasn't the case. See also a new benchmark in Figure 5. > I was using GLM-PCA as a generic example, but I then realized that coincidentally in the GLM-PCA paper they describe a fast analytical approximation using deviance residuals, which is not compared to in the analytical Pearson residuals manuscript (and again highlights the potential role of peer-review IMO). Re peer review -- as I already mentioned, none of the actual reviewers asked us about deviance residuals ;-) So thanks a lot for voicing these concerns here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-828228025
https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292:86,Testability,test,tests,86,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak !. I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292
https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292:240,Testability,test,tests,240,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak !. I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292
https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292:310,Testability,test,tests,310,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak !. I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292
https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292:325,Testability,test,tests,325,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak !. I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292
https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292:343,Testability,test,test,343,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak !. I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292
https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292:556,Testability,test,tests,556,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak !. I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292
https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292:613,Testability,test,tests,613,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak !. I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292
https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292:708,Testability,test,tests,708,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak !. I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292
https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292:644,Usability,feedback,feedback,644,"Hey @giovp @LuckyMD @ivirshup @adamgayoso @dkobak !. I just finished writing a set of tests for all four functions I currently have implemented! I also made some minor changes to the original code of the PR because (as probably intended by tests in general ;)) I found some inconsistencies when developing the tests. For the tests, I tried to test all input arguments and outputs. Only exception was when a bundle function (e.g. `sc.pp.recipe_pearson_residuals`) passes on an argument directly to a lower level function (e.g. `sc.pp.pca`) that has its own tests. But of course, also here, one could include extra tests. Looking forward to your feedback here, as this is my first time writing a larger set of tests. I will be on vacation until June 27th, but after that I can prioritize working on your suggestions for this! Thanks in advance :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-859688292
https://github.com/scverse/scanpy/pull/1715#issuecomment-872954063:119,Performance,perform,perform,119,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-872954063
https://github.com/scverse/scanpy/pull/1715#issuecomment-872954063:532,Safety,avoid,avoid,532,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-872954063
https://github.com/scverse/scanpy/pull/1715#issuecomment-872954063:140,Testability,test,test,140,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-872954063
https://github.com/scverse/scanpy/pull/1715#issuecomment-873639813:57,Testability,test,tests,57,"@jlause sorry for not getting back. I just had a look at tests and it looks good!; as discussed via email, we would like to first add this to an `experimental` API before including it in code. Tomorrow I will arrange the code slots were you'd have to copy over the functions from their current places. It's a bit of tedious work but shouldn't take much. Will elaborate better on comments!; I will then take care of fixing docs and links. . Another thing still left to be done would be a tutorial on how to use these function and a more elaborate explanation. We will add that tutorial to `scanpy-tutorials` and link from main docs. Maybe you could start already briefly fleshing it out? I'd move the conversation of the tutorial to https://github.com/theislab/scanpy-tutorials",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-873639813
https://github.com/scverse/scanpy/pull/1715#issuecomment-874227014:249,Modifiability,variab,variable,249,I've also initialized the `experimental` module. I think it should be straightforward to copy over the functions currently in core.; e.g.; ```; sc.pp.normalize_pearson_residuals() -> sc.experimental.pp.normalize_pearson_residuals(); ```. For highly variable genes it might be a bit ugly because it essentially only supports one modality. We really need to start thinking about #1739 ...; Let me know if it makes sense and if there is something unclear.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-874227014
https://github.com/scverse/scanpy/pull/1715#issuecomment-874558246:39,Performance,perform,perform,39,"> One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. @ktpolanski Thanks for the suggestion. Can you clarify which implementation you used where this is implemented?. I think one can essentially convert the variance of Pearson residuals into a p-value using a chi-square test. Then instead of the fixed number of HVG genes one could use some p-value cutoff. We have not experimented with this approach at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-874558246
https://github.com/scverse/scanpy/pull/1715#issuecomment-874558246:60,Testability,test,test,60,"> One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. @ktpolanski Thanks for the suggestion. Can you clarify which implementation you used where this is implemented?. I think one can essentially convert the variance of Pearson residuals into a p-value using a chi-square test. Then instead of the fixed number of HVG genes one could use some p-value cutoff. We have not experimented with this approach at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-874558246
https://github.com/scverse/scanpy/pull/1715#issuecomment-874558246:492,Testability,test,test,492,"> One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. @ktpolanski Thanks for the suggestion. Can you clarify which implementation you used where this is implemented?. I think one can essentially convert the variance of Pearson residuals into a p-value using a chi-square test. Then instead of the fixed number of HVG genes one could use some p-value cutoff. We have not experimented with this approach at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-874558246
https://github.com/scverse/scanpy/pull/1715#issuecomment-874842313:12,Testability,test,testing,12,"I found the testing in the [original R implementation](https://github.com/willtownes/scrna2019/blob/master/util/functions.R) of the deviances, and then mirrored it in [my implementation](https://github.com/theislab/scanpy/pull/1765). Glad to hear there seems to be potential for something analogous here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-874842313
https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467:843,Deployability,release,release,843,"Hey @giovp !. Thanks for your review and sorry for the delay, but I think I addressed all requests now:; - code moved to experimental; - fixed broken column ordering when batch argument was used with HVG selection; - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that?. Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467
https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467:223,Energy Efficiency,adapt,adapted,223,"Hey @giovp !. Thanks for your review and sorry for the delay, but I think I addressed all requests now:; - code moved to experimental; - fixed broken column ordering when batch argument was used with HVG selection; - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that?. Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467
https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467:223,Modifiability,adapt,adapted,223,"Hey @giovp !. Thanks for your review and sorry for the delay, but I think I addressed all requests now:; - code moved to experimental; - fixed broken column ordering when batch argument was used with HVG selection; - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that?. Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467
https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467:217,Testability,test,tests,217,"Hey @giovp !. Thanks for your review and sorry for the delay, but I think I addressed all requests now:; - code moved to experimental; - fixed broken column ordering when batch argument was used with HVG selection; - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that?. Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467
https://github.com/scverse/scanpy/pull/1715#issuecomment-889285607:212,Usability,feedback,feedback,212,Hey all!. Just wanted to let you know that I've also made a PR for the tutorial as @giovp suggested:. https://github.com/theislab/scanpy-tutorials/pull/43. Let me know what's left to do - looking forward to your feedback!. Jan,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-889285607
https://github.com/scverse/scanpy/pull/1715#issuecomment-890768314:196,Availability,error,error,196,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen?. Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs; - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-890768314
https://github.com/scverse/scanpy/pull/1715#issuecomment-890768314:600,Availability,ping,ping,600,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen?. Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs; - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-890768314
https://github.com/scverse/scanpy/pull/1715#issuecomment-890768314:498,Performance,perform,performed,498,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen?. Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs; - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-890768314
https://github.com/scverse/scanpy/pull/1715#issuecomment-890959567:388,Performance,perform,performed,388,"Hey @giovp,; thanks for going over the PR once more - I'm sorry about the problem with the docs, anything I can do here? I am not very experienced with readthedocs.. > I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen?. Yes, thanks for catching that! Seems I just forgot to `git add _recipe.py`. Regarding the name of the output field: I decided to call it `X_pearson_residuals_pca` as it is the data in `X` after Pearson residuals plus PCA. I thought that adds some clarity to how that PCA was obtained. . On the other hand, if one were to apply Pearson residuals and PCA ""manually"" in sequence and with default settings, one would get an `adata` with `X` holding the Pearson residuals and `obs['X_pca']` holding the PCA results.. that is also how the `recipe_weinreb17()` returns its PCA. So maybe it would be cleaner the way you suggested. Same goes for the `adata.uns['pearson_residuals_pca']` field btw, which I would then rename to `adata.uns['pca']`. I will make a quick commit including that change!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-890959567
https://github.com/scverse/scanpy/pull/1715#issuecomment-894153042:563,Energy Efficiency,green,green,563,"@jlause I think I caught the remaining bug for the docs, now they should pass. ; EDIT: they do!. I also fixed the `X_pca` notation in the `normalize_pearson_residuals_pca` method. Since there are many arguments that overlap between functions, do you mind going through all of them again and make sure they are consistent? We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. Thanks again for the effort! It's been quite a ride 😅 hope it was enjoyable to some extent... now let's wait for green light from @ivirshup and then we can merge this as well as the tutorial . remaining TODO before merging:; - [ ] link tutorial to documentation",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-894153042
https://github.com/scverse/scanpy/pull/1715#issuecomment-900656064:24,Usability,feedback,feedback,24,"Thanks for the detailed feedback @ivirshup , I've started to and will be working on that in the next days!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-900656064
https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395:245,Energy Efficiency,efficient,efficient,245,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:; - [ ] Make tests faster (re-use results where possible); - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:; - on the keyword/positional argument issue; - on the the ""is median rank a good way to do HVG selection across batches""-issue; - on the question what the final names of the functions should be - your suggestions were:; > `normalize_pearson_residuals` -> `pearson_residuals`; > It's a bit more like log1p; >; > `sc.experimental.pp.highly_variable_genes` -> something else; > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) ; Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395
https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395:986,Integrability,wrap,wraps,986,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:; - [ ] Make tests faster (re-use results where possible); - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:; - on the keyword/positional argument issue; - on the the ""is median rank a good way to do HVG selection across batches""-issue; - on the question what the final names of the functions should be - your suggestions were:; > `normalize_pearson_residuals` -> `pearson_residuals`; > It's a bit more like log1p; >; > `sc.experimental.pp.highly_variable_genes` -> something else; > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) ; Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395
https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395:172,Testability,test,tests,172,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:; - [ ] Make tests faster (re-use results where possible); - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:; - on the keyword/positional argument issue; - on the the ""is median rank a good way to do HVG selection across batches""-issue; - on the question what the final names of the functions should be - your suggestions were:; > `normalize_pearson_residuals` -> `pearson_residuals`; > It's a bit more like log1p; >; > `sc.experimental.pp.highly_variable_genes` -> something else; > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) ; Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395
https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395:229,Testability,test,tests,229,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:; - [ ] Make tests faster (re-use results where possible); - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:; - on the keyword/positional argument issue; - on the the ""is median rank a good way to do HVG selection across batches""-issue; - on the question what the final names of the functions should be - your suggestions were:; > `normalize_pearson_residuals` -> `pearson_residuals`; > It's a bit more like log1p; >; > `sc.experimental.pp.highly_variable_genes` -> something else; > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) ; Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395
https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463:33,Availability,failure,failures,33,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463
https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463:93,Availability,failure,failures,93,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463
https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463:28,Testability,test,test,28,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463
https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463:69,Testability,test,tests,69,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463
https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463:102,Usability,clear,clearly,102,"PPS: I see that I'm getting test failures with some github automatic tests, with none of the failures clearly coming from the code I edited -- do you know what is going on here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902986463
https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698:1225,Availability,avail,available,1225,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix; > ; > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. ; This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know!. > ## Docs consistency; > ; > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698
https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698:468,Performance,optimiz,optimized,468,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix; > ; > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. ; This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know!. > ## Docs consistency; > ; > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698
https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698:848,Usability,simpl,simply,848,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix; > ; > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. ; This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know!. > ## Docs consistency; > ; > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698
https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207:150,Energy Efficiency,efficient,efficient,150,"Hey, just as a quick summary of how things stand from my view:. ; - [x] Make tests faster (re-use results where possible); - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it?; - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment); - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)); - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?!. I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207
https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207:77,Testability,test,tests,77,"Hey, just as a quick summary of how things stand from my view:. ; - [x] Make tests faster (re-use results where possible); - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it?; - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment); - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)); - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?!. I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207
https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207:134,Testability,test,tests,134,"Hey, just as a quick summary of how things stand from my view:. ; - [x] Make tests faster (re-use results where possible); - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it?; - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment); - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)); - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?!. I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207
https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207:283,Testability,test,tests,283,"Hey, just as a quick summary of how things stand from my view:. ; - [x] Make tests faster (re-use results where possible); - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it?; - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment); - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)); - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?!. I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207
https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207:1228,Testability,test,tests,1228,"Hey, just as a quick summary of how things stand from my view:. ; - [x] Make tests faster (re-use results where possible); - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it?; - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment); - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)); - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?!. I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207
https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207:1402,Testability,test,tests,1402,"Hey, just as a quick summary of how things stand from my view:. ; - [x] Make tests faster (re-use results where possible); - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it?; - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment); - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)); - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?!. I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207
https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207:1551,Usability,feedback,feedback,1551,"Hey, just as a quick summary of how things stand from my view:. ; - [x] Make tests faster (re-use results where possible); - [x] Make tests more code-efficient by code-sharing between functions where possible. Both done, hopefully enough to address @ivirshup 's comments :) Now both tests take less than 20secs (which is a lot shorter than before). These issues are still up for discussion/here I need your input to finish up:. - the keyword/positional argument issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687448287) code comment) -- here @giovp also mentioned that he could fix it?; - the ""is median rank a good way to do HVG selection across batches""-issue (see [this](https://github.com/theislab/scanpy/pull/1715#discussion_r687465687) code comment); - the question what the final names of the functions should be (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - add an option for fast-lane feature selection? (see my [last post](https://github.com/theislab/scanpy/pull/1715#issuecomment-903315698)); - docs consistency (see @ivirshup's [last post](https://github.com/theislab/scanpy/pull/1715#pullrequestreview-728217616)); - [failing tests](https://github.com/theislab/scanpy/pull/1715#issuecomment-902986463) - I hope I did not break anything here, but I don't really understand how the problems in `scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k` could be caused by changes in my code?!. I'll be off for vacation until Thursday and can respond to any feedback after that - looking forward!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-907829207
https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513:1841,Availability,avail,available,1841,"move `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. ; If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post); > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513
https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513:1930,Availability,avail,available,1930,"move `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. ; If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post); > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513
https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513:1271,Modifiability,refactor,refactor,1271,"move `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. ; If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post); > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513
https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513:497,Usability,clear,clear,497,"thanks a lot for the extensive summary @jlause . . > the keyword/positional argument issue (see this code comment) -- here @giovp also mentioned that he could fix it?. enforced keyword for both pearson residual and hvg function. @jlause please revert (remove `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. ; If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post); > A number of parameters are available in multiple functions. Would it make sense to use some of our",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513
https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513:1656,Usability,clear,clearer,1656,"move `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. ; If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post); > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513
https://github.com/scverse/scanpy/pull/1715#issuecomment-912511459:198,Testability,test,tests,198,"@giovp thanks for your reply, I agree on all points :) Have a good vacation!; @ivirshup Let me know if you have any feedback on the open points or if I can do anything in the meantime (e.g. failing tests, docs, fast-lane HVG).; Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-912511459
https://github.com/scverse/scanpy/pull/1715#issuecomment-912511459:116,Usability,feedback,feedback,116,"@giovp thanks for your reply, I agree on all points :) Have a good vacation!; @ivirshup Let me know if you have any feedback on the open points or if I can do anything in the meantime (e.g. failing tests, docs, fast-lane HVG).; Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-912511459
https://github.com/scverse/scanpy/pull/1715#issuecomment-949761007:56,Deployability,update,updates,56,"Hey @ivirshup @giovp @LuckyMD & @dkobak ,. Is there any updates on this PR or the 1.9 timeline? I'll be off for a week now but once I'm back I'd be happy to work on any remaining tasks that are needed to get this merged! See my above posts for what I think is still left to do, mainly waiting on input from @ivirshup I think. I already posted a tutorial draft here: https://github.com/theislab/scanpy-tutorials/pull/43 and can also work on that if there is more feedback to address. Looking forward to finishing this up!; Cheers,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-949761007
https://github.com/scverse/scanpy/pull/1715#issuecomment-949761007:462,Usability,feedback,feedback,462,"Hey @ivirshup @giovp @LuckyMD & @dkobak ,. Is there any updates on this PR or the 1.9 timeline? I'll be off for a week now but once I'm back I'd be happy to work on any remaining tasks that are needed to get this merged! See my above posts for what I think is still left to do, mainly waiting on input from @ivirshup I think. I already posted a tutorial draft here: https://github.com/theislab/scanpy-tutorials/pull/43 and can also work on that if there is more feedback to address. Looking forward to finishing this up!; Cheers,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-949761007
https://github.com/scverse/scanpy/pull/1715#issuecomment-959860881:63,Usability,feedback,feedback,63,"Hey @ivirshup,. thanks for your comments, that looks like good feedback! Hope your moving went well :); I'm busier than expected this week, but will take some time next week to respond / make the changes you suggested.; Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-959860881
https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467:419,Performance,cache,cache,419,"Hey all!; Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182).; - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that?. Hope you enjoy your Christmas holidays!; Best,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467
https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467:453,Testability,test,testing,453,"Hey all!; Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182).; - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that?. Hope you enjoy your Christmas holidays!; Best,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467
https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467:99,Usability,feedback,feedback,99,"Hey all!; Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182).; - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that?. Hope you enjoy your Christmas holidays!; Best,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467
https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467:208,Usability,simpl,simple,208,"Hey all!; Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182).; - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that?. Hope you enjoy your Christmas holidays!; Best,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467
https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133:283,Availability,ping,ping,283,"Hey @giovp & @ivirshup,; hope you had a good start into 2022! I was getting a twitter request recently asking about when this PR will be merged - are there any news on the timeline yet?. For the PR itself I made suggestions for the few remaining points (see my previous post) - just ping me here if you have feedback on that or if there is anything else to do!. Looking forward to wrap this up :); Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133
https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133:381,Integrability,wrap,wrap,381,"Hey @giovp & @ivirshup,; hope you had a good start into 2022! I was getting a twitter request recently asking about when this PR will be merged - are there any news on the timeline yet?. For the PR itself I made suggestions for the few remaining points (see my previous post) - just ping me here if you have feedback on that or if there is anything else to do!. Looking forward to wrap this up :); Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133
https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133:308,Usability,feedback,feedback,308,"Hey @giovp & @ivirshup,; hope you had a good start into 2022! I was getting a twitter request recently asking about when this PR will be merged - are there any news on the timeline yet?. For the PR itself I made suggestions for the few remaining points (see my previous post) - just ping me here if you have feedback on that or if there is anything else to do!. Looking forward to wrap this up :); Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133
https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277:274,Availability,error,error,274,"Hi @giovp,; no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. If there is anything I should look into, let me know - I have some time for this next week!; Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277
https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277:164,Deployability,integrat,integration,164,"Hi @giovp,; no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. If there is anything I should look into, let me know - I have some time for this next week!; Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277
https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277:164,Integrability,integrat,integration,164,"Hi @giovp,; no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. If there is anything I should look into, let me know - I have some time for this next week!; Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277
https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277:280,Integrability,message,messages,280,"Hi @giovp,; no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. If there is anything I should look into, let me know - I have some time for this next week!; Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277
https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277:212,Testability,test,tests,212,"Hi @giovp,; no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. If there is anything I should look into, let me know - I have some time for this next week!; Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277
https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277:218,Testability,test,test,218,"Hi @giovp,; no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. If there is anything I should look into, let me know - I have some time for this next week!; Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610:99,Availability,error,error,99,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610:105,Integrability,message,messages,105,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610:240,Performance,cache,cache,240,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610:37,Testability,test,tests,37,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610:43,Testability,test,test,43,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:421,Integrability,wrap,wraps,421,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:489,Integrability,wrap,wraps,489,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:506,Integrability,wrap,wrapper,506,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:591,Integrability,wrap,wrapper,591,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:44,Performance,cache,cache,44,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:103,Performance,cache,cache,103,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:159,Performance,cache,cache,159,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:355,Performance,cache,cached,355,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:375,Testability,test,testing,375,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241
https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:130,Usability,Simpl,Simple,130,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241
https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:331,Integrability,wrap,wrapper,331,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705
https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:109,Performance,cache,cached,109,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705
https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:213,Performance,cache,cache,213,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705
https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:129,Testability,test,testing,129,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705
https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:199,Testability,test,test,199,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705
https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:168,Usability,simpl,simply,168,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705
https://github.com/scverse/scanpy/pull/1715#issuecomment-1054373951:748,Deployability,release,release,748,">> for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p. > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix?. I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs; and check that:; - arguments and doc params match; - typo and other minor issues still present (e.g. difficult phrasing). . if this gets approval, before merging to master todo:; - [x] add release note; - [ ] go over scanpy_tutorials and re run tutorial and merge; - [x] link tutorial in docs. p.s. docs are failing for reasons I have haven't figured out yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1054373951
https://github.com/scverse/scanpy/pull/1715#issuecomment-1065345395:1253,Deployability,release,release,1253,"Hi!. > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p.; > ; > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix?; > ; > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > ; > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:; > ; > * arguments and doc params match; > ; > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree.; - I think the `.._pca` function is missing from the release note. should I add it there?; - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that?. > ; > ; > if this gets approval, before merging to master todo:; > ; > * [x] add release note; > ; > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :); Best! Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065345395
https://github.com/scverse/scanpy/pull/1715#issuecomment-1065345395:1525,Deployability,release,release,1525,"Hi!. > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p.; > ; > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix?; > ; > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > ; > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:; > ; > * arguments and doc params match; > ; > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree.; - I think the `.._pca` function is missing from the release note. should I add it there?; - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that?. > ; > ; > if this gets approval, before merging to master todo:; > ; > * [x] add release note; > ; > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :); Best! Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065345395
https://github.com/scverse/scanpy/pull/1715#issuecomment-1065345395:1701,Integrability,wrap,wrapped,1701,"Hi!. > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p.; > ; > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix?; > ; > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > ; > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:; > ; > * arguments and doc params match; > ; > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree.; - I think the `.._pca` function is missing from the release note. should I add it there?; - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that?. > ; > ; > if this gets approval, before merging to master todo:; > ; > * [x] add release note; > ; > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :); Best! Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065345395
https://github.com/scverse/scanpy/pull/1715#issuecomment-1065373944:310,Deployability,release,release,310,"> sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in code format or just plain text?. I'm not sure, I think with math is nicer but not aware of any convention. @ivirshup ?. > I think the .._pca function is missing from the release note. should I add it there?; The ..pca function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that?. must say I missed those sorry, feel free to add and I'll take a look again tomorrow and wrap it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065373944
https://github.com/scverse/scanpy/pull/1715#issuecomment-1065373944:585,Integrability,wrap,wrap,585,"> sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in code format or just plain text?. I'm not sure, I think with math is nicer but not aware of any convention. @ivirshup ?. > I think the .._pca function is missing from the release note. should I add it there?; The ..pca function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that?. must say I missed those sorry, feel free to add and I'll take a look again tomorrow and wrap it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065373944
https://github.com/scverse/scanpy/pull/1715#issuecomment-1065878828:84,Deployability,release,release,84,"Hi, I worked over the docs completely once but still need to do a few small things (release note, final spell check) but need to leave my desk now - will do the final bits either later today or tomorrow!; Best jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065878828
https://github.com/scverse/scanpy/pull/1715#issuecomment-1065994090:165,Deployability,release,release,165,"Hi @giovp ,; I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the `returns` sections of the docs a bit more consistent. Also, it seems that building the docs is failing again on github (locally it works with some warnings). Again I'm not sure why / if it is even related to my changes :thinking: . Let me know if I can help with fixing that or if anything else comes up!. Best, Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065994090
https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112:188,Deployability,release,release,188,"ok tutorial is merged, you can have a look how it renders here: https://scanpy-tutorials.readthedocs.io/en/latest/tutorial_pearson_residuals.html. I've fixed the tutorial.rst page and the release note. To me it looks good, I'd like to get @ivirshup approval on this before merging. > I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the returns sections of the docs a bit more consistent. really clear and coincise btw, great job",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112
https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112:436,Deployability,release,release,436,"ok tutorial is merged, you can have a look how it renders here: https://scanpy-tutorials.readthedocs.io/en/latest/tutorial_pearson_residuals.html. I've fixed the tutorial.rst page and the release note. To me it looks good, I'd like to get @ivirshup approval on this before merging. > I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the returns sections of the docs a bit more consistent. really clear and coincise btw, great job",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112
https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112:530,Usability,clear,clear,530,"ok tutorial is merged, you can have a look how it renders here: https://scanpy-tutorials.readthedocs.io/en/latest/tutorial_pearson_residuals.html. I've fixed the tutorial.rst page and the release note. To me it looks good, I'd like to get @ivirshup approval on this before merging. > I'm done from my side of things: I have re-worded some parts of the docstrings (hopefully to better readability ;) ), added the missing function to the release note and tried to make the returns sections of the docs a bit more consistent. really clear and coincise btw, great job",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1069479112
https://github.com/scverse/scanpy/pull/1715#issuecomment-1578401813:307,Availability,down,downstream,307,"> . Hi, @jlause . There's a issue when using `normalize_pearson_residuals`, it seems that we can't calculated the `log2foldchange` in `rank_genes_groups` will be failed. That's because `np.expm1` can't restore the `adata.X` after `normalize_pearson_residuals`. Could you solve this issue that completed the downstream currently?. <img width=""770"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/46667721/a8e64ab1-360d-43a2-a07b-a766049bcbcd"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1578401813
https://github.com/scverse/scanpy/issues/1718#issuecomment-791195880:76,Usability,simpl,simply,76,"Retrieve cell names in the Seurat object used to create the embedding, then simply reorder AnnData accordingly (adata = adata[cell_names]).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718#issuecomment-791195880
https://github.com/scverse/scanpy/issues/1718#issuecomment-799993218:333,Testability,log,logically,333,"Hi @dawe ,; I follow your code and it work well, but the result showing weird, the scvelo arrow indicate the development direction just was in contrast to the monocle direction.; In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result.; I guess what i make the cell order was wrong ? i follow your code-adata = adata[cell_names]- to order the cell , i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix?; why was the order runing so quickly that the matrix of annData not be sorted at the same time?; Best,; hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718#issuecomment-799993218
https://github.com/scverse/scanpy/issues/1718#issuecomment-801970805:723,Availability,robust,robust,723,"> In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. As @LuckyMD said, this is a question for `scvelo`. . > I guess what i make the cell order was wrong ? . The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time?. Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718#issuecomment-801970805
https://github.com/scverse/scanpy/issues/1718#issuecomment-801970805:156,Testability,log,logically,156,"> In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result. As @LuckyMD said, this is a question for `scvelo`. . > I guess what i make the cell order was wrong ? . The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done. > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time?. Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names. d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718#issuecomment-801970805
https://github.com/scverse/scanpy/issues/1718#issuecomment-802436872:755,Availability,robust,robust,755,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result.; > ; > As @LuckyMD said, this is a question for `scvelo`.; > ; > > I guess what i make the cell order was wrong ?; > ; > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done.; > ; > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time?; > ; > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names.; > ; > d; Thanks i would check currently, and reported the result as soon as possible. ; Best,; hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718#issuecomment-802436872
https://github.com/scverse/scanpy/issues/1718#issuecomment-802436872:158,Testability,log,logically,158,"> > In the other word, the scvelo's 'scv.pl.velocity_embedding_stream' showing terminal differentiation cells develop to original cells. this was incorrected logically. why the scvelo showed the inverted result contrast with monocle result.; > ; > As @LuckyMD said, this is a question for `scvelo`.; > ; > > I guess what i make the cell order was wrong ?; > ; > The best way to check if ordering went wrong is to plot an embedding colored by some known grouping. If colors are all mixed up you know a mistake has done.; > ; > > i wonder whether the code just sorted the cell barcode on annData.obs but the annData.X's matrix? why was the order runing so quickly that the matrix of annData not be sorted at the same time?; > ; > Luckily `AnnData` is quite robust and it reorder any slot (`obs`, `obsp`, `obsm`…) according to the specified cell names.; > ; > d; Thanks i would check currently, and reported the result as soon as possible. ; Best,; hanhuihong",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1718#issuecomment-802436872
https://github.com/scverse/scanpy/issues/1719#issuecomment-793713405:256,Safety,avoid,avoid,256,"Actually, I just realised that ; ```python; adata.var_names = adata.var_names + ""-"" + adata.var[""ensembl_id""]; ```; is not optimal because it adds ENSEMBL to every single var name, making the names very long (which is what most people are likely trying to avoid)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719#issuecomment-793713405
https://github.com/scverse/scanpy/issues/1719#issuecomment-793798821:493,Availability,error,error,493,"How about MultiIndex?; ```python; df = adata.var.reset_index().set_index(['gene_ids', 'index']); adata2 = sc.AnnData(X = adata.X, var = df); ```. user can subset genes based on:; ```python; genes = ['CD69', 'CD44', 'CXCR5']; idx = adata2.var.index.get_level_values('index').isin(genes); adata2[:,idx]; ```. Seems to initialize ok but throws an issue with `get.py` line 139 when trying to plot with `sc.pl.violin`:; ```python; genes2 = adata2[:,idx].var.index; sc.pl.violin(adata2, genes2). ## error; gene_names = pd.Series(adata.var_names, index=adata.var_names); ```; `initializing a Series from a MultiIndex is not supported`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1719#issuecomment-793798821
https://github.com/scverse/scanpy/issues/1720#issuecomment-792236673:88,Deployability,configurat,configurations,88,"@gokceneraslan A few thoughts on this:. 1. In general, I don't like messing with global configurations for other packages.; 2. I generate many plots, but only a few are actually going to go to a manuscript. I'm not sure it's justified to increase all PDF sizes so that they're all editable. Maybe it should be opt-in? And do we need to mess with global settings to do this, or can we just make finding out about this easier for users?. What if there was an ""Plotting output options"" tutorial, which went over saving of figures? This could include a section on how to export for publication.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720#issuecomment-792236673
https://github.com/scverse/scanpy/issues/1720#issuecomment-792236673:88,Modifiability,config,configurations,88,"@gokceneraslan A few thoughts on this:. 1. In general, I don't like messing with global configurations for other packages.; 2. I generate many plots, but only a few are actually going to go to a manuscript. I'm not sure it's justified to increase all PDF sizes so that they're all editable. Maybe it should be opt-in? And do we need to mess with global settings to do this, or can we just make finding out about this easier for users?. What if there was an ""Plotting output options"" tutorial, which went over saving of figures? This could include a section on how to export for publication.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720#issuecomment-792236673
https://github.com/scverse/scanpy/issues/1720#issuecomment-792304104:381,Integrability,wrap,wrapping,381,"> can we just make finding out about this easier for users?. That’s what I feel like most of our plot options are. Kinda like an executable FAQ. Python’s `itertools` docs have a [recipes](https://docs.python.org/3/library/itertools.html#itertools-recipes) section instead, which are copyable code for less common use cases. I think going for docs here make sense because we’re not wrapping something complex, just increasing visibility for some useful option. Maybe we can add a table that shows what some of our plot options do with `rcParams` behind the scenes (and also some `rcParams` like this one which aren’t covered by our plot options)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720#issuecomment-792304104
https://github.com/scverse/scanpy/issues/1720#issuecomment-792413196:37,Usability,guid,guide,37,I think this could be a good [how to guide](https://documentation.divio.com/how-to-guides/). Arguably this should be a how to guide for `matplotlib` though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720#issuecomment-792413196
https://github.com/scverse/scanpy/issues/1720#issuecomment-792413196:83,Usability,guid,guides,83,I think this could be a good [how to guide](https://documentation.divio.com/how-to-guides/). Arguably this should be a how to guide for `matplotlib` though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720#issuecomment-792413196
https://github.com/scverse/scanpy/issues/1720#issuecomment-792413196:126,Usability,guid,guide,126,I think this could be a good [how to guide](https://documentation.divio.com/how-to-guides/). Arguably this should be a how to guide for `matplotlib` though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720#issuecomment-792413196
https://github.com/scverse/scanpy/pull/1722#issuecomment-792235187:144,Deployability,release,release,144,"Thanks for the PR!. On content, I think it would be helpful if this had a short description of the method. Also, what you like it to say in the release notes?. I've changed the base from `1.7.x` to `master` since it looks like you've added the commit on the `master` branch. I think it makes the most sense to add this to the master branch for now, and I'll get back to you on whether the docs will be updated with the `1.7.2` or the `1.8.0` release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722#issuecomment-792235187
https://github.com/scverse/scanpy/pull/1722#issuecomment-792235187:402,Deployability,update,updated,402,"Thanks for the PR!. On content, I think it would be helpful if this had a short description of the method. Also, what you like it to say in the release notes?. I've changed the base from `1.7.x` to `master` since it looks like you've added the commit on the `master` branch. I think it makes the most sense to add this to the master branch for now, and I'll get back to you on whether the docs will be updated with the `1.7.2` or the `1.8.0` release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722#issuecomment-792235187
https://github.com/scverse/scanpy/pull/1722#issuecomment-792235187:442,Deployability,release,release,442,"Thanks for the PR!. On content, I think it would be helpful if this had a short description of the method. Also, what you like it to say in the release notes?. I've changed the base from `1.7.x` to `master` since it looks like you've added the commit on the `master` branch. I think it makes the most sense to add this to the master branch for now, and I'll get back to you on whether the docs will be updated with the `1.7.2` or the `1.8.0` release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722#issuecomment-792235187
https://github.com/scverse/scanpy/pull/1722#issuecomment-793841669:36,Deployability,release,release,36,"Hi! Thanks for the addition. In the release notes I guess something like ""Triku, a new feature selection method was added to our ecosystem"" would be fine. . As for committing to the master, sorry for that. I am eager to see it in the 1.8.0 release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722#issuecomment-793841669
https://github.com/scverse/scanpy/pull/1722#issuecomment-793841669:240,Deployability,release,release,240,"Hi! Thanks for the addition. In the release notes I guess something like ""Triku, a new feature selection method was added to our ecosystem"" would be fine. . As for committing to the master, sorry for that. I am eager to see it in the 1.8.0 release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722#issuecomment-793841669
https://github.com/scverse/scanpy/pull/1722#issuecomment-794836848:48,Deployability,release,release,48,"No problem! . Also, this will go into the 1.7.2 release, but all changes should be made to master while only some get back ported to the current release branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722#issuecomment-794836848
https://github.com/scverse/scanpy/pull/1722#issuecomment-794836848:145,Deployability,release,release,145,"No problem! . Also, this will go into the 1.7.2 release, but all changes should be made to master while only some get back ported to the current release branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1722#issuecomment-794836848
https://github.com/scverse/scanpy/issues/1724#issuecomment-794274079:727,Integrability,wrap,wrapper,727,"Hi @PauBadiaM,. I have always viewed Dorothea and Progeny as methods to aid in the interpretation of my data. Hence, I would assume this might be most useful as a targeted approach to plot activity of a particular TF or pathway. This is something I would probably find most useful as a function where i can either ask for the activity of a single TF/pathway or to get the activity score that explains most variation/correlates with a particular PC. Hence I would err on the side of storing the activities in `.obsm` and then have some functionality around analysing which activity scores are most useful to a user. It will be hard for users to go through all of the data in the end for further analysis. You can always write a wrapper around things like `sc.tl.rank_genes_groups` where the `.obsm` data is copied into a new `adata_tmp.X` for rank genes groups output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-794274079
https://github.com/scverse/scanpy/issues/1724#issuecomment-794802960:15,Security,access,access,15,"I think making access to entires in `obsm` for plotting functions is a good idea. This is definitely on our roadmap, and has started to be implemented (https://github.com/theislab/anndata/pull/342), but is a bit stalled at the moment. Am I correct in understanding that being able to things like:. ```python; adata.obsm[""pathways""] = pathway_dataframe_func(adata); sc.pl.heatmap(adata, groupby=""leiden"", obsm=""pathway""); sc.pl.umap(adata, color=[""pathways/pathway-1"", ""leiden""]); ```. would solve most of the barriers you're facing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-794802960
https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056:331,Deployability,update,update,331,"Thanks for the quick responses @LuckyMD and @ivirshup.; If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it.; Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056
https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056:232,Integrability,wrap,wrapper,232,"Thanks for the quick responses @LuckyMD and @ivirshup.; If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it.; Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056
https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056:79,Security,access,accessible,79,"Thanks for the quick responses @LuckyMD and @ivirshup.; If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it.; Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056
https://github.com/scverse/scanpy/issues/1724#issuecomment-795155685:34,Usability,simpl,simple,34,"on the same line, we wrote a very simple `extract` function in squidpy that we ended up using quite a lot: https://squidpy.readthedocs.io/en/latest/api/squidpy.pl.extract.html. see for instance a usage example here: https://squidpy.readthedocs.io/en/latest/auto_examples/image/compute_texture_features.html#sphx-glr-auto-examples-image-compute-texture-features-py. I think what you guys are working in theislab/anndata#342 has much broader scope, and in general more useful for multi modal data etc. but if you think `sq.pl.extract()` could be a quick and dirty way to get the results you want, we could think of moving it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-795155685
https://github.com/scverse/scanpy/issues/1724#issuecomment-807060037:22,Usability,feedback,feedback,22,"Thank you all for the feedback!. In the end the best solution has been to store activities in `.obsm` and then use the plotting functions via an `extract` function like in `squidpy`. Now that both tools are AnnData compatible, should I open a pull request to add them into the Ecosystem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-807060037
https://github.com/scverse/scanpy/issues/1729#issuecomment-794866272:96,Modifiability,variab,variables,96,"This is already allowed, we just don't have a separate argument for it. Just pass a list of the variables you'd like to groupby. For example:. ```python; import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); genes = list(pbmc.uns[""rank_genes_groups""][""names""][0]). pbmc.obs[""batch""] = np.random.choice([""a"", ""b""], pbmc.n_obs); sc.pl.dotplot(pbmc, genes, [""louvain"", ""batch""]); ```. ![tmp](https://user-images.githubusercontent.com/8238804/110577481-d30fef80-81b6-11eb-93f3-4adb2f269e78.jpg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1729#issuecomment-794866272
https://github.com/scverse/scanpy/issues/1731#issuecomment-803518963:455,Security,expose,expose,455,"I'm not super into the idea of supporting much beyond exactly what `cellranger` outputs for these functions. We expect very specific things from these files, and I think it's difficult to say what's a reasonable amount of modification once we start supporting any. I'd be open to exposing some of the internally used functions so it's easier to write a custom reading function here, if that's a reasonable alternative to you? I think all that we'd really expose here is a faster [`scipy.io.mmread`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.mmread.html#scipy.io.mmread), since the other files are read with `pd.read_csv`. --------------------. @LuckyMD . > At some point scanpy switched to non-gzipped files by default as file I/O is faster that way. Reading files quickly was regarded as a more important that storage minimization. I believe this only applies to writing `h5ad`. But really, `lzf` is probably ideal here. It's much faster than `gzip`, has similar compression, and is barely slower than no compression. But `lzf` is vendored with `h5py` not `hdf5` (last I checked), so you might not be able to read a file compressed that way from `R` or something else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731#issuecomment-803518963
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:1321,Availability,toler,tolerance,1321,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:2,Deployability,update,updated,2,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:10,Deployability,release,release,10,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:36,Testability,test,test,36,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:86,Testability,test,tests,86,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:125,Testability,test,tests,125,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:217,Testability,test,test,217,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:291,Testability,test,test,291,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:370,Testability,test,test,370,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:598,Testability,Test,Test,598,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:1096,Testability,test,testing,1096,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:1179,Testability,test,testing,1179,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:1267,Testability,Test,Test,1267,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:1288,Testability,Assert,AssertionError,1288,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:1583,Testability,test,tests,1583,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072:1624,Testability,Assert,AssertionError,1624,"I updated release notes and added a test for this specific case. I did not write many tests before, so I looked at the other tests and tried to stick to what I saw there. I noted something unexpected when writing the test: When used `np.mean` and `np.var(.., ddof=1)` to compare against the test failed because some of the variances were off. The current version of the test uses `sc.pp._utils._get_mean_var()` (thats what `highly_variable_genes()` uses internally...), and does not fail.. Is it ok to use that instead? Is it expected that numpy and `_get_mean_var()` are slightly different here?. Test code with numpy ground truth:; ```; def test_seurat_v3_mean_var_output_with_batchkey_vs_numpy():; pbmc = sc.datasets.pbmc3k(); pbmc.var_names_make_unique(); n_cells = pbmc.shape[0]; batch = np.zeros((n_cells), dtype=int); batch[1500:] = 1; pbmc.obs[""batch""] = batch. true_mean = np.mean(pbmc.X.toarray(), axis=0); true_var = np.var(pbmc.X.toarray(), axis=0, ddof=1). result_df = sc.pp.highly_variable_genes(; pbmc, batch_key='batch', flavor='seurat_v3', n_top_genes=4000, inplace=False; ); np.testing.assert_allclose(true_mean, result_df['means'], rtol=2e-05, atol=2e-05); np.testing.assert_allclose(true_var, result_df['variances'], rtol=2e-05, atol=2e-05); ```; Test output:; ```; E AssertionError: ; E Not equal to tolerance rtol=2e-05, atol=2e-05; E ; E Mismatched elements: 172 / 32738 (0.525%); E Max absolute difference: 0.01117667; E Max relative difference: 0.00013328; E x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32); E y: array([0., 0., 0., ..., 0., 0., 0.]). tests/test_highly_variable_genes.py:279: AssertionError; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797052072
https://github.com/scverse/scanpy/pull/1732#issuecomment-797693464:497,Energy Efficiency,efficient,efficient,497,"> > Is it ok to use that instead? Is it expected that numpy and _get_mean_var() are slightly different here?; > ; > very interesting, just checked `sc.pp._utils._get_mean_var()` and noticed that variance is not calcualted with `np.var`. Would that makes sense to change in `_utils` to use `np.var` ?; > ; > https://github.com/theislab/scanpy/blob/af8f323ebca87bc98d51e556742acf3c2cdf56e9/scanpy/preprocessing/_utils.py#L6. I'm not experienced enough to say if what currently happens is maybe more efficient than `np.var()`?!. I think the deviating values I reported above come from what happens in the sparse case (`sparse_mean_variance_axis()`).. I can check next week if the same deviations occurs for the nonsparse case. PS: As I said before, not even sure if these deviations are maybe even expected because of the different way of computing the variance here?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-797693464
https://github.com/scverse/scanpy/pull/1732#issuecomment-799050440:526,Testability,test,testing,526,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python; import scanpy as sc, numpy as np; from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1); var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64); _, var_scanpy = _get_mean_var(pbmc.X). # These are close; np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different; assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(); ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-799050440
https://github.com/scverse/scanpy/pull/1732#issuecomment-799050440:602,Testability,assert,assert,602,"Calculating the variance on 32 bit floats, especially in the value ranges you have for counts, will result in a fair bit of inaccuracy. Because of this, we calculate variance with 64 bit values internally. To demonstrate:. ```python; import scanpy as sc, numpy as np; from scanpy.pp._utils import _get_mean_var. pbmc = sc.datasets.pbmc3k(). var_np32 = np.var(pbmc.X.toarray(), axis=0, ddof=1); var_np64 = np.var(pbmc.X.toarray(), axis=0, ddof=1, dtype=np.float64); _, var_scanpy = _get_mean_var(pbmc.X). # These are close; np.testing.assert_allclose(var_np64, var_scanpy). # Same values are different; assert (np.isclose(var_np32, var_np64) == np.isclose(var_np32, var_scanpy)).all(); ```. We don't use the numpy function for variance because then we'd need to create a dense intermediate array. We've previously used `sklearn.utils.sparsefuncs.mean_variance_axis` but that didn't let us control `ddof` or collect to 64 bit values from 32 bit input.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-799050440
https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131:49,Energy Efficiency,adapt,adapted,49,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131
https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131:49,Modifiability,adapt,adapted,49,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131
https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131:61,Testability,test,test,61,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131
https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131:34,Usability,clear,clear,34,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131
https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402:131,Deployability,integrat,integrating,131,"Hey, thanks for your reply!. I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402
https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402:131,Integrability,integrat,integrating,131,"Hey, thanks for your reply!. I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402
https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402:594,Testability,test,test,594,"Hey, thanks for your reply!. I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402
https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402:908,Testability,test,test,908,"Hey, thanks for your reply!. I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402
https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402:1003,Testability,test,tests,1003,"Hey, thanks for your reply!. I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402
https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791:523,Deployability,integrat,integration,523,"Very good catch! It does indeed look like in the function itself it should be. ```; df.sort_values( ; ['highly_variable_nbatches', 'highly_variable_rank'], ; ascending=[False, True], ; na_position='last', ; inplace=True, ; ) ; ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791
https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791:523,Integrability,integrat,integration,523,"Very good catch! It does indeed look like in the function itself it should be. ```; df.sort_values( ; ['highly_variable_nbatches', 'highly_variable_rank'], ; ascending=[False, True], ; na_position='last', ; inplace=True, ; ) ; ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791
https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791:248,Testability,test,test,248,"Very good catch! It does indeed look like in the function itself it should be. ```; df.sort_values( ; ['highly_variable_nbatches', 'highly_variable_rank'], ; ascending=[False, True], ; na_position='last', ; inplace=True, ; ) ; ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791
https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791:291,Testability,test,testing,291,"Very good catch! It does indeed look like in the function itself it should be. ```; df.sort_values( ; ['highly_variable_nbatches', 'highly_variable_rank'], ; ascending=[False, True], ; na_position='last', ; inplace=True, ; ) ; ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791
https://github.com/scverse/scanpy/issues/1733#issuecomment-802390464:200,Testability,test,test,200,"Thanks for digging up the seurat code! Somehow failed to find it when I looked for it.. Should I add the correction to PR #1732, which fixes another little issue in the same function?; And should the test also be changed? Currently only the intersection of the hvgs is checked between scanpy and seurat, so instead of sorting the output again and take the top4000 manually (as done now), one could just take `df['highly_variable']` directly for comparison:. ```; df = sc.pp.highly_variable_genes(; pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=""batch"", inplace=False; ). df = df.loc[df['highly_variable'],:]; seurat_hvg_info_batch = pd.read_csv(; FILE_V3_BATCH, sep=' ', dtype={""variances_norm"": np.float64}; ). # ranks might be slightly different due to many genes having same normalized var; seu = pd.Index(seurat_hvg_info_batch['x'].values); assert len(seu.intersection(df.index)) / 4000 > 0.95; ```. Unfortunately I have no lead where to start with the remaining discrepancy and not enough time at the moment to look into this.. :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802390464
https://github.com/scverse/scanpy/issues/1733#issuecomment-802390464:853,Testability,assert,assert,853,"Thanks for digging up the seurat code! Somehow failed to find it when I looked for it.. Should I add the correction to PR #1732, which fixes another little issue in the same function?; And should the test also be changed? Currently only the intersection of the hvgs is checked between scanpy and seurat, so instead of sorting the output again and take the top4000 manually (as done now), one could just take `df['highly_variable']` directly for comparison:. ```; df = sc.pp.highly_variable_genes(; pbmc, n_top_genes=4000, flavor='seurat_v3', batch_key=""batch"", inplace=False; ). df = df.loc[df['highly_variable'],:]; seurat_hvg_info_batch = pd.read_csv(; FILE_V3_BATCH, sep=' ', dtype={""variances_norm"": np.float64}; ). # ranks might be slightly different due to many genes having same normalized var; seu = pd.Index(seurat_hvg_info_batch['x'].values); assert len(seu.intersection(df.index)) / 4000 > 0.95; ```. Unfortunately I have no lead where to start with the remaining discrepancy and not enough time at the moment to look into this.. :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802390464
https://github.com/scverse/scanpy/issues/1733#issuecomment-802581847:118,Testability,test,tests,118,"@jlause, thanks for figuring this out as well!. A separate PR for this fix would be great. There should definitely be tests targeting this, since changing how the sort is done doesn't seem to break anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802581847
https://github.com/scverse/scanpy/pull/1735#issuecomment-796324421:132,Safety,avoid,avoid,132,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735#issuecomment-796324421
https://github.com/scverse/scanpy/pull/1735#issuecomment-796324421:84,Testability,test,tests,84,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735#issuecomment-796324421
https://github.com/scverse/scanpy/pull/1735#issuecomment-796324421:336,Testability,test,testing,336,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735#issuecomment-796324421
https://github.com/scverse/scanpy/pull/1735#issuecomment-796324421:848,Testability,test,tests,848,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735#issuecomment-796324421
https://github.com/scverse/scanpy/pull/1735#issuecomment-796324421:925,Testability,test,tests,925,"@fidelram Since we are not using tight layout when we save figures for the plotting tests, axis labels are cut off. I enabled it to avoid that with the following modification:. <img width=""759"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110717685-87ba0900-81d7-11eb-8cfd-a1c71155d276.png"">. Here is the new plot testing this PR without the tight layout:. ![master_dotplot_groupby_list_catorder](https://user-images.githubusercontent.com/1140359/110726467-6cef9080-81e7-11eb-971d-e6b87dd92f6e.png). which is pretty bad because what really matters in this plot for this PR is the labels of the x axis. Here is the same plot with the tight layout:. ![master_dotplot_groupby_list_catorder-tightlayout](https://user-images.githubusercontent.com/1140359/110726408-534e4900-81e7-11eb-9931-adae793d099e.png). However, many plotting tests fail now due to this change :/ Do you mind helping me with the failing tests?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735#issuecomment-796324421
https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772:1021,Modifiability,extend,extend,1021,"> ## Question; > ; > Does this interact with group colors and dendrograms at all?. Dendrogram and colors seem not affected. > ## Test change; > ; > All of the plots will start failing because this will change the output for every test. I have a few concerns here:; > ; > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%; > ; > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)?; > ; > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again.; > ; > ; > Proposed solution:; > ; > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772
https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772:129,Testability,Test,Test,129,"> ## Question; > ; > Does this interact with group colors and dendrograms at all?. Dendrogram and colors seem not affected. > ## Test change; > ; > All of the plots will start failing because this will change the output for every test. I have a few concerns here:; > ; > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%; > ; > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)?; > ; > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again.; > ; > ; > Proposed solution:; > ; > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772
https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772:230,Testability,test,test,230,"> ## Question; > ; > Does this interact with group colors and dendrograms at all?. Dendrogram and colors seem not affected. > ## Test change; > ; > All of the plots will start failing because this will change the output for every test. I have a few concerns here:; > ; > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%; > ; > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)?; > ; > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again.; > ; > ; > Proposed solution:; > ; > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772
https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772:320,Testability,test,tests,320,"> ## Question; > ; > Does this interact with group colors and dendrograms at all?. Dendrogram and colors seem not affected. > ## Test change; > ; > All of the plots will start failing because this will change the output for every test. I have a few concerns here:; > ; > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%; > ; > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)?; > ; > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again.; > ; > ; > Proposed solution:; > ; > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772
https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772:851,Testability,test,test,851,"> ## Question; > ; > Does this interact with group colors and dendrograms at all?. Dendrogram and colors seem not affected. > ## Test change; > ; > All of the plots will start failing because this will change the output for every test. I have a few concerns here:; > ; > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%; > ; > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)?; > ; > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again.; > ; > ; > Proposed solution:; > ; > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772
https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772:1049,Testability,test,test,1049,"> ## Question; > ; > Does this interact with group colors and dendrograms at all?. Dendrogram and colors seem not affected. > ## Test change; > ; > All of the plots will start failing because this will change the output for every test. I have a few concerns here:; > ; > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%; > ; > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)?; > ; > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again.; > ; > ; > Proposed solution:; > ; > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772
https://github.com/scverse/scanpy/issues/1739#issuecomment-796944318:321,Deployability,pipeline,pipeline,321,"This sounds interesting, and definitely makes things more clean in the long run... but a big issue I think would be backward compatibility for everything that relies on Scanpy. Also, I wonder if this makes it a bit more difficult for new users as they would need to know what steps are required in a single-cell analysis pipeline to understand the organization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739#issuecomment-796944318
https://github.com/scverse/scanpy/pull/1740#issuecomment-799062288:882,Performance,perform,performance,882,"> only working on genes. technically it could work on continuos covariates as well, should I add that option?. Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in?. > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-799062288
https://github.com/scverse/scanpy/pull/1740#issuecomment-799062288:1028,Testability,test,testing,1028,"> only working on genes. technically it could work on continuos covariates as well, should I add that option?. Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in?. > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-799062288
https://github.com/scverse/scanpy/pull/1740#issuecomment-799760109:392,Testability,test,tests,392,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells.; I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-799760109
https://github.com/scverse/scanpy/pull/1740#issuecomment-799760109:402,Testability,benchmark,benchmark,402,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells.; I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-799760109
https://github.com/scverse/scanpy/pull/1740#issuecomment-799760109:341,Usability,simpl,simplified,341,"damn spent way too much time debugging numba, only to realize that set `parallel=True` in the dot product was clashing with `parallel=True` for the collection. Now it's fast, 10s for 18k genes and 2k cells.; I completely copied over the design from gearys c with the `singledispatch` (btw cool usage for handling different value types!) and simplified a bit the numba part. Tomorrow I'll add tests and benchmark against pysal and then we are ready to go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-799760109
https://github.com/scverse/scanpy/pull/1740#issuecomment-800169773:1268,Testability,test,tests,1268,"reproducibility with pysal: ✅ . ```python; import scanpy as sc; import squidpy as sq. adata = sq.datasets.visium_hne_adata(); adj = adata.obsp[""connectivities""]; dat = adata.X.T; moran_scanpy = sc.metrics.morans_i(adj, dat). import libpysal; import esda. adj = adj.tolil(); neighbors = dict(enumerate(adj.rows)); weights = dict(enumerate(adj.data)); W = libpysal.weights.W(neighbors, weights, ids=adata.obs.index.values). def _compute_moran(y: np.ndarray, w: W, transformation: str, permutations: int):; mi = esda.moran.Moran(; y, w, transformation=transformation, permutations=permutations; ); return mi.I. moran_list = []; transformation = ""O"" # original weights; layer = None; permutations = None; for g in adata.var_names.values:; mi = _compute_moran(; adata.obs_vector(g, layer=layer), W, transformation, permutations; ); moran_list.append(mi). moran_esda = np.array(moran_list); from scipy.stats import pearsonr; import seaborn as sns. g = sns.scatterplot(x=moran_scanpy, y=moran_esda); g.set_xlabel(""scanpy""); g.set_ylabel(""esda""); p = pearsonr(moran_scanpy, moran_esda)[0]; g.set_title(f""pearson_{round(p, 5)}""); ```. ![image](https://user-images.githubusercontent.com/25887487/111299950-d4c33280-8650-11eb-8ace-eafb81ff1aad.png). have a couple of issues with tests but should fix them by today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-800169773
https://github.com/scverse/scanpy/pull/1740#issuecomment-800582076:181,Availability,error,errors,181,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you!; btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-800582076
https://github.com/scverse/scanpy/pull/1740#issuecomment-800582076:23,Testability,test,tests,23,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you!; btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-800582076
https://github.com/scverse/scanpy/pull/1740#issuecomment-800582076:76,Testability,test,tested,76,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you!; btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-800582076
https://github.com/scverse/scanpy/pull/1740#issuecomment-800582076:302,Testability,test,tests,302,"ok, finished also with tests (I took what you had already for gearys C that tested for different types and consistency). Had to change to float32 cause I was having reproducibility errors (possibly due to overflow). ready to review, thank you!; btw I took a fair bit of code from gearysc re design and tests, so if you think should add better acknowledgment or co-author this PR, please go ahead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-800582076
https://github.com/scverse/scanpy/pull/1740#issuecomment-802562555:603,Performance,perform,performance,603,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results.; * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`.; * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-802562555
https://github.com/scverse/scanpy/pull/1740#issuecomment-802562555:940,Testability,test,tests,940,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results.; * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`.; * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-802562555
https://github.com/scverse/scanpy/pull/1740#issuecomment-802659709:248,Deployability,release,release,248,"> a np.sum in a prange loop, plus some minor other things. You can check the linked comment info for more details. 🤦 remember having a look but not quite getting it the first time... thank you!. > Argument order. 👍 . > Minor things. 👍 🙏 . I'll had release note, than we can merge it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-802659709
https://github.com/scverse/scanpy/issues/1742#issuecomment-802755481:28,Usability,clear,clear,28,"Hi @venomandvenus, it's not clear if this is really a bug or you are just getting a bunch of warnings from seaborn, which can happen quite often. Did `sc.pl.violin` eventually output the plot you wanted? Also can you post a link to the tutorial you are following?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1742#issuecomment-802755481
https://github.com/scverse/scanpy/issues/1744#issuecomment-800775264:46,Availability,mask,mask,46,"I've been thinking it would be good to add a `mask` argument to a number of functions. I think `mask_vars=~(adata.var[""mito""] | adata.var[""ribo""])` could work here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1744#issuecomment-800775264
https://github.com/scverse/scanpy/issues/1746#issuecomment-801541188:62,Deployability,update,update,62,"Fixed, we've got a bit of custom plan with readthedocs and an update to their billing code broke that url.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1746#issuecomment-801541188
https://github.com/scverse/scanpy/issues/1747#issuecomment-800776373:280,Safety,avoid,avoided,280,"Initial justification was that it makes a number of computations much faster and doesn't seem to cause problems. If you have examples of problems being caused, that would be great to know. I get this is sort of like `stringsAsFactors`, but I think a lot the problems with that is avoided by categorical having a more sane api than factors. For instance, you can just interact with categorical arrays of strings as though it was an array of strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-800776373
https://github.com/scverse/scanpy/issues/1747#issuecomment-800904479:598,Modifiability,variab,variable,598,"So admittedly this is mostly a philosophical objection, it's probably unlikely to cause any computational problems. The main practical issue I have is converting between Python and R where this causes the type of columns to change. I realise this is kinda a niche problem though and doesn't practically make a lot of difference. My main two philosophically objects are:. 1. Users should have control over what type things are (unless this is required for computational reasons); 2. Functions shouldn't have side effects (i.e. I expect the `highly_variable_genes()` function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in `obs`/`var`). I can see the potential computational benefits in some cases but I wonder if that is actually true for everywhere it is used. It seems mostly like a case of ""this might potentially make a difference so let's do it just in case"". Where it does matter it's probably possible to get the same benefit without making permanent changes to the object. (Also I realise this is a pretty minor thing so not trying to make a big deal out of it. It annoys me but maybe not worth making major changes for 😝.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-800904479
https://github.com/scverse/scanpy/issues/1747#issuecomment-800937743:90,Availability,error,errors,90,"Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-800937743
https://github.com/scverse/scanpy/issues/1747#issuecomment-801601445:624,Modifiability,variab,variable,624,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors?. -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801601445
https://github.com/scverse/scanpy/issues/1747#issuecomment-801601445:811,Modifiability,variab,variable,811,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors?. -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801601445
https://github.com/scverse/scanpy/issues/1747#issuecomment-801719754:92,Availability,error,errors,92,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801719754
https://github.com/scverse/scanpy/issues/1747#issuecomment-801719754:296,Availability,error,error,296,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801719754
https://github.com/scverse/scanpy/issues/1747#issuecomment-801719754:305,Usability,clear,clear,305,"> Hey! Just to chime in, I believe plotting functions also expect categoricals and I've had errors from other functions as well about obs columns not being categorical. I think that was `rank_genes_groups`, but I'm not sure. This is definitely true but easy enough for the user to address if the error is clear (or handle internally as needed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801719754
https://github.com/scverse/scanpy/issues/1747#issuecomment-801731499:540,Modifiability,variab,variable,540,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this.; > ; > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter?. I don't think there is any g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801731499
https://github.com/scverse/scanpy/issues/1747#issuecomment-801731499:1019,Modifiability,variab,variable,1019,"took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this.; > ; > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter?. I don't think there is any good way for a conver",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801731499
https://github.com/scverse/scanpy/issues/1749#issuecomment-803253215:51,Deployability,release,release,51,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>; <summary> My environment </summary>. ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; IPython 7.21.0; PIL 8.1.0; anndata 0.7.5; backcall 0.2.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; ipython_genutils 0.2.0; jedi 0.17.2; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; ptyprocess 0.7.0; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; scanpy 1.7.1; scipy 1.6.1; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; storemagic NA; tables 3.6.1; traitlets 5.0.5; wcwidth 0.2.5; -----; Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]; macOS-10.15.7-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2021-03-20 16:27; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-803253215
https://github.com/scverse/scanpy/issues/1749#issuecomment-803253215:1070,Deployability,update,updated,1070,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>; <summary> My environment </summary>. ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; IPython 7.21.0; PIL 8.1.0; anndata 0.7.5; backcall 0.2.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; ipython_genutils 0.2.0; jedi 0.17.2; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; ptyprocess 0.7.0; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; scanpy 1.7.1; scipy 1.6.1; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; storemagic NA; tables 3.6.1; traitlets 5.0.5; wcwidth 0.2.5; -----; Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]; macOS-10.15.7-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2021-03-20 16:27; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-803253215
https://github.com/scverse/scanpy/issues/1749#issuecomment-803253215:1018,Testability,log,logical,1018,"Hmm, I'm having trouble reproducing using the same release. Could be an issue with an underlying library? I'm using a slightly newer scipy. <details>; <summary> My environment </summary>. ```; -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; IPython 7.21.0; PIL 8.1.0; anndata 0.7.5; backcall 0.2.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; ipython_genutils 0.2.0; jedi 0.17.2; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; llvmlite 0.35.0; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; ptyprocess 0.7.0; pygments 2.8.1; pyparsing 2.4.7; pytz 2021.1; scanpy 1.7.1; scipy 1.6.1; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; storemagic NA; tables 3.6.1; traitlets 5.0.5; wcwidth 0.2.5; -----; Python 3.8.5 (default, Sep 4 2020, 02:22:02) [Clang 10.0.0 ]; macOS-10.15.7-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2021-03-20 16:27; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-803253215
https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453:25,Deployability,update,updated,25,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python; %env PYTHONHASHSEED=0; import numpy as np; np.random.seed(42); import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []; for i in range(10):; adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'); adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal); ```; Output:; ```pytd; env: PYTHONHASHSEED=0. 0.6; ```; In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; soc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453
https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453:100,Deployability,update,updated,100,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python; %env PYTHONHASHSEED=0; import numpy as np; np.random.seed(42); import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []; for i in range(10):; adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'); adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal); ```; Output:; ```pytd; env: PYTHONHASHSEED=0. 0.6; ```; In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; soc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453
https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453:731,Deployability,update,updated,731,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python; %env PYTHONHASHSEED=0; import numpy as np; np.random.seed(42); import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []; for i in range(10):; adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'); adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal); ```; Output:; ```pytd; env: PYTHONHASHSEED=0. 0.6; ```; In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; soc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453
https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453:2491,Deployability,update,updated,2491,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal); ```; Output:; ```pytd; env: PYTHONHASHSEED=0. 0.6; ```; In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; tqdm 4.59.0; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.3; wcwidth 0.2.5; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; jupyterlab 3.0.10; notebook 6.2.0; -----; Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]; Linux-5.8.0-44-generic-x86_64-with-glibc2.10; 28 logical CPU cores; -----; Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453
https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453:902,Performance,bottleneck,bottleneck,902,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python; %env PYTHONHASHSEED=0; import numpy as np; np.random.seed(42); import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []; for i in range(10):; adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'); adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal); ```; Output:; ```pytd; env: PYTHONHASHSEED=0. 0.6; ```; In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; soc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453
https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453:2445,Testability,log,logical,2445,"adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal); ```; Output:; ```pytd; env: PYTHONHASHSEED=0. 0.6; ```; In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; tqdm 4.59.0; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.3; wcwidth 0.2.5; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.21.0; jupyter_client 6.1.11; jupyter_core 4.7.1; jupyterlab 3.0.10; notebook 6.2.0; -----; Python 3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]; Linux-5.8.0-44-generic-x86_64-with-glibc2.10; 28 logical CPU cores; -----; Session information updated at 2021-03-25 10:43. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453
https://github.com/scverse/scanpy/pull/1753#issuecomment-804517933:55,Usability,clear,clear,55,"@flying-sheep, any suggestions on how to automatically clear out all the rst files from `api` and `extension`? I suppose we could add a temporary `git clean`, but that seems a bit harsh.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753#issuecomment-804517933
https://github.com/scverse/scanpy/pull/1753#issuecomment-813822197:546,Testability,stub,stub,546,"> Regarding generated, there’s both no need. I disagree with this. I think we definitely should not be intermingling source and generated files, especially when it's one source file to many generated. This is not a pleasant way to navigate files:. <details>; <summary> </summary>. <img width=""182"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/113657454-543ca280-96e1-11eb-901c-675e8f248150.png"">. </details>. Also, looking around at other big projects with sphinx documentation, this is the way they all handled autosummary stub files. > `git clean -fx -- ./docs`. This is a bit of a nuke. I've added removing the generated docs to `make clean`, but we wouldn't want to run `git clean -fx -- ./docs` for `make clean`. > 2. Changing anything wide-reaching this would break many of our incoming links. This would be solved with [redirects](https://docs.readthedocs.io/en/stable/user-defined-redirects.html). I would be happy to have `docs.scanpy.org/en/latest/api/dotplot.html` be the canonical url, but not if it requires mixing generated and source files. > 3. For case insensitivity let’s use the feature that specifically exists for that problem instead: autosummary_filename_map. Doesn't this only half solve the issue? Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too? Do we have special names for urls for classes which have an associated function? I don't think that would be cleaner than just having all classes be put in a `classes` subdirectory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753#issuecomment-813822197
https://github.com/scverse/scanpy/pull/1753#issuecomment-813954380:103,Modifiability,plugin,plugin,103,"> This is not a pleasant way to navigate files:. <details>; <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select “Hide Ignored Files”. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if it’s possible!. > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too?. We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and don’t do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html` → `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753#issuecomment-813954380
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:37,Availability,failure,failure,37,"Hello,; I am now facing a problem of failure in computing neighbours when using scanpy or scvelo; when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`; or; `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`; it will always reports that. ```pytb; `computing neighbors; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 743 try:; --> 744 yield; 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst); 327 val = self.lower_assign(ty, inst); --> 328 self.storevar(val, inst.target.name); 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name); 1277 name=name); -> 1278 raise AssertionError(msg); 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last); <ipython-input-37-db298150880d> in <module>; ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy); 62 ; 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):; ---> 64 neighbors(; 65 adata,; 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:474,Availability,error,errors,474,"Hello,; I am now facing a problem of failure in computing neighbours when using scanpy or scvelo; when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`; or; `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`; it will always reports that. ```pytb; `computing neighbors; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 743 try:; --> 744 yield; 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst); 327 val = self.lower_assign(ty, inst); --> 328 self.storevar(val, inst.target.name); 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name); 1277 name=name); -> 1278 raise AssertionError(msg); 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last); <ipython-input-37-db298150880d> in <module>; ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy); 62 ; 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):; ---> 64 neighbors(; 65 adata,; 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:3974,Availability,error,errors,3974,"y in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_typ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:4482,Availability,error,errors,4482," y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:5866,Availability,avail,available,5866,"conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385 try:; --> 386 pm.run(self.state); 387 if self.state.cr is not None:; 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 337 (self.pipeline_name, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass_inst, CompilerPass):; --> 330 self._runPass(idx, pass_inst, state); 331 else:; 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:9559,Availability,error,errors,9559,"ion(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self); 214 bb = self.blkmap[offset]; 215 self.builder.position_at_end(bb); --> 216 self.lower_block(block); 217 self.post_lower(); 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback); 133 value = type(); 134 try:; --> 135 self.gen.throw(type, value, traceback); 136 except StopIteration as exc:; 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 751 raise newerr.with_traceback(tb); 752 ; 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:; def rdist(x, y):; <source elided>; result = 0.0; dim = x.shape[0]; ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52); ```; ​; sc.pp.filter_cells(unspliced, min_genes=200); dyn.pl.basic_stats(spliced)`; I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:2771,Deployability,install,installed,2771,"scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata); --> 163 neighbors.compute_neighbors(; 164 n_neighbors=n_neighbors,; 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 748 # we need self._distances also for method == 'gauss' if we didn't; 749 # use dense distances; --> 750 self._distances, self._connectivities = _compute_connectivities_umap(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:5006,Deployability,pipeline,pipeline,5006,"f folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:5118,Deployability,pipeline,pipeline,5118,"f folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:5876,Deployability,pipeline,pipelines,5876,"conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385 try:; --> 386 pm.run(self.state); 387 if self.state.cr is not None:; 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 337 (self.pipeline_name, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass_inst, CompilerPass):; --> 330 self._runPass(idx, pass_inst, state); 331 else:; 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:7735,Deployability,pipeline,pipeline,7735,"ler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state); 287 mutated |= check(pss.run_initialization, internal_state); 288 with SimpleTimer() as pass_time:; --> 289 mutated |= check(pss.run_pass, internal_state); 290 with SimpleTimer() as finalize_time:; 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 260 ; 261 def check(func, compiler_state):; --> 262 mangled = func(compiler_state); 263 if mangled not in (True, False):; 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 461 ; 462 # TODO: Pull this out into the pipeline; --> 463 NativeLowering().run_pass(state); 464 lowered = state['cr']; 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 382 lower = lowering.Lower(targetctx, library, fndesc, interp,; 383 metadata=metadata); --> 384 lower.lower(); 385 if not flags.no_cpython_wrapper:; 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower(self); 134 if self.generator_info is None:; 135 self.genlower = None; --> 136 self.lower_normal_function(self.fndesc); 137 else:; 138 self.genlower = self.GeneratorLower(self). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/env",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:9850,Deployability,pipeline,pipeline,9850,"ion(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self); 214 bb = self.blkmap[offset]; 215 self.builder.position_at_end(bb); --> 216 self.lower_block(block); 217 self.post_lower(); 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback); 133 value = type(); 134 try:; --> 135 self.gen.throw(type, value, traceback); 136 except StopIteration as exc:; 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 751 raise newerr.with_traceback(tb); 752 ; 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:; def rdist(x, y):; <source elided>; result = 0.0; dim = x.shape[0]; ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52); ```; ​; sc.pp.filter_cells(unspliced, min_genes=200); dyn.pl.basic_stats(spliced)`; I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:3475,Energy Efficiency,Reduce,Reduced,3475,"p(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:2746,Integrability,message,message,2746,"scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata); --> 163 neighbors.compute_neighbors(; 164 n_neighbors=n_neighbors,; 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 748 # we need self._distances also for method == 'gauss' if we didn't; 749 # use dense distances; --> 750 self._distances, self._connectivities = _compute_connectivities_umap(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:3583,Integrability,wrap,wrapper,3583,"ties_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:9724,Modifiability,config,config,9724,"ion(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self); 214 bb = self.blkmap[offset]; 215 self.builder.position_at_end(bb); --> 216 self.lower_block(block); 217 self.post_lower(); 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback); 133 value = type(); 134 try:; --> 135 self.gen.throw(type, value, traceback); 136 except StopIteration as exc:; 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 751 raise newerr.with_traceback(tb); 752 ; 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:; def rdist(x, y):; <source elided>; result = 0.0; dim = x.shape[0]; ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52); ```; ​; sc.pp.filter_cells(unspliced, min_genes=200); dyn.pl.basic_stats(spliced)`; I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:367,Testability,Assert,AssertionError,367,"Hello,; I am now facing a problem of failure in computing neighbours when using scanpy or scvelo; when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`; or; `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`; it will always reports that. ```pytb; `computing neighbors; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 743 try:; --> 744 yield; 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst); 327 val = self.lower_assign(ty, inst); --> 328 self.storevar(val, inst.target.name); 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name); 1277 name=name); -> 1278 raise AssertionError(msg); 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last); <ipython-input-37-db298150880d> in <module>; ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy); 62 ; 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):; ---> 64 neighbors(; 65 adata,; 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:1102,Testability,Assert,AssertionError,1102,"anpy or scvelo; when I tried to use the . `sc.pp.neighbors(labelled, n_neighbors=5, n_pcs=4)`; or; `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`; it will always reports that. ```pytb; `computing neighbors; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 743 try:; --> 744 yield; 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst); 327 val = self.lower_assign(ty, inst); --> 328 self.storevar(val, inst.target.name); 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name); 1277 name=name); -> 1278 raise AssertionError(msg); 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last); <ipython-input-37-db298150880d> in <module>; ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy); 62 ; 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):; ---> 64 neighbors(; 65 adata,; 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata); --> 163 neighbors.compute_neighbors(; 164 n_neighbors=n_neighbors,; 165 knn=knn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:1130,Testability,Assert,AssertionError,1130,"eighbors=5, n_pcs=4)`; or; `scv.pp.moments(raw, n_pcs=30, n_neighbors=30)`; it will always reports that. ```pytb; `computing neighbors; ---------------------------------------------------------------------------; AssertionError Traceback (most recent call last); ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 743 try:; --> 744 yield; 745 except NumbaError as e:. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_inst(self, inst); 327 val = self.lower_assign(ty, inst); --> 328 self.storevar(val, inst.target.name); 329 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name); 1277 name=name); -> 1278 raise AssertionError(msg); 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last); <ipython-input-37-db298150880d> in <module>; ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy); 62 ; 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):; ---> 64 neighbors(; 65 adata,; 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata); --> 163 neighbors.compute_neighbors(; 164 n_neighbors=n_neighbors,; 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:5544,Testability,assert,assert,5544,"e[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); --> 606 return pipeline.compile_extra(func); 607 ; 608 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in compile_extra(self, func); 351 self.state.lifted = (); 352 self.state.lifted_from = None; --> 353 return self._compile_bytecode(); 354 ; 355 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_bytecode(self); 413 """"""; 414 assert self.state.func_ir is None; --> 415 return self._compile_core(); 416 ; 417 def _compile_ir(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 393 self.state.status.fail_reason = e; 394 if is_final_pipeline:; --> 395 raise e; 396 else:; 397 raise CompilerError(""All available pipelines exhausted""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler.py in _compile_core(self); 384 res = None; 385 try:; --> 386 pm.run(self.state); 387 if self.state.cr is not None:; 388 break. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 337 (self.pipeline_name, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:1946,Usability,simpl,simplefilter,1946,"rpy/lib/python3.9/site-packages/numba/core/lowering.py in storevar(self, value, name); 1277 name=name); -> 1278 raise AssertionError(msg); 1279 . AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. During handling of the above exception, another exception occurred:. LoweringError Traceback (most recent call last); <ipython-input-37-db298150880d> in <module>; ----> 1 scv.pp.moments(raw, n_pcs=30, n_neighbors=30). ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/moments.py in moments(data, n_neighbors, n_pcs, mode, method, use_rep, use_highly_variable, copy); 62 ; 63 if n_neighbors is not None and n_neighbors > get_n_neighs(adata):; ---> 64 neighbors(; 65 adata,; 66 n_neighbors=n_neighbors,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata); --> 163 neighbors.compute_neighbors(; 164 n_neighbors=n_neighbors,; 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 748 # we need self._distances also for method == 'gauss' if we didn't; 749 # use dense distances; --> 750 self._distances, self._connectivities = _compute_connectivities_umap(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 fro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:7108,Usability,Simpl,SimpleTimer,7108,"python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 337 (self.pipeline_name, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass_inst, CompilerPass):; --> 330 self._runPass(idx, pass_inst, state); 331 else:; 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state); 287 mutated |= check(pss.run_initialization, internal_state); 288 with SimpleTimer() as pass_time:; --> 289 mutated |= check(pss.run_pass, internal_state); 290 with SimpleTimer() as finalize_time:; 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 260 ; 261 def check(func, compiler_state):; --> 262 mangled = func(compiler_state); 263 if mangled not in (True, False):; 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 461 ; 462 # TODO: Pull this out into the pipeline; --> 463 NativeLowering().run_pass(state); 464 lowered = state['cr']; 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 382 lower = lowering.Lower(targetctx, library, fndesc, interp,; 383 metadata=metadata); --> 384 lower.lower(); 385 if not flags.no_cpyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:7202,Usability,Simpl,SimpleTimer,7202,"me, pass_desc); 338 patched_exception = self._patch_error(msg, e); --> 339 raise patched_exception; 340 ; 341 def dependency_analysis(self):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in run(self, state); 328 pass_inst = _pass_registry.get(pss).pass_inst; 329 if isinstance(pass_inst, CompilerPass):; --> 330 self._runPass(idx, pass_inst, state); 331 else:; 332 raise BaseException(""Legacy pass in use""). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in _runPass(self, index, pss, internal_state); 287 mutated |= check(pss.run_initialization, internal_state); 288 with SimpleTimer() as pass_time:; --> 289 mutated |= check(pss.run_pass, internal_state); 290 with SimpleTimer() as finalize_time:; 291 mutated |= check(pss.run_finalizer, internal_state). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/compiler_machinery.py in check(func, compiler_state); 260 ; 261 def check(func, compiler_state):; --> 262 mangled = func(compiler_state); 263 if mangled not in (True, False):; 264 msg = (""CompilerPass implementations should return True/False. "". ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 461 ; 462 # TODO: Pull this out into the pipeline; --> 463 NativeLowering().run_pass(state); 464 lowered = state['cr']; 465 signature = typing.signature(state.return_type, *state.args). ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/typed_passes.py in run_pass(self, state); 382 lower = lowering.Lower(targetctx, library, fndesc, interp,; 383 metadata=metadata); --> 384 lower.lower(); 385 if not flags.no_cpython_wrapper:; 386 lower.create_cpython_wrapper(flags.release_gil). ~/.conda/envs/rpy/lib/python3.9/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796
https://github.com/scverse/scanpy/issues/1756#issuecomment-812989390:11,Availability,down,downgrade,11,"Hi, I have downgrade my numab version to 0.51, it works",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-812989390
https://github.com/scverse/scanpy/issues/1756#issuecomment-813536425:9,Availability,down,downgraded,9,"Ok, so I downgraded to numba version 0.52.0 and that seems to be working well as of 5 minutes ago. . Thanks for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-813536425
https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512:253,Deployability,release,releases,253,"@giovp, how would you like to continue with this? We could either set an upper bound on `numba`, i.e. `numba<0.53.0`, or change how `umap-learn` is pinned. In the latter case, `umap-learn>=0.5.1` should work (see [here](https://github.com/lmcinnes/umap/releases/tag/0.5.1)). I think this approach would be best, since `numba>=0.53` supports `python>=3.9`.; Happy to open the PR if you agree. Would be good to decide on how to proceed ASAP as people keep running into issues when using `scvelo` or `cellrank`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512
https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512:138,Usability,learn,learn,138,"@giovp, how would you like to continue with this? We could either set an upper bound on `numba`, i.e. `numba<0.53.0`, or change how `umap-learn` is pinned. In the latter case, `umap-learn>=0.5.1` should work (see [here](https://github.com/lmcinnes/umap/releases/tag/0.5.1)). I think this approach would be best, since `numba>=0.53` supports `python>=3.9`.; Happy to open the PR if you agree. Would be good to decide on how to proceed ASAP as people keep running into issues when using `scvelo` or `cellrank`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512
https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512:182,Usability,learn,learn,182,"@giovp, how would you like to continue with this? We could either set an upper bound on `numba`, i.e. `numba<0.53.0`, or change how `umap-learn` is pinned. In the latter case, `umap-learn>=0.5.1` should work (see [here](https://github.com/lmcinnes/umap/releases/tag/0.5.1)). I think this approach would be best, since `numba>=0.53` supports `python>=3.9`.; Happy to open the PR if you agree. Would be good to decide on how to proceed ASAP as people keep running into issues when using `scvelo` or `cellrank`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-845723512
https://github.com/scverse/scanpy/issues/1756#issuecomment-845733520:23,Usability,learn,learn,23,"hey @WeilerP ,; > umap-learn>=0.5.1 should work (see here). I think this approach would be best, since numba>=0.53 supports python>=3.9. I agree this is the best solution, and don't think there is any drawback from scanpy side. Can @Koncopd @ivirshup comment on this? If so, I think it would be easy to inlcude it in 1.8",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-845733520
https://github.com/scverse/scanpy/issues/1756#issuecomment-846837983:23,Usability,learn,learn,23,"FWIW, confirming `umap-learn 0.5.1` works with `numba==0.53.1` on my machine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846837983
https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:61,Availability,error,error,61,"with `umap-lean==0.5.0` and `numba=0.53.1` I get a different error. ```python; import scanpy as sc; adata = sc.datasets.pbmc3k_processed(); sc.pp.neighbors(adata); ```. <details>; <summary>Details</summary>. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-4-5d47edb05ae7> in <module>; ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import nump",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466
https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:2794,Availability,error,errors,2794,"in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466
https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:3324,Availability,error,errors,3324,". ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466
https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:1364,Deployability,install,installed,1364,"-4-5d47edb05ae7> in <module>; ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466
https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:3879,Deployability,pipeline,pipeline,3879,"n3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466
https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:3906,Deployability,pipeline,pipeline,3906,"n3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466
https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:4014,Deployability,pipeline,pipeline,4014,"n3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466
https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:1339,Integrability,message,message,1339,"-4-5d47edb05ae7> in <module>; ----> 1 sc.pp.neighbors(adata). ~/Projects/scanpy/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466
https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:2392,Integrability,wrap,wrapper,2392,"t fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466
https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:4212,Modifiability,config,config,4212,"mpile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466
https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:4726,Performance,load,load,4726,"mpile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466
https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:1622,Usability,simpl,simplefilter,1622,"data.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. ~/Projects/scanpy/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466
https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:4926,Usability,learn,learn,4926,"mpile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466
https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206:354,Availability,down,downstream,354,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206
https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206:61,Deployability,install,install,61,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206
https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206:19,Usability,learn,learn,19,"If we pinned `umap-learn>=0.5.1`.1 it would be impossible to install scvelo, since [it pins umap<0.5](https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/requirements.txt#L9). We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846949206
https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729:533,Availability,down,downstream,533,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5?. This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729
https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729:155,Testability,test,tests,155,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5?. This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729
https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729:662,Usability,learn,learn,662,"> This is actually something I've been meaning to bug you about @WeilerP, why does scvelo pin umap below 0.5?. This was only a dirty hack to make our unit tests pass (see e.g. [here](https://github.com/WeilerP/scvelo/runs/2112241472?check_suite_focus=true)). It's no longer pinned on `scvelo@develop` which we plan on merging into master in the following days to tag a new version. > We can ban umap 0.5.0 specifically. It's generally important that scanpy has a broad-ish range of versions it's comparable with, since there's a lot downstream. I'd be happy bump umap to above 0.4 though, since it has been a while for that. I believe the problem is using `umap-learn<=0.5.0` with new `numba` versions (I think `numba>=0.53.0`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846973729
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:1067,Availability,error,errors,1067,"* ; scanpy ; During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); computing neighbors; using 'X_pca' with n_pcs = 40; ; LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last); C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs); 822 try:; --> 823 yield; 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block); 264 loc=self.loc, errcls_=defaulterrcls):; --> 265 self.lower_inst(inst); 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst); 438 ty = self.typeof(inst.target.name); --> 439 val = self.lower_assign(ty, inst); 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst); 625 elif isinstance(value, ir.Expr):; --> 626 return self.lower_expr(ty, value); 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr); 1161 elif expr.op == 'call':; -> 1162 res = self.lower_call(resty, expr); 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:6060,Availability,error,errors,6060,"p\__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig); 963 with ev.trigger_event(""numba:compile"", data=ev_details):; 964 try:; --> 965 cres = self._compiler.compile(args, return_type); 966 except errors.ForceLiteralArg as e:; 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type); 123 ; 124 def compile(self, args, return_type):; --> 125 status, retval = self._compile_cached(args, return_type); 126 if status:; 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type); 137 ; 138 try:; --> 139 retval = self._compile_core(args, return_type); 140 except errors.TypingError as e:; 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type); 150 ; 151 impl = self._get_implementation(args, {}); --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,; 153 self.targetdescr.target_context,; 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:6569,Availability,error,errors,6569,"st(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig); 963 with ev.trigger_event(""numba:compile"", data=ev_details):; 964 try:; --> 965 cres = self._compiler.compile(args, return_type); 966 except errors.ForceLiteralArg as e:; 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type); 123 ; 124 def compile(self, args, return_type):; --> 125 status, retval = self._compile_cached(args, return_type); 126 if status:; 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type); 137 ; 138 try:; --> 139 retval = self._compile_core(args, return_type); 140 except errors.TypingError as e:; 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type); 150 ; 151 impl = self._get_implementation(args, {}); --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,; 153 self.targetdescr.target_context,; 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 150 ; 151 impl = self._get_implementation(args, {}); --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,; 153 self.targetdescr.target_context,; 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 714 pipeline = pipeline_class(typingctx, targetctx, library,; 715 args, return_type, flags, locals); --> 716 return pipel",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:8284,Availability,avail,available,8284,"4 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 714 pipeline = pipeline_class(typingctx, targetctx, library,; 715 args, return_type, flags, locals); --> 716 return pipeline.compile_extra(func); 717 ; 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func); 450 self.state.lifted = (); 451 self.state.lifted_from = None; --> 452 return self._compile_bytecode(); 453 ; 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self); 518 """"""; 519 assert self.state.func_ir is None; --> 520 return self._compile_core(); 521 ; 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 497 self.state.status.fail_reason = e; 498 if is_final_pipeline:; --> 499 raise e; 500 else:; 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 484 res = None; 485 try:; --> 486 pm.run(self.state); 487 if self.state.cr is not None:; 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 366 (self.pipeline_name, pass_desc); 367 patched_exception = self._patch_error(msg, e); --> 368 raise patched_exception; 369 ; 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 354 pass_inst = _pass_registry.get(pss).pass_inst; 355 if isinstance(pass_inst, CompilerPass):; --> 356 self._runPass(idx, pass_inst, state); 357 else:; 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:11687,Availability,error,errors,11687," mangled = func(compiler_state); 274 if mangled not in (True, False):; 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 392 lower = lowering.Lower(targetctx, library, fndesc, interp,; 393 metadata=metadata); --> 394 lower.lower(); 395 if not flags.no_cpython_wrapper:; 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self); 166 if self.generator_info is None:; 167 self.genlower = None; --> 168 self.lower_normal_function(self.fndesc); 169 else:; 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_normal_function(self, fndesc); 220 # Init argument values; 221 self.extract_function_arguments(); --> 222 entry_block_tail = self.lower_function_body(); 223 ; 224 # Close tail of entry block, do not emit debug metadata else the. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_function_body(self); 249 bb = self.blkmap[offset]; 250 self.builder.position_at_end(bb); --> 251 self.lower_block(block); 252 self.post_lower(); 253 return entry_block_tail. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block); 263 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 264 loc=self.loc, errcls_=defaulterrcls):; --> 265 self.lower_inst(inst); 266 self.post_block(block); 267 . C:\ProgramData\Anaconda3\lib\contextlib.py in __exit__(self, typ, value, traceback); 135 value = typ(); 136 try:; --> 137 self.gen.throw(typ, value, traceback); 138 except StopIteration as exc:; 139 # Suppress StopIteration *unless* it's the same exception that. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs); 835 else:; 836 tb = None; --> 837 raise newerr.with_traceback(tb); 838 elif use_new_style_errors():; 839 raise e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:410,Deployability,pipeline,pipeline,410,"**solution please :** ; scanpy ; During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py. sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); computing neighbors; using 'X_pca' with n_pcs = 40; ; LoweringError: Failed in nopython mode pipeline (step: native lowering); Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). File ""C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py"", line 53:; def rdist(x, y):; <source elided>; dim = x.shape[0]; for i in range(dim):; ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, varkwarg=None, target=None)"" at C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py (53). TypeError Traceback (most recent call last); C:\ProgramData\Anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs); 822 try:; --> 823 yield; 824 except NumbaError as e:. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block); 264 loc=self.loc, errcls_=defaulterrcls):; --> 265 self.lower_inst(inst); 266 self.post_block(block). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_inst(self, inst); 438 ty = self.typeof(inst.target.name); --> 439 val = self.lower_assign(ty, inst); 440 argidx = None. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_assign(self, ty, inst); 625 elif isinstance(value, ir.Expr):; --> 626 return self.lower_expr(ty, value); 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr); 1161 elif expr.op == 'call':; -> 1162 res = self.lower_call(resty, expr); 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:4872,Deployability,install,installed,4872,"a\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 809 # we need self._distances also for method == 'gauss' if we didn't; 810 # use dense distances; --> 811 self._distances, self._connectivities = _compute_connectivities_umap(; 812 knn_indices,; 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 390 # umap 0.5.0; 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 392 from umap.umap_ import fuzzy_simplicial_set; 393 ; 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:7433,Deployability,pipeline,pipeline,7433,".py in _compile_cached(self, args, return_type); 137 ; 138 try:; --> 139 retval = self._compile_core(args, return_type); 140 except errors.TypingError as e:; 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type); 150 ; 151 impl = self._get_implementation(args, {}); --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,; 153 self.targetdescr.target_context,; 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 150 ; 151 impl = self._get_implementation(args, {}); --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,; 153 self.targetdescr.target_context,; 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 714 pipeline = pipeline_class(typingctx, targetctx, library,; 715 args, return_type, flags, locals); --> 716 return pipeline.compile_extra(func); 717 ; 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func); 450 self.state.lifted = (); 451 self.state.lifted_from = None; --> 452 return self._compile_bytecode(); 453 ; 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self); 518 """"""; 519 assert self.state.func_ir is None; --> 520 return self._compile_core(); 521 ; 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 497 self.state.status.fail_reason = e; 498 if is_final_pipeline:; --> 499 raise e; 500 else:; 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 484 res = None; 485 try:; --> 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:7545,Deployability,pipeline,pipeline,7545,".py in _compile_cached(self, args, return_type); 137 ; 138 try:; --> 139 retval = self._compile_core(args, return_type); 140 except errors.TypingError as e:; 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_core(self, args, return_type); 150 ; 151 impl = self._get_implementation(args, {}); --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,; 153 self.targetdescr.target_context,; 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 150 ; 151 impl = self._get_implementation(args, {}); --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,; 153 self.targetdescr.target_context,; 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 714 pipeline = pipeline_class(typingctx, targetctx, library,; 715 args, return_type, flags, locals); --> 716 return pipeline.compile_extra(func); 717 ; 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func); 450 self.state.lifted = (); 451 self.state.lifted_from = None; --> 452 return self._compile_bytecode(); 453 ; 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self); 518 """"""; 519 assert self.state.func_ir is None; --> 520 return self._compile_core(); 521 ; 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 497 self.state.status.fail_reason = e; 498 if is_final_pipeline:; --> 499 raise e; 500 else:; 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 484 res = None; 485 try:; --> 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:8294,Deployability,pipeline,pipelines,8294,"4 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 714 pipeline = pipeline_class(typingctx, targetctx, library,; 715 args, return_type, flags, locals); --> 716 return pipeline.compile_extra(func); 717 ; 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func); 450 self.state.lifted = (); 451 self.state.lifted_from = None; --> 452 return self._compile_bytecode(); 453 ; 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self); 518 """"""; 519 assert self.state.func_ir is None; --> 520 return self._compile_core(); 521 ; 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 497 self.state.status.fail_reason = e; 498 if is_final_pipeline:; --> 499 raise e; 500 else:; 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 484 res = None; 485 try:; --> 486 pm.run(self.state); 487 if self.state.cr is not None:; 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 366 (self.pipeline_name, pass_desc); 367 patched_exception = self._patch_error(msg, e); --> 368 raise patched_exception; 369 ; 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 354 pass_inst = _pass_registry.get(pss).pass_inst; 355 if isinstance(pass_inst, CompilerPass):; --> 356 self._runPass(idx, pass_inst, state); 357 else:; 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:5567,Energy Efficiency,Reduce,Reduced,5567,"tivities_umap(; 812 knn_indices,; 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 390 # umap 0.5.0; 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 392 from umap.umap_ import fuzzy_simplicial_set; 393 ; 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig); 963 with ev.trigger_event(""numba:compile"", data=ev_details):; 964 try:; --> 965 cres = self._compiler.compile(args, return_type); 966 except errors.ForceLiteralArg as e:; 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type); 123 ; 124 def compile(self, args, return_type):; --> 125 status, retval = self._compile_cached(args, return_type); 126 if status:; 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type); 137 ; 138 try:; --> 139 retval = self._compile_core(args, return",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:2675,Integrability,wrap,wrapper,2675,"e, ir.Expr):; --> 626 return self.lower_expr(ty, value); 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr); 1161 elif expr.op == 'call':; -> 1162 res = self.lower_call(resty, expr); 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr); 890 else:; --> 891 res = self._lower_call_normal(fnty, expr, signature); 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature); 1132 ; -> 1133 res = impl(self.builder, argvals, self.loc); 1134 return res; C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc); 1189 def __call__(self, builder, args, loc=None):; -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc); 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs); 1219 kwargs.pop('loc') # drop unused loc; -> 1220 return fn(*args, **kwargs); 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args); 37 state.start = context.get_constant(int_type, 0); ---> 38 state.stop = stop; 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value); 163 return super(_StructProxy, self).__setattr__(field, value); --> 164 self[self._datamodel.get_field_position(field)] = value; 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value); 187 else:; --> 188 raise TypeError(""Invalid store of {value.type} to ""; 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringErro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:4847,Integrability,message,message,4847,"a\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 809 # we need self._distances also for method == 'gauss' if we didn't; 810 # use dense distances; --> 811 self._distances, self._connectivities = _compute_connectivities_umap(; 812 knn_indices,; 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 390 # umap 0.5.0; 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 392 from umap.umap_ import fuzzy_simplicial_set; 393 ; 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:5672,Integrability,wrap,wrapper,5672,"e_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 390 # umap 0.5.0; 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 392 from umap.umap_ import fuzzy_simplicial_set; 393 ; 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig); 963 with ev.trigger_event(""numba:compile"", data=ev_details):; 964 try:; --> 965 cres = self._compiler.compile(args, return_type); 966 except errors.ForceLiteralArg as e:; 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type); 123 ; 124 def compile(self, args, return_type):; --> 125 status, retval = self._compile_cached(args, return_type); 126 if status:; 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type); 137 ; 138 try:; --> 139 retval = self._compile_core(args, return_type); 140 except errors.TypingError as e:; 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\disp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:7965,Testability,assert,assert,7965,"ckages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 150 ; 151 impl = self._get_implementation(args, {}); --> 152 cres = compiler.compile_extra(self.targetdescr.typing_context,; 153 self.targetdescr.target_context,; 154 impl,. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 714 pipeline = pipeline_class(typingctx, targetctx, library,; 715 args, return_type, flags, locals); --> 716 return pipeline.compile_extra(func); 717 ; 718 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in compile_extra(self, func); 450 self.state.lifted = (); 451 self.state.lifted_from = None; --> 452 return self._compile_bytecode(); 453 ; 454 def compile_ir(self, func_ir, lifted=(), lifted_from=None):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_bytecode(self); 518 """"""; 519 assert self.state.func_ir is None; --> 520 return self._compile_core(); 521 ; 522 def _compile_ir(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 497 self.state.status.fail_reason = e; 498 if is_final_pipeline:; --> 499 raise e; 500 else:; 501 raise CompilerError(""All available pipelines exhausted""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler.py in _compile_core(self); 484 res = None; 485 try:; --> 486 pm.run(self.state); 487 if self.state.cr is not None:; 488 break. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 366 (self.pipeline_name, pass_desc); 367 patched_exception = self._patch_error(msg, e); --> 368 raise patched_exception; 369 ; 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 354 pass_inst = _pass_registry.get(pss).pass_inst; 355 if isinstance(pass_inst, Compi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:9511,Usability,Simpl,SimpleTimer,9511,"Data\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 366 (self.pipeline_name, pass_desc); 367 patched_exception = self._patch_error(msg, e); --> 368 raise patched_exception; 369 ; 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 354 pass_inst = _pass_registry.get(pss).pass_inst; 355 if isinstance(pass_inst, CompilerPass):; --> 356 self._runPass(idx, pass_inst, state); 357 else:; 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 ; C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state); 309 mutated |= check(pss.run_initialization, internal_state); 310 with SimpleTimer() as pass_time:; --> 311 mutated |= check(pss.run_pass, internal_state); 312 with SimpleTimer() as finalize_time:; 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state); 271 ; 272 def check(func, compiler_state):; --> 273 mangled = func(compiler_state); 274 if mangled not in (True, False):; 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 392 lower = lowering.Lower(targetctx, library, fndesc, interp,; 393 metadata=metadata); --> 394 lower.lower(); 395 if not flags.no_cpython_wrapper:; 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self); 166 if self.generator_info is None:; 167 self.genlower = None; --> 168 self.lower_normal_function(self.fndesc); 169 else:; 170 self.genlower =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:9605,Usability,Simpl,SimpleTimer,9605,"peline_name, pass_desc); 367 patched_exception = self._patch_error(msg, e); --> 368 raise patched_exception; 369 ; 370 def dependency_analysis(self):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in run(self, state); 354 pass_inst = _pass_registry.get(pss).pass_inst; 355 if isinstance(pass_inst, CompilerPass):; --> 356 self._runPass(idx, pass_inst, state); 357 else:; 358 raise BaseException(""Legacy pass in use""). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 33 def _acquire_compile_lock(*args, **kwargs):; 34 with self:; ---> 35 return func(*args, **kwargs); 36 return _acquire_compile_lock; 37 ; C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in _runPass(self, index, pss, internal_state); 309 mutated |= check(pss.run_initialization, internal_state); 310 with SimpleTimer() as pass_time:; --> 311 mutated |= check(pss.run_pass, internal_state); 312 with SimpleTimer() as finalize_time:; 313 mutated |= check(pss.run_finalizer, internal_state). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\compiler_machinery.py in check(func, compiler_state); 271 ; 272 def check(func, compiler_state):; --> 273 mangled = func(compiler_state); 274 if mangled not in (True, False):; 275 msg = (""CompilerPass implementations should return True/False. "". C:\ProgramData\Anaconda3\lib\site-packages\numba\core\typed_passes.py in run_pass(self, state); 392 lower = lowering.Lower(targetctx, library, fndesc, interp,; 393 metadata=metadata); --> 394 lower.lower(); 395 if not flags.no_cpython_wrapper:; 396 lower.create_cpython_wrapper(flags.release_gil). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower(self); 166 if self.generator_info is None:; 167 self.genlower = None; --> 168 self.lower_normal_function(self.fndesc); 169 else:; 170 self.genlower = self.GeneratorLower(self). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325
https://github.com/scverse/scanpy/issues/1757#issuecomment-804509620:1101,Availability,avail,available,1101,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python; normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None); Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable.; normalization_axis: Literal[""var"", ""group""] (default: ""var""); If a `normalization` is passed, which dimension of the data to normalize along.; ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-804509620
https://github.com/scverse/scanpy/issues/1757#issuecomment-804509620:1014,Integrability,wrap,wrapping,1014,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python; normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None); Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable.; normalization_axis: Literal[""var"", ""group""] (default: ""var""); If a `normalization` is passed, which dimension of the data to normalize along.; ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-804509620
https://github.com/scverse/scanpy/issues/1757#issuecomment-873078527:243,Modifiability,flexible,flexible,243,"I also like the `normalization` and `normalization_axis` suggestion, I use z-score also all the time but it's very painful right now. I also agree with Stephen's concerns, minmax is not perfect and can be misleading, and it's safer to be more flexible, provide more normalization options and let the user be responsible for how the plots look like IMO. As I mentioned in #1913, we have to write in the legend that it's min-max scaled expression. We can even remove the color legend labels (0.0, 0.5, 1.0) if minmax is applied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-873078527
https://github.com/scverse/scanpy/issues/1757#issuecomment-873078527:226,Safety,safe,safer,226,"I also like the `normalization` and `normalization_axis` suggestion, I use z-score also all the time but it's very painful right now. I also agree with Stephen's concerns, minmax is not perfect and can be misleading, and it's safer to be more flexible, provide more normalization options and let the user be responsible for how the plots look like IMO. As I mentioned in #1913, we have to write in the legend that it's min-max scaled expression. We can even remove the color legend labels (0.0, 0.5, 1.0) if minmax is applied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-873078527
https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638:1218,Modifiability,variab,variable,1218,"or highly expressed genes is probably more meaningful difference than for a lowly expressed one.; - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher; - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe).; - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:; - [0,1] on groups - seems very variable; ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png); - no normalisation (currently only other option) - bad for multiple marker comparison; ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png); - max_abs scale on groups - probably still exaggerates variability; ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png); - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups.; ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638
https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638:1596,Modifiability,variab,variability,1596,"a lowly expressed one.; - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher; - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe).; - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:; - [0,1] on groups - seems very variable; ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png); - no normalisation (currently only other option) - bad for multiple marker comparison; ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png); - max_abs scale on groups - probably still exaggerates variability; ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png); - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups.; ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638
https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638:1812,Modifiability,variab,variability,1812,"a lowly expressed one.; - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher; - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe).; - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:; - [0,1] on groups - seems very variable; ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png); - no normalisation (currently only other option) - bad for multiple marker comparison; ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png); - max_abs scale on groups - probably still exaggerates variability; ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png); - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups.; ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638
https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638:712,Testability,log,log-norm,712,"A few comments from my endeavors:; - Not sure how good is max scaling on groups for differently strongly expressed genes, especially due to mean-var bias - e.g. 0.8 of max expression for highly expressed genes is probably more meaningful difference than for a lowly expressed one.; - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher; - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe).; - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:; - [0,1] on groups - seems very variable; ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png); - no normalisation (currently only other option) - bad for multiple marker comparison; ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png); - max_abs scale on groups - probably still exaggerates variability; ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png); - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes dif",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638
https://github.com/scverse/scanpy/issues/1757#issuecomment-1084726453:458,Availability,avail,available,458,"I'm pushing this from 1.9, but am also considering how well posed the issue is. There's a fairly complicated relationship between this kind of ""normalization"" and the `norm`, `vmin`, `vmax`, etc. arguments. I think this would need a tutorial (at least) to go with it. Some thoughts:. * Basically, what this feature is is an additional transformation applied to the summarized values before they are mapped to colors. This could (and probably should) also be available for sizes of the dots.; * This is kinda covered by matplotlib's `norm` values, but those can only be applied to all the data at once – not per group.; * If we make it easier to split out getting summarized dataframe then plotting the values, this could largely be handled in user code.; * If you are doing a `z-score` normalization, surely you'd want a centered colorbar and diverging palette?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-1084726453
https://github.com/scverse/scanpy/issues/1758#issuecomment-814597837:129,Deployability,release,release,129,"This seems mostly fine. I would definitely suggest updating to a more recent version, as there could definitely be issues in pre-release builds. If that doesn't solve your problem, could you confirm if `""KY.Chr1.1190"" in adata.var[""Uniq_Name""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-814597837
https://github.com/scverse/scanpy/issues/1758#issuecomment-816317537:426,Availability,error,error,426,```; sc.__version__; '1.8.0.dev78+gc488909a'; ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. ; In other words if adata.raw was missing the notation it failed for me (different error though). ; I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-816317537
https://github.com/scverse/scanpy/issues/1758#issuecomment-816317537:458,Deployability,update,update,458,```; sc.__version__; '1.8.0.dev78+gc488909a'; ```. It seems to be working but I'm currently on a different dataset. What I noticed was that if I didn't have the same ID columns in my `adata.var` when setting adata.raw I couldn't use `gene_symbols`. After setting `adata.var` so it had the same IDs before setting `adata.raw` made it possible. ; In other words if adata.raw was missing the notation it failed for me (different error though). ; I will give an update when I get back to the dataset above. Just to make sure it's the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-816317537
https://github.com/scverse/scanpy/issues/1758#issuecomment-816352446:307,Modifiability,variab,variables,307,"> What I noticed was that if I didn't have the same ID columns in my adata.var when setting adata.raw I couldn't use gene_symbols. Ah, if you're using the values in `raw` for differential expression the column used for `gene_symbols` should be in `raw.var` this is because `raw` can have a different set of variables than the main object. So this case, at least, is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-816352446
https://github.com/scverse/scanpy/issues/1758#issuecomment-851701172:216,Testability,test,test,216,"Getting back to this. I still have the same issue as before. For some reason this does not work on my data. I don't have any `raw` set at all in this data.; ```; sc.tl.rank_genes_groups(adata, 'celltypes', method='t-test'); sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, cmap='viridis', gene_symbols='Uniq_Name'); ```. ```; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-41-3b246f8b6bcc> in <module>; ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds); 823 """"""; 824 ; --> 825 return _rank_genes_groups_plot(; 826 adata,; 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds); 448 from .._matrixplot import matrixplot; 449 ; --> 450 _pl = matrixplot(; 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds; 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds); 345 """"""; 346 ; --> 347 mp = MatrixPlot(; 348 adata,; 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds); 109 **kwds,; 110",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-851701172
https://github.com/scverse/scanpy/issues/1758#issuecomment-851701172:1358,Testability,log,log,1358,"e>; ----> 1 sc.pl.rank_genes_groups_matrixplot(adata, n_genes=5, use_raw=False, cmap='viridis', gene_symbols='Uniq_Name'). ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in rank_genes_groups_matrixplot(adata, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds); 823 """"""; 824 ; --> 825 return _rank_genes_groups_plot(; 826 adata,; 827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds); 448 from .._matrixplot import matrixplot; 449 ; --> 450 _pl = matrixplot(; 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds; 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds); 345 """"""; 346 ; --> 347 mp = MatrixPlot(; 348 adata,; 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds); 109 **kwds,; 110 ):; --> 111 BasePlot.__init__(; 112 self,; 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds); 109 self._update_var_groups(); 110 ; --> 111 self.categories, self.obs_tidy = _prepare_dataframe(; 112 adata,; 113 self.var_names,. ~/projects/scanpy/sca",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-851701172
https://github.com/scverse/scanpy/issues/1758#issuecomment-851701172:1783,Testability,log,log,1783,"827 plot_type='matrixplot',. ~/projects/scanpy/scanpy/plotting/_tools/__init__.py in _rank_genes_groups_plot(adata, plot_type, groups, n_genes, groupby, values_to_plot, min_logfoldchange, key, show, save, return_fig, **kwds); 448 from .._matrixplot import matrixplot; 449 ; --> 450 _pl = matrixplot(; 451 adata, var_names, groupby, values_df=values_df, return_fig=True, **kwds; 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds); 345 """"""; 346 ; --> 347 mp = MatrixPlot(; 348 adata,; 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds); 109 **kwds,; 110 ):; --> 111 BasePlot.__init__(; 112 self,; 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds); 109 self._update_var_groups(); 110 ; --> 111 self.categories, self.obs_tidy = _prepare_dataframe(; 112 adata,; 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols); 1864 groupby.remove(groupby_index); 1865 keys = list(groupby) + list(np.unique(var_names)); -> 1866 obs_tidy = get.obs_df(; 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols; 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-851701172
https://github.com/scverse/scanpy/issues/1758#issuecomment-851701172:2164,Testability,log,log,2164,"return_fig=True, **kwds; 452 ). ~/projects/scanpy/scanpy/plotting/_matrixplot.py in matrixplot(adata, var_names, groupby, use_raw, log, num_categories, figsize, dendrogram, title, cmap, colorbar_title, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, values_df, swap_axes, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds); 345 """"""; 346 ; --> 347 mp = MatrixPlot(; 348 adata,; 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds); 109 **kwds,; 110 ):; --> 111 BasePlot.__init__(; 112 self,; 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds); 109 self._update_var_groups(); 110 ; --> 111 self.categories, self.obs_tidy = _prepare_dataframe(; 112 adata,; 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols); 1864 groupby.remove(groupby_index); 1865 keys = list(groupby) + list(np.unique(var_names)); -> 1866 obs_tidy = get.obs_df(; 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols; 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 270 alias_index = None; 271 ; --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(; 273 adata.obs,; 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw); 165 not_found.append(key); 166 if len(not_found) > 0:; --> 167 ra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-851701172
https://github.com/scverse/scanpy/issues/1758#issuecomment-851701172:2575,Testability,log,log,2575,"g, vmin, vmax, vcenter, norm, **kwds); 345 """"""; 346 ; --> 347 mp = MatrixPlot(; 348 adata,; 349 var_names,. ~/projects/scanpy/scanpy/plotting/_matrixplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, standard_scale, ax, values_df, vmin, vmax, vcenter, norm, **kwds); 109 **kwds,; 110 ):; --> 111 BasePlot.__init__(; 112 self,; 113 adata,. ~/projects/scanpy/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, vmin, vmax, vcenter, norm, **kwds); 109 self._update_var_groups(); 110 ; --> 111 self.categories, self.obs_tidy = _prepare_dataframe(; 112 adata,; 113 self.var_names,. ~/projects/scanpy/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols); 1864 groupby.remove(groupby_index); 1865 keys = list(groupby) + list(np.unique(var_names)); -> 1866 obs_tidy = get.obs_df(; 1867 adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols; 1868 ). ~/projects/scanpy/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 270 alias_index = None; 271 ; --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(; 273 adata.obs,; 274 var.index,. ~/projects/scanpy/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw); 165 not_found.append(key); 166 if len(not_found) > 0:; --> 167 raise KeyError(; 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in""; 169 f"" {alt_repr}.{alt_search_repr}."". KeyError: ""Could not find keys '['KY.Chr1.1388', 'KY.Chr1.1475', 'KY.Chr1.2070', 'KY.Chr1.2214', 'KY.Chr1.2297', 'KY.Chr1.686', 'KY.Chr11.93', 'KY.Chr13.413', 'KY.Chr2.1171', 'KY.Chr2.1545', 'KY.Chr3.405', 'KY.Chr4.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-851701172
https://github.com/scverse/scanpy/issues/1759#issuecomment-806685821:56,Deployability,install,installed,56,"Hello, could you write what version of sklearn you have installed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759#issuecomment-806685821
https://github.com/scverse/scanpy/issues/1760#issuecomment-807905537:680,Performance,Load,Loading,680,"Ah, I think I see what you're asking now. At the moment, I don't think we have a function for that. But this should be fairly straightforward to work around. Something like this should work:. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from sklearn.metrics import pairwise_distances; import seaborn as sns. def groupby_mean(adata, groupby):; grouped = adata.obs.groupby(groupby); results = np.zeros((grouped.ngroups, adata.n_vars), dtype=np.float64). for idx, indices in enumerate(grouped.indices.values()):; results[idx] = np.ravel(adata.X[indices].mean(axis=0)). return pd.DataFrame(results, columns=adata.var_names, index=grouped.groups.keys()). # Loading data; pbmc_full = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc_small = sc.datasets.pbmc68k_reduced().raw.to_adata(); var_intersect = pbmc_full.var_names.intersection(pbmc_small.var_names). # Calculate mean expression per cell type; full_means = groupby_mean(pbmc_full[:, var_intersect], ""louvain""); small_means = groupby_mean(pbmc_small[:, var_intersect], ""louvain""). # Correlation distance between celltypes; corr_mtx = pd.DataFrame(; pairwise_distances(full_means, small_means, metric=""correlation""),; index= full_means.index,; columns=small_means.index,; ); ```. Is this more of what you were thinking?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1760#issuecomment-807905537
https://github.com/scverse/scanpy/issues/1762#issuecomment-807908934:51,Testability,test,tests,51,"So, that dataset is used pretty extensively in the tests, and especially around plotting (plus it's actually shipped with the library). I don't think we're likely to modify it, given that it's used so heavily as a reference. What do you need it for, and could you use `pbmc3k_processed` for that purpose?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762#issuecomment-807908934
https://github.com/scverse/scanpy/issues/1762#issuecomment-808189078:18,Deployability,integrat,integration,18,"I was trying some integration methods between the two pbmc datasets. Maybe, could you add a sc.datasets.pbmc68k_full() where the whole transcriptome is included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762#issuecomment-808189078
https://github.com/scverse/scanpy/issues/1762#issuecomment-808189078:18,Integrability,integrat,integration,18,"I was trying some integration methods between the two pbmc datasets. Maybe, could you add a sc.datasets.pbmc68k_full() where the whole transcriptome is included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762#issuecomment-808189078
https://github.com/scverse/scanpy/issues/1764#issuecomment-815287672:233,Performance,perform,performing,233,"Dear @wangjiawen2013,. what is the interest behind your question? Do you have many datasets with very few cells?; Scanpy itself can easily work with very small datasets, but you should always be aware of statistical limitations when performing statistical tests etc on very few cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764#issuecomment-815287672
https://github.com/scverse/scanpy/issues/1764#issuecomment-815287672:256,Testability,test,tests,256,"Dear @wangjiawen2013,. what is the interest behind your question? Do you have many datasets with very few cells?; Scanpy itself can easily work with very small datasets, but you should always be aware of statistical limitations when performing statistical tests etc on very few cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764#issuecomment-815287672
https://github.com/scverse/scanpy/issues/1764#issuecomment-815387135:252,Performance,throughput,throughput,252,"We knocked out a gene, then wanna to reveal the difference between KO embro cells and wild type embryo cells and illustrate the function of the gene in development process. The single cell data was generated using Smart-seq2 technology, which is a low throughput technology. And besides, there were only a few cells in eary embros because they are very small.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764#issuecomment-815387135
https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884:347,Availability,down,downstream,347,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this.; * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause?. <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884
https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884:506,Modifiability,variab,variable,506,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this.; * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause?. <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884
https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884:563,Modifiability,variab,variable,563,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this.; * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause?. <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884
https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884:34,Usability,simpl,simple,34,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this.; * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause?. <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884
https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884:663,Usability,pause,pause,663,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this.; * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause?. <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884
https://github.com/scverse/scanpy/pull/1767#issuecomment-810829669:126,Availability,down,downstream,126,"It is an enrichment analysis but foot-print based: we don't just look at the elements of a pathway/TF but also the biological downstream effects that occur when said biological process is active. ; ""Annotation/ Enrichment Analysis"" fits but it would be good to also mention somewhere that they are foot-print based. Would this be okay?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1767#issuecomment-810829669
https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499:157,Deployability,update,update,157,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 ce508c4084e8df272163f4e17136386cfaec2605; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1768: Fix correlation plot test for new version of matplotlib'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1768-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1768 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499
https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499:895,Deployability,continuous,continuous,895,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 ce508c4084e8df272163f4e17136386cfaec2605; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1768: Fix correlation plot test for new version of matplotlib'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1768-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1768 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499
https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499:906,Deployability,integrat,integration,906,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 ce508c4084e8df272163f4e17136386cfaec2605; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1768: Fix correlation plot test for new version of matplotlib'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1768-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1768 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499
https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499:906,Integrability,integrat,integration,906,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 ce508c4084e8df272163f4e17136386cfaec2605; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1768: Fix correlation plot test for new version of matplotlib'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1768-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1768 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499
https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499:516,Testability,test,test,516,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 ce508c4084e8df272163f4e17136386cfaec2605; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1768: Fix correlation plot test for new version of matplotlib'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1768-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1768 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499
https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499:881,Testability,test,tested,881,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 ce508c4084e8df272163f4e17136386cfaec2605; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1768: Fix correlation plot test for new version of matplotlib'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1768-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1768 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499
https://github.com/scverse/scanpy/pull/1771#issuecomment-844219743:64,Availability,failure,failure,64,"Hi @ivirshup , I replaced the one test image that was causing a failure, as you suggested. (And I checked to make sure the image makes sense... it does...) I think this should do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1771#issuecomment-844219743
https://github.com/scverse/scanpy/pull/1771#issuecomment-844219743:34,Testability,test,test,34,"Hi @ivirshup , I replaced the one test image that was causing a failure, as you suggested. (And I checked to make sure the image makes sense... it does...) I think this should do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1771#issuecomment-844219743
https://github.com/scverse/scanpy/pull/1775#issuecomment-813543994:31,Usability,feedback,feedback,31,"Hey @ivirshup,. Thanks for the feedback. > why do you want this to be in external and not ecosystem? Generally, I think of external as a place to put provide a ""scanpy like"" API, but scNym already provides this kind of API. Two reasons:. 1. This is my first introduction to `ecosystem`, so I hadn't actually considered it.; 2. Given that the `scnym` API is a single function, adding an `sce.tl` endpoint seems like a parsimonious way to improve discoverability for users. Happy to remove this PR and suggest an edit to `ecosystem.rst` instead if that's preferable to the team. All the best,; Jacob",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1775#issuecomment-813543994
https://github.com/scverse/scanpy/issues/1776#issuecomment-959756849:49,Deployability,install,installs,49,"Also, consider setting `flit >=3.4` for editable installs via [PEP 660](https://www.python.org/dev/peps/pep-0660/) 😃",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1776#issuecomment-959756849
https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367:318,Testability,log,logfoldchanges,318,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot?. I've used something like this snipped (using `hvplots`) for these:. ```python; import hvplot.pandas. def plot_volcano(dedf):; dedf = dedf.copy(); dedf = dedf[dedf[""pvals""].notnull()]; dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10; dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10; return dedf.hvplot.scatter(; ""logfoldchanges"",; ""pvals"",; xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),; ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),; hover_cols=list(dedf.columns),; logy=True,; flip_yaxis=True; ). plot_volcano(sc.get.rank_genes_groups(adata, ...)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367
https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367:347,Testability,log,logfoldchanges,347,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot?. I've used something like this snipped (using `hvplots`) for these:. ```python; import hvplot.pandas. def plot_volcano(dedf):; dedf = dedf.copy(); dedf = dedf[dedf[""pvals""].notnull()]; dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10; dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10; return dedf.hvplot.scatter(; ""logfoldchanges"",; ""pvals"",; xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),; ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),; hover_cols=list(dedf.columns),; logy=True,; flip_yaxis=True; ). plot_volcano(sc.get.rank_genes_groups(adata, ...)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367
https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367:385,Testability,log,logfoldchanges,385,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot?. I've used something like this snipped (using `hvplots`) for these:. ```python; import hvplot.pandas. def plot_volcano(dedf):; dedf = dedf.copy(); dedf = dedf[dedf[""pvals""].notnull()]; dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10; dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10; return dedf.hvplot.scatter(; ""logfoldchanges"",; ""pvals"",; xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),; ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),; hover_cols=list(dedf.columns),; logy=True,; flip_yaxis=True; ). plot_volcano(sc.get.rank_genes_groups(adata, ...)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367
https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367:415,Testability,log,logfoldchanges,415,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot?. I've used something like this snipped (using `hvplots`) for these:. ```python; import hvplot.pandas. def plot_volcano(dedf):; dedf = dedf.copy(); dedf = dedf[dedf[""pvals""].notnull()]; dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10; dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10; return dedf.hvplot.scatter(; ""logfoldchanges"",; ""pvals"",; xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),; ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),; hover_cols=list(dedf.columns),; logy=True,; flip_yaxis=True; ). plot_volcano(sc.get.rank_genes_groups(adata, ...)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367
https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367:469,Testability,log,logfoldchanges,469,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot?. I've used something like this snipped (using `hvplots`) for these:. ```python; import hvplot.pandas. def plot_volcano(dedf):; dedf = dedf.copy(); dedf = dedf[dedf[""pvals""].notnull()]; dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10; dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10; return dedf.hvplot.scatter(; ""logfoldchanges"",; ""pvals"",; xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),; ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),; hover_cols=list(dedf.columns),; logy=True,; flip_yaxis=True; ). plot_volcano(sc.get.rank_genes_groups(adata, ...)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367
https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367:509,Testability,log,logfoldchanges,509,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot?. I've used something like this snipped (using `hvplots`) for these:. ```python; import hvplot.pandas. def plot_volcano(dedf):; dedf = dedf.copy(); dedf = dedf[dedf[""pvals""].notnull()]; dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10; dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10; return dedf.hvplot.scatter(; ""logfoldchanges"",; ""pvals"",; xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),; ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),; hover_cols=list(dedf.columns),; logy=True,; flip_yaxis=True; ). plot_volcano(sc.get.rank_genes_groups(adata, ...)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367
https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367:532,Testability,log,logfoldchanges,532,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot?. I've used something like this snipped (using `hvplots`) for these:. ```python; import hvplot.pandas. def plot_volcano(dedf):; dedf = dedf.copy(); dedf = dedf[dedf[""pvals""].notnull()]; dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10; dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10; return dedf.hvplot.scatter(; ""logfoldchanges"",; ""pvals"",; xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),; ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),; hover_cols=list(dedf.columns),; logy=True,; flip_yaxis=True; ). plot_volcano(sc.get.rank_genes_groups(adata, ...)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367
https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367:579,Testability,log,logfoldchanges,579,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot?. I've used something like this snipped (using `hvplots`) for these:. ```python; import hvplot.pandas. def plot_volcano(dedf):; dedf = dedf.copy(); dedf = dedf[dedf[""pvals""].notnull()]; dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10; dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10; return dedf.hvplot.scatter(; ""logfoldchanges"",; ""pvals"",; xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),; ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),; hover_cols=list(dedf.columns),; logy=True,; flip_yaxis=True; ). plot_volcano(sc.get.rank_genes_groups(adata, ...)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367
https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367:602,Testability,log,logfoldchanges,602,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot?. I've used something like this snipped (using `hvplots`) for these:. ```python; import hvplot.pandas. def plot_volcano(dedf):; dedf = dedf.copy(); dedf = dedf[dedf[""pvals""].notnull()]; dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10; dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10; return dedf.hvplot.scatter(; ""logfoldchanges"",; ""pvals"",; xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),; ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),; hover_cols=list(dedf.columns),; logy=True,; flip_yaxis=True; ). plot_volcano(sc.get.rank_genes_groups(adata, ...)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367
https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367:794,Testability,log,logy,794,"I don't think you'll be able to do this with the tracksplot. Were you looking for something more like a volcano plot?. I've used something like this snipped (using `hvplots`) for these:. ```python; import hvplot.pandas. def plot_volcano(dedf):; dedf = dedf.copy(); dedf = dedf[dedf[""pvals""].notnull()]; dedf.loc[dedf[""logfoldchanges""] == np.inf, ""logfoldchanges""] = 10; dedf.loc[dedf[""logfoldchanges""] == -np.inf, ""logfoldchanges""] = -10; return dedf.hvplot.scatter(; ""logfoldchanges"",; ""pvals"",; xlim=(dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].min(), dedf[""logfoldchanges""][dedf[""logfoldchanges""].abs() != np.inf].max()),; ylim=(dedf[""pvals""][dedf[""pvals""].abs() != np.inf].min(), dedf[""pvals""][dedf[""pvals""].abs() != np.inf].max() + .5),; hover_cols=list(dedf.columns),; logy=True,; flip_yaxis=True; ). plot_volcano(sc.get.rank_genes_groups(adata, ...)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1778#issuecomment-814594367
https://github.com/scverse/scanpy/pull/1780#issuecomment-822961476:485,Deployability,release,release,485,"Thanks for opening this PR!. Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-822961476
https://github.com/scverse/scanpy/pull/1780#issuecomment-822961476:429,Testability,log,log,429,"Thanks for opening this PR!. Similar to #1775, I think this might fit better in ecosystem than `external`. Initially, we started `external` as a way of providing a `scanpy`-like API for tools which didn't use `scanpy`. Since your tool already has this kind of API, I think it's a better fit for the ecosystem page. We are working on making this page more visible to users (#1801), but the addition of this tool will be in change log and mentioned in the announcement of the next minor release. How does this sound?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-822961476
https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:782,Deployability,release,releases,782,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662
https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:35,Energy Efficiency,reduce,reduce,35,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662
https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:377,Energy Efficiency,reduce,reduce,377,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662
https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:600,Energy Efficiency,reduce,reduce,600,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662
https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:106,Integrability,wrap,wrapper,106,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662
https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:351,Integrability,wrap,wrapper,351,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662
https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:531,Integrability,wrap,wrapper,531,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662
https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:588,Integrability,wrap,wrapper,588,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662
https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:669,Integrability,interface,interfaces,669,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662
https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:632,Safety,redund,redundancy,632,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662
https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:261,Security,access,access,261,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662
https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:99,Usability,simpl,simple,99,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662
https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577:744,Deployability,release,release,744,"Sorry about the late reply to this!. > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577
https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577:107,Energy Efficiency,reduce,reduce,107,"Sorry about the late reply to this!. > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577
https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577:82,Integrability,wrap,wrapper,82,"Sorry about the late reply to this!. > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577
https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577:235,Integrability,wrap,wrapper,235,"Sorry about the late reply to this!. > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577
https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577:627,Integrability,interface,interface,627,"Sorry about the late reply to this!. > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577
https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:262,Availability,down,downloading,262,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808
https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:645,Availability,mainten,maintenance,645,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808
https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:1439,Availability,mainten,maintenance,1439,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808
https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:1365,Deployability,release,releases,1365,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808
https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:765,Integrability,wrap,wrapper,765,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808
https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:803,Integrability,wrap,wrapper,803,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808
https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:383,Performance,load,loaded,383,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808
https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:290,Usability,learn,learning,290,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808
https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:1617,Usability,simpl,simply,1617,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808
https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000:174,Deployability,release,release,174,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000
https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000:81,Integrability,wrap,wrapper,81,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000
https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000:138,Integrability,wrap,wrapper,138,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000
https://github.com/scverse/scanpy/issues/1781#issuecomment-814593014:55,Integrability,wrap,wrapper,55,"Since the API being used here is deprecated, as is the wrapper, I'm inclined leave this as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781#issuecomment-814593014
https://github.com/scverse/scanpy/issues/1782#issuecomment-814591832:179,Availability,error,error,179,"In the current release, we check for the counts being integer valued. kallisto can assign partial counts, (e.g a gene can have 1.5 counts) which triggers the check, triggering an error. For the next bugfix release we've softened consequences of this check failing to a warning, and the check can be skipped. See discussion in #1642 and #1679 for details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782#issuecomment-814591832
https://github.com/scverse/scanpy/issues/1782#issuecomment-814591832:15,Deployability,release,release,15,"In the current release, we check for the counts being integer valued. kallisto can assign partial counts, (e.g a gene can have 1.5 counts) which triggers the check, triggering an error. For the next bugfix release we've softened consequences of this check failing to a warning, and the check can be skipped. See discussion in #1642 and #1679 for details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782#issuecomment-814591832
https://github.com/scverse/scanpy/issues/1782#issuecomment-814591832:206,Deployability,release,release,206,"In the current release, we check for the counts being integer valued. kallisto can assign partial counts, (e.g a gene can have 1.5 counts) which triggers the check, triggering an error. For the next bugfix release we've softened consequences of this check failing to a warning, and the check can be skipped. See discussion in #1642 and #1679 for details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782#issuecomment-814591832
https://github.com/scverse/scanpy/issues/1782#issuecomment-814613904:46,Deployability,release,released,46,"Should be fixed as of `1.7.2`, which was just released.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782#issuecomment-814613904
https://github.com/scverse/scanpy/issues/1791#issuecomment-815761921:335,Usability,simpl,simply,335,"Thanks very much for your reply. . Yes, I am aware ‘dpi’ can help increase the resolution. However, I noticed that with the same ‘dpi’ , in the latest version of scanpy, the plots look more blurry than before. (You can easily see the difference between the notebook attached here and the tutorial on scanpy website. on the other hand, simply increasing ‘dpi’ will also increase the plot size, which is not really wanted in most cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1791#issuecomment-815761921
https://github.com/scverse/scanpy/issues/1793#issuecomment-816682033:116,Deployability,deploy,deployed,116,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-816682033
https://github.com/scverse/scanpy/issues/1793#issuecomment-816682033:213,Deployability,update,updated,213,"In case this helps, all gpu accelerated code implemented in scanpy use rapids related packages, which can be easily deployed by using their [docker images](https://hub.docker.com/r/rapidsai/rapidsai-core) and are updated on a regular basis! There is the choice of multiple os and python versions, although windows is not present.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-816682033
https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172:712,Integrability,interface,interface,712,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/; https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172
https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172:683,Testability,log,logs,683,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/; https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172
https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172:417,Usability,simpl,simple,417,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/; https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172
https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172:485,Usability,simpl,simple,485,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/; https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172
https://github.com/scverse/scanpy/issues/1793#issuecomment-881369655:135,Availability,avail,available,135,"Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-881369655
https://github.com/scverse/scanpy/issues/1793#issuecomment-882740958:145,Availability,avail,available,145,"> ; > ; > Thanks for your response. As of today that's correct: AWS, GCP and DO. Azure support is a work in progress at the moment. It should be available by the end of this month most likely. Do you have a hard deadline on this?. No we don't. But before using it Cirun I would have to look at more closely first.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-882740958
https://github.com/scverse/scanpy/issues/1793#issuecomment-1102829655:52,Deployability,update,update,52,"Hi @Zethson,; I'm curious whether you have a status update on this. Would be really excited to have GPU-accelerated Leiden, but I understand the issues you mention here. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-1102829655
https://github.com/scverse/scanpy/issues/1793#issuecomment-1102832260:132,Availability,ping,ping,132,@maarten-hifibio we are indeed actively working on this again. Feel free to join our zulip https://scverse.zulipchat.com/login/ and ping me. I can add you to the conversation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-1102832260
https://github.com/scverse/scanpy/issues/1793#issuecomment-1102832260:121,Testability,log,login,121,@maarten-hifibio we are indeed actively working on this again. Feel free to join our zulip https://scverse.zulipchat.com/login/ and ping me. I can add you to the conversation.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-1102832260
https://github.com/scverse/scanpy/issues/1793#issuecomment-1106331321:84,Availability,avail,available,84,@maarten-hifibio in the mean time if you need it I have just made some GPU wrappers available on the following gist:; https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. It includes one for leiden that would exactly act like sc.tl.leiden (it is part of my PR referenced here),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-1106331321
https://github.com/scverse/scanpy/issues/1793#issuecomment-1106331321:75,Integrability,wrap,wrappers,75,@maarten-hifibio in the mean time if you need it I have just made some GPU wrappers available on the following gist:; https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. It includes one for leiden that would exactly act like sc.tl.leiden (it is part of my PR referenced here),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-1106331321
https://github.com/scverse/scanpy/issues/1795#issuecomment-817677727:75,Availability,error,error,75,"Please provide more details. What is `folder` in your case and what is the error message? Follow the issue template, please.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817677727
https://github.com/scverse/scanpy/issues/1795#issuecomment-817677727:81,Integrability,message,message,81,"Please provide more details. What is `folder` in your case and what is the error message? Follow the issue template, please.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817677727
https://github.com/scverse/scanpy/issues/1795#issuecomment-817682376:86,Availability,error,error,86,"Hello, I am using the folder where I store the raw data for the analysis. There is no error message when I run the command but it does not generate any file or object with this name or any name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817682376
https://github.com/scverse/scanpy/issues/1795#issuecomment-817682376:92,Integrability,message,message,92,"Hello, I am using the folder where I store the raw data for the analysis. There is no error message when I run the command but it does not generate any file or object with this name or any name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817682376
https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273:63,Availability,error,error,63,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273
https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273:69,Integrability,message,message,69,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273
https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273:27,Modifiability,variab,variable,27,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273
https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943:226,Availability,error,error,226,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943
https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943:232,Integrability,message,message,232,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943
https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943:4,Modifiability,variab,variable,4,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943
https://github.com/scverse/scanpy/issues/1795#issuecomment-817788753:209,Availability,error,error,209,"> I find out the solution. Thank You. Although I think that this is not an issue with Scanpy, it is usually common courtesy to post the solution to the corresponding issue. If other people search for the same error they can find your solution :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817788753
https://github.com/scverse/scanpy/issues/1798#issuecomment-819683830:265,Modifiability,layers,layers,265,"I like the thought... for exactly the reason you brought up, I recommended storing log-normalized data in `adata.raw` in my best practices workflow. That way DE analysis and plotting is done on that data type rather than raw counts. I have been working with `adata.layers['counts']` for count data and don't keep filtered out cells/genes (easy to recreate anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819683830
https://github.com/scverse/scanpy/issues/1798#issuecomment-819683830:83,Testability,log,log-normalized,83,"I like the thought... for exactly the reason you brought up, I recommended storing log-normalized data in `adata.raw` in my best practices workflow. That way DE analysis and plotting is done on that data type rather than raw counts. I have been working with `adata.layers['counts']` for count data and don't keep filtered out cells/genes (easy to recreate anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819683830
https://github.com/scverse/scanpy/issues/1798#issuecomment-819784468:182,Energy Efficiency,adapt,adapt,182,"Yes, but if the user needs the raw counts of all genes, he/she shouldn't deal with ""unnormalizing"" things (which is non-trivial for beginners, but not for you 😄). So, it's better to adapt scanpy to easier workflows, not the other way around due to the limitations of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819784468
https://github.com/scverse/scanpy/issues/1798#issuecomment-819784468:182,Modifiability,adapt,adapt,182,"Yes, but if the user needs the raw counts of all genes, he/she shouldn't deal with ""unnormalizing"" things (which is non-trivial for beginners, but not for you 😄). So, it's better to adapt scanpy to easier workflows, not the other way around due to the limitations of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819784468
https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442:660,Integrability,depend,depends,660,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no?. This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442
https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442:561,Performance,load,load,561,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no?. This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442
https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442:180,Usability,simpl,simply,180,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no?. This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442
https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:857,Availability,mask,mask,857,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988
https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:896,Availability,mask,mask,896,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988
https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:1284,Availability,avail,available,1284,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988
https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:497,Deployability,integrat,integration,497,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988
https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:497,Integrability,integrat,integration,497,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988
https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:599,Safety,safe,safely,599,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988
https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:1239,Usability,usab,usable,1239,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988
https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:1098,Availability,mask,masks,1098,"te the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449
https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:1168,Availability,mask,mask,1168,"te the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449
https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:532,Deployability,integrat,integration,532,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449
https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:546,Deployability,Integrat,Integration,546,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449
https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:1473,Deployability,pipeline,pipeline,1473," no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449
https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:1663,Deployability,pipeline,pipeline,1663," no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement for `.raw` as a version of the data that is used for DE analysis but not `.X`. This distinction is quite useful as it is becoming more frequent that you have 1 version of the data for further embedding-based analysis, and one for moecular analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449
https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:532,Integrability,integrat,integration,532,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449
https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:546,Integrability,Integrat,Integration,546,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449
https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:75,Usability,simpl,simply,75,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449
https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:434,Usability,clear,clear,434,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:3427,Availability,mask,masks,3427,"⠈⢀⠌⠚⠀⠀⠃; ⠁⠂⡃⠈⠀⢀⠀⠙⢀⠥⠀⠀⠄⡁⠀⠠⠈⠀⠈⠃⠂⠠⣀⠀⠈⣁⠁⠆; ⡀⠐⠐⠠⠀⠐⢐⡄⣂⠀⠀⠘⠀⠀⠀⠠⠂⠀⡀⠨⠁⠀⠀⠀⠁⠁⠣⠤; ⠀⡐⢀⢢⠀⠁⠔⠀⠁⠀⠃⠀⢀⢀⠐⠃⠄⠀⡇⠊⠄⠀⡈⢀⠀⠀⣀⠆; ⠀⢐⣤⡄⠠⠂⠃⡈⠘⠀⠀⠀⡂⠰⢄⠊⡂⠀⠐⠂⠀⠄⠀⠀⢱⠩⠈⢀; ⢁⠀⠑⠚⠁⠂⠂⠐⠁⠀⠀⢀⠠⠀⠐⠈⠈⡨⠀⠂⠀⡈⠈⠁⡐⣀⢁⠂; ⠀⠀⠀⠁⠀⠠⠅⠁⡠⠇⢐⠀⠀⠖⢉⣀⠀⢀⠀⠠⡀⠀⡀⢰⠁⠂⢉⠂; ⠀⠀⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work wi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:3497,Availability,mask,mask,3497,"⠈⢀⠌⠚⠀⠀⠃; ⠁⠂⡃⠈⠀⢀⠀⠙⢀⠥⠀⠀⠄⡁⠀⠠⠈⠀⠈⠃⠂⠠⣀⠀⠈⣁⠁⠆; ⡀⠐⠐⠠⠀⠐⢐⡄⣂⠀⠀⠘⠀⠀⠀⠠⠂⠀⡀⠨⠁⠀⠀⠀⠁⠁⠣⠤; ⠀⡐⢀⢢⠀⠁⠔⠀⠁⠀⠃⠀⢀⢀⠐⠃⠄⠀⡇⠊⠄⠀⡈⢀⠀⠀⣀⠆; ⠀⢐⣤⡄⠠⠂⠃⡈⠘⠀⠀⠀⡂⠰⢄⠊⡂⠀⠐⠂⠀⠄⠀⠀⢱⠩⠈⢀; ⢁⠀⠑⠚⠁⠂⠂⠐⠁⠀⠀⢀⠠⠀⠐⠈⠈⡨⠀⠂⠀⡈⠈⠁⡐⣀⢁⠂; ⠀⠀⠀⠁⠀⠠⠅⠁⡠⠇⢐⠀⠀⠖⢉⣀⠀⢀⠀⠠⡀⠀⡀⢰⠁⠂⢉⠂; ⠀⠀⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work wi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:4105,Availability,mask,masked,4105,"⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:917,Deployability,Integrat,Integration,917,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:1047,Deployability,integrat,integrate,1047,"aslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:3796,Deployability,pipeline,pipeline,3796,"⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:917,Integrability,Integrat,Integration,917,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:1047,Integrability,integrat,integrate,1047,"aslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:285,Modifiability,variab,variables,285,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:337,Modifiability,variab,variables,337,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:1075,Modifiability,variab,variable,1075,"s this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when y",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:2007,Modifiability,variab,variables,2007,"that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```; ⡠⠄⠀⠨⡯⢀⠀⠐⢡⠀⠠⢀⠠⢂⠀⠐⠀⠐⠐⣢⠀⢂⠀⠈⠒⣂⠂⠀; ⠆⢌⠁⡁⠈⠀⡀⠖⠂⠀⠁⠂⠀⠉⠐⡀⠀⠀⠈⠠⠄⠉⢀⡀⠀⠀⠀⠂; ⠑⠀⠠⠀⠃⠀⠀⠅⢀⠠⠄⡀⠅⠂⢀⠪⠀⠦⢀⠀⢃⠈⢀⠌⠚⠀⠀⠃; ⠁⠂⡃⠈⠀⢀⠀⠙⢀⠥⠀⠀⠄⡁⠀⠠⠈⠀⠈⠃⠂⠠⣀⠀⠈⣁⠁⠆; ⡀⠐⠐⠠⠀⠐⢐⡄⣂⠀⠀⠘⠀⠀⠀⠠⠂⠀⡀⠨⠁⠀⠀⠀⠁⠁⠣⠤; ⠀⡐⢀⢢⠀⠁⠔⠀⠁⠀⠃⠀⢀⢀⠐⠃⠄⠀⡇⠊⠄⠀⡈⢀⠀⠀⣀⠆; ⠀⢐⣤⡄⠠⠂⠃⡈⠘⠀⠀⠀⡂⠰⢄⠊⡂⠀⠐⠂⠀⠄⠀⠀⢱⠩⠈⢀; ⢁⠀⠑⠚⠁⠂⠂⠐⠁⠀⠀⢀⠠⠀⠐⠈⠈⡨⠀⠂⠀⡈⠈⠁⡐⣀⢁⠂; ⠀⠀⠀⠁⠀⠠⠅⠁⡠⠇⢐⠀⠀⠖⢉⣀⠀⢀⠀⠠⡀⠀⡀⢰⠁⠂⢉⠂; ⠀⠀⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:4201,Modifiability,layers,layers,4201,"⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:4278,Modifiability,variab,variables,4278,"⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:4390,Modifiability,variab,variables,4390,"⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:406,Safety,detect,detection,406,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:91,Usability,simpl,simply,91,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472
https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661:201,Availability,mask,masks,201,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no?. > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661
https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661:547,Availability,mask,masking,547,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no?. > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661
https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661:267,Modifiability,layers,layers,267,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no?. > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661
https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661:623,Modifiability,variab,variable,623,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no?. > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661
https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661:504,Testability,test,testing,504,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no?. > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661
https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305:85,Availability,mask,masks,85,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers?. Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no?. I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level – but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union?. > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305
https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305:262,Availability,mask,masking,262,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers?. Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no?. I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level – but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union?. > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305
https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305:538,Availability,mainten,maintenance,538,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers?. Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no?. I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level – but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union?. > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305
https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305:150,Modifiability,layers,layers,150,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers?. Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no?. I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level – but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union?. > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305
https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305:597,Testability,log,logic,597,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers?. Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no?. I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level – but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union?. > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305
https://github.com/scverse/scanpy/issues/1798#issuecomment-824590137:23,Availability,mask,mask,23,"One more point for the mask argument, would be useful in plotting to allow things like plotting expression with some clusters masked out (#759).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-824590137
https://github.com/scverse/scanpy/issues/1798#issuecomment-824590137:126,Availability,mask,masked,126,"One more point for the mask argument, would be useful in plotting to allow things like plotting expression with some clusters masked out (#759).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-824590137
https://github.com/scverse/scanpy/issues/1799#issuecomment-822498064:68,Integrability,depend,dependency,68,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-822498064
https://github.com/scverse/scanpy/issues/1799#issuecomment-822545856:14,Deployability,update,updated,14,"@dawe, I just updated the requirements for `scvelo`. The latest version on `develop/` should now work for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-822545856
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:182,Availability,error,error,182,"Thanks for reporting @dawe and thanks for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:232,Availability,error,error,232,"Thanks for reporting @dawe and thanks for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:307,Availability,error,error,307,"Thanks for reporting @dawe and thanks for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:314,Availability,ERROR,ERROR,314,"Thanks for reporting @dawe and thanks for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:329,Availability,error,errored,329,"Thanks for reporting @dawe and thanks for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1909,Availability,error,error,1909,"-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>; from scvelo.core import cleanup, SplicingDynamics; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>; from ._anndata import (; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>; from typing_extensions import Literal; ModuleNotFoundError: No module named 't",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1998,Availability,ERROR,ERROR,1998,"-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>; from scvelo.core import cleanup, SplicingDynamics; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>; from ._anndata import (; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>; from typing_extensions import Literal; ModuleNotFoundError: No module named 't",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:493,Deployability,install,install-,493,"Thanks for reporting @dawe and thanks for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:590,Deployability,install,install-,590,"Thanks for reporting @dawe and thanks for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:976,Deployability,install,install-,976,"for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1141,Deployability,install,install-,1141,"for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1291,Deployability,install,install-,1291,"p.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <modu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1411,Deployability,install,install-,1411,"c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from sc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1547,Deployability,install,install-,1547,"le__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:2150,Deployability,install,installation,2150,"python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>; from scvelo.core import cleanup, SplicingDynamics; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>; from ._anndata import (; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>; from typing_extensions import Literal; ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:3000,Deployability,install,install,3000,"python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>; from scvelo.core import cleanup, SplicingDynamics; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>; from ._anndata import (; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>; from typing_extensions import Literal; ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:238,Integrability,message,message,238,"Thanks for reporting @dawe and thanks for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:385,Testability,test,test,385,"Thanks for reporting @dawe and thanks for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1101,Testability,test,test,1101,"for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:1939,Testability,test,test,1939,"-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>; from scvelo.core import cleanup, SplicingDynamics; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>; from ._anndata import (; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>; from typing_extensions import Literal; ModuleNotFoundError: No module named 't",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:2323,Testability,test,test,2323,"python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>; from scvelo.core import cleanup, SplicingDynamics; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>; from ._anndata import (; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>; from typing_extensions import Literal; ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:2442,Testability,log,logging,2442,"python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>; from scvelo.core import cleanup, SplicingDynamics; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>; from ._anndata import (; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>; from typing_extensions import Literal; ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:2500,Testability,test,test,2500,"python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>; from scvelo.core import cleanup, SplicingDynamics; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>; from ._anndata import (; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>; from typing_extensions import Literal; ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:2662,Testability,test,test,2662,"python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>; from scvelo.core import cleanup, SplicingDynamics; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>; from ._anndata import (; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>; from typing_extensions import Literal; ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:2802,Testability,test,test,2802,"python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERROR: Failed building wheel for llvmlite; ```. </details>. Any ideas about that?. When using **python 3.8** in a fresh new virtual environment, I get, installation of the development version works fine, but when importing scvelo. `Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/__init__.py"", line 5, in <module>; from scvelo import datasets, logging, pl, pp, settings, tl, utils; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/datasets.py"", line 10, in <module>; from scvelo.core import cleanup, SplicingDynamics; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/__init__.py"", line 1, in <module>; from ._anndata import (; File ""/home/mischko/test/python_virtual/lib/python3.8/site-packages/scvelo/core/_anndata.py"", line 4, in <module>; from typing_extensions import Literal; ModuleNotFoundError: No module named 'typing_extensions'`. `pip install typing_extensions` resolves the issue, so @WeilerP you might consider adding this to the setup.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752
https://github.com/scverse/scanpy/issues/1799#issuecomment-835257528:236,Availability,error,error,236,"@mihem, thanks for pointing this out. I opened [this](https://github.com/theislab/scvelo/issues/443) issue on the `scvelo` repo to resolve the `typing_extensions` problem. You may want to open a new issue on `scanpy` for the `llvmlite` error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-835257528
https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309:22,Availability,error,error,22,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:; conda install -c conda-forge umap-learn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309
https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309:163,Deployability,install,install,163,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:; conda install -c conda-forge umap-learn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309
https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309:297,Deployability,install,install,297,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:; conda install -c conda-forge umap-learn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309
https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309:61,Testability,Assert,AssertionError,61,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:; conda install -c conda-forge umap-learn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309
https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309:234,Usability,learn,learn,234,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:; conda install -c conda-forge umap-learn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309
https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309:325,Usability,learn,learn,325,"I am also getting the error when running. sc.pp.neighbors(). AssertionError: Storing i64 to ptr of i32 ('dim'). FE type int32. I tried pip uninstall numba and pip install numba==0.52.0 and numba==0.51.0, but nothing works. I had umap-learn 0.4.6, and updating it resolved the issue for me:; conda install -c conda-forge umap-learn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-867004309
https://github.com/scverse/scanpy/issues/1800#issuecomment-822136254:38,Testability,log,log,38,"Yes, you should probably see negative log fold changes, but they would be in the lower ranked genes (since we sort by test statistic). For example:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); sc.tl.rank_genes_groups(pbmc, 'louvain', method='wilcoxon'); print(sc.get.rank_genes_groups_df(pbmc, ""B cells"").tail().to_markdown()); ```. | | names | scores | logfoldchanges | pvals | pvals_adj |; |------:|:--------|---------:|-----------------:|-------------:|------------:|; | 13709 | IL32 | -16.6762 | -4.0097 | 1.95318e-62 | 1.4881e-59 |; | 13710 | ANXA1 | -17.0521 | -3.27545 | 3.37183e-65 | 2.72007e-62 |; | 13711 | S100A6 | -19.6524 | -2.86864 | 5.51413e-86 | 5.40148e-83 |; | 13712 | TMSB4X | -21.137 | -1.17325 | 3.63515e-99 | 3.8348e-96 |; | 13713 | S100A4 | -22.1246 | -3.50364 | 1.83143e-108 | 2.7907e-105 |",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1800#issuecomment-822136254
https://github.com/scverse/scanpy/issues/1800#issuecomment-822136254:118,Testability,test,test,118,"Yes, you should probably see negative log fold changes, but they would be in the lower ranked genes (since we sort by test statistic). For example:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); sc.tl.rank_genes_groups(pbmc, 'louvain', method='wilcoxon'); print(sc.get.rank_genes_groups_df(pbmc, ""B cells"").tail().to_markdown()); ```. | | names | scores | logfoldchanges | pvals | pvals_adj |; |------:|:--------|---------:|-----------------:|-------------:|------------:|; | 13709 | IL32 | -16.6762 | -4.0097 | 1.95318e-62 | 1.4881e-59 |; | 13710 | ANXA1 | -17.0521 | -3.27545 | 3.37183e-65 | 2.72007e-62 |; | 13711 | S100A6 | -19.6524 | -2.86864 | 5.51413e-86 | 5.40148e-83 |; | 13712 | TMSB4X | -21.137 | -1.17325 | 3.63515e-99 | 3.8348e-96 |; | 13713 | S100A4 | -22.1246 | -3.50364 | 1.83143e-108 | 2.7907e-105 |",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1800#issuecomment-822136254
https://github.com/scverse/scanpy/issues/1800#issuecomment-822136254:375,Testability,log,logfoldchanges,375,"Yes, you should probably see negative log fold changes, but they would be in the lower ranked genes (since we sort by test statistic). For example:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); sc.tl.rank_genes_groups(pbmc, 'louvain', method='wilcoxon'); print(sc.get.rank_genes_groups_df(pbmc, ""B cells"").tail().to_markdown()); ```. | | names | scores | logfoldchanges | pvals | pvals_adj |; |------:|:--------|---------:|-----------------:|-------------:|------------:|; | 13709 | IL32 | -16.6762 | -4.0097 | 1.95318e-62 | 1.4881e-59 |; | 13710 | ANXA1 | -17.0521 | -3.27545 | 3.37183e-65 | 2.72007e-62 |; | 13711 | S100A6 | -19.6524 | -2.86864 | 5.51413e-86 | 5.40148e-83 |; | 13712 | TMSB4X | -21.137 | -1.17325 | 3.63515e-99 | 3.8348e-96 |; | 13713 | S100A4 | -22.1246 | -3.50364 | 1.83143e-108 | 2.7907e-105 |",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1800#issuecomment-822136254
https://github.com/scverse/scanpy/issues/1802#issuecomment-824423022:78,Integrability,Depend,Depending,78,"I think if you add `show=False` to the `sc.pl.umap` commands it might work? . Depending on the type grid and subplots, it could be easier to plot things manually with matplotlib (to have more control over things like legend placement etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1802#issuecomment-824423022
https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301:453,Modifiability,variab,variable,453,"@moqri bit difficult to answer this question. What are you referring to?. if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: ; ```; The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301
https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301:213,Usability,clear,clear,213,"@moqri bit difficult to answer this question. What are you referring to?. if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: ; ```; The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301
https://github.com/scverse/scanpy/issues/1803#issuecomment-827063521:452,Modifiability,variab,variable,452,"Ok so you are probably using this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html. you can check the definition in the docs:; ```; For the dispersion-based methods ([Satija15] and [Zheng17]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected.; ```. this slightly changes according to the `flavour`, all citations are also mentioned in the docs (link above). does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803#issuecomment-827063521
https://github.com/scverse/scanpy/pull/1805#issuecomment-827226658:1464,Testability,log,logging,1464,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1805?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > Merging [#1805](https://codecov.io/gh/theislab/scanpy/pull/1805?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c6dc5f2) into [master](https://codecov.io/gh/theislab/scanpy/commit/7abd0724fc1684c018f55951108a07fa9eaf9911?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (7abd072) will **increase** coverage by `0.00%`.; > The diff coverage is `0.00%`. ```diff; @@ Coverage Diff @@; ## master #1805 +/- ##; =======================================; Coverage 71.18% 71.19% ; =======================================; Files 92 92 ; Lines 11190 11192 +2 ; =======================================; + Hits 7966 7968 +2 ; Misses 3224 3224 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1805?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/tools/\_louvain.py](https://codecov.io/gh/theislab/scanpy/pull/1805/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Rvb2xzL19sb3V2YWluLnB5) | `59.57% <0.00%> (-1.30%)` | :arrow_down: |; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1805/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <0.00%> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1805#issuecomment-827226658
https://github.com/scverse/scanpy/issues/1806#issuecomment-833218975:34,Energy Efficiency,efficient,efficient,34,"I think that we can be a bit more efficient than `var`. Also, as this has come up before (with `scale`) it's probably worth a utility function. I think I've got something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1806#issuecomment-833218975
https://github.com/scverse/scanpy/pull/1807#issuecomment-827457169:1198,Testability,log,logging,1198,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1807?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > Merging [#1807](https://codecov.io/gh/theislab/scanpy/pull/1807?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (b12b4ea) into [master](https://codecov.io/gh/theislab/scanpy/commit/c488909a54e9ab1462186cca35b537426e4630db?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c488909) will **increase** coverage by `0.01%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #1807 +/- ##; ==========================================; + Coverage 71.18% 71.20% +0.01% ; ==========================================; Files 92 92 ; Lines 11190 11190 ; ==========================================; + Hits 7966 7968 +2 ; + Misses 3224 3222 -2 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1807?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1807/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <ø> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1807#issuecomment-827457169
https://github.com/scverse/scanpy/pull/1808#issuecomment-827474435:1766,Testability,log,logging,1766,codecov.io/gh/theislab/scanpy/pull/1808?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > Merging [#1808](https://codecov.io/gh/theislab/scanpy/pull/1808?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (1ae578b) into [master](https://codecov.io/gh/theislab/scanpy/commit/c488909a54e9ab1462186cca35b537426e4630db?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c488909) will **increase** coverage by `0.01%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #1808 +/- ##; ==========================================; + Coverage 71.18% 71.20% +0.01% ; ==========================================; Files 92 92 ; Lines 11190 11190 ; ==========================================; + Hits 7966 7968 +2 ; + Misses 3224 3222 -2 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1808?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/plotting/\_matrixplot.py](https://codecov.io/gh/theislab/scanpy/pull/1808/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL19tYXRyaXhwbG90LnB5) | `97.87% <ø> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1808/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.27% <ø> (ø)` | |; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1808/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <0.00%> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1808#issuecomment-827474435
https://github.com/scverse/scanpy/pull/1809#issuecomment-827482585:2079,Testability,log,logging,2079,n=pr+comments&utm_term=theislab) (6fa84c9) into [master](https://codecov.io/gh/theislab/scanpy/commit/c488909a54e9ab1462186cca35b537426e4630db?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c488909) will **increase** coverage by `0.02%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #1809 +/- ##; ==========================================; + Coverage 71.18% 71.21% +0.02% ; ==========================================; Files 92 92 ; Lines 11190 11180 -10 ; ==========================================; - Hits 7966 7962 -4 ; + Misses 3224 3218 -6 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1809?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/plotting/\_anndata.py](https://codecov.io/gh/theislab/scanpy/pull/1809/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL19hbm5kYXRhLnB5) | `84.47% <ø> (ø)` | |; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1809/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9zY2F0dGVycGxvdHMucHk=) | `86.95% <ø> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1809/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.74% <0.00%> (+0.47%)` | :arrow_up: |; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1809/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <0.00%> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1809#issuecomment-827482585
https://github.com/scverse/scanpy/pull/1810#issuecomment-827491040:1510,Testability,log,logging,1510,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1810?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > Merging [#1810](https://codecov.io/gh/theislab/scanpy/pull/1810?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (2a449c7) into [master](https://codecov.io/gh/theislab/scanpy/commit/c488909a54e9ab1462186cca35b537426e4630db?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c488909) will **increase** coverage by `0.02%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #1810 +/- ##; ==========================================; + Coverage 71.18% 71.21% +0.02% ; ==========================================; Files 92 92 ; Lines 11190 11180 -10 ; ==========================================; - Hits 7966 7962 -4 ; + Misses 3224 3218 -6 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1810?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1810/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.74% <ø> (+0.47%)` | :arrow_up: |; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1810/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <0.00%> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1810#issuecomment-827491040
https://github.com/scverse/scanpy/pull/1811#issuecomment-827499178:1790,Testability,log,logging,1790,canpy/pull/1811?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > Merging [#1811](https://codecov.io/gh/theislab/scanpy/pull/1811?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (3e25aa8) into [master](https://codecov.io/gh/theislab/scanpy/commit/c488909a54e9ab1462186cca35b537426e4630db?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c488909) will **increase** coverage by `0.02%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #1811 +/- ##; ==========================================; + Coverage 71.18% 71.21% +0.02% ; ==========================================; Files 92 92 ; Lines 11190 11180 -10 ; ==========================================; - Hits 7966 7962 -4 ; + Misses 3224 3218 -6 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1811?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/plotting/\_tools/paga.py](https://codecov.io/gh/theislab/scanpy/pull/1811/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9wYWdhLnB5) | `70.62% <ø> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1811/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.74% <0.00%> (+0.47%)` | :arrow_up: |; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1811/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <0.00%> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811#issuecomment-827499178
https://github.com/scverse/scanpy/pull/1811#issuecomment-827726391:86,Testability,log,logs,86,"Hm, I am a bit lost wrt to the checks failing @giovp ; the doc build doesn't show any logs and travis fails because of black complaining about scanpy/tools/_sim.py and some version checking problem. None of this seems related to my PR 🤔",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811#issuecomment-827726391
https://github.com/scverse/scanpy/pull/1811#issuecomment-827747018:55,Deployability,install,installed,55,I don't think `igraph` or `louvain` are actually being installed on readthedocs. . I think you'll need to modify the `.readthedocs.yml` for this. But not calling `louvain` could also work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1811#issuecomment-827747018
https://github.com/scverse/scanpy/pull/1812#issuecomment-827502118:1510,Testability,log,logging,1510,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1812?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > Merging [#1812](https://codecov.io/gh/theislab/scanpy/pull/1812?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (d655718) into [master](https://codecov.io/gh/theislab/scanpy/commit/c488909a54e9ab1462186cca35b537426e4630db?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c488909) will **increase** coverage by `0.02%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #1812 +/- ##; ==========================================; + Coverage 71.18% 71.21% +0.02% ; ==========================================; Files 92 92 ; Lines 11190 11180 -10 ; ==========================================; - Hits 7966 7962 -4 ; + Misses 3224 3218 -6 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1812?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1812/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.74% <ø> (+0.47%)` | :arrow_up: |; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1812/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <0.00%> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1812#issuecomment-827502118
https://github.com/scverse/scanpy/pull/1813#issuecomment-827506879:1810,Testability,log,logging,1810,pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > Merging [#1813](https://codecov.io/gh/theislab/scanpy/pull/1813?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (078ce3e) into [master](https://codecov.io/gh/theislab/scanpy/commit/c488909a54e9ab1462186cca35b537426e4630db?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c488909) will **increase** coverage by `0.02%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #1813 +/- ##; ==========================================; + Coverage 71.18% 71.21% +0.02% ; ==========================================; Files 92 92 ; Lines 11190 11180 -10 ; ==========================================; - Hits 7966 7962 -4 ; + Misses 3224 3218 -6 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1813?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/plotting/\_tools/scatterplots.py](https://codecov.io/gh/theislab/scanpy/pull/1813/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9zY2F0dGVycGxvdHMucHk=) | `86.95% <ø> (ø)` | |; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1813/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.74% <0.00%> (+0.47%)` | :arrow_up: |; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1813/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <0.00%> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1813#issuecomment-827506879
https://github.com/scverse/scanpy/pull/1814#issuecomment-827514615:1467,Testability,log,logging,1467,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1814?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > Merging [#1814](https://codecov.io/gh/theislab/scanpy/pull/1814?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (2c1d01a) into [master](https://codecov.io/gh/theislab/scanpy/commit/c488909a54e9ab1462186cca35b537426e4630db?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c488909) will **increase** coverage by `0.01%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #1814 +/- ##; ==========================================; + Coverage 71.18% 71.20% +0.01% ; ==========================================; Files 92 92 ; Lines 11190 11190 ; ==========================================; + Hits 7966 7968 +2 ; + Misses 3224 3222 -2 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1814?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/plotting/\_anndata.py](https://codecov.io/gh/theislab/scanpy/pull/1814/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL19hbm5kYXRhLnB5) | `84.47% <ø> (ø)` | |; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1814/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <0.00%> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1814#issuecomment-827514615
https://github.com/scverse/scanpy/pull/1815#issuecomment-827526265:1490,Testability,log,logging,1490,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1815?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > Merging [#1815](https://codecov.io/gh/theislab/scanpy/pull/1815?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (aefcf1d) into [master](https://codecov.io/gh/theislab/scanpy/commit/c488909a54e9ab1462186cca35b537426e4630db?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c488909) will **increase** coverage by `0.01%`.; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## master #1815 +/- ##; ==========================================; + Coverage 71.18% 71.20% +0.01% ; ==========================================; Files 92 92 ; Lines 11190 11190 ; ==========================================; + Hits 7966 7968 +2 ; + Misses 3224 3222 -2 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1815?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/plotting/\_tools/\_\_init\_\_.py](https://codecov.io/gh/theislab/scanpy/pull/1815/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL190b29scy9fX2luaXRfXy5weQ==) | `76.27% <ø> (ø)` | |; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1815/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <0.00%> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1815#issuecomment-827526265
https://github.com/scverse/scanpy/issues/1818#issuecomment-828326883:40,Safety,detect,detection,40,"Sorry, I didn't use multiplex community detection. I focused more on preprocessing and briefly on graph construction. I stored the CITE-Seq data in `adata.obsm` and the gene-protein mapping as a column in `adata.var` - if this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-828326883
https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504:294,Deployability,integrat,integration,294,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504
https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504:294,Integrability,integrat,integration,294,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504
https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504:408,Performance,tune,tuned,408,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504
https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:254,Deployability,integrat,integration,254,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212
https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:254,Integrability,integrat,integration,254,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212
https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:570,Performance,tune,tune,570,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212
https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:20,Usability,feedback,feedback,20,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212
https://github.com/scverse/scanpy/issues/1818#issuecomment-830832355:550,Usability,clear,clearly,550,"Hmm... I wonder what the policy should be for Scanpy in these kinds of situations. So far I believe we have mainly added tools that have previously been used for sc analysis (either published tools or ones that have been used in sc papers). I'm not aware of that being the case for multiplex clustering yet. Do we really want to add methods to core that are ML tools, but not necessarily used for SC analysis yet? That would open quite a large range to methods to possible contributions (but might take us out of scanpy core remit... assuming that's clearly defined). Especially something as experimental as multiplex clustering I would be a bit hesitant about. Users will have a particular expectation of a tool in scanpy core. If there isn't a canonical use case example (something we can use as tutorial, or point to as a reason for when this can work), then might not meet those expectations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830832355
https://github.com/scverse/scanpy/issues/1818#issuecomment-857832028:129,Safety,risk,risky,129,"I agree with malte that there's so much more ML out there that just adding a function cause it can be quickly implemented can be risky.; however if we're not the ones to try then who else should. so what if we test the leiden_multiplex in comparison to seurat's WNN on the tutorial data, and decide then? I would be surprised if we didn't find a set of params for leiden_multiplex that allows to replicate the seurat clustering results. also comes to mind similarity network fusion (implemented for citeseq in the citefuse package). prob a project of its own sake tbh. ; happy to help with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-857832028
https://github.com/scverse/scanpy/issues/1818#issuecomment-857832028:210,Testability,test,test,210,"I agree with malte that there's so much more ML out there that just adding a function cause it can be quickly implemented can be risky.; however if we're not the ones to try then who else should. so what if we test the leiden_multiplex in comparison to seurat's WNN on the tutorial data, and decide then? I would be surprised if we didn't find a set of params for leiden_multiplex that allows to replicate the seurat clustering results. also comes to mind similarity network fusion (implemented for citeseq in the citefuse package). prob a project of its own sake tbh. ; happy to help with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-857832028
https://github.com/scverse/scanpy/issues/1818#issuecomment-860807165:238,Testability,test,testing,238,"@giovp Cool! I hadn't seen this. If this is referenced in their paper, then multiplex leiden would fit into the category of ""used in sc analysis"" that I was arguing before, and I would be happy with it being in here. I do think that some testing should ideally happen on our side, so it would be great if you want to take this on, @bio-la !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-860807165
https://github.com/scverse/scanpy/issues/1818#issuecomment-2061253111:22,Availability,avail,available,22,"This functionality is available through `muon`: https://muon.readthedocs.io/en/latest/api/generated/muon.tl.leiden.html. Sorry for the confusion, but that tutorial is based on a development branch which is out of date and should be taken down. I'm also going to close this issue, since multimodal analysis in general is handled through `muon`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-2061253111
https://github.com/scverse/scanpy/issues/1818#issuecomment-2061253111:238,Availability,down,down,238,"This functionality is available through `muon`: https://muon.readthedocs.io/en/latest/api/generated/muon.tl.leiden.html. Sorry for the confusion, but that tutorial is based on a development branch which is out of date and should be taken down. I'm also going to close this issue, since multimodal analysis in general is handled through `muon`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-2061253111
https://github.com/scverse/scanpy/pull/1819#issuecomment-828341861:1466,Testability,log,logging,1466,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1819?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > Merging [#1819](https://codecov.io/gh/theislab/scanpy/pull/1819?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (9b117ae) into [master](https://codecov.io/gh/theislab/scanpy/commit/c488909a54e9ab1462186cca35b537426e4630db?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c488909) will **increase** coverage by `0.01%`.; > The diff coverage is `0.00%`. ```diff; @@ Coverage Diff @@; ## master #1819 +/- ##; ==========================================; + Coverage 71.18% 71.20% +0.01% ; ==========================================; Files 92 92 ; Lines 11190 11190 ; ==========================================; + Hits 7966 7968 +2 ; + Misses 3224 3222 -2 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1819?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/tools/\_louvain.py](https://codecov.io/gh/theislab/scanpy/pull/1819/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Rvb2xzL19sb3V2YWluLnB5) | `60.86% <0.00%> (ø)` | |; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1819/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <0.00%> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1819#issuecomment-828341861
https://github.com/scverse/scanpy/pull/1820#issuecomment-828458610:1466,Testability,log,logging,1466,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1820?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > Merging [#1820](https://codecov.io/gh/theislab/scanpy/pull/1820?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (0949bcb) into [master](https://codecov.io/gh/theislab/scanpy/commit/c488909a54e9ab1462186cca35b537426e4630db?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c488909) will **increase** coverage by `0.01%`.; > The diff coverage is `0.00%`. ```diff; @@ Coverage Diff @@; ## master #1820 +/- ##; ==========================================; + Coverage 71.18% 71.20% +0.01% ; ==========================================; Files 92 92 ; Lines 11190 11190 ; ==========================================; + Hits 7966 7968 +2 ; + Misses 3224 3222 -2 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1820?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/tools/\_louvain.py](https://codecov.io/gh/theislab/scanpy/pull/1820/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Rvb2xzL19sb3V2YWluLnB5) | `60.86% <0.00%> (ø)` | |; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1820/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <0.00%> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1820#issuecomment-828458610
https://github.com/scverse/scanpy/pull/1821#issuecomment-829900825:85,Deployability,continuous,continuous,85,"would also make sense to have this as `colorbar_loc` as this only really applies for continuous coloring, right? I can definetly see the benefit of removing colorbar from figure if this is wanted by the user",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821#issuecomment-829900825
https://github.com/scverse/scanpy/pull/1821#issuecomment-848578199:141,Deployability,continuous,continuous,141,"Sorry for the late reply, thought I responded to this!. > would also make sense to have this as colorbar_loc as this only really applies for continuous coloring. I think that would make sense",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821#issuecomment-848578199
https://github.com/scverse/scanpy/pull/1821#issuecomment-895935559:159,Deployability,continuous,continuous,159,Hi!; Could you finally solve the issue of removing colorbar from the figure? I have tried with the legend_loc=None and legend_loc='none' and they don't remove continuous colorbars. I have tried to fix this issue myself but couldn't find the exact place in the code to do that. Any help would be more than welcome!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821#issuecomment-895935559
https://github.com/scverse/scanpy/pull/1821#issuecomment-939952451:65,Deployability,continuous,continuous,65,"Hi!; Could you finally solve this issue? I am trying to remove a continuous colorbar from a umap and I cannot make it.; Thanks,. Lídia",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821#issuecomment-939952451
https://github.com/scverse/scanpy/pull/1821#issuecomment-1084806101:47,Testability,test,test,47,Just changed the argument a little and added a test. Will be out soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1821#issuecomment-1084806101
https://github.com/scverse/scanpy/pull/1822#issuecomment-829484368:1496,Testability,log,logging,1496,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1822?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > Merging [#1822](https://codecov.io/gh/theislab/scanpy/pull/1822?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (62e48bc) into [master](https://codecov.io/gh/theislab/scanpy/commit/c488909a54e9ab1462186cca35b537426e4630db?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) (c488909) will **increase** coverage by `0.01%`.; > The diff coverage is `50.00%`. ```diff; @@ Coverage Diff @@; ## master #1822 +/- ##; ==========================================; + Coverage 71.18% 71.20% +0.01% ; ==========================================; Files 92 92 ; Lines 11190 11192 +2 ; ==========================================; + Hits 7966 7969 +3 ; + Misses 3224 3223 -1 ; ```. | [Impacted Files](https://codecov.io/gh/theislab/scanpy/pull/1822?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) | Coverage Δ | |; |---|---|---|; | [scanpy/plotting/\_anndata.py](https://codecov.io/gh/theislab/scanpy/pull/1822/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L3Bsb3R0aW5nL19hbm5kYXRhLnB5) | `84.39% <50.00%> (-0.08%)` | :arrow_down: |; | [scanpy/logging.py](https://codecov.io/gh/theislab/scanpy/pull/1822/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#diff-c2NhbnB5L2xvZ2dpbmcucHk=) | `98.38% <0.00%> (+1.61%)` | :arrow_up: |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822#issuecomment-829484368
https://github.com/scverse/scanpy/pull/1822#issuecomment-829878212:112,Testability,test,tests,112,"Thanks for the PR. I think if this is added it should be added as a function parameter. It should also get some tests, I'm a bit concerned how just passing `hue` will work with the groupby and palette parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1822#issuecomment-829878212
https://github.com/scverse/scanpy/issues/1823#issuecomment-963435024:149,Deployability,release,release,149,"Closing as:. * The original issue looks like a numba related problem, which I believe didn't work with python 3.9 at the time.; * We're past the 1.7 release series and aren't supporting it anymore",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-963435024
https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937:46,Availability,error,error,46,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937
https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937:397,Energy Efficiency,monitor,monitor,397,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937
https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937:739,Usability,pause,pausefilter,739,"Please re-open this; currently receiving this error with Python 3.9.7 and scanpy 1.8.2. Just in case it's useful, CPU flags including instruction sets are pasted below. fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 popcnt aes xsave avx f16c lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs xop skinit wdt lwp fma4 tce nodeid_msr tbm topoext perfctr_core perfctr_nb cpb hw_pstate ssbd ibpb vmmcall bmi1 arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-983551937
https://github.com/scverse/scanpy/issues/1823#issuecomment-983618113:109,Integrability,depend,dependencies,109,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-983618113
https://github.com/scverse/scanpy/issues/1823#issuecomment-983618113:143,Testability,log,logging,143,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-983618113
https://github.com/scverse/scanpy/issues/1824#issuecomment-835867261:74,Testability,test,test,74,"Thank you, do you have sankey plot also. another point, do you know which test to use to see which cell types are significant between groups?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824#issuecomment-835867261
https://github.com/scverse/scanpy/issues/1824#issuecomment-953260643:2047,Deployability,patch,patches,2047,"thor', xlabel='leiden_0.6', condition=None); print(relative_frequencies); sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None); ```. I am getting this output; ```. 0 1 2 3 4 5 \; Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 ; Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 ; Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 ; Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author ; Benitez 0.111111 0.133191 0.021368 0.000000 Benitez ; Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari ; Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari ; Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-44-a5b07a2bfb6d> in <module>; 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None); 7 print(relative_frequencies); ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save); 835 for i, typ in enumerate(reversed(cell_types)):; 836 fig = sb.barplot(; --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize; 838 ); 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824#issuecomment-953260643
https://github.com/scverse/scanpy/issues/1824#issuecomment-953260643:2071,Deployability,Patch,Patch,2071,"thor', xlabel='leiden_0.6', condition=None); print(relative_frequencies); sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None); ```. I am getting this output; ```. 0 1 2 3 4 5 \; Benitez 0.087607 0.175214 0.076923 0.183761 0.059829 0.150997 ; Rajbhandari 0.106852 0.079310 0.063879 0.098888 0.023395 0.243239 ; Sarvari 0.078359 0.252695 0.120431 0.116487 0.224560 0.028662 ; Sun 0.408022 0.163329 0.151518 0.108632 0.089583 0.009960 . 6 7 8 9 Author ; Benitez 0.111111 0.133191 0.021368 0.000000 Benitez ; Rajbhandari 0.221171 0.011448 0.151485 0.000332 Rajbhandari ; Sarvari 0.033921 0.084407 0.021299 0.039180 Sarvari ; Sun 0.008545 0.056112 0.004245 0.000054 Sun . ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-44-a5b07a2bfb6d> in <module>; 6 relative_frequencies=sct.calc.relative_frequency_per_cluster(adata2, group_by='Author', xlabel='leiden_0.6', condition=None); 7 print(relative_frequencies); ----> 8 sct.plot.cluster_composition_stacked_barplot(relative_frequencies, xlabel='Author', figsize=(6, 10), width=0.8, order=None, error_bar=None, label_size=15, tick_size=13, capsize=None, margins=(0.02, 0.04), cols=None, save=None). /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/sc_toolbox/api/plot/__init__.py in cluster_composition_stacked_barplot(relative_frequencies, xlabel, figsize, width, order, error_bar, label_size, tick_size, capsize, margins, cols, save); 835 for i, typ in enumerate(reversed(cell_types)):; 836 fig = sb.barplot(; --> 837 data=plot_data, x=xlabel, y=typ, order=order, ci=ci, errcolor=""black"", color=cols[i], capsize=capsize; 838 ); 839 patches.append(mpatches.Patch(color=cols[i], label=typ)). TypeError: 'NoneType' object is not subscriptable. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824#issuecomment-953260643
https://github.com/scverse/scanpy/issues/1824#issuecomment-953646449:62,Availability,avail,available,62,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824#issuecomment-953646449
https://github.com/scverse/scanpy/issues/1824#issuecomment-953646449:92,Deployability,integrat,integrating,92,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824#issuecomment-953646449
https://github.com/scverse/scanpy/issues/1824#issuecomment-953646449:92,Integrability,integrat,integrating,92,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824#issuecomment-953646449
https://github.com/scverse/scanpy/issues/1825#issuecomment-1140150507:94,Availability,error,error,94,"I encounter this same problem, if I call `sc.pp.log1p(x)` before culate hvg with seurat3, the error is gone, it have correlation with the adata.X is sparse or dense in my view.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1825#issuecomment-1140150507
https://github.com/scverse/scanpy/pull/1828#issuecomment-840023795:14,Availability,failure,failure,14,The travis CI failure looks unrelated to this PR.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-840023795
https://github.com/scverse/scanpy/pull/1828#issuecomment-1004540184:66,Deployability,release,release,66,"@Zethson Ready to merge. Thanks for your feedback; I added to the release notes, and rebased on master.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004540184
https://github.com/scverse/scanpy/pull/1828#issuecomment-1004540184:41,Usability,feedback,feedback,41,"@Zethson Ready to merge. Thanks for your feedback; I added to the release notes, and rebased on master.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004540184
https://github.com/scverse/scanpy/pull/1828#issuecomment-1004638735:643,Modifiability,variab,variables,643,"```; Traceback (most recent call last):; File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1828/lib/python3.8/site-packages/sphinx/events.py"", line 101, in emit; results.append(listener.handler(self.app, *args)); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1828/lib/python3.8/site-packages/sphinx_autodoc_typehints/__init__.py"", line 446, in process_docstring; formatted_annotation = format_annotation(; File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1828/lib/python3.8/site-packages/scanpydoc/elegant_typehints/formatting.py"", line 126, in format_annotation; arg_name = variables[""argname""].replace(r""\_"", ""_""); KeyError: 'argname'; ```. Not sure why RTD fails with this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004638735
https://github.com/scverse/scanpy/pull/1828#issuecomment-1005072811:294,Availability,error,error,294,"@Zethson I believe that's an upstream issue. Looks like the docs broke when `sphinx-autodoc-typehints` bumped versions from `1.12.0` to `1.13.0`. I can build the docs locally from `master` and from this branch with `sphinx-autodoc-typehints` v1.12, but not v1.13. (You'll also see an identical error in #2099, despite that just being a dependency bump for pre-commit.). I'll submit a PR to pin `sphinx-autodoc-typehints` to version 1.12.0 shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1005072811
https://github.com/scverse/scanpy/pull/1828#issuecomment-1005072811:336,Integrability,depend,dependency,336,"@Zethson I believe that's an upstream issue. Looks like the docs broke when `sphinx-autodoc-typehints` bumped versions from `1.12.0` to `1.13.0`. I can build the docs locally from `master` and from this branch with `sphinx-autodoc-typehints` v1.12, but not v1.13. (You'll also see an identical error in #2099, despite that just being a dependency bump for pre-commit.). I'll submit a PR to pin `sphinx-autodoc-typehints` to version 1.12.0 shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1005072811
https://github.com/scverse/scanpy/pull/1828#issuecomment-1005073549:303,Availability,error,error,303,"> @Zethson I believe that's an upstream issue. Looks like the docs broke when `sphinx-autodoc-typehints` bumped versions from `1.12.0` to `1.13.0`.; > ; > I can build the docs locally from `master` and from this branch with `sphinx-autodoc-typehints` v1.12, but not v1.13. (You'll also see an identical error in #2099, despite that just being a dependency bump for pre-commit.); > ; > I'll submit a PR to pin `sphinx-autodoc-typehints` to version 1.12.0 shortly. Thank you for taking the time to dig into this! Much appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1005073549
https://github.com/scverse/scanpy/pull/1828#issuecomment-1005073549:345,Integrability,depend,dependency,345,"> @Zethson I believe that's an upstream issue. Looks like the docs broke when `sphinx-autodoc-typehints` bumped versions from `1.12.0` to `1.13.0`.; > ; > I can build the docs locally from `master` and from this branch with `sphinx-autodoc-typehints` v1.12, but not v1.13. (You'll also see an identical error in #2099, despite that just being a dependency bump for pre-commit.); > ; > I'll submit a PR to pin `sphinx-autodoc-typehints` to version 1.12.0 shortly. Thank you for taking the time to dig into this! Much appreciated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1005073549
https://github.com/scverse/scanpy/pull/1828#issuecomment-1006739637:78,Energy Efficiency,green,green,78,Thank you both. @Zethson I've rebased off master. Please merge when this goes green.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1006739637
https://github.com/scverse/scanpy/issues/1829#issuecomment-835850158:278,Usability,intuit,intuitive,278,"> Do we give any indication of how many cells are in a group right now? I feel like this would be important for the user to even know the stats could be unreliable. `sc.pl.dotplot(..., return_fig=True).add_totals().show()`. is one way to check the cell numbers, but there is no intuitive way to do the filtering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1829#issuecomment-835850158
https://github.com/scverse/scanpy/issues/1831#issuecomment-845906745:99,Availability,down,downstream,99,"Hi @FADHLyemen,. You can export the raw count table (before calculating the percentage) into R for downstream analysis using `edgeR`. You can follow the following link to find further information: https://bioconductor.org/books/release/OSCA/multi-sample-comparisons.html#differential-abundance. Regards,; Mikhael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1831#issuecomment-845906745
https://github.com/scverse/scanpy/issues/1831#issuecomment-845906745:228,Deployability,release,release,228,"Hi @FADHLyemen,. You can export the raw count table (before calculating the percentage) into R for downstream analysis using `edgeR`. You can follow the following link to find further information: https://bioconductor.org/books/release/OSCA/multi-sample-comparisons.html#differential-abundance. Regards,; Mikhael",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1831#issuecomment-845906745
https://github.com/scverse/scanpy/issues/1831#issuecomment-845926484:89,Testability,test,testing,89,"Hey! Check out [scCoda](https://github.com/theislab/scCODA) for a differential abundance testing framework in python that is anndata compatible. As statistics are calculated on the sample level, you would however need more than 2 samples to be able to assess significance.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1831#issuecomment-845926484
https://github.com/scverse/scanpy/issues/1832#issuecomment-838305749:449,Availability,mainten,maintenance,449,"I think there's definitely room for more plotting libraries in the ecosystem, but have some doubts about whether all needs can be met by one library. I personally use `seaborn`/ `matplotlib`, `bokeh`, `datashader`, and `altair` for different cases. I also think making a good plotting API is exceedingly difficult, especially if you target both high and low level use cases. I would note that the plotting code in scanpy feels like some of the most maintenance intensive code in the library. > provides helper functions for handling colors, saving figures, etc. We can do a bit more of this here. But of course, much of it would end up being `matplotlib` specific. > encourages a consistent plotting API (e.g. by defining abstract base classes). I'd be interested in hearing specific thoughts on this. I've personally been thinking it would be nice to lean on `seaborn` plotting classes more heavily here, potentially contributing features upstream. Here's one example https://github.com/mwaskom/seaborn/issues/2487 of a feature which could fit the `AnnData` data model nicely. > there is quite some duplicated code in the plotting section. We'd definitely like to reduce the amount of duplicated code, which is what drove the addition of `sc.get`. This seems to be working out internally, if slowly. > All the scanpy helper functions for plotting (e.g. savefig_or_show, _set_color_for_categorical_obs etc.) are private scanpy functions. I'd like to move towards stabilizing this. I'm not sure how much we'd want to provide plotting library specific code, vs. more generic helpers. Right now the most obvious addition is `_set_color_for_categorical_obs`, which I'd also like to make accessible through `sc.get`. Adding `groupby` support to `anndata` would help a lot here too (https://github.com/theislab/anndata/issues/556). `save_fig_or_show` is something that I don't think we should export, and may need a rework (#1508).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1832#issuecomment-838305749
https://github.com/scverse/scanpy/issues/1832#issuecomment-838305749:1165,Energy Efficiency,reduce,reduce,1165,"I think there's definitely room for more plotting libraries in the ecosystem, but have some doubts about whether all needs can be met by one library. I personally use `seaborn`/ `matplotlib`, `bokeh`, `datashader`, and `altair` for different cases. I also think making a good plotting API is exceedingly difficult, especially if you target both high and low level use cases. I would note that the plotting code in scanpy feels like some of the most maintenance intensive code in the library. > provides helper functions for handling colors, saving figures, etc. We can do a bit more of this here. But of course, much of it would end up being `matplotlib` specific. > encourages a consistent plotting API (e.g. by defining abstract base classes). I'd be interested in hearing specific thoughts on this. I've personally been thinking it would be nice to lean on `seaborn` plotting classes more heavily here, potentially contributing features upstream. Here's one example https://github.com/mwaskom/seaborn/issues/2487 of a feature which could fit the `AnnData` data model nicely. > there is quite some duplicated code in the plotting section. We'd definitely like to reduce the amount of duplicated code, which is what drove the addition of `sc.get`. This seems to be working out internally, if slowly. > All the scanpy helper functions for plotting (e.g. savefig_or_show, _set_color_for_categorical_obs etc.) are private scanpy functions. I'd like to move towards stabilizing this. I'm not sure how much we'd want to provide plotting library specific code, vs. more generic helpers. Right now the most obvious addition is `_set_color_for_categorical_obs`, which I'd also like to make accessible through `sc.get`. Adding `groupby` support to `anndata` would help a lot here too (https://github.com/theislab/anndata/issues/556). `save_fig_or_show` is something that I don't think we should export, and may need a rework (#1508).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1832#issuecomment-838305749
https://github.com/scverse/scanpy/issues/1832#issuecomment-838305749:1683,Security,access,accessible,1683,"I think there's definitely room for more plotting libraries in the ecosystem, but have some doubts about whether all needs can be met by one library. I personally use `seaborn`/ `matplotlib`, `bokeh`, `datashader`, and `altair` for different cases. I also think making a good plotting API is exceedingly difficult, especially if you target both high and low level use cases. I would note that the plotting code in scanpy feels like some of the most maintenance intensive code in the library. > provides helper functions for handling colors, saving figures, etc. We can do a bit more of this here. But of course, much of it would end up being `matplotlib` specific. > encourages a consistent plotting API (e.g. by defining abstract base classes). I'd be interested in hearing specific thoughts on this. I've personally been thinking it would be nice to lean on `seaborn` plotting classes more heavily here, potentially contributing features upstream. Here's one example https://github.com/mwaskom/seaborn/issues/2487 of a feature which could fit the `AnnData` data model nicely. > there is quite some duplicated code in the plotting section. We'd definitely like to reduce the amount of duplicated code, which is what drove the addition of `sc.get`. This seems to be working out internally, if slowly. > All the scanpy helper functions for plotting (e.g. savefig_or_show, _set_color_for_categorical_obs etc.) are private scanpy functions. I'd like to move towards stabilizing this. I'm not sure how much we'd want to provide plotting library specific code, vs. more generic helpers. Right now the most obvious addition is `_set_color_for_categorical_obs`, which I'd also like to make accessible through `sc.get`. Adding `groupby` support to `anndata` would help a lot here too (https://github.com/theislab/anndata/issues/556). `save_fig_or_show` is something that I don't think we should export, and may need a rework (#1508).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1832#issuecomment-838305749
https://github.com/scverse/scanpy/issues/1832#issuecomment-841139143:2011,Deployability,continuous,continuous,2011," this can quickly get out of bounds, I'd thus suggest to; ; - constrain this discussion to `matplotlib`/`seaborn` (as this is what scanpy and afaik most of the ecosystem projects are using); - only focus on the low-level use-cases. . In brief all that is required to implement a plotting API that behaves like scanpy's. . ---. > I'd be interested in hearing specific thoughts on this. I've personally been thinking it would be nice to lean on seaborn plotting classes more heavily here, potentially contributing features upstream. Here's one example mwaskom/seaborn#2487 of a feature which could fit the AnnData data model nicely. I was mostly referring to @fidelram's idea how to make plot styling more ""modular"" instead of having a vast amount of arguments for a single plotting function (#956). If this idea was to be implemented for all scanpy plotting functions, I thought that maybe an abstract base-class could provide the method signatures to ensure consistency within scanpy and ecosystem packages. Even with the current ""keyword approach"" it would be great if there was some way to ensure that common keywords are always named consistently. . What would be an example of a plot object you would like to ""move"" to seaborn? Something like a multi-panel UMAP plot? . ---. > I'd like to move towards stabilizing this. I'm not sure how much we'd want to provide plotting library specific code, vs. more generic helpers. Right now the most obvious addition is _set_color_for_categorical_obs, which I'd also like to make accessible through sc.get. Adding groupby support to anndata would help a lot here too (theislab/anndata#556). that sounds great! . ---. Finally, in terms of ""reusable building blocks"" I was thinking of, for instance, . - the ""dot size legend"" ; ![image](https://user-images.githubusercontent.com/7051479/118252952-a378ae80-b4a9-11eb-8a11-72bf46cdae20.png). - Setting up axes for a scatter plot together with the appropriate legend (continuous color bar or categorical legend)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1832#issuecomment-841139143
https://github.com/scverse/scanpy/issues/1832#issuecomment-841139143:1578,Security,access,accessible,1578," this can quickly get out of bounds, I'd thus suggest to; ; - constrain this discussion to `matplotlib`/`seaborn` (as this is what scanpy and afaik most of the ecosystem projects are using); - only focus on the low-level use-cases. . In brief all that is required to implement a plotting API that behaves like scanpy's. . ---. > I'd be interested in hearing specific thoughts on this. I've personally been thinking it would be nice to lean on seaborn plotting classes more heavily here, potentially contributing features upstream. Here's one example mwaskom/seaborn#2487 of a feature which could fit the AnnData data model nicely. I was mostly referring to @fidelram's idea how to make plot styling more ""modular"" instead of having a vast amount of arguments for a single plotting function (#956). If this idea was to be implemented for all scanpy plotting functions, I thought that maybe an abstract base-class could provide the method signatures to ensure consistency within scanpy and ecosystem packages. Even with the current ""keyword approach"" it would be great if there was some way to ensure that common keywords are always named consistently. . What would be an example of a plot object you would like to ""move"" to seaborn? Something like a multi-panel UMAP plot? . ---. > I'd like to move towards stabilizing this. I'm not sure how much we'd want to provide plotting library specific code, vs. more generic helpers. Right now the most obvious addition is _set_color_for_categorical_obs, which I'd also like to make accessible through sc.get. Adding groupby support to anndata would help a lot here too (theislab/anndata#556). that sounds great! . ---. Finally, in terms of ""reusable building blocks"" I was thinking of, for instance, . - the ""dot size legend"" ; ![image](https://user-images.githubusercontent.com/7051479/118252952-a378ae80-b4a9-11eb-8a11-72bf46cdae20.png). - Setting up axes for a scatter plot together with the appropriate legend (continuous color bar or categorical legend)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1832#issuecomment-841139143
https://github.com/scverse/scanpy/issues/1832#issuecomment-1028414654:0,Availability,Ping,Ping,0,"Ping @WeilerP @adamgayoso, since you've both raised this idea today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1832#issuecomment-1028414654
https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974:60,Deployability,continuous,continuous,60,"So, it look like it does fit all elements at once if it's a continuous variable (I'm not completley sure why this doesn't seem to be the case for categorical). . I think your solution would work, but it may be worthwhile to spot check. It would probably also be nice to have a nice API for this on our end, like being able to just provide a patsy formula. I did a quick check comparing your suggestion to the results of adding features with the function below, and it seems fine. ```python; import statsmodels.formula.api as smf. def regress_out_poly(y, x, degree=2):; poly = "" + "".join(f""np.power(x, {i})"" for i in range(1, degree + 1)); mod = smf.glm(f""y ~ {poly}"", {""y"": y, ""x"": x}, family=sm.families.Gaussian()); return mod.fit().resid_response; ```. @LuckyMD may have more to say on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974
https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974:592,Energy Efficiency,power,power,592,"So, it look like it does fit all elements at once if it's a continuous variable (I'm not completley sure why this doesn't seem to be the case for categorical). . I think your solution would work, but it may be worthwhile to spot check. It would probably also be nice to have a nice API for this on our end, like being able to just provide a patsy formula. I did a quick check comparing your suggestion to the results of adding features with the function below, and it seems fine. ```python; import statsmodels.formula.api as smf. def regress_out_poly(y, x, degree=2):; poly = "" + "".join(f""np.power(x, {i})"" for i in range(1, degree + 1)); mod = smf.glm(f""y ~ {poly}"", {""y"": y, ""x"": x}, family=sm.families.Gaussian()); return mod.fit().resid_response; ```. @LuckyMD may have more to say on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974
https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974:71,Modifiability,variab,variable,71,"So, it look like it does fit all elements at once if it's a continuous variable (I'm not completley sure why this doesn't seem to be the case for categorical). . I think your solution would work, but it may be worthwhile to spot check. It would probably also be nice to have a nice API for this on our end, like being able to just provide a patsy formula. I did a quick check comparing your suggestion to the results of adding features with the function below, and it seems fine. ```python; import statsmodels.formula.api as smf. def regress_out_poly(y, x, degree=2):; poly = "" + "".join(f""np.power(x, {i})"" for i in range(1, degree + 1)); mod = smf.glm(f""y ~ {poly}"", {""y"": y, ""x"": x}, family=sm.families.Gaussian()); return mod.fit().resid_response; ```. @LuckyMD may have more to say on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974
https://github.com/scverse/scanpy/issues/1840#issuecomment-844133832:69,Deployability,install,installing,69,"@wniu721 We had similar issues, but everything seems to be solved by installing recent versions of UMAP. I have to say we were working with python 3.8 (IDK if 3.9 has other issues)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-844133832
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:463,Availability,down,downgrade,463,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:486,Availability,error,error,486,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:542,Availability,ERROR,ERROR,542,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:557,Availability,error,errored,557,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:63,Deployability,install,install,63,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:286,Deployability,install,installed,286,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:407,Deployability,install,install,407,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:516,Deployability,install,install,516,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:770,Deployability,install,install-,770,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:915,Deployability,install,install-,915,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:1243,Deployability,install,install,1243,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:1337,Deployability,install,install-record,1337,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:1404,Deployability,install,install-headers,1404,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004:1491,Testability,log,logs,1491,"@dawe Could you also please provide a brief tutorial on how to install `scanpy` on M1? I am having troubles. I have followed [this tutorial ](https://medium.com/geekculture/the-best-way-to-setup-your-m1-mac-for-python-development-fb5dffd08fd) to set up python on my M1 Mac. Thus I have installed `miniforge` with `brew`. My versions are `Python 3.9.6` and `pip 21.2.4`. Also I have read that you succeed in install `scanpy` with `python 3.8` but I am not able to downgrade version. The error I face when I run `pip3 install scanpy` is:. ```; ERROR: Command errored out with exit status 1: /opt/homebrew/Caskroom/miniforge/base/bin/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""'; __file__='""'""'/private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-install-6blz73pw/h5py_c0efce6062af4b4d9f6564a97c24d1a7/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/y_/5kkrlhbj2v1bch8snxxws28c0000gn/T/pip-record-lf5rwuj7/install-record.txt --single-version-externally-managed --compile --install-headers /opt/homebrew/Caskroom/miniforge/base/include/python3.9/h5py Check the logs for full command output.```. Thank you in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1840#issuecomment-930949004
https://github.com/scverse/scanpy/issues/1845#issuecomment-848077091:66,Deployability,continuous,continuous,66,@giovp I want to make correlation plot between cell types and the continuous variables stored in .obs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848077091
https://github.com/scverse/scanpy/issues/1845#issuecomment-848077091:77,Modifiability,variab,variables,77,@giovp I want to make correlation plot between cell types and the continuous variables stored in .obs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848077091
https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:149,Deployability,continuous,continuous,149,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984
https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:331,Deployability,continuous,continuous,331,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984
https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:448,Integrability,depend,dependent,448,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984
https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:109,Modifiability,variab,variable,109,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984
https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:160,Modifiability,variab,variable,160,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984
https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:205,Modifiability,variab,variable,205,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984
https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:288,Modifiability,variab,variable,288,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984
https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:342,Modifiability,variab,variable,342,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984
https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:434,Modifiability,variab,variables,434,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984
https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:693,Modifiability,variab,variable-and-a-categorical-variab,693,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984
https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:377,Testability,test,test,377,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984
https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:486,Testability,log,logistic,486,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984
https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:54,Usability,clear,clear,54,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984
https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263:41,Deployability,continuous,continuous,41,@Koncopd it is a correlation between two continuous variables as celltypes are continuous and age is also continuous. how to correlate X with continuous variables stored in .obs ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263
https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263:79,Deployability,continuous,continuous,79,@Koncopd it is a correlation between two continuous variables as celltypes are continuous and age is also continuous. how to correlate X with continuous variables stored in .obs ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263
https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263:106,Deployability,continuous,continuous,106,@Koncopd it is a correlation between two continuous variables as celltypes are continuous and age is also continuous. how to correlate X with continuous variables stored in .obs ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263
https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263:142,Deployability,continuous,continuous,142,@Koncopd it is a correlation between two continuous variables as celltypes are continuous and age is also continuous. how to correlate X with continuous variables stored in .obs ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263
https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263:52,Modifiability,variab,variables,52,@Koncopd it is a correlation between two continuous variables as celltypes are continuous and age is also continuous. how to correlate X with continuous variables stored in .obs ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263
https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263:153,Modifiability,variab,variables,153,@Koncopd it is a correlation between two continuous variables as celltypes are continuous and age is also continuous. how to correlate X with continuous variables stored in .obs ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263
https://github.com/scverse/scanpy/issues/1845#issuecomment-849646102:21,Deployability,continuous,continuous,21,"Are celltypes really continuous? How does this variable look like?; for continuous you can do; `from scipy.stats import pearsonr`; `r, _ = pearsonr(adata.obs[""celltypes""], adata.obs[""age""])`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-849646102
https://github.com/scverse/scanpy/issues/1845#issuecomment-849646102:72,Deployability,continuous,continuous,72,"Are celltypes really continuous? How does this variable look like?; for continuous you can do; `from scipy.stats import pearsonr`; `r, _ = pearsonr(adata.obs[""celltypes""], adata.obs[""age""])`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-849646102
https://github.com/scverse/scanpy/issues/1845#issuecomment-849646102:47,Modifiability,variab,variable,47,"Are celltypes really continuous? How does this variable look like?; for continuous you can do; `from scipy.stats import pearsonr`; `r, _ = pearsonr(adata.obs[""celltypes""], adata.obs[""age""])`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-849646102
https://github.com/scverse/scanpy/issues/1845#issuecomment-849666917:297,Integrability,depend,depends,297,@Koncopd it is the # of celltypes per each cohort or the relative_frequencies per each group:; ![image](https://user-images.githubusercontent.com/23288387/119840907-5ea23e00-bed3-11eb-9738-17b267889bb5.png). is it something researchers looking for? or do you think this not good approach as cells depends on how many cells per sample,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-849666917
https://github.com/scverse/scanpy/issues/1847#issuecomment-845420561:50,Deployability,integrat,integrating-data-using-ingest,50,"https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html; Hi, this should be relevant.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1847#issuecomment-845420561
https://github.com/scverse/scanpy/issues/1847#issuecomment-845420561:50,Integrability,integrat,integrating-data-using-ingest,50,"https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html; Hi, this should be relevant.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1847#issuecomment-845420561
https://github.com/scverse/scanpy/pull/1848#issuecomment-847710888:284,Availability,down,downloaded,284,"> Could you try building the docs and checking if anything gets added? The `scanpy.plotting.rst` file has been weird in the past. I think it's at least started being consistent with newer versions of sphinx, but it'd be good to check. The pbmc data in the docs folder does indeed get downloaded when building the docs.; Do you want me to include this path into the `.gitignore`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1848#issuecomment-847710888
https://github.com/scverse/scanpy/pull/1848#issuecomment-848593645:38,Testability,test,test,38,"One last thing, could you exclude the test data files from any automatic formatting? I guess it's good to know that new lines at the end of files doesn't matter, but I'd prefer to keep those files exactly as they were written by `spaceranger`/ `zarr`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1848#issuecomment-848593645
https://github.com/scverse/scanpy/pull/1848#issuecomment-848597315:29,Testability,test,tests,29,@ivirshup . `exclude: scanpy/tests/_data`. Added for trailing-whitespace and end-of-file-fixer.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1848#issuecomment-848597315
https://github.com/scverse/scanpy/issues/1850#issuecomment-846951987:10,Deployability,install,install,10,"Could you install the newest bugfix release of anndata and try again? Failing that, I'd recommend upgrading `h5py` and seeing if that works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-846951987
https://github.com/scverse/scanpy/issues/1850#issuecomment-846951987:36,Deployability,release,release,36,"Could you install the newest bugfix release of anndata and try again? Failing that, I'd recommend upgrading `h5py` and seeing if that works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-846951987
https://github.com/scverse/scanpy/issues/1850#issuecomment-847090754:64,Deployability,update,update,64,"The issue persists with anndata 0.7.6. I've also been trying to update h5py, but it has conflicts with other packages. I'll post an update if I get it updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847090754
https://github.com/scverse/scanpy/issues/1850#issuecomment-847090754:132,Deployability,update,update,132,"The issue persists with anndata 0.7.6. I've also been trying to update h5py, but it has conflicts with other packages. I'll post an update if I get it updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847090754
https://github.com/scverse/scanpy/issues/1850#issuecomment-847090754:151,Deployability,update,updated,151,"The issue persists with anndata 0.7.6. I've also been trying to update h5py, but it has conflicts with other packages. I'll post an update if I get it updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847090754
https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613:58,Deployability,update,update,58,"What packages are conflicting with `h5py` 3.0? The 2 -> 3 update had some fairly hard to manage changes to how string dtypes are handled, and it'd be nice to drop 2.0 support once the ecosystem is caught up. ----------------. I'm actually not so sure this is h5py or anndata though, those are just common culprits. I've tried this in a conda environment with h5py 2.10.0 and it doesn't reproduce. I've even tried to make a conda environment from your `sinfo` and could not reproduce. <details>; <summary> Here's how I tried to create a replicate environment </summary>. ```python; $ mamba create -n issue-1850 'anndata==0.7.6' 'scanpy==1.7.2' 'sinfo==0.3.1' 'pillow==8.0.1' 'backcall==0.2.0' 'bottleneck==1.3.2' 'cffi==1.14.0' 'colorama==0.4.4' 'cycler==0.10.0' 'decorator==4.4.2' 'fcsparser==0.2.1' 'get_version==2.1' 'h5py==2.10.0' 'python-igraph>=0.7.1' 'ipykernel==5.3.4' 'ipython_genutils==0.2.0' 'ipywidgets==7.5.1' 'jedi==0.17.2' 'joblib==0.17.0' 'kiwisolver==1.2.0' 'leidenalg==0.8.2' 'llvmlite==0.34.0' 'lxml==4.6.1' 'matplotlib==3.3.2' 'natsort==7.0.1' 'networkx==2.5' 'numba==0.51.2' 'numexpr==2.7.1' 'numpy==1.19.2' 'packaging==20.4' 'pandas==1.2.4' 'parso==0.7.0' 'pexpect==4.8.0' 'pickleshare==0.7.5' 'prompt_toolkit==3.0.8' 'psutil==5.8.0' 'ptyprocess==0.6.0' 'pycparser==2.20' 'pygments==2.7.1' 'pyparsing==2.4.7' 'pytz==2020.1' 'scipy==1.5.2' 'scvelo==0.2.3' 'seaborn==0.11.1' 'sinfo==0.3.1' 'six==1.15.0' 'scikit-learn==0.23.2' 'statsmodels==0.12.0' 'pytables==3.6.1' 'traitlets==5.0.5' 'umap-learn==0.4.6' 'wcwidth==0.2.5' 'IPython==7.18.1' 'jupyter_client==6.1.7' 'jupyter_core==4.6.3' 'notebook==6.1.4'; ```. </details>. Could you create a fresh environment, and try again? I'm really confused about how you are ending up with a multi index anywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613
https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613:693,Performance,bottleneck,bottleneck,693,"What packages are conflicting with `h5py` 3.0? The 2 -> 3 update had some fairly hard to manage changes to how string dtypes are handled, and it'd be nice to drop 2.0 support once the ecosystem is caught up. ----------------. I'm actually not so sure this is h5py or anndata though, those are just common culprits. I've tried this in a conda environment with h5py 2.10.0 and it doesn't reproduce. I've even tried to make a conda environment from your `sinfo` and could not reproduce. <details>; <summary> Here's how I tried to create a replicate environment </summary>. ```python; $ mamba create -n issue-1850 'anndata==0.7.6' 'scanpy==1.7.2' 'sinfo==0.3.1' 'pillow==8.0.1' 'backcall==0.2.0' 'bottleneck==1.3.2' 'cffi==1.14.0' 'colorama==0.4.4' 'cycler==0.10.0' 'decorator==4.4.2' 'fcsparser==0.2.1' 'get_version==2.1' 'h5py==2.10.0' 'python-igraph>=0.7.1' 'ipykernel==5.3.4' 'ipython_genutils==0.2.0' 'ipywidgets==7.5.1' 'jedi==0.17.2' 'joblib==0.17.0' 'kiwisolver==1.2.0' 'leidenalg==0.8.2' 'llvmlite==0.34.0' 'lxml==4.6.1' 'matplotlib==3.3.2' 'natsort==7.0.1' 'networkx==2.5' 'numba==0.51.2' 'numexpr==2.7.1' 'numpy==1.19.2' 'packaging==20.4' 'pandas==1.2.4' 'parso==0.7.0' 'pexpect==4.8.0' 'pickleshare==0.7.5' 'prompt_toolkit==3.0.8' 'psutil==5.8.0' 'ptyprocess==0.6.0' 'pycparser==2.20' 'pygments==2.7.1' 'pyparsing==2.4.7' 'pytz==2020.1' 'scipy==1.5.2' 'scvelo==0.2.3' 'seaborn==0.11.1' 'sinfo==0.3.1' 'six==1.15.0' 'scikit-learn==0.23.2' 'statsmodels==0.12.0' 'pytables==3.6.1' 'traitlets==5.0.5' 'umap-learn==0.4.6' 'wcwidth==0.2.5' 'IPython==7.18.1' 'jupyter_client==6.1.7' 'jupyter_core==4.6.3' 'notebook==6.1.4'; ```. </details>. Could you create a fresh environment, and try again? I'm really confused about how you are ending up with a multi index anywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613
https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613:1431,Usability,learn,learn,1431,"What packages are conflicting with `h5py` 3.0? The 2 -> 3 update had some fairly hard to manage changes to how string dtypes are handled, and it'd be nice to drop 2.0 support once the ecosystem is caught up. ----------------. I'm actually not so sure this is h5py or anndata though, those are just common culprits. I've tried this in a conda environment with h5py 2.10.0 and it doesn't reproduce. I've even tried to make a conda environment from your `sinfo` and could not reproduce. <details>; <summary> Here's how I tried to create a replicate environment </summary>. ```python; $ mamba create -n issue-1850 'anndata==0.7.6' 'scanpy==1.7.2' 'sinfo==0.3.1' 'pillow==8.0.1' 'backcall==0.2.0' 'bottleneck==1.3.2' 'cffi==1.14.0' 'colorama==0.4.4' 'cycler==0.10.0' 'decorator==4.4.2' 'fcsparser==0.2.1' 'get_version==2.1' 'h5py==2.10.0' 'python-igraph>=0.7.1' 'ipykernel==5.3.4' 'ipython_genutils==0.2.0' 'ipywidgets==7.5.1' 'jedi==0.17.2' 'joblib==0.17.0' 'kiwisolver==1.2.0' 'leidenalg==0.8.2' 'llvmlite==0.34.0' 'lxml==4.6.1' 'matplotlib==3.3.2' 'natsort==7.0.1' 'networkx==2.5' 'numba==0.51.2' 'numexpr==2.7.1' 'numpy==1.19.2' 'packaging==20.4' 'pandas==1.2.4' 'parso==0.7.0' 'pexpect==4.8.0' 'pickleshare==0.7.5' 'prompt_toolkit==3.0.8' 'psutil==5.8.0' 'ptyprocess==0.6.0' 'pycparser==2.20' 'pygments==2.7.1' 'pyparsing==2.4.7' 'pytz==2020.1' 'scipy==1.5.2' 'scvelo==0.2.3' 'seaborn==0.11.1' 'sinfo==0.3.1' 'six==1.15.0' 'scikit-learn==0.23.2' 'statsmodels==0.12.0' 'pytables==3.6.1' 'traitlets==5.0.5' 'umap-learn==0.4.6' 'wcwidth==0.2.5' 'IPython==7.18.1' 'jupyter_client==6.1.7' 'jupyter_core==4.6.3' 'notebook==6.1.4'; ```. </details>. Could you create a fresh environment, and try again? I'm really confused about how you are ending up with a multi index anywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613
https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613:1511,Usability,learn,learn,1511,"What packages are conflicting with `h5py` 3.0? The 2 -> 3 update had some fairly hard to manage changes to how string dtypes are handled, and it'd be nice to drop 2.0 support once the ecosystem is caught up. ----------------. I'm actually not so sure this is h5py or anndata though, those are just common culprits. I've tried this in a conda environment with h5py 2.10.0 and it doesn't reproduce. I've even tried to make a conda environment from your `sinfo` and could not reproduce. <details>; <summary> Here's how I tried to create a replicate environment </summary>. ```python; $ mamba create -n issue-1850 'anndata==0.7.6' 'scanpy==1.7.2' 'sinfo==0.3.1' 'pillow==8.0.1' 'backcall==0.2.0' 'bottleneck==1.3.2' 'cffi==1.14.0' 'colorama==0.4.4' 'cycler==0.10.0' 'decorator==4.4.2' 'fcsparser==0.2.1' 'get_version==2.1' 'h5py==2.10.0' 'python-igraph>=0.7.1' 'ipykernel==5.3.4' 'ipython_genutils==0.2.0' 'ipywidgets==7.5.1' 'jedi==0.17.2' 'joblib==0.17.0' 'kiwisolver==1.2.0' 'leidenalg==0.8.2' 'llvmlite==0.34.0' 'lxml==4.6.1' 'matplotlib==3.3.2' 'natsort==7.0.1' 'networkx==2.5' 'numba==0.51.2' 'numexpr==2.7.1' 'numpy==1.19.2' 'packaging==20.4' 'pandas==1.2.4' 'parso==0.7.0' 'pexpect==4.8.0' 'pickleshare==0.7.5' 'prompt_toolkit==3.0.8' 'psutil==5.8.0' 'ptyprocess==0.6.0' 'pycparser==2.20' 'pygments==2.7.1' 'pyparsing==2.4.7' 'pytz==2020.1' 'scipy==1.5.2' 'scvelo==0.2.3' 'seaborn==0.11.1' 'sinfo==0.3.1' 'six==1.15.0' 'scikit-learn==0.23.2' 'statsmodels==0.12.0' 'pytables==3.6.1' 'traitlets==5.0.5' 'umap-learn==0.4.6' 'wcwidth==0.2.5' 'IPython==7.18.1' 'jupyter_client==6.1.7' 'jupyter_core==4.6.3' 'notebook==6.1.4'; ```. </details>. Could you create a fresh environment, and try again? I'm really confused about how you are ending up with a multi index anywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2144,Availability,down,down,2144,"e 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:1971,Deployability,update,updated,1971,"format 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2032,Deployability,update,update,2032,"e 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2198,Deployability,install,installing,2198,"; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64::harfbuzz==1.8.8=hffaf4a1_0; - conda-forge/linux-64::leidenalg==0.8.2=py38habedc41_0; - defaults/linux",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2258,Deployability,update,update,2258,"anpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64::harfbuzz==1.8.8=hffaf4a1_0; - conda-forge/linux-64::leidenalg==0.8.2=py38habedc41_0; - defaults/linux-64::pango==1.42.4=h049681c_0; - defaults/linux-64::pycairo==1.19.1=py38h708ec4a_0; - conda-for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2286,Deployability,update,update,2286,"anpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64::harfbuzz==1.8.8=hffaf4a1_0; - conda-forge/linux-64::leidenalg==0.8.2=py38habedc41_0; - defaults/linux-64::pango==1.42.4=h049681c_0; - defaults/linux-64::pycairo==1.19.1=py38h708ec4a_0; - conda-for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:2699,Deployability,update,updated,2699," 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=py38_1; - defaults/linux-64::cairo==1.14.12=h8948797_3; - defaults/linux-64::graphviz==2.40.1=h21bd128_2; - defaults/linux-64::harfbuzz==1.8.8=hffaf4a1_0; - conda-forge/linux-64::leidenalg==0.8.2=py38habedc41_0; - defaults/linux-64::pango==1.42.4=h049681c_0; - defaults/linux-64::pycairo==1.19.1=py38h708ec4a_0; - conda-forge/linux-64::python-igraph==0.7.1.post7=py38h516909a_0; - r/linux-64::r==3.6.0=r36_0; - r/linux-64::r-base==3.6.1=haffb61f_2; - r/noarch::r-boot==1.3_20=r36h6115d3f_0; - r/linux-64::r-class==7.3_15=r36h96ca727_0; - r/linux-64::r-cluster==2.0.8=r36ha65eedd_0; - r/noarch::r-codetools==0.2_16=r36h6115d3f_0; - r/linux-64::r-foreign==0.8_71=r36h96ca727_0; - r/linux-64::r-kernsmooth==2.23_15=r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:6569,Deployability,patch,patch,6569,.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h06a4308_0 ; docrep 0.3.2 pyh44b312d_0 conda-forge; docutils 0.17.1 py38h06a4308_1 ; dorothea-py 1.0.3 pypi_0 pypi; entrypoints 0.3 py38_0 ; et_xmlfile 1.1.0 py38h06a4308_0 ; expat 2.4.1 h2531618_2 ; fa2 0.3.5 pypi_0 pypi; fastcache 1.1.0 py38h7b6447c_0 ; fbpca 1.0 pypi_0 pypi; fcsparser 0.2.1 pypi_0 pypi; filelock 3.0.12 pyhd3eb1b0_1 ; flake8 3.9.0 pyhd3eb1b0_0 ; flask 1.1.2 pyhd3eb1b0_0 ; fontconfig 2.13.1 h6c09931_0 ; freetype 2.10.4 h5ab3b9f_0 ; fribidi 1.0.10 h7b6447c_0 ; fsspec 0.9.0 pyhd3eb1b0_0 ; funcargparse 0.2.3 pypi_0 pypi; future 0.18.2 py38_1 ; future_fstrings 1.2.0 py38h32f6830_2 conda-forge; gcc_impl_linux-64 7.3.0 habb00fd_1 ; gcc_linux-64 7.3.0 h553295d_15 ; geosketch 1.2 pypi_0 pypi; get_terminal_size 1.0.0 haa9412d_0 ; get_version 2.1 py_1 conda-forge; gevent 21.1.2 py38h27cfd23_1 ; gfortran_impl_linux-64 7.3.0 hdf63c60_1 ; gfortran_linux-64 7.3.0 h553295d_15 ; glib 2.63.1 h5a9c865_0 ; glob2 0.7 pyhd3eb1b0_0 ; gmp 6.2.1 h2531,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:12434,Deployability,patch,patchelf,12434,bd54_1 ; multicoretsne 0.1 pypi_0 pypi; multidict 5.1.0 pypi_0 pypi; multipledispatch 0.6.0 py38_0 ; mypy_extensions 0.4.3 py38_0 ; natsort 7.1.1 pyhd3eb1b0_0 ; nbclassic 0.2.6 pyhd3eb1b0_0 ; nbclient 0.5.3 pyhd3eb1b0_0 ; nbconvert 6.0.7 py38_0 ; nbformat 5.1.3 pyhd3eb1b0_0 ; ncurses 6.2 he6710b0_1 ; nest-asyncio 1.5.1 pyhd3eb1b0_0 ; networkx 2.5 py_0 ; nltk 3.6.2 pyhd3eb1b0_0 ; nose 1.3.7 pyhd3eb1b0_1006 ; notebook 6.4.0 py38h06a4308_0 ; numba 0.53.1 py38ha9443f7_0 ; numexpr 2.7.3 py38h22e1b3c_1 ; numpy 1.20.2 py38h2d18471_0 ; numpy-base 1.20.2 py38hfae3a4d_0 ; numpydoc 1.1.0 pyhd3eb1b0_1 ; nvidia-ml-py3 7.352.0 pypi_0 pypi; olefile 0.46 py_0 ; opencensus 0.7.13 pypi_0 pypi; opencensus-context 0.1.2 pypi_0 pypi; openpyxl 3.0.7 pyhd3eb1b0_0 ; openssl 1.1.1k h27cfd23_0 ; packaging 20.9 pyhd3eb1b0_0 ; palantir 1.0.0 pypi_0 pypi; pandas 1.2.4 py38h2531618_0 ; pandoc 2.12 h06a4308_0 ; pandocfilters 1.4.3 py38h06a4308_1 ; pango 1.42.4 h049681c_0 ; parso 0.7.0 py_0 ; partd 1.2.0 pyhd3eb1b0_0 ; patchelf 0.12 h2531618_1 ; path 15.1.2 py38h06a4308_0 ; path.py 12.5.0 0 ; pathlib2 2.3.5 py38h06a4308_2 ; pathspec 0.7.0 py_0 ; pathtools 0.1.2 py_1 ; patsy 0.5.1 py38_0 ; pcre 8.44 he6710b0_0 ; pep8 1.7.1 py38_0 ; pexpect 4.8.0 pyhd3eb1b0_3 ; phenograph 1.5.7 pypi_0 pypi; pickleshare 0.7.5 pyhd3eb1b0_1003 ; pillow 8.2.0 py38he98fc37_0 ; pip 21.1.1 py38h06a4308_0 ; pixman 0.40.0 h7b6447c_0 ; pkginfo 1.7.0 py38h06a4308_0 ; pluggy 0.13.1 py38h06a4308_0 ; ply 3.11 py38_0 ; progeny-py 1.0.3 pypi_0 pypi; progressbar2 3.37.1 py38h06a4308_0 ; prometheus_client 0.10.1 pyhd3eb1b0_0 ; prompt-toolkit 3.0.17 pyh06a4308_0 ; prompt_toolkit 3.0.17 hd3eb1b0_0 ; protobuf 3.17.0 pypi_0 pypi; psutil 5.8.0 py38h27cfd23_1 ; ptyprocess 0.7.0 pyhd3eb1b0_2 ; py 1.10.0 pyhd3eb1b0_0 ; py-lief 0.10.1 py38h403a769_0 ; py-spy 0.3.7 pypi_0 pypi; pyasn1 0.4.8 pypi_0 pypi; pyasn1-modules 0.2.8 pypi_0 pypi; pycairo 1.19.1 py38h708ec4a_0 ; pycodestyle 2.6.0 pyhd3eb1b0_0 ; pycosat 0.6.3 py38h7b6447c_1 ; pycparser 2.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:20564,Deployability,update,update,20564,"-----------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-2-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 3 from ._metadata import __version__, __author__, __email__; 4 ; ----> 5 from ._utils import check_versions; 6 ; 7 check_versions(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/_utils.py in <module>; 16 from numpy import random; 17 from scipy import sparse; ---> 18 from anndata import AnnData, __version__ as anndata_version; 19 from textwrap import dedent; 20 from packaging import version. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/anndata/__init__.py in <module>; 5 if not within_flit():; 6 del within_flit; ----> 7 from ._core.anndata import AnnData, ImplicitModificationWarning; 8 from ._core.merge import concat; 9 from ._core.raw import Raw. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/anndata/_core/anndata.py in <module>; 15 from typing import Tuple, List # Generic; 16 ; ---> 17 import h5py; 18 from natsort import natsorted; 19 import numpy as np. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/__init__.py in <module>; 31 raise; 32 ; ---> 33 from . import version; 34 ; 35 if version.hdf5_version_tuple != version.hdf5_built_version_tuple:. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/version.py in <module>; 13 ; 14 from collections import namedtuple; ---> 15 from . import h5 as _h5; 16 import sys; 17 import numpy. h5py/h5.pyx in init h5py.h5(). ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3; ```. </Details>. Before, when I was trying to update `h5py` and getting lots of conflicts, I let conda try to figure out all of them, printing out a huge list that reached the output limit of my console (~6000 lines). Let me know if I should post the list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:7841,Energy Efficiency,green,greenlet,7841,fastcache 1.1.0 py38h7b6447c_0 ; fbpca 1.0 pypi_0 pypi; fcsparser 0.2.1 pypi_0 pypi; filelock 3.0.12 pyhd3eb1b0_1 ; flake8 3.9.0 pyhd3eb1b0_0 ; flask 1.1.2 pyhd3eb1b0_0 ; fontconfig 2.13.1 h6c09931_0 ; freetype 2.10.4 h5ab3b9f_0 ; fribidi 1.0.10 h7b6447c_0 ; fsspec 0.9.0 pyhd3eb1b0_0 ; funcargparse 0.2.3 pypi_0 pypi; future 0.18.2 py38_1 ; future_fstrings 1.2.0 py38h32f6830_2 conda-forge; gcc_impl_linux-64 7.3.0 habb00fd_1 ; gcc_linux-64 7.3.0 h553295d_15 ; geosketch 1.2 pypi_0 pypi; get_terminal_size 1.0.0 haa9412d_0 ; get_version 2.1 py_1 conda-forge; gevent 21.1.2 py38h27cfd23_1 ; gfortran_impl_linux-64 7.3.0 hdf63c60_1 ; gfortran_linux-64 7.3.0 h553295d_15 ; glib 2.63.1 h5a9c865_0 ; glob2 0.7 pyhd3eb1b0_0 ; gmp 6.2.1 h2531618_2 ; gmpy2 2.0.8 py38hd5f6e3b_3 ; google-api-core 1.27.0 pypi_0 pypi; google-auth 1.30.0 pypi_0 pypi; googleapis-common-protos 1.53.0 pypi_0 pypi; gpustat 0.6.0 pypi_0 pypi; graphite2 1.3.14 h23475e2_0 ; graphtools 1.5.2 pypi_0 pypi; graphviz 2.40.1 h21bd128_2 ; greenlet 1.1.0 py38h2531618_0 ; grpcio 1.37.1 pypi_0 pypi; gsl 2.4 h14c3975_4 ; gst-plugins-base 1.14.0 hbbd80ab_1 ; gstreamer 1.14.0 hb453b48_1 ; gxx_impl_linux-64 7.3.0 hdf63c60_1 ; gxx_linux-64 7.3.0 h553295d_15 ; h5py 3.2.1 nompi_py38h9915d05_100 conda-forge; harfbuzz 1.8.8 hffaf4a1_0 ; harmonypy 0.0.5 pypi_0 pypi; harmonyts 0.1.4 pypi_0 pypi; hdf5 1.10.6 nompi_h3c11f04_101 conda-forge; heapdict 1.0.1 py_0 ; hiredis 2.0.0 pypi_0 pypi; html5lib 1.1 py_0 ; icu 58.2 he6710b0_3 ; idna 2.10 pyhd3eb1b0_0 ; igraph 0.7.1 h2166141_1005 conda-forge; imageio 2.9.0 pyhd3eb1b0_0 ; imagesize 1.2.0 pyhd3eb1b0_0 ; importlib-metadata 3.10.0 py38h06a4308_0 ; importlib_metadata 3.10.0 hd3eb1b0_0 ; iniconfig 1.1.1 pyhd3eb1b0_0 ; intel-openmp 2021.2.0 h06a4308_610 ; intervaltree 2.1.0 pypi_0 pypi; ipykernel 5.3.4 py38h5ca1d4c_0 ; ipython 7.22.0 py38hb070fc8_0 ; ipython_genutils 0.2.0 pyhd3eb1b0_1 ; ipywidgets 7.6.3 pyhd3eb1b0_1 ; isort 5.8.0 pyhd3eb1b0_0 ; itsdangerous 2.0.1 pyhd3eb1b0_0 ; jbig 2.1 h,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:9905,Integrability,wrap,wrap,9905,1 ; jeepney 0.6.0 pyhd3eb1b0_0 ; jinja2 3.0.0 pyhd3eb1b0_0 ; joblib 1.0.1 pyhd3eb1b0_0 ; jpeg 9b h024ee3a_2 ; json5 0.9.5 py_0 ; jsonschema 3.2.0 py_2 ; jupyter 1.0.0 py38_7 ; jupyter-packaging 0.7.12 pyhd3eb1b0_0 ; jupyter_client 6.1.12 pyhd3eb1b0_0 ; jupyter_console 6.4.0 pyhd3eb1b0_0 ; jupyter_contrib_core 0.3.3 py_2 conda-forge; jupyter_contrib_nbextensions 0.5.1 pyhd8ed1ab_2 conda-forge; jupyter_core 4.7.1 py38h06a4308_0 ; jupyter_highlight_selected_word 0.2.0 py38h578d9bd_1002 conda-forge; jupyter_latex_envs 1.4.6 pyhd8ed1ab_1002 conda-forge; jupyter_nbextensions_configurator 0.4.1 py38h578d9bd_2 conda-forge; jupyter_server 1.4.1 py38h06a4308_0 ; jupyterlab 3.0.14 pyhd3eb1b0_1 ; jupyterlab_pygments 0.1.2 py_0 ; jupyterlab_server 2.4.0 pyhd3eb1b0_0 ; jupyterlab_widgets 1.0.0 pyhd3eb1b0_1 ; keyring 23.0.1 py38h06a4308_0 ; kiwisolver 1.3.1 py38h2531618_0 ; krb5 1.17.1 h173b8e3_0 ; lazy-object-proxy 1.6.0 py38h27cfd23_0 ; lcms2 2.12 h3be6417_0 ; ld_impl_linux-64 2.33.1 h53a641e_7 ; legacy-api-wrap 1.2 py_0 conda-forge; leidenalg 0.8.2 py38habedc41_0 conda-forge; libarchive 3.4.2 h62408e4_0 ; libcurl 7.69.1 h20c2e04_0 ; libedit 3.1.20210216 h27cfd23_1 ; libev 4.33 h7b6447c_0 ; libffi 3.2.1 hf484d3e_1007 ; libgcc-ng 9.3.0 h2828fa1_19 conda-forge; libgfortran-ng 7.3.0 hdf63c60_0 ; libgomp 9.3.0 h2828fa1_19 conda-forge; libiconv 1.15 h63c8f33_5 ; liblief 0.10.1 he6710b0_0 ; libllvm10 10.0.1 hbcb73fb_5 ; libllvm9 9.0.1 h4a3c616_1 ; libpng 1.6.37 hbc83047_0 ; libsodium 1.0.18 h7b6447c_0 ; libspatialindex 1.9.3 h2531618_0 ; libssh2 1.9.0 h1ba5d50_1 ; libstdcxx-ng 9.1.0 hdf63c60_0 ; libtiff 4.2.0 h85742a9_0 ; libtool 2.4.6 h7b6447c_1005 ; libuuid 1.0.3 h1bed415_2 ; libuv 1.40.0 h7b6447c_0 ; libwebp-base 1.2.0 h27cfd23_0 ; libxcb 1.14 h7b6447c_0 ; libxml2 2.9.10 hb55368b_3 ; libxslt 1.1.34 hc22bd24_0 ; llvmlite 0.36.0 py38h612dafd_4 ; locket 0.2.1 py38h06a4308_1 ; loompy 2.0.16 py_0 bioconda; lxml 4.6.3 py38h9120a33_0 ; lz4-c 1.9.3 h2531618_0 ; lzo 2.10 h7b6447c_2 ; magic-i,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:18220,Integrability,wrap,wrapt,18220,"tblib 1.7.0 py_0 ; terminado 0.9.4 py38h06a4308_0 ; testpath 0.4.4 pyhd3eb1b0_0 ; textdistance 4.2.1 pyhd3eb1b0_0 ; threadpoolctl 2.1.0 pyh5ca1d4c_0 ; three-merge 0.1.1 pyhd3eb1b0_0 ; tk 8.6.10 hbc83047_0 ; tktable 2.10 h14c3975_0 ; tokenize-rt 4.1.0 pyhd8ed1ab_0 conda-forge; toml 0.10.2 pyhd3eb1b0_0 ; toolz 0.11.1 pyhd3eb1b0_0 ; tornado 6.1 py38h27cfd23_0 ; tqdm 4.59.0 pyhd3eb1b0_1 ; traitlets 5.0.5 pyhd3eb1b0_0 ; triku 1.3.1 pypi_0 pypi; tslearn 0.5.0.5 pypi_0 pypi; typed-ast 1.4.2 py38h27cfd23_1 ; typing 3.10.0.0 py38h06a4308_0 ; typing-extensions 3.10.0.0 pypi_0 pypi; typing_extensions 3.7.4.3 pyha847dfd_0 ; tzlocal 2.1 py38_0 ; ujson 4.0.2 py38h2531618_0 ; umap-learn 0.5.1 py38h578d9bd_0 conda-forge; unicodecsv 0.14.1 py38_0 ; unixodbc 2.3.9 h7b6447c_0 ; urllib3 1.26.4 pyhd3eb1b0_0 ; vendorize 0.2.1 pypi_0 pypi; watchdog 1.0.2 py38h06a4308_1 ; wcwidth 0.2.5 py_0 ; webencodings 0.5.1 py38_1 ; werkzeug 1.0.1 pyhd3eb1b0_0 ; wheel 0.36.2 pyhd3eb1b0_0 ; widgetsnbextension 3.5.1 py38_0 ; wrapt 1.12.1 py38h7b6447c_1 ; wurlitzer 2.1.0 py38h06a4308_0 ; xlrd 1.2.0 pypi_0 pypi; xlsxwriter 1.3.8 pyhd3eb1b0_0 ; xlwt 1.3.0 py38_0 ; xz 5.2.5 h7b6447c_0 ; yaml 0.2.5 h7b6447c_0 ; yapf 0.31.0 pyhd3eb1b0_0 ; yarl 1.6.3 pypi_0 pypi; zeromq 4.3.4 h2531618_0 ; zict 2.0.0 pyhd3eb1b0_0 ; zipp 3.4.1 pyhd3eb1b0_0 ; zlib 1.2.11 h7b6447c_3 ; zope 1.0 py38_1 ; zope.event 4.5.0 py38_0 ; zope.interface 5.3.0 py38h27cfd23_0 ; zstd 1.4.5 h9ceee32_0 ; ```. </Details>. <Details>; <Summary>ImportError traceback</Summary>. ```python; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-2-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 3 from ._metadata import __version__, __author__, __email__; 4 ; ----> 5 from ._utils import check_versions; 6 ; 7 check_versions(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:18608,Integrability,interface,interface,18608,"s 5.0.5 pyhd3eb1b0_0 ; triku 1.3.1 pypi_0 pypi; tslearn 0.5.0.5 pypi_0 pypi; typed-ast 1.4.2 py38h27cfd23_1 ; typing 3.10.0.0 py38h06a4308_0 ; typing-extensions 3.10.0.0 pypi_0 pypi; typing_extensions 3.7.4.3 pyha847dfd_0 ; tzlocal 2.1 py38_0 ; ujson 4.0.2 py38h2531618_0 ; umap-learn 0.5.1 py38h578d9bd_0 conda-forge; unicodecsv 0.14.1 py38_0 ; unixodbc 2.3.9 h7b6447c_0 ; urllib3 1.26.4 pyhd3eb1b0_0 ; vendorize 0.2.1 pypi_0 pypi; watchdog 1.0.2 py38h06a4308_1 ; wcwidth 0.2.5 py_0 ; webencodings 0.5.1 py38_1 ; werkzeug 1.0.1 pyhd3eb1b0_0 ; wheel 0.36.2 pyhd3eb1b0_0 ; widgetsnbextension 3.5.1 py38_0 ; wrapt 1.12.1 py38h7b6447c_1 ; wurlitzer 2.1.0 py38h06a4308_0 ; xlrd 1.2.0 pypi_0 pypi; xlsxwriter 1.3.8 pyhd3eb1b0_0 ; xlwt 1.3.0 py38_0 ; xz 5.2.5 h7b6447c_0 ; yaml 0.2.5 h7b6447c_0 ; yapf 0.31.0 pyhd3eb1b0_0 ; yarl 1.6.3 pypi_0 pypi; zeromq 4.3.4 h2531618_0 ; zict 2.0.0 pyhd3eb1b0_0 ; zipp 3.4.1 pyhd3eb1b0_0 ; zlib 1.2.11 h7b6447c_3 ; zope 1.0 py38_1 ; zope.event 4.5.0 py38_0 ; zope.interface 5.3.0 py38h27cfd23_0 ; zstd 1.4.5 h9ceee32_0 ; ```. </Details>. <Details>; <Summary>ImportError traceback</Summary>. ```python; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-2-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 3 from ._metadata import __version__, __author__, __email__; 4 ; ----> 5 from ._utils import check_versions; 6 ; 7 check_versions(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/_utils.py in <module>; 16 from numpy import random; 17 from scipy import sparse; ---> 18 from anndata import AnnData, __version__ as anndata_version; 19 from textwrap import dedent; 20 from packaging import version. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/anndata/__init__.py in <module>; 5 if not within_flit():; 6 del within_flit; ----> 7 from ._core.anndata import AnnDat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:7925,Modifiability,plugin,plugins-base,7925, filelock 3.0.12 pyhd3eb1b0_1 ; flake8 3.9.0 pyhd3eb1b0_0 ; flask 1.1.2 pyhd3eb1b0_0 ; fontconfig 2.13.1 h6c09931_0 ; freetype 2.10.4 h5ab3b9f_0 ; fribidi 1.0.10 h7b6447c_0 ; fsspec 0.9.0 pyhd3eb1b0_0 ; funcargparse 0.2.3 pypi_0 pypi; future 0.18.2 py38_1 ; future_fstrings 1.2.0 py38h32f6830_2 conda-forge; gcc_impl_linux-64 7.3.0 habb00fd_1 ; gcc_linux-64 7.3.0 h553295d_15 ; geosketch 1.2 pypi_0 pypi; get_terminal_size 1.0.0 haa9412d_0 ; get_version 2.1 py_1 conda-forge; gevent 21.1.2 py38h27cfd23_1 ; gfortran_impl_linux-64 7.3.0 hdf63c60_1 ; gfortran_linux-64 7.3.0 h553295d_15 ; glib 2.63.1 h5a9c865_0 ; glob2 0.7 pyhd3eb1b0_0 ; gmp 6.2.1 h2531618_2 ; gmpy2 2.0.8 py38hd5f6e3b_3 ; google-api-core 1.27.0 pypi_0 pypi; google-auth 1.30.0 pypi_0 pypi; googleapis-common-protos 1.53.0 pypi_0 pypi; gpustat 0.6.0 pypi_0 pypi; graphite2 1.3.14 h23475e2_0 ; graphtools 1.5.2 pypi_0 pypi; graphviz 2.40.1 h21bd128_2 ; greenlet 1.1.0 py38h2531618_0 ; grpcio 1.37.1 pypi_0 pypi; gsl 2.4 h14c3975_4 ; gst-plugins-base 1.14.0 hbbd80ab_1 ; gstreamer 1.14.0 hb453b48_1 ; gxx_impl_linux-64 7.3.0 hdf63c60_1 ; gxx_linux-64 7.3.0 h553295d_15 ; h5py 3.2.1 nompi_py38h9915d05_100 conda-forge; harfbuzz 1.8.8 hffaf4a1_0 ; harmonypy 0.0.5 pypi_0 pypi; harmonyts 0.1.4 pypi_0 pypi; hdf5 1.10.6 nompi_h3c11f04_101 conda-forge; heapdict 1.0.1 py_0 ; hiredis 2.0.0 pypi_0 pypi; html5lib 1.1 py_0 ; icu 58.2 he6710b0_3 ; idna 2.10 pyhd3eb1b0_0 ; igraph 0.7.1 h2166141_1005 conda-forge; imageio 2.9.0 pyhd3eb1b0_0 ; imagesize 1.2.0 pyhd3eb1b0_0 ; importlib-metadata 3.10.0 py38h06a4308_0 ; importlib_metadata 3.10.0 hd3eb1b0_0 ; iniconfig 1.1.1 pyhd3eb1b0_0 ; intel-openmp 2021.2.0 h06a4308_610 ; intervaltree 2.1.0 pypi_0 pypi; ipykernel 5.3.4 py38h5ca1d4c_0 ; ipython 7.22.0 py38hb070fc8_0 ; ipython_genutils 0.2.0 pyhd3eb1b0_1 ; ipywidgets 7.6.3 pyhd3eb1b0_1 ; isort 5.8.0 pyhd3eb1b0_0 ; itsdangerous 2.0.1 pyhd3eb1b0_0 ; jbig 2.1 hdba287a_0 ; jdcal 1.4.1 py_0 ; jedi 0.17.2 py38h06a4308_1 ; jeepney 0.6.0 pyhd3eb1b0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:323,Performance,bottleneck,bottleneck,323,"I created a new environment (see below for package details) and there everything works as it should. . <Details>; <summary>Versions in the new working environment</summary>. -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.6; anyio NA; argon2 20.1.0; attr 21.2.0; babel 2.9.1; backcall 0.2.0; bottleneck 1.3.2; brotli NA; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.05.0; dateutil 2.8.1; decorator 5.0.9; fsspec 2021.05.0; get_version 2.1; h5py 3.2.1; idna 2.10; igraph 0.9.1; ipykernel 5.5.5; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.8.0; jupyterlab_server 2.5.2; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; markupsafe 2.0.1; matplotlib 3.4.2; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5616,Performance,bottleneck,bottleneck,5616,pyhd3eb1b0_0 ; anaconda custom py38_1 ; anaconda-client 1.7.2 py38_0 ; anaconda-project 0.10.0 pyhd3eb1b0_0 ; anndata 0.7.6 pypi_0 pypi; anyio 2.2.0 py38h06a4308_1 ; appdirs 1.4.4 py_0 ; argh 0.26.2 py38_0 ; argon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5795,Performance,cache,cached-property,5795, py_0 ; argh 0.26.2 py38_0 ; argon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h06a4308_0 ; docrep 0.3.2 pyh44b312d_0 conda-forge; docutils 0.17.1 py38h06a4308_1 ; dorothea-py 1.0.3 pypi_0 pypi; entrypoints 0.3 py38_0 ; et_xmlfile 1.1.0 py38h06a4308_0 ; expat,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5824,Performance,cache,cachetools,5824,rgon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h06a4308_0 ; docrep 0.3.2 pyh44b312d_0 conda-forge; docutils 0.17.1 py38h06a4308_1 ; dorothea-py 1.0.3 pypi_0 pypi; entrypoints 0.3 py38_0 ; et_xmlfile 1.1.0 py38h06a4308_0 ; expat 2.4.1 h2531618_2 ; fa2 0.3.5 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:4951,Safety,timeout,timeout,4951,h96ca727_0; - r/linux-64::r-recommended==3.6.0=r36_0; - r/linux-64::r-rpart==4.1_15=r36h96ca727_0; - r/linux-64::r-spatial==7.3_11=r36h96ca727_4; - r/linux-64::r-survival==2.44_1.1=r36h96ca727_0; ```. </Details>. <Details>; <Summary>Package versions in old environment</Summary>. ```# packages in environment at /home/karl/anaconda3/envs/scanpy1_7:; #; # Name Version Build Channel; _anaconda_depends 2020.07 py38_0 ; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 1_gnu conda-forge; _r-mutex 1.0.0 anacondar_1 ; adjusttext 0.7.3.1 py_1 conda-forge; aiohttp 3.7.4.post0 pypi_0 pypi; aiohttp-cors 0.7.0 pypi_0 pypi; aioredis 1.3.1 pypi_0 pypi; alabaster 0.7.12 pyhd3eb1b0_0 ; anaconda custom py38_1 ; anaconda-client 1.7.2 py38_0 ; anaconda-project 0.10.0 pyhd3eb1b0_0 ; anndata 0.7.6 pypi_0 pypi; anyio 2.2.0 py38h06a4308_1 ; appdirs 1.4.4 py_0 ; argh 0.26.2 py38_0 ; argon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5759,Security,certificate,certificates,5759, 2.2.0 py38h06a4308_1 ; appdirs 1.4.4 py_0 ; argh 0.26.2 py38_0 ; argon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h06a4308_0 ; docrep 0.3.2 pyh44b312d_0 conda-forge; docutils 0.17.1 py38h06a4308_1 ; dorothea-py 1.0.3 pypi_0 pypi; entrypoints 0.3 py38_0 ; et,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:1917,Testability,log,logical,1917,"format 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. </Details>. I'm still trying to update h5py in the old environment, which has quite some inconsistencies in it, considerably slowing everything down. At some point it looked like I had success with installing h5py 3.2.1 from conda-forge after running `conda update anaconda` and `conda update --all` (as per [here](https://stackoverflow.com/questions/56072846/how-to-resolve-inconsistent-package-warnings-in-conda)). But now this environment leads to an ImportError when importing scanpy: `ImportError: /home/karl/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/h5py/defs.cpython-38-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_ros3`; Can it be that pip version of scanpy doesn't see the updated conda version of h5py?. <Details>; <summary>Inconsistencies in the old environment</summary>. ```; The following packages are causing the inconsistency:. - defaults/linux-64::_anaconda_depends==2020.07=py38_0; - defaults/linux-64::anaconda==custom=",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:11263,Testability,mock,mock,11263, ; liblief 0.10.1 he6710b0_0 ; libllvm10 10.0.1 hbcb73fb_5 ; libllvm9 9.0.1 h4a3c616_1 ; libpng 1.6.37 hbc83047_0 ; libsodium 1.0.18 h7b6447c_0 ; libspatialindex 1.9.3 h2531618_0 ; libssh2 1.9.0 h1ba5d50_1 ; libstdcxx-ng 9.1.0 hdf63c60_0 ; libtiff 4.2.0 h85742a9_0 ; libtool 2.4.6 h7b6447c_1005 ; libuuid 1.0.3 h1bed415_2 ; libuv 1.40.0 h7b6447c_0 ; libwebp-base 1.2.0 h27cfd23_0 ; libxcb 1.14 h7b6447c_0 ; libxml2 2.9.10 hb55368b_3 ; libxslt 1.1.34 hc22bd24_0 ; llvmlite 0.36.0 py38h612dafd_4 ; locket 0.2.1 py38h06a4308_1 ; loompy 2.0.16 py_0 bioconda; lxml 4.6.3 py38h9120a33_0 ; lz4-c 1.9.3 h2531618_0 ; lzo 2.10 h7b6447c_2 ; magic-impute 2.0.4 pypi_0 pypi; make 4.2.1 h1bed415_1 ; markupsafe 2.0.1 py38h27cfd23_0 ; matplotlib 3.3.4 py38h06a4308_0 ; matplotlib-base 3.3.4 py38h62a2d02_0 ; mccabe 0.6.1 py38_1 ; mistune 0.8.4 py38h7b6447c_1000 ; mkl 2021.2.0 h06a4308_296 ; mkl-service 2.3.0 py38h27cfd23_1 ; mkl_fft 1.3.0 py38h42c9631_2 ; mkl_random 1.2.1 py38ha9443f7_2 ; mnnpy 0.1.9.5 pypi_0 pypi; mock 4.0.3 pyhd3eb1b0_0 ; more-itertools 8.7.0 pyhd3eb1b0_0 ; mpc 1.1.0 h10f8cd9_1 ; mpfr 4.0.2 hb69a4c5_1 ; mpmath 1.2.1 py38h06a4308_0 ; msgpack-python 1.0.2 py38hff7bd54_1 ; multicoretsne 0.1 pypi_0 pypi; multidict 5.1.0 pypi_0 pypi; multipledispatch 0.6.0 py38_0 ; mypy_extensions 0.4.3 py38_0 ; natsort 7.1.1 pyhd3eb1b0_0 ; nbclassic 0.2.6 pyhd3eb1b0_0 ; nbclient 0.5.3 pyhd3eb1b0_0 ; nbconvert 6.0.7 py38_0 ; nbformat 5.1.3 pyhd3eb1b0_0 ; ncurses 6.2 he6710b0_1 ; nest-asyncio 1.5.1 pyhd3eb1b0_0 ; networkx 2.5 py_0 ; nltk 3.6.2 pyhd3eb1b0_0 ; nose 1.3.7 pyhd3eb1b0_1006 ; notebook 6.4.0 py38h06a4308_0 ; numba 0.53.1 py38ha9443f7_0 ; numexpr 2.7.3 py38h22e1b3c_1 ; numpy 1.20.2 py38h2d18471_0 ; numpy-base 1.20.2 py38hfae3a4d_0 ; numpydoc 1.1.0 pyhd3eb1b0_1 ; nvidia-ml-py3 7.352.0 pypi_0 pypi; olefile 0.46 py_0 ; opencensus 0.7.13 pypi_0 pypi; opencensus-context 0.1.2 pypi_0 pypi; openpyxl 3.0.7 pyhd3eb1b0_0 ; openssl 1.1.1k h27cfd23_0 ; packaging 20.9 pyhd3eb1b0_0 ; palantir 1.0.0 py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:17270,Testability,test,testpath,17270,3.6.1 pyhd3eb1b0_1001 ; sip 4.19.13 py38he6710b0_0 ; six 1.15.0 py38h06a4308_0 ; sklearn 0.0 pypi_0 pypi; snappy 1.1.8 he6710b0_0 ; sniffio 1.2.0 py38h06a4308_1 ; snowballstemmer 2.1.0 pyhd3eb1b0_0 ; sortedcollections 2.1.0 pyhd3eb1b0_0 ; sortedcontainers 2.3.0 pyhd3eb1b0_0 ; soupsieve 2.2.1 pyhd3eb1b0_0 ; sphinx 4.0.1 pyhd3eb1b0_0 ; sphinxcontrib 1.0 py38_1 ; sphinxcontrib-applehelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-devhelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-htmlhelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-jsmath 1.0.1 pyhd3eb1b0_0 ; sphinxcontrib-qthelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-serializinghtml 1.1.4 pyhd3eb1b0_0 ; sphinxcontrib-websupport 1.2.4 py_0 ; spyder 4.2.5 py38h06a4308_0 ; spyder-kernels 1.10.2 py38h06a4308_0 ; sqlalchemy 1.4.15 py38h27cfd23_0 ; sqlite 3.35.4 hdfb4753_0 ; statsmodels 0.12.2 py38h27cfd23_0 ; stdlib-list 0.7.0 py_2 conda-forge; sympy 1.8 py38h06a4308_0 ; tasklogger 1.0.0 pypi_0 pypi; tbb 2020.3 hfd86e86_0 ; tblib 1.7.0 py_0 ; terminado 0.9.4 py38h06a4308_0 ; testpath 0.4.4 pyhd3eb1b0_0 ; textdistance 4.2.1 pyhd3eb1b0_0 ; threadpoolctl 2.1.0 pyh5ca1d4c_0 ; three-merge 0.1.1 pyhd3eb1b0_0 ; tk 8.6.10 hbc83047_0 ; tktable 2.10 h14c3975_0 ; tokenize-rt 4.1.0 pyhd8ed1ab_0 conda-forge; toml 0.10.2 pyhd3eb1b0_0 ; toolz 0.11.1 pyhd3eb1b0_0 ; tornado 6.1 py38h27cfd23_0 ; tqdm 4.59.0 pyhd3eb1b0_1 ; traitlets 5.0.5 pyhd3eb1b0_0 ; triku 1.3.1 pypi_0 pypi; tslearn 0.5.0.5 pypi_0 pypi; typed-ast 1.4.2 py38h27cfd23_1 ; typing 3.10.0.0 py38h06a4308_0 ; typing-extensions 3.10.0.0 pypi_0 pypi; typing_extensions 3.7.4.3 pyha847dfd_0 ; tzlocal 2.1 py38_0 ; ujson 4.0.2 py38h2531618_0 ; umap-learn 0.5.1 py38h578d9bd_0 conda-forge; unicodecsv 0.14.1 py38_0 ; unixodbc 2.3.9 h7b6447c_0 ; urllib3 1.26.4 pyhd3eb1b0_0 ; vendorize 0.2.1 pypi_0 pypi; watchdog 1.0.2 py38h06a4308_1 ; wcwidth 0.2.5 py_0 ; webencodings 0.5.1 py38_1 ; werkzeug 1.0.1 pyhd3eb1b0_0 ; wheel 0.36.2 pyhd3eb1b0_0 ; widgetsnbextension 3.5.1 py38_0 ; wrapt 1.12.1 py38h7b6447c_1 ; wurlitzer 2.1.0 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:15790,Usability,learn,learn,15790,tconsole 5.0.3 pyhd3eb1b0_0 ; qtpy 1.9.0 py_0 ; r 3.6.0 r36_0 r; r-base 3.6.1 haffb61f_2 r; r-boot 1.3_20 r36h6115d3f_0 r; r-class 7.3_15 r36h96ca727_0 r; r-cluster 2.0.8 r36ha65eedd_0 r; r-codetools 0.2_16 r36h6115d3f_0 r; r-foreign 0.8_71 r36h96ca727_0 r; r-kernsmooth 2.23_15 r36ha65eedd_4 r; r-lattice 0.20_38 r36h96ca727_0 r; r-mass 7.3_51.3 r36h96ca727_0 r; r-matrix 1.2_17 r36h96ca727_0 r; r-mgcv 1.8_28 r36h96ca727_0 r; r-nlme 3.1_139 r36ha65eedd_0 r; r-nnet 7.3_12 r36h96ca727_0 r; r-recommended 3.6.0 r36_0 r; r-rpart 4.1_15 r36h96ca727_0 r; r-spatial 7.3_11 r36h96ca727_4 r; r-survival 2.44_1.1 r36h96ca727_0 r; ray 1.3.0 pypi_0 pypi; readline 8.1 h27cfd23_0 ; redis 3.5.3 pypi_0 pypi; regex 2021.4.4 py38h27cfd23_0 ; requests 2.25.1 pyhd3eb1b0_0 ; ripgrep 12.1.1 0 ; rope 0.18.0 py_0 ; rpy2 3.4.4 pypi_0 pypi; rsa 4.7.2 pypi_0 pypi; rtree 0.9.7 py38h06a4308_1 ; ruamel_yaml 0.15.100 py38h27cfd23_0 ; scanorama 1.7 pypi_0 pypi; scanpy 1.7.2 pypi_0 pypi; scikit-image 0.16.2 py38h0573a6f_0 ; scikit-learn 0.24.2 py38ha9443f7_0 ; scikit-learn-extra 0.1.0b2 py38h8790de6_0 conda-forge; scikit-misc 0.1.4 pypi_0 pypi; scipy 1.6.2 py38had2a1c9_1 ; scprep 1.0.13 pypi_0 pypi; scvelo 0.2.3 pypi_0 pypi; seaborn 0.11.1 pyhd3eb1b0_0 ; secretstorage 3.3.1 py38h06a4308_0 ; send2trash 1.5.0 pyhd3eb1b0_1 ; setuptools 52.0.0 py38h06a4308_0 ; setuptools-scm 6.0.1 pyhd3eb1b0_1 ; setuptools_scm 6.0.1 hd3eb1b0_1 ; simplegeneric 0.8.1 py38_2 ; sinfo 0.3.1 py_0 conda-forge; singledispatch 3.6.1 pyhd3eb1b0_1001 ; sip 4.19.13 py38he6710b0_0 ; six 1.15.0 py38h06a4308_0 ; sklearn 0.0 pypi_0 pypi; snappy 1.1.8 he6710b0_0 ; sniffio 1.2.0 py38h06a4308_1 ; snowballstemmer 2.1.0 pyhd3eb1b0_0 ; sortedcollections 2.1.0 pyhd3eb1b0_0 ; sortedcontainers 2.3.0 pyhd3eb1b0_0 ; soupsieve 2.2.1 pyhd3eb1b0_0 ; sphinx 4.0.1 pyhd3eb1b0_0 ; sphinxcontrib 1.0 py38_1 ; sphinxcontrib-applehelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-devhelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-htmlhelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-jsmath,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:15827,Usability,learn,learn-extra,15827, py_0 ; r 3.6.0 r36_0 r; r-base 3.6.1 haffb61f_2 r; r-boot 1.3_20 r36h6115d3f_0 r; r-class 7.3_15 r36h96ca727_0 r; r-cluster 2.0.8 r36ha65eedd_0 r; r-codetools 0.2_16 r36h6115d3f_0 r; r-foreign 0.8_71 r36h96ca727_0 r; r-kernsmooth 2.23_15 r36ha65eedd_4 r; r-lattice 0.20_38 r36h96ca727_0 r; r-mass 7.3_51.3 r36h96ca727_0 r; r-matrix 1.2_17 r36h96ca727_0 r; r-mgcv 1.8_28 r36h96ca727_0 r; r-nlme 3.1_139 r36ha65eedd_0 r; r-nnet 7.3_12 r36h96ca727_0 r; r-recommended 3.6.0 r36_0 r; r-rpart 4.1_15 r36h96ca727_0 r; r-spatial 7.3_11 r36h96ca727_4 r; r-survival 2.44_1.1 r36h96ca727_0 r; ray 1.3.0 pypi_0 pypi; readline 8.1 h27cfd23_0 ; redis 3.5.3 pypi_0 pypi; regex 2021.4.4 py38h27cfd23_0 ; requests 2.25.1 pyhd3eb1b0_0 ; ripgrep 12.1.1 0 ; rope 0.18.0 py_0 ; rpy2 3.4.4 pypi_0 pypi; rsa 4.7.2 pypi_0 pypi; rtree 0.9.7 py38h06a4308_1 ; ruamel_yaml 0.15.100 py38h27cfd23_0 ; scanorama 1.7 pypi_0 pypi; scanpy 1.7.2 pypi_0 pypi; scikit-image 0.16.2 py38h0573a6f_0 ; scikit-learn 0.24.2 py38ha9443f7_0 ; scikit-learn-extra 0.1.0b2 py38h8790de6_0 conda-forge; scikit-misc 0.1.4 pypi_0 pypi; scipy 1.6.2 py38had2a1c9_1 ; scprep 1.0.13 pypi_0 pypi; scvelo 0.2.3 pypi_0 pypi; seaborn 0.11.1 pyhd3eb1b0_0 ; secretstorage 3.3.1 py38h06a4308_0 ; send2trash 1.5.0 pyhd3eb1b0_1 ; setuptools 52.0.0 py38h06a4308_0 ; setuptools-scm 6.0.1 pyhd3eb1b0_1 ; setuptools_scm 6.0.1 hd3eb1b0_1 ; simplegeneric 0.8.1 py38_2 ; sinfo 0.3.1 py_0 conda-forge; singledispatch 3.6.1 pyhd3eb1b0_1001 ; sip 4.19.13 py38he6710b0_0 ; six 1.15.0 py38h06a4308_0 ; sklearn 0.0 pypi_0 pypi; snappy 1.1.8 he6710b0_0 ; sniffio 1.2.0 py38h06a4308_1 ; snowballstemmer 2.1.0 pyhd3eb1b0_0 ; sortedcollections 2.1.0 pyhd3eb1b0_0 ; sortedcontainers 2.3.0 pyhd3eb1b0_0 ; soupsieve 2.2.1 pyhd3eb1b0_0 ; sphinx 4.0.1 pyhd3eb1b0_0 ; sphinxcontrib 1.0 py38_1 ; sphinxcontrib-applehelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-devhelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-htmlhelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-jsmath 1.0.1 pyhd3eb1b0_0 ; sphinxcontrib-qthe,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:16192,Usability,simpl,simplegeneric,16192,r36h96ca727_0 r; r-nlme 3.1_139 r36ha65eedd_0 r; r-nnet 7.3_12 r36h96ca727_0 r; r-recommended 3.6.0 r36_0 r; r-rpart 4.1_15 r36h96ca727_0 r; r-spatial 7.3_11 r36h96ca727_4 r; r-survival 2.44_1.1 r36h96ca727_0 r; ray 1.3.0 pypi_0 pypi; readline 8.1 h27cfd23_0 ; redis 3.5.3 pypi_0 pypi; regex 2021.4.4 py38h27cfd23_0 ; requests 2.25.1 pyhd3eb1b0_0 ; ripgrep 12.1.1 0 ; rope 0.18.0 py_0 ; rpy2 3.4.4 pypi_0 pypi; rsa 4.7.2 pypi_0 pypi; rtree 0.9.7 py38h06a4308_1 ; ruamel_yaml 0.15.100 py38h27cfd23_0 ; scanorama 1.7 pypi_0 pypi; scanpy 1.7.2 pypi_0 pypi; scikit-image 0.16.2 py38h0573a6f_0 ; scikit-learn 0.24.2 py38ha9443f7_0 ; scikit-learn-extra 0.1.0b2 py38h8790de6_0 conda-forge; scikit-misc 0.1.4 pypi_0 pypi; scipy 1.6.2 py38had2a1c9_1 ; scprep 1.0.13 pypi_0 pypi; scvelo 0.2.3 pypi_0 pypi; seaborn 0.11.1 pyhd3eb1b0_0 ; secretstorage 3.3.1 py38h06a4308_0 ; send2trash 1.5.0 pyhd3eb1b0_1 ; setuptools 52.0.0 py38h06a4308_0 ; setuptools-scm 6.0.1 pyhd3eb1b0_1 ; setuptools_scm 6.0.1 hd3eb1b0_1 ; simplegeneric 0.8.1 py38_2 ; sinfo 0.3.1 py_0 conda-forge; singledispatch 3.6.1 pyhd3eb1b0_1001 ; sip 4.19.13 py38he6710b0_0 ; six 1.15.0 py38h06a4308_0 ; sklearn 0.0 pypi_0 pypi; snappy 1.1.8 he6710b0_0 ; sniffio 1.2.0 py38h06a4308_1 ; snowballstemmer 2.1.0 pyhd3eb1b0_0 ; sortedcollections 2.1.0 pyhd3eb1b0_0 ; sortedcontainers 2.3.0 pyhd3eb1b0_0 ; soupsieve 2.2.1 pyhd3eb1b0_0 ; sphinx 4.0.1 pyhd3eb1b0_0 ; sphinxcontrib 1.0 py38_1 ; sphinxcontrib-applehelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-devhelp 1.0.2 pyhd3eb1b0_0 ; sphinxcontrib-htmlhelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-jsmath 1.0.1 pyhd3eb1b0_0 ; sphinxcontrib-qthelp 1.0.3 pyhd3eb1b0_0 ; sphinxcontrib-serializinghtml 1.1.4 pyhd3eb1b0_0 ; sphinxcontrib-websupport 1.2.4 py_0 ; spyder 4.2.5 py38h06a4308_0 ; spyder-kernels 1.10.2 py38h06a4308_0 ; sqlalchemy 1.4.15 py38h27cfd23_0 ; sqlite 3.35.4 hdfb4753_0 ; statsmodels 0.12.2 py38h27cfd23_0 ; stdlib-list 0.7.0 py_2 conda-forge; sympy 1.8 py38h06a4308_0 ; tasklogger 1.0.0 pypi_0 pypi;,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:17893,Usability,learn,learn,17893,3eb1b0_0 ; sphinxcontrib-websupport 1.2.4 py_0 ; spyder 4.2.5 py38h06a4308_0 ; spyder-kernels 1.10.2 py38h06a4308_0 ; sqlalchemy 1.4.15 py38h27cfd23_0 ; sqlite 3.35.4 hdfb4753_0 ; statsmodels 0.12.2 py38h27cfd23_0 ; stdlib-list 0.7.0 py_2 conda-forge; sympy 1.8 py38h06a4308_0 ; tasklogger 1.0.0 pypi_0 pypi; tbb 2020.3 hfd86e86_0 ; tblib 1.7.0 py_0 ; terminado 0.9.4 py38h06a4308_0 ; testpath 0.4.4 pyhd3eb1b0_0 ; textdistance 4.2.1 pyhd3eb1b0_0 ; threadpoolctl 2.1.0 pyh5ca1d4c_0 ; three-merge 0.1.1 pyhd3eb1b0_0 ; tk 8.6.10 hbc83047_0 ; tktable 2.10 h14c3975_0 ; tokenize-rt 4.1.0 pyhd8ed1ab_0 conda-forge; toml 0.10.2 pyhd3eb1b0_0 ; toolz 0.11.1 pyhd3eb1b0_0 ; tornado 6.1 py38h27cfd23_0 ; tqdm 4.59.0 pyhd3eb1b0_1 ; traitlets 5.0.5 pyhd3eb1b0_0 ; triku 1.3.1 pypi_0 pypi; tslearn 0.5.0.5 pypi_0 pypi; typed-ast 1.4.2 py38h27cfd23_1 ; typing 3.10.0.0 py38h06a4308_0 ; typing-extensions 3.10.0.0 pypi_0 pypi; typing_extensions 3.7.4.3 pyha847dfd_0 ; tzlocal 2.1 py38_0 ; ujson 4.0.2 py38h2531618_0 ; umap-learn 0.5.1 py38h578d9bd_0 conda-forge; unicodecsv 0.14.1 py38_0 ; unixodbc 2.3.9 h7b6447c_0 ; urllib3 1.26.4 pyhd3eb1b0_0 ; vendorize 0.2.1 pypi_0 pypi; watchdog 1.0.2 py38h06a4308_1 ; wcwidth 0.2.5 py_0 ; webencodings 0.5.1 py38_1 ; werkzeug 1.0.1 pyhd3eb1b0_0 ; wheel 0.36.2 pyhd3eb1b0_0 ; widgetsnbextension 3.5.1 py38_0 ; wrapt 1.12.1 py38h7b6447c_1 ; wurlitzer 2.1.0 py38h06a4308_0 ; xlrd 1.2.0 pypi_0 pypi; xlsxwriter 1.3.8 pyhd3eb1b0_0 ; xlwt 1.3.0 py38_0 ; xz 5.2.5 h7b6447c_0 ; yaml 0.2.5 h7b6447c_0 ; yapf 0.31.0 pyhd3eb1b0_0 ; yarl 1.6.3 pypi_0 pypi; zeromq 4.3.4 h2531618_0 ; zict 2.0.0 pyhd3eb1b0_0 ; zipp 3.4.1 pyhd3eb1b0_0 ; zlib 1.2.11 h7b6447c_3 ; zope 1.0 py38_1 ; zope.event 4.5.0 py38_0 ; zope.interface 5.3.0 py38h27cfd23_0 ; zstd 1.4.5 h9ceee32_0 ; ```. </Details>. <Details>; <Summary>ImportError traceback</Summary>. ```python; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310
https://github.com/scverse/scanpy/issues/1850#issuecomment-848441096:263,Availability,error,errors,263,"> I created a new environment (see below for package details) and there everything works as it should. Can you use this new environment to do your analysis?. I expect that the previous environment managed to get into a messy state, which can lead to very strange errors. Because of this, I generally avoid trying to update old environments much and instead opt for creating fresh ones frequently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-848441096
https://github.com/scverse/scanpy/issues/1850#issuecomment-848441096:316,Deployability,update,update,316,"> I created a new environment (see below for package details) and there everything works as it should. Can you use this new environment to do your analysis?. I expect that the previous environment managed to get into a messy state, which can lead to very strange errors. Because of this, I generally avoid trying to update old environments much and instead opt for creating fresh ones frequently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-848441096
https://github.com/scverse/scanpy/issues/1850#issuecomment-848441096:300,Safety,avoid,avoid,300,"> I created a new environment (see below for package details) and there everything works as it should. Can you use this new environment to do your analysis?. I expect that the previous environment managed to get into a messy state, which can lead to very strange errors. Because of this, I generally avoid trying to update old environments much and instead opt for creating fresh ones frequently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-848441096
https://github.com/scverse/scanpy/issues/1850#issuecomment-848678562:241,Deployability,install,installing,241,"I might have overextended the old environment a bit too much indeed, so I'll just continue with a fresh one. If it's of any help to you, then before the problems started I wanted to try out some of the newer additions to scanpy ecosystem by installing triku, dorothea and progeny. Thanks for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-848678562
https://github.com/scverse/scanpy/issues/1850#issuecomment-862972961:84,Deployability,install,installed,84,"@karlann yeah, conda environments can run into these problems, especially if you've installed some packages with `pip` and some with `conda`. I generally try to create fresh environments very frequently instead of updating old ones. `mamba` (faster conda) definitely makes this less painful. @OnlyBelter have you tried the same solution of trying a fresh environment?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-862972961
https://github.com/scverse/scanpy/issues/1850#issuecomment-863089065:86,Availability,error,error,86,"I think this problem may cause by `seaborn`!. The following code should reproduce the error:. ```; import scanpy as sc; import seaborn as sns; sns.set() # <--- here. pbmc = sc.datasets.pbmc68k_reduced(); sc.pl.umap(pbmc, color = 'phase'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-863089065
https://github.com/scverse/scanpy/issues/1850#issuecomment-863733680:31,Availability,error,error,31,"@OnlyBelter I'm not getting an error from that, but I am getting a bunch of warnings, which makes it seem like something weird is going on. I'm going to split this into a new issue while we investigate, since it seems a bit different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-863733680
https://github.com/scverse/scanpy/pull/1858#issuecomment-864529782:90,Testability,test,test,90,Done! I've changed the random vector generation to use the code you suggested and added a test to test_embedding.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1858#issuecomment-864529782
https://github.com/scverse/scanpy/issues/1859#issuecomment-861373568:134,Deployability,patch,patch,134,"Thanks for the thorough issue report! I recall running into this before with some other igraph algorithm, so there might be a similar patch somewhere else in the codebase. Would you be interested in opening a PR to fix this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1859#issuecomment-861373568
https://github.com/scverse/scanpy/issues/1859#issuecomment-866125544:347,Energy Efficiency,schedul,schedule,347,"I suppose to do this properly one ought to scan the code base for uses of igraph, check which among them require the RNG and then add the seeding to those modules?. Since I'm still very new to scanpy and rather swamped at the moment, I won't have the time to do this anytime soon. Maybe add a help wanted tag? If nobody takes it up by the time my schedule lightens I'll see what I can do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1859#issuecomment-866125544
https://github.com/scverse/scanpy/issues/1859#issuecomment-866576277:301,Testability,test,test,301,"> I suppose to do this properly one ought to scan the code base for uses of igraph, check which among them require the RNG and then add the seeding to those modules?. This would be nice, but would also be a lot of work. I was thinking a PR would just make the layouts for PAGA reproducible, and add a test making sure this is the case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1859#issuecomment-866576277
https://github.com/scverse/scanpy/issues/1860#issuecomment-873990253:529,Availability,avail,available,529,"> @WeilerP, do you think this would be more appropriate in `scvelo`? (Side note, I have thought that tutorial; > of going from BAMs through `scvelo` would be quite useful). Hm, not sure if the functionality would match the expectation. In `scvelo`, we'd store only unspliced and spliced counts (spliced both in `adata.X` and `adata.layers`). Based on the proposed code snippet in [alexdobin/STAR#774 (comment)](https://github.com/alexdobin/STAR/issues/774#issuecomment-850477636), the expected output would be to read all of the available information?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1860#issuecomment-873990253
https://github.com/scverse/scanpy/issues/1860#issuecomment-873990253:332,Modifiability,layers,layers,332,"> @WeilerP, do you think this would be more appropriate in `scvelo`? (Side note, I have thought that tutorial; > of going from BAMs through `scvelo` would be quite useful). Hm, not sure if the functionality would match the expectation. In `scvelo`, we'd store only unspliced and spliced counts (spliced both in `adata.X` and `adata.layers`). Based on the proposed code snippet in [alexdobin/STAR#774 (comment)](https://github.com/alexdobin/STAR/issues/774#issuecomment-850477636), the expected output would be to read all of the available information?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1860#issuecomment-873990253
https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735:168,Modifiability,variab,variable,168,"@ivirshup, @JBreunig using the absolute counts isn't a problem per se. It's simply that the scvelo paper used the spliced counts in `adata.X` based on which the highly variable genes are selected and PCA, neighbor graph and UMAP embedding are calculated.; @JBreunig, shouldn't be a problem to put spliced into `adata.X` as the dimensions of spliced and total counts are the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735
https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735:76,Usability,simpl,simply,76,"@ivirshup, @JBreunig using the absolute counts isn't a problem per se. It's simply that the scvelo paper used the spliced counts in `adata.X` based on which the highly variable genes are selected and PCA, neighbor graph and UMAP embedding are calculated.; @JBreunig, shouldn't be a problem to put spliced into `adata.X` as the dimensions of spliced and total counts are the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735
https://github.com/scverse/scanpy/issues/1861#issuecomment-867334904:184,Modifiability,variab,variable,184,"Definitely been an abstract todo for a while. Tracking for 1.9. Some questions:. * What about methods where more than one element is added to the AnnData? E.g. for PCA we also add the variable loadings to `varm`; * How do these parameters get tracked in the `uns` metadata? Currently the key added there is largely fixed, but maybe it should vary too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1861#issuecomment-867334904
https://github.com/scverse/scanpy/issues/1861#issuecomment-867334904:193,Performance,load,loadings,193,"Definitely been an abstract todo for a while. Tracking for 1.9. Some questions:. * What about methods where more than one element is added to the AnnData? E.g. for PCA we also add the variable loadings to `varm`; * How do these parameters get tracked in the `uns` metadata? Currently the key added there is largely fixed, but maybe it should vary too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1861#issuecomment-867334904
https://github.com/scverse/scanpy/issues/1861#issuecomment-1081974157:78,Modifiability,variab,variable,78,"@adamgayoso, have you had any thoughts here about how we manage the `key` for variable loadings?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1861#issuecomment-1081974157
https://github.com/scverse/scanpy/issues/1861#issuecomment-1081974157:87,Performance,load,loadings,87,"@adamgayoso, have you had any thoughts here about how we manage the `key` for variable loadings?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1861#issuecomment-1081974157
https://github.com/scverse/scanpy/issues/1862#issuecomment-861376891:8,Modifiability,variab,variable,8,"Are the variable names in your anndata object unique? If not, there could be an issue with ambiguous gene names being used (e.g. two genes are given the same name, so we can't tell which column to use). If this is the issue, making the variable names unique with `adata.var_names_make_unique()` should solve the issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1862#issuecomment-861376891
https://github.com/scverse/scanpy/issues/1862#issuecomment-861376891:236,Modifiability,variab,variable,236,"Are the variable names in your anndata object unique? If not, there could be an issue with ambiguous gene names being used (e.g. two genes are given the same name, so we can't tell which column to use). If this is the issue, making the variable names unique with `adata.var_names_make_unique()` should solve the issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1862#issuecomment-861376891
https://github.com/scverse/scanpy/issues/1865#issuecomment-861467621:38,Integrability,depend,dependent,38,"I think the level of threading can be dependent on BLAS/ LAPACK etc. However, it should generally be multithreaded. Does the issue persist in a conda environment?. If you still run into issues I'd be interested in seeing some details (e.g. what size and kind of matrix, cpu usage during computation). A counter example of a faster way to compute it could be useful to see too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1865#issuecomment-861467621
https://github.com/scverse/scanpy/issues/1866#issuecomment-862113537:9,Integrability,depend,depend,9,"It would depend on what data was in the `Batch` column. HDF5 store are typed, so we can't store columns with mixed kinds of values. If the column's dtype is `object`, we check to see if it's string values, otherwise we say we don't know how to write it, since it could be any mix of things. What kinds of values did you have in `Batch`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866#issuecomment-862113537
https://github.com/scverse/scanpy/issues/1867#issuecomment-1814616200:165,Testability,test,tests,165,"Nice catch - agree on all points regarding inconsistency, the causing sections & the solution with @jlause. Made the PR implementing the ""half-pseudocode"" and added tests which catch your described unexpected behavior for all `flavor`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1867#issuecomment-1814616200
https://github.com/scverse/scanpy/pull/1868#issuecomment-861393180:481,Deployability,rolling,rolling,481,"It seems this is of some relevance to users, as shown by it showing up twice independently over the course of the past week. For some reason, my tweaks have killed off ReadTheDocs, and I can't check why as upon pressing the ""details"" button I get the 404 equivalent :) I am now importing `types` so I can correctly define `metric` as also including `types.FunctionType`. This is probably what is causing whatever hiccup is happening. @giovp , some assistance with getting the ball rolling on this? The changes are a result of me expanding BBKNN with pynndescent on your recommendation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1868#issuecomment-861393180
https://github.com/scverse/scanpy/pull/1868#issuecomment-861412696:143,Availability,error,errors,143,@ktpolanski following from https://scanpy.readthedocs.io/en/stable/dev/code.html#code-style and building the docs locally gives me (among some errors that were not relevant):. ```; ...; /path/to/scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp._bbknn.bbknn:28: WARNING: py:class reference target not found: function; /path/to/scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp._bbknn.bbknn:28: WARNING: py:class reference target not found: function; ...; ```. Maybe that gives you some pointers?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1868#issuecomment-861412696
https://github.com/scverse/scanpy/pull/1868#issuecomment-861433547:12,Deployability,install,install,12,"I ran `pip3 install scanpy[doc]`, as instructed by that page, and the doc seems to have built fine locally. ![image](https://user-images.githubusercontent.com/14993986/122047761-cbc53700-cde0-11eb-8fb2-5a180d306554.png). The only fishy thing is the `function` is not clickable, while the other two are, but it built. Also, line 28 is `trim: Optional[int] = None,`, which was there previously already. It's the metric change that's causing this - your argument renaming PR did not hiccup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1868#issuecomment-861433547
https://github.com/scverse/scanpy/pull/1868#issuecomment-863801606:15,Deployability,update,update,15,Thanks for the update!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1868#issuecomment-863801606
https://github.com/scverse/scanpy/issues/1870#issuecomment-1058394641:117,Availability,error,error,117,"While the single command works `adata = adata[adata[: , 'A'].X > 1, :]`. The compound command gives me the following error: ; TypeError: unsupported operand type(s) for &: 'SparseCSRView' and 'SparseCSRView'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1870#issuecomment-1058394641
https://github.com/scverse/scanpy/issues/1870#issuecomment-1192133374:125,Availability,error,error,125,"> While the single command works `adata = adata[adata[: , 'A'].X > 1, :]`; > ; > The compound command gives me the following error: TypeError: unsupported operand type(s) for &: 'SparseCSRView' and 'SparseCSRView'. Have you solved it? I have a similar problem. . My code:; adata = adata[(adata[: , 'A'].X > 0) & (adata[:, 'B'].X > 0), :]; # TypeError: unsupported operand type(s) for &: 'SparseCSRView' and 'SparseCSRView'. However, ; adata = adata[(adata[: , 'A'].X > 0), :]; It works greatly.; Hope to get help, thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1870#issuecomment-1192133374
https://github.com/scverse/scanpy/issues/1872#issuecomment-862230678:22,Testability,log,logging,22,Any idea why `scanpy. logging.print_versions()` is reporting a different version that you've reported above it? . Could be that there's an issue with this environment. Can you replicate the issue in a fresh conda environment?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1872#issuecomment-862230678
https://github.com/scverse/scanpy/pull/1873#issuecomment-872823146:140,Deployability,update,update,140,"Hi, . I also met this problem. I am using Scanpy 1.7.2, and could you please suggest which version of BBKNN I should use if I don't want to update Scanpy to 1.8.0. Thanks,; Min",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1873#issuecomment-872823146
https://github.com/scverse/scanpy/pull/1873#issuecomment-872833609:75,Integrability,wrap,wrapper,75,Two options:; - bbknn < 1.5.0; - use `bbknn.bbknn()` instead of the scanpy wrapper,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1873#issuecomment-872833609
https://github.com/scverse/scanpy/pull/1873#issuecomment-872835230:85,Integrability,wrap,wrapper,85,"> Two options:; > ; > * bbknn < 1.5.0; > * use `bbknn.bbknn()` instead of the scanpy wrapper. Thank you for your rapid reply, bbknn 1.4 works for me (scanpy 1.7.2). The second option is good.; Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1873#issuecomment-872835230
https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801:128,Integrability,depend,depends,128,You can store different forms of the matrix in `layers` and often choose which one to use with the `layers` argument. It really depends on the function whether it expects normalized or count data. Most functions should mention it in the documentation if the expect count data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801
https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801:48,Modifiability,layers,layers,48,You can store different forms of the matrix in `layers` and often choose which one to use with the `layers` argument. It really depends on the function whether it expects normalized or count data. Most functions should mention it in the documentation if the expect count data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801
https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801:100,Modifiability,layers,layers,100,You can store different forms of the matrix in `layers` and often choose which one to use with the `layers` argument. It really depends on the function whether it expects normalized or count data. Most functions should mention it in the documentation if the expect count data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801
https://github.com/scverse/scanpy/issues/1875#issuecomment-868781210:135,Modifiability,layers,layers,135,"how to add the raw counts to my h5ad object?; Any idea how to convert the three data slot from Seurat [raw, data, scaled.data] to h5ad layers?. ```; SeuratObject@assays$RNA@counts; SeuratObject@assays$RNA@data; SeuratObject@assays$RNA@scale.data; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1875#issuecomment-868781210
https://github.com/scverse/scanpy/issues/1876#issuecomment-863787914:213,Modifiability,variab,variables,213,"That's an interesting idea and I see how it would be useful. I don't think it's going to be easy to implement, since I believe our code is heavily based around having groups of observations on one axis, groups of variables on the other. . Definitely something to keep in mind for a refactor though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-863787914
https://github.com/scverse/scanpy/issues/1876#issuecomment-863787914:282,Modifiability,refactor,refactor,282,"That's an interesting idea and I see how it would be useful. I don't think it's going to be easy to implement, since I believe our code is heavily based around having groups of observations on one axis, groups of variables on the other. . Definitely something to keep in mind for a refactor though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-863787914
https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315:1644,Availability,error,error,1644," was a single variable which would be used to fill cell in the plot. As an example:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2). sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); ```. ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG). Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments. I think you'd be able to get an output close to what you would currently like with:. ```python; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```. I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:. ```python; # Imaginary implementation:; sc.pl.heatmap(; pbmc,; var_names=""LDHB"",; row_groups=""louvain"",; col_groups=""sampleid""; ); ```. ![image](https://user-images.githubusercontent.com/8238804/144901891-45c3a8aa-1b56-4521-abc1-66f968a59d23.png). ```python; sc.pl.heatmap(; pbmc,; var_names=[""LDHB"", ""LYZ"", ""CD79A""],; row_groups=""louvain"",; col_groups=""sampleid""; ); ```. ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png). What do you think about that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315
https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315:260,Modifiability,variab,variable,260,"@zhangguy, adding on to some thoughts from your PR https://github.com/theislab/scanpy/pull/2055#issuecomment-987012001. From my reading of that PR, you added a boolean argument `groupby_expand` which, when `True`, assumed `group_by` had two values: a grouping variable for the rows of the plot and a grouping variable for the columns of the plot. It also assumed `var_names` was a single variable which would be used to fill cell in the plot. As an example:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2). sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); ```. ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG). Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments. I think you'd be able to get an output close to what you would currently like with:. ```python; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```. I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:. ```python; # Imaginary implementation:; sc.pl.heatmap(; pbmc,; var_names=""LDHB"",; row_groups=""louvain"",; col_groups=""sampleid""; ); ```. ![ima",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315
https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315:309,Modifiability,variab,variable,309,"@zhangguy, adding on to some thoughts from your PR https://github.com/theislab/scanpy/pull/2055#issuecomment-987012001. From my reading of that PR, you added a boolean argument `groupby_expand` which, when `True`, assumed `group_by` had two values: a grouping variable for the rows of the plot and a grouping variable for the columns of the plot. It also assumed `var_names` was a single variable which would be used to fill cell in the plot. As an example:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2). sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); ```. ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG). Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments. I think you'd be able to get an output close to what you would currently like with:. ```python; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```. I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:. ```python; # Imaginary implementation:; sc.pl.heatmap(; pbmc,; var_names=""LDHB"",; row_groups=""louvain"",; col_groups=""sampleid""; ); ```. ![ima",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315
https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315:388,Modifiability,variab,variable,388,"@zhangguy, adding on to some thoughts from your PR https://github.com/theislab/scanpy/pull/2055#issuecomment-987012001. From my reading of that PR, you added a boolean argument `groupby_expand` which, when `True`, assumed `group_by` had two values: a grouping variable for the rows of the plot and a grouping variable for the columns of the plot. It also assumed `var_names` was a single variable which would be used to fill cell in the plot. As an example:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2). sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); ```. ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG). Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments. I think you'd be able to get an output close to what you would currently like with:. ```python; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```. I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:. ```python; # Imaginary implementation:; sc.pl.heatmap(; pbmc,; var_names=""LDHB"",; row_groups=""louvain"",; col_groups=""sampleid""; ); ```. ![ima",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315
https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315:1505,Modifiability,variab,variables,1505," was a single variable which would be used to fill cell in the plot. As an example:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2). sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); ```. ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG). Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments. I think you'd be able to get an output close to what you would currently like with:. ```python; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```. I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:. ```python; # Imaginary implementation:; sc.pl.heatmap(; pbmc,; var_names=""LDHB"",; row_groups=""louvain"",; col_groups=""sampleid""; ); ```. ![image](https://user-images.githubusercontent.com/8238804/144901891-45c3a8aa-1b56-4521-abc1-66f968a59d23.png). ```python; sc.pl.heatmap(; pbmc,; var_names=[""LDHB"", ""LYZ"", ""CD79A""],; row_groups=""louvain"",; col_groups=""sampleid""; ); ```. ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png). What do you think about that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315
https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:1756,Availability,error,error,1756," ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG); > ; > Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments.; > ; > I think you'd be able to get an output close to what you would currently like with:; > ; > ```python; > import scanpy as sc, pandas as pd, numpy as np; > ; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]); > ; > summarized = df.pivot_table(; > index=[""louvain"", ""sampleid""],; > values=""LDHB"",; > aggfunc=[np.mean, np.count_nonzero]; > ); > color_df = summarized[""mean""].unstack(); > size_df = summarized[""count_nonzero""].unstack(); > ; > # I don't think the var_names or groupby variables are actually important here; > sc.pl.DotPlot(; > pbmc,; > var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; > dot_color_df=color_df, dot_size_df=size_df,; > ).style(cmap=""Reds"").show(); > ```; > ; > I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:; > ; > ```python; > # Imaginary implementation:; > sc.pl.heatmap(; > pbmc,; > var_names=""LDHB"",; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144901891-45c3a8aa-1b56-4521-abc1-66f968a59d23.png); > ; > ```python; > sc.pl.heatmap(; > pbmc,; > var_names=[""LDHB"", ""LYZ"", ""CD79A""],; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png); > ; > What do you think about that?. Thanks @ivirshup !. I like these lines you suggested- perhaps I can adopt to make it more elegant when creating color_df/size_df:; ```; import scanpy as sc, pandas as pd, numpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664
https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:3309,Availability,error,error,3309," ```python; > sc.pl.heatmap(; > pbmc,; > var_names=[""LDHB"", ""LYZ"", ""CD79A""],; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png); > ; > What do you think about that?. Thanks @ivirshup !. I like these lines you suggested- perhaps I can adopt to make it more elegant when creating color_df/size_df:; ```; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```; this is the output:; ![image](https://user-images.githubusercontent.com/10910559/145053489-c550d5a7-a8fe-4a61-b672-9103ccf1d228.png); some work are needed to modify the grid/axis size, legend and scale. Actually this is the reason I work on top of the _dotplot and _baseplot function/ classes to implement the solution- to make the plots the same style with scanpy dotplot without doing too much work on the cosmetics. But I can certainly change grouby_expand from bool to an actual variable `group_cols` as you suggested in #2055 . Or should we call it `col_groups` as you did in your sc.pl.heatmap pseudo code? ; I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664
https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:287,Modifiability,variab,variable,287,"> @zhangguy, adding on to some thoughts from your PR [#2055 (comment)](https://github.com/theislab/scanpy/pull/2055#issuecomment-987012001); > ; > From my reading of that PR, you added a boolean argument `groupby_expand` which, when `True`, assumed `group_by` had two values: a grouping variable for the rows of the plot and a grouping variable for the columns of the plot. It also assumed `var_names` was a single variable which would be used to fill cell in the plot. As an example:; > ; > ```python; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > ; > sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); > ```; > ; > ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG); > ; > Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments.; > ; > I think you'd be able to get an output close to what you would currently like with:; > ; > ```python; > import scanpy as sc, pandas as pd, numpy as np; > ; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]); > ; > summarized = df.pivot_table(; > index=[""louvain"", ""sampleid""],; > values=""LDHB"",; > aggfunc=[np.mean, np.count_nonzero]; > ); > color_df = summarized[""mean""].unstack(); > size_df = summarized[""count_nonzero""].unstack(); > ; > # I don't think the var_names or groupby variables are actually important here; > sc.pl.DotPlot(; > pbmc,; > var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; > dot_color_df=color_df, dot_size_df=size_df,; > ).style(cmap=""Reds"").show(); > ```; > ; > I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:; > ; > ```python; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664
https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:336,Modifiability,variab,variable,336,"> @zhangguy, adding on to some thoughts from your PR [#2055 (comment)](https://github.com/theislab/scanpy/pull/2055#issuecomment-987012001); > ; > From my reading of that PR, you added a boolean argument `groupby_expand` which, when `True`, assumed `group_by` had two values: a grouping variable for the rows of the plot and a grouping variable for the columns of the plot. It also assumed `var_names` was a single variable which would be used to fill cell in the plot. As an example:; > ; > ```python; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > ; > sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); > ```; > ; > ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG); > ; > Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments.; > ; > I think you'd be able to get an output close to what you would currently like with:; > ; > ```python; > import scanpy as sc, pandas as pd, numpy as np; > ; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]); > ; > summarized = df.pivot_table(; > index=[""louvain"", ""sampleid""],; > values=""LDHB"",; > aggfunc=[np.mean, np.count_nonzero]; > ); > color_df = summarized[""mean""].unstack(); > size_df = summarized[""count_nonzero""].unstack(); > ; > # I don't think the var_names or groupby variables are actually important here; > sc.pl.DotPlot(; > pbmc,; > var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; > dot_color_df=color_df, dot_size_df=size_df,; > ).style(cmap=""Reds"").show(); > ```; > ; > I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:; > ; > ```python; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664
https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:415,Modifiability,variab,variable,415,"> @zhangguy, adding on to some thoughts from your PR [#2055 (comment)](https://github.com/theislab/scanpy/pull/2055#issuecomment-987012001); > ; > From my reading of that PR, you added a boolean argument `groupby_expand` which, when `True`, assumed `group_by` had two values: a grouping variable for the rows of the plot and a grouping variable for the columns of the plot. It also assumed `var_names` was a single variable which would be used to fill cell in the plot. As an example:; > ; > ```python; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > ; > sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); > ```; > ; > ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG); > ; > Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments.; > ; > I think you'd be able to get an output close to what you would currently like with:; > ; > ```python; > import scanpy as sc, pandas as pd, numpy as np; > ; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]); > ; > summarized = df.pivot_table(; > index=[""louvain"", ""sampleid""],; > values=""LDHB"",; > aggfunc=[np.mean, np.count_nonzero]; > ); > color_df = summarized[""mean""].unstack(); > size_df = summarized[""count_nonzero""].unstack(); > ; > # I don't think the var_names or groupby variables are actually important here; > sc.pl.DotPlot(; > pbmc,; > var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; > dot_color_df=color_df, dot_size_df=size_df,; > ).style(cmap=""Reds"").show(); > ```; > ; > I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:; > ; > ```python; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664
https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:1611,Modifiability,variab,variables,1611,"""], pbmc.n_obs / 2); > ; > sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); > ```; > ; > ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG); > ; > Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments.; > ; > I think you'd be able to get an output close to what you would currently like with:; > ; > ```python; > import scanpy as sc, pandas as pd, numpy as np; > ; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]); > ; > summarized = df.pivot_table(; > index=[""louvain"", ""sampleid""],; > values=""LDHB"",; > aggfunc=[np.mean, np.count_nonzero]; > ); > color_df = summarized[""mean""].unstack(); > size_df = summarized[""count_nonzero""].unstack(); > ; > # I don't think the var_names or groupby variables are actually important here; > sc.pl.DotPlot(; > pbmc,; > var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; > dot_color_df=color_df, dot_size_df=size_df,; > ).style(cmap=""Reds"").show(); > ```; > ; > I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:; > ; > ```python; > # Imaginary implementation:; > sc.pl.heatmap(; > pbmc,; > var_names=""LDHB"",; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144901891-45c3a8aa-1b56-4521-abc1-66f968a59d23.png); > ; > ```python; > sc.pl.heatmap(; > pbmc,; > var_names=[""LDHB"", ""LYZ"", ""CD79A""],; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png); > ; > What do you think about that?. Thanks @ivirshup !. I like these lines you s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664
https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:3170,Modifiability,variab,variables,3170,"images.githubusercontent.com/8238804/144901891-45c3a8aa-1b56-4521-abc1-66f968a59d23.png); > ; > ```python; > sc.pl.heatmap(; > pbmc,; > var_names=[""LDHB"", ""LYZ"", ""CD79A""],; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png); > ; > What do you think about that?. Thanks @ivirshup !. I like these lines you suggested- perhaps I can adopt to make it more elegant when creating color_df/size_df:; ```; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```; this is the output:; ![image](https://user-images.githubusercontent.com/10910559/145053489-c550d5a7-a8fe-4a61-b672-9103ccf1d228.png); some work are needed to modify the grid/axis size, legend and scale. Actually this is the reason I work on top of the _dotplot and _baseplot function/ classes to implement the solution- to make the plots the same style with scanpy dotplot without doing too much work on the cosmetics. But I can certainly change grouby_expand from bool to an actual variable `group_cols` as you suggested in #2055 . Or should we call it `col_groups` as you did in your sc.pl.heatmap pseudo code? ; I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664
https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:3878,Modifiability,variab,variable,3878," ```python; > sc.pl.heatmap(; > pbmc,; > var_names=[""LDHB"", ""LYZ"", ""CD79A""],; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png); > ; > What do you think about that?. Thanks @ivirshup !. I like these lines you suggested- perhaps I can adopt to make it more elegant when creating color_df/size_df:; ```; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```; this is the output:; ![image](https://user-images.githubusercontent.com/10910559/145053489-c550d5a7-a8fe-4a61-b672-9103ccf1d228.png); some work are needed to modify the grid/axis size, legend and scale. Actually this is the reason I work on top of the _dotplot and _baseplot function/ classes to implement the solution- to make the plots the same style with scanpy dotplot without doing too much work on the cosmetics. But I can certainly change grouby_expand from bool to an actual variable `group_cols` as you suggested in #2055 . Or should we call it `col_groups` as you did in your sc.pl.heatmap pseudo code? ; I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664
https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524:109,Integrability,depend,depends,109,"> Or should we call it col_groups as you did in your sc.pl.heatmap pseudo code?. That could be up to you. It depends on what the user is trying to achieve, which makes more sense. For instance, I'm not sure if it makes sense to allow splitting the columns by both variables and groups, or if that's the wrong abstraction. > I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). This code could definitely be a lot more simple. Would definitely appreciate help here! I think some of the concepts used in `seaborn` could be quite useful here, though it looks like they're under heavy refactoring at the moment ([relevant seaborn branch](https://github.com/mwaskom/seaborn/tree/skunkworks/features)). Maybe a good first step would be to fix how so the dotplot would look right if the user provides the dot size and dot color dataframes? Would make these plots possible, and gives an interface to try later approaches with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524
https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524:1075,Integrability,interface,interface,1075,"> Or should we call it col_groups as you did in your sc.pl.heatmap pseudo code?. That could be up to you. It depends on what the user is trying to achieve, which makes more sense. For instance, I'm not sure if it makes sense to allow splitting the columns by both variables and groups, or if that's the wrong abstraction. > I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). This code could definitely be a lot more simple. Would definitely appreciate help here! I think some of the concepts used in `seaborn` could be quite useful here, though it looks like they're under heavy refactoring at the moment ([relevant seaborn branch](https://github.com/mwaskom/seaborn/tree/skunkworks/features)). Maybe a good first step would be to fix how so the dotplot would look right if the user provides the dot size and dot color dataframes? Would make these plots possible, and gives an interface to try later approaches with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524
https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524:264,Modifiability,variab,variables,264,"> Or should we call it col_groups as you did in your sc.pl.heatmap pseudo code?. That could be up to you. It depends on what the user is trying to achieve, which makes more sense. For instance, I'm not sure if it makes sense to allow splitting the columns by both variables and groups, or if that's the wrong abstraction. > I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). This code could definitely be a lot more simple. Would definitely appreciate help here! I think some of the concepts used in `seaborn` could be quite useful here, though it looks like they're under heavy refactoring at the moment ([relevant seaborn branch](https://github.com/mwaskom/seaborn/tree/skunkworks/features)). Maybe a good first step would be to fix how so the dotplot would look right if the user provides the dot size and dot color dataframes? Would make these plots possible, and gives an interface to try later approaches with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524
https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524:777,Modifiability,refactor,refactoring,777,"> Or should we call it col_groups as you did in your sc.pl.heatmap pseudo code?. That could be up to you. It depends on what the user is trying to achieve, which makes more sense. For instance, I'm not sure if it makes sense to allow splitting the columns by both variables and groups, or if that's the wrong abstraction. > I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). This code could definitely be a lot more simple. Would definitely appreciate help here! I think some of the concepts used in `seaborn` could be quite useful here, though it looks like they're under heavy refactoring at the moment ([relevant seaborn branch](https://github.com/mwaskom/seaborn/tree/skunkworks/features)). Maybe a good first step would be to fix how so the dotplot would look right if the user provides the dot size and dot color dataframes? Would make these plots possible, and gives an interface to try later approaches with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524
https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524:614,Usability,simpl,simple,614,"> Or should we call it col_groups as you did in your sc.pl.heatmap pseudo code?. That could be up to you. It depends on what the user is trying to achieve, which makes more sense. For instance, I'm not sure if it makes sense to allow splitting the columns by both variables and groups, or if that's the wrong abstraction. > I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). This code could definitely be a lot more simple. Would definitely appreciate help here! I think some of the concepts used in `seaborn` could be quite useful here, though it looks like they're under heavy refactoring at the moment ([relevant seaborn branch](https://github.com/mwaskom/seaborn/tree/skunkworks/features)). Maybe a good first step would be to fix how so the dotplot would look right if the user provides the dot size and dot color dataframes? Would make these plots possible, and gives an interface to try later approaches with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524
https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049:27,Deployability,update,updates,27,"Hi @ivirshup ; I made some updates to PR #2055 . The column grouping argument was changed to a string/list argument 'col_groups'.; A few examples:; ```; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); pbmc.obs[""condition""] = np.tile([""c1"", ""c2""], int(pbmc.n_obs / 2)). ## plot one gene, one column grouping variable; sc.pl.dotplot(pbmc, var_names='C1QA', groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171329-f5fafb2b-0695-41d9-b313-eac9ea218836.png); ```; ## plot two genes, one column grouping variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171410-45f77f03-3487-4b7f-86da-658284608b05.png); ```; ## plot two genes, tow column group variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171470-58df0907-a15b-4b7f-afa3-3578728177e0.png); ```; ## or we could use the same varaibles as y axis; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby=['sampleid', 'condition'], col_groups='louvain'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171544-849a93f4-99cd-493e-9f2b-f5662f03e797.png). For the heatmap, I think you were referring to `sc.pl.matrixplot`. `sc.pl.heatmap` is a different function which plot a cell as a row and a gene as a column. `col_groups` was also added to `sc.pl.matrixplot`:; ```; ## plot two genes, tow column group variable; sc.pl.matrixplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171604-183f7210-276c-4fdb-b173-477e00e636c0.png); For the `row_groups` you proposed in your hypothetical `sc.pl.heatmap` implementation, it is equivalent to the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049
https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049:2180,Deployability,update,update,2180,"obs[""condition""] = np.tile([""c1"", ""c2""], int(pbmc.n_obs / 2)). ## plot one gene, one column grouping variable; sc.pl.dotplot(pbmc, var_names='C1QA', groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171329-f5fafb2b-0695-41d9-b313-eac9ea218836.png); ```; ## plot two genes, one column grouping variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171410-45f77f03-3487-4b7f-86da-658284608b05.png); ```; ## plot two genes, tow column group variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171470-58df0907-a15b-4b7f-afa3-3578728177e0.png); ```; ## or we could use the same varaibles as y axis; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby=['sampleid', 'condition'], col_groups='louvain'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171544-849a93f4-99cd-493e-9f2b-f5662f03e797.png). For the heatmap, I think you were referring to `sc.pl.matrixplot`. `sc.pl.heatmap` is a different function which plot a cell as a row and a gene as a column. `col_groups` was also added to `sc.pl.matrixplot`:; ```; ## plot two genes, tow column group variable; sc.pl.matrixplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171604-183f7210-276c-4fdb-b173-477e00e636c0.png); For the `row_groups` you proposed in your hypothetical `sc.pl.heatmap` implementation, it is equivalent to the current `groupby` argument in `sc.pl.dotplot`/`sc.pl.matrixplot`. I think it might be good to keep it as is for now- for this kind of changes it might be good to do a coordinated update on all plotting functions because I see quite a few functions use the `groupby` argument.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049
https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049:377,Modifiability,variab,variable,377,"Hi @ivirshup ; I made some updates to PR #2055 . The column grouping argument was changed to a string/list argument 'col_groups'.; A few examples:; ```; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); pbmc.obs[""condition""] = np.tile([""c1"", ""c2""], int(pbmc.n_obs / 2)). ## plot one gene, one column grouping variable; sc.pl.dotplot(pbmc, var_names='C1QA', groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171329-f5fafb2b-0695-41d9-b313-eac9ea218836.png); ```; ## plot two genes, one column grouping variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171410-45f77f03-3487-4b7f-86da-658284608b05.png); ```; ## plot two genes, tow column group variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171470-58df0907-a15b-4b7f-afa3-3578728177e0.png); ```; ## or we could use the same varaibles as y axis; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby=['sampleid', 'condition'], col_groups='louvain'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171544-849a93f4-99cd-493e-9f2b-f5662f03e797.png). For the heatmap, I think you were referring to `sc.pl.matrixplot`. `sc.pl.heatmap` is a different function which plot a cell as a row and a gene as a column. `col_groups` was also added to `sc.pl.matrixplot`:; ```; ## plot two genes, tow column group variable; sc.pl.matrixplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171604-183f7210-276c-4fdb-b173-477e00e636c0.png); For the `row_groups` you proposed in your hypothetical `sc.pl.heatmap` implementation, it is equivalent to the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049
https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049:630,Modifiability,variab,variable,630,"Hi @ivirshup ; I made some updates to PR #2055 . The column grouping argument was changed to a string/list argument 'col_groups'.; A few examples:; ```; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); pbmc.obs[""condition""] = np.tile([""c1"", ""c2""], int(pbmc.n_obs / 2)). ## plot one gene, one column grouping variable; sc.pl.dotplot(pbmc, var_names='C1QA', groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171329-f5fafb2b-0695-41d9-b313-eac9ea218836.png); ```; ## plot two genes, one column grouping variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171410-45f77f03-3487-4b7f-86da-658284608b05.png); ```; ## plot two genes, tow column group variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171470-58df0907-a15b-4b7f-afa3-3578728177e0.png); ```; ## or we could use the same varaibles as y axis; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby=['sampleid', 'condition'], col_groups='louvain'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171544-849a93f4-99cd-493e-9f2b-f5662f03e797.png). For the heatmap, I think you were referring to `sc.pl.matrixplot`. `sc.pl.heatmap` is a different function which plot a cell as a row and a gene as a column. `col_groups` was also added to `sc.pl.matrixplot`:; ```; ## plot two genes, tow column group variable; sc.pl.matrixplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171604-183f7210-276c-4fdb-b173-477e00e636c0.png); For the `row_groups` you proposed in your hypothetical `sc.pl.heatmap` implementation, it is equivalent to the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049
https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049:890,Modifiability,variab,variable,890,"Hi @ivirshup ; I made some updates to PR #2055 . The column grouping argument was changed to a string/list argument 'col_groups'.; A few examples:; ```; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); pbmc.obs[""condition""] = np.tile([""c1"", ""c2""], int(pbmc.n_obs / 2)). ## plot one gene, one column grouping variable; sc.pl.dotplot(pbmc, var_names='C1QA', groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171329-f5fafb2b-0695-41d9-b313-eac9ea218836.png); ```; ## plot two genes, one column grouping variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171410-45f77f03-3487-4b7f-86da-658284608b05.png); ```; ## plot two genes, tow column group variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171470-58df0907-a15b-4b7f-afa3-3578728177e0.png); ```; ## or we could use the same varaibles as y axis; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby=['sampleid', 'condition'], col_groups='louvain'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171544-849a93f4-99cd-493e-9f2b-f5662f03e797.png). For the heatmap, I think you were referring to `sc.pl.matrixplot`. `sc.pl.heatmap` is a different function which plot a cell as a row and a gene as a column. `col_groups` was also added to `sc.pl.matrixplot`:; ```; ## plot two genes, tow column group variable; sc.pl.matrixplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171604-183f7210-276c-4fdb-b173-477e00e636c0.png); For the `row_groups` you proposed in your hypothetical `sc.pl.heatmap` implementation, it is equivalent to the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049
https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049:1653,Modifiability,variab,variable,1653,"obs[""condition""] = np.tile([""c1"", ""c2""], int(pbmc.n_obs / 2)). ## plot one gene, one column grouping variable; sc.pl.dotplot(pbmc, var_names='C1QA', groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171329-f5fafb2b-0695-41d9-b313-eac9ea218836.png); ```; ## plot two genes, one column grouping variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171410-45f77f03-3487-4b7f-86da-658284608b05.png); ```; ## plot two genes, tow column group variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171470-58df0907-a15b-4b7f-afa3-3578728177e0.png); ```; ## or we could use the same varaibles as y axis; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby=['sampleid', 'condition'], col_groups='louvain'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171544-849a93f4-99cd-493e-9f2b-f5662f03e797.png). For the heatmap, I think you were referring to `sc.pl.matrixplot`. `sc.pl.heatmap` is a different function which plot a cell as a row and a gene as a column. `col_groups` was also added to `sc.pl.matrixplot`:; ```; ## plot two genes, tow column group variable; sc.pl.matrixplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171604-183f7210-276c-4fdb-b173-477e00e636c0.png); For the `row_groups` you proposed in your hypothetical `sc.pl.heatmap` implementation, it is equivalent to the current `groupby` argument in `sc.pl.dotplot`/`sc.pl.matrixplot`. I think it might be good to keep it as is for now- for this kind of changes it might be good to do a coordinated update on all plotting functions because I see quite a few functions use the `groupby` argument.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049
https://github.com/scverse/scanpy/issues/1881#issuecomment-863776992:525,Integrability,wrap,wrapped,525,"The additional cases that would need to be handled (and what I believe the current internal solutions are):. * What if colors haven't been saved to uns yet; * If it's an view, we just return what the colors would be by default; * If it's an ""actual"" we return the colors, but also assign them to the object; * What if there's a different number of colors and categories; * We warn and reassign the colors. If it's a view, we don't modify the object, but probably still warn. Most of the logic for handling this internally is wrapped in `_get_palette`: https://github.com/theislab/scanpy/blob/ed364a887db2eb604d0a09cd72325cb5e1f4e27e/scanpy/plotting/_tools/scatterplots.py#L1192-L1204",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1881#issuecomment-863776992
https://github.com/scverse/scanpy/issues/1881#issuecomment-863776992:487,Testability,log,logic,487,"The additional cases that would need to be handled (and what I believe the current internal solutions are):. * What if colors haven't been saved to uns yet; * If it's an view, we just return what the colors would be by default; * If it's an ""actual"" we return the colors, but also assign them to the object; * What if there's a different number of colors and categories; * We warn and reassign the colors. If it's a view, we don't modify the object, but probably still warn. Most of the logic for handling this internally is wrapped in `_get_palette`: https://github.com/theislab/scanpy/blob/ed364a887db2eb604d0a09cd72325cb5e1f4e27e/scanpy/plotting/_tools/scatterplots.py#L1192-L1204",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1881#issuecomment-863776992
https://github.com/scverse/scanpy/pull/1884#issuecomment-864681671:195,Availability,error,errors,195,There was an issue where values were being set as rgb values unintentionally which should be fixed by #1886. Can you give an example of how this was causing problems? What would cause you to hit errors here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1884#issuecomment-864681671
https://github.com/scverse/scanpy/pull/1884#issuecomment-864689573:313,Availability,error,error,313,"I tried debug by myself and this is what I found at the break point: when the color represented by RGB values, such as `[array([0.29803922, 0.44705882, 0.69019608]), array([0.86666667, 0.51764706, 0.32156863]), array([0.33333333, 0.65882353, 0.40784314]), array([0.76862745, 0.30588235, 0.32156863])]`, it throws error. It may also cause by the default setting of `seaborn`. Since you have fixed it in #1886 , this problem should be fixed too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1884#issuecomment-864689573
https://github.com/scverse/scanpy/pull/1884#issuecomment-865741927:23,Availability,error,error,23,"Once again, I got this error, even if I didn't import `seaborn`. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-10-a5c62c51242c> in <module>; 1 # plt.figure(figsize=(10, 10)); 2 rcParams['figure.figsize'] = 8, 8; ----> 3 sc.pl.umap(all_dataset['pdac_pengj_02'], color='cluster', legend_loc='on data', legend_fontsize='small', title='', frameon=False). f:\tools\miniconda3\envs\deside2\lib\site-packages\scanpy\plotting\_tools\scatterplots.py in umap(adata, **kwargs); 601 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 602 """"""; --> 603 return embedding(adata, 'umap', **kwargs); 604 ; 605 . f:\tools\miniconda3\envs\deside2\lib\site-packages\scanpy\plotting\_tools\scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 244 groups=groups,; 245 ); --> 246 color_vector, categorical = _color_vector(; 247 adata,; 248 value_to_plot,. f:\tools\miniconda3\envs\deside2\lib\site-packages\scanpy\plotting\_tools\scatterplots.py in _color_vector(adata, values_key, values, palette, na_color); 1128 return values, False; 1129 else: # is_categorical_dtype(values); -> 1130 color_map = _get_palette(adata, values_key, palette=palette); 1131 color_vector = values.map(color_map).map(to_hex); 1132 . f:\tools\miniconda3\envs\deside2\lib\site-packages\scanpy\plotting\_tools\scatterplots.py in _get_palette(adata, values_key, palette); 1103 _utils._set_default_colors_for_categorical_obs(adata, values_key); 1104 else:; -> 1105 _utils._validate_palette(adata, values_key); 1106 return dict",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1884#issuecomment-865741927
https://github.com/scverse/scanpy/pull/1884#issuecomment-873725271:15,Deployability,update,update,15,Thanks for the update! Glad to hear it works now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1884#issuecomment-873725271
https://github.com/scverse/scanpy/pull/1890#issuecomment-866352349:435,Deployability,update,updates,435,"Thank you for checking the code. I have also been thinking about possible test(s), the only thing that comes to my mind is to check the output against a fixed reference result, which I could verify on a few different machines. But I'm not sure whether this wouldn't cause problems in the future. This could be troublesome to maintain as the matrix will need be stored somewhere and may need re-checking if e.g. numpy has some relevant updates. I am open to suggestions!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-866352349
https://github.com/scverse/scanpy/pull/1890#issuecomment-866352349:74,Testability,test,test,74,"Thank you for checking the code. I have also been thinking about possible test(s), the only thing that comes to my mind is to check the output against a fixed reference result, which I could verify on a few different machines. But I'm not sure whether this wouldn't cause problems in the future. This could be troublesome to maintain as the matrix will need be stored somewhere and may need re-checking if e.g. numpy has some relevant updates. I am open to suggestions!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-866352349
https://github.com/scverse/scanpy/pull/1890#issuecomment-866651210:289,Testability,test,test,289,"Hmm, I also consistently get a slightly different result for computing the mean between numpy and numba (I think it's one floating point step). When you were seeing differences, did they show up as different with `np.allclose`? If they did, I think we can go with adding a small reference test that checks with `np.testing.assert_almost_equal`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-866651210
https://github.com/scverse/scanpy/pull/1890#issuecomment-866651210:315,Testability,test,testing,315,"Hmm, I also consistently get a slightly different result for computing the mean between numpy and numba (I think it's one floating point step). When you were seeing differences, did they show up as different with `np.allclose`? If they did, I think we can go with adding a small reference test that checks with `np.testing.assert_almost_equal`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-866651210
https://github.com/scverse/scanpy/pull/1890#issuecomment-867445206:160,Testability,test,test,160,I haven't used np.allclose for comparison but if I remember correctly (it was a while ago) the differences were around 4th or 5th decimal point. I will write a test using np.allclose against a reference and run it on different machines to see how it looks. I will post here the results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-867445206
https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597:15,Testability,test,test,15,"I've added the test mentioned above and checked on two machines (MacOS and CentOS) with the same results. ; The differences in output are after 5th decimal point, the test:; - passes np.allclose both with and without the fix (master branch vs this PR); - passes np.array_equal only with the fix (this PR); Hence, I've kept only the np.array_equal test. This series of tests uses _create_sparse_nan_matrix, so I had to set the np.random.seed to make the test reproducible and agree with the reference.; The reference is stored in tests/_data/ as a pkl file, as I've noticed that there were some analogous files stored there for other tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597
https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597:167,Testability,test,test,167,"I've added the test mentioned above and checked on two machines (MacOS and CentOS) with the same results. ; The differences in output are after 5th decimal point, the test:; - passes np.allclose both with and without the fix (master branch vs this PR); - passes np.array_equal only with the fix (this PR); Hence, I've kept only the np.array_equal test. This series of tests uses _create_sparse_nan_matrix, so I had to set the np.random.seed to make the test reproducible and agree with the reference.; The reference is stored in tests/_data/ as a pkl file, as I've noticed that there were some analogous files stored there for other tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597
https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597:347,Testability,test,test,347,"I've added the test mentioned above and checked on two machines (MacOS and CentOS) with the same results. ; The differences in output are after 5th decimal point, the test:; - passes np.allclose both with and without the fix (master branch vs this PR); - passes np.array_equal only with the fix (this PR); Hence, I've kept only the np.array_equal test. This series of tests uses _create_sparse_nan_matrix, so I had to set the np.random.seed to make the test reproducible and agree with the reference.; The reference is stored in tests/_data/ as a pkl file, as I've noticed that there were some analogous files stored there for other tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597
https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597:368,Testability,test,tests,368,"I've added the test mentioned above and checked on two machines (MacOS and CentOS) with the same results. ; The differences in output are after 5th decimal point, the test:; - passes np.allclose both with and without the fix (master branch vs this PR); - passes np.array_equal only with the fix (this PR); Hence, I've kept only the np.array_equal test. This series of tests uses _create_sparse_nan_matrix, so I had to set the np.random.seed to make the test reproducible and agree with the reference.; The reference is stored in tests/_data/ as a pkl file, as I've noticed that there were some analogous files stored there for other tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597
https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597:453,Testability,test,test,453,"I've added the test mentioned above and checked on two machines (MacOS and CentOS) with the same results. ; The differences in output are after 5th decimal point, the test:; - passes np.allclose both with and without the fix (master branch vs this PR); - passes np.array_equal only with the fix (this PR); Hence, I've kept only the np.array_equal test. This series of tests uses _create_sparse_nan_matrix, so I had to set the np.random.seed to make the test reproducible and agree with the reference.; The reference is stored in tests/_data/ as a pkl file, as I've noticed that there were some analogous files stored there for other tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597
https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597:529,Testability,test,tests,529,"I've added the test mentioned above and checked on two machines (MacOS and CentOS) with the same results. ; The differences in output are after 5th decimal point, the test:; - passes np.allclose both with and without the fix (master branch vs this PR); - passes np.array_equal only with the fix (this PR); Hence, I've kept only the np.array_equal test. This series of tests uses _create_sparse_nan_matrix, so I had to set the np.random.seed to make the test reproducible and agree with the reference.; The reference is stored in tests/_data/ as a pkl file, as I've noticed that there were some analogous files stored there for other tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597
https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597:633,Testability,test,tests,633,"I've added the test mentioned above and checked on two machines (MacOS and CentOS) with the same results. ; The differences in output are after 5th decimal point, the test:; - passes np.allclose both with and without the fix (master branch vs this PR); - passes np.array_equal only with the fix (this PR); Hence, I've kept only the np.array_equal test. This series of tests uses _create_sparse_nan_matrix, so I had to set the np.random.seed to make the test reproducible and agree with the reference.; The reference is stored in tests/_data/ as a pkl file, as I've noticed that there were some analogous files stored there for other tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-869646597
https://github.com/scverse/scanpy/pull/1890#issuecomment-871135647:44,Testability,test,test,44,I'm not sure I totally trust that generated test data to be reproducible across scipy versions. Could you use one of the small example datasets modified in a deterministic way instead? Something like `sc.datasets.krumsiek11()`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-871135647
https://github.com/scverse/scanpy/pull/1890#issuecomment-873623806:349,Testability,test,test,349,"I see your point, no problem. krumsiek11 dataset does not actually cause inconsistencies float32/float64 (without/with the fix). I suspect this is because it has fewer decimal digits and fewer genes so does not accumulate enough imprecision.; Paul15 data behaves as reported before, so I used that one instead. Hope this is fine!. As previously the test passes with the fix but not without it on the two machines I tested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-873623806
https://github.com/scverse/scanpy/pull/1890#issuecomment-873623806:415,Testability,test,tested,415,"I see your point, no problem. krumsiek11 dataset does not actually cause inconsistencies float32/float64 (without/with the fix). I suspect this is because it has fewer decimal digits and fewer genes so does not accumulate enough imprecision.; Paul15 data behaves as reported before, so I used that one instead. Hope this is fine!. As previously the test passes with the fix but not without it on the two machines I tested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1890#issuecomment-873623806
https://github.com/scverse/scanpy/issues/1892#issuecomment-864860024:112,Deployability,update,updated,112,"I see that it can process arrays now, i should check if it better to replace the custom implementation with the updated `stats.mannwhitneyu`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1892#issuecomment-864860024
https://github.com/scverse/scanpy/issues/1892#issuecomment-864875522:112,Deployability,update,updated,112,"> see that it can process arrays now, i should check if it better to replace the custom implementation with the updated stats.mannwhitneyu. It still doesn't work with sparse arrays, and our version on a sparse array was much faster than theirs on a dense. I did not compare speeds for dense v dense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1892#issuecomment-864875522
https://github.com/scverse/scanpy/issues/1896#issuecomment-866565353:95,Deployability,install,installed,95,"Hi. So @vals `gprofiler` and `gprofiler-official` are different packages, and having them both installed could cause an issue like this. Is this the case here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1896#issuecomment-866565353
https://github.com/scverse/scanpy/pull/1900#issuecomment-869127933:137,Availability,Ping,Ping,137,"Looks like the issue goes deeper, to `get_version` being called on RTD. Seems like some interaction of that package and the environment. Ping @flying-sheep",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1900#issuecomment-869127933
https://github.com/scverse/scanpy/issues/1901#issuecomment-869271735:89,Deployability,toggle,toggle,89,"Also, does the `enrich()` function run GProfiler as an ordered or unordered query? Can I toggle this using `gprofiler_kwargs`? (This question may be more appropriate for the gprofiler-official page, but I can't find one). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1901#issuecomment-869271735
https://github.com/scverse/scanpy/issues/1909#issuecomment-874177090:113,Deployability,Install,Installing,113,"Did you fix this already? The `CheckBuild` job’s task “Build & Twine check” looks fine to me:. For master:. ```; Installing collected packages: numpy, threadpoolctl, ...; Successfully installed anndata-0.7.6 cycler-0.10.0 ...; Checking dist/scanpy-1.9.0.dev2+gc9b59137-py3-none-any.whl: PASSED; Checking dist/scanpy-1.9.0.dev2+gc9b59137.tar.gz: PASSED, with warnings; warning: `long_description_content_type` missing. defaulting to `text/x-rst`.; warning: `long_description` missing.; ```. For the 1.8.x branch:. ```; Installing collected packages: numpy, threadpoolctl, ...; Successfully installed anndata-0.7.6 cycler-0.10.0 ...; Checking dist/scanpy-1.8.1.dev5+gbcbcbccf-py3-none-any.whl: PASSED; Checking dist/scanpy-1.8.1.dev5+gbcbcbccf.tar.gz: PASSED, with warnings; warning: `long_description_content_type` missing. defaulting to `text/x-rst`.; warning: `long_description` missing.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874177090
https://github.com/scverse/scanpy/issues/1909#issuecomment-874177090:184,Deployability,install,installed,184,"Did you fix this already? The `CheckBuild` job’s task “Build & Twine check” looks fine to me:. For master:. ```; Installing collected packages: numpy, threadpoolctl, ...; Successfully installed anndata-0.7.6 cycler-0.10.0 ...; Checking dist/scanpy-1.9.0.dev2+gc9b59137-py3-none-any.whl: PASSED; Checking dist/scanpy-1.9.0.dev2+gc9b59137.tar.gz: PASSED, with warnings; warning: `long_description_content_type` missing. defaulting to `text/x-rst`.; warning: `long_description` missing.; ```. For the 1.8.x branch:. ```; Installing collected packages: numpy, threadpoolctl, ...; Successfully installed anndata-0.7.6 cycler-0.10.0 ...; Checking dist/scanpy-1.8.1.dev5+gbcbcbccf-py3-none-any.whl: PASSED; Checking dist/scanpy-1.8.1.dev5+gbcbcbccf.tar.gz: PASSED, with warnings; warning: `long_description_content_type` missing. defaulting to `text/x-rst`.; warning: `long_description` missing.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874177090
https://github.com/scverse/scanpy/issues/1909#issuecomment-874177090:518,Deployability,Install,Installing,518,"Did you fix this already? The `CheckBuild` job’s task “Build & Twine check” looks fine to me:. For master:. ```; Installing collected packages: numpy, threadpoolctl, ...; Successfully installed anndata-0.7.6 cycler-0.10.0 ...; Checking dist/scanpy-1.9.0.dev2+gc9b59137-py3-none-any.whl: PASSED; Checking dist/scanpy-1.9.0.dev2+gc9b59137.tar.gz: PASSED, with warnings; warning: `long_description_content_type` missing. defaulting to `text/x-rst`.; warning: `long_description` missing.; ```. For the 1.8.x branch:. ```; Installing collected packages: numpy, threadpoolctl, ...; Successfully installed anndata-0.7.6 cycler-0.10.0 ...; Checking dist/scanpy-1.8.1.dev5+gbcbcbccf-py3-none-any.whl: PASSED; Checking dist/scanpy-1.8.1.dev5+gbcbcbccf.tar.gz: PASSED, with warnings; warning: `long_description_content_type` missing. defaulting to `text/x-rst`.; warning: `long_description` missing.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874177090
https://github.com/scverse/scanpy/issues/1909#issuecomment-874177090:589,Deployability,install,installed,589,"Did you fix this already? The `CheckBuild` job’s task “Build & Twine check” looks fine to me:. For master:. ```; Installing collected packages: numpy, threadpoolctl, ...; Successfully installed anndata-0.7.6 cycler-0.10.0 ...; Checking dist/scanpy-1.9.0.dev2+gc9b59137-py3-none-any.whl: PASSED; Checking dist/scanpy-1.9.0.dev2+gc9b59137.tar.gz: PASSED, with warnings; warning: `long_description_content_type` missing. defaulting to `text/x-rst`.; warning: `long_description` missing.; ```. For the 1.8.x branch:. ```; Installing collected packages: numpy, threadpoolctl, ...; Successfully installed anndata-0.7.6 cycler-0.10.0 ...; Checking dist/scanpy-1.8.1.dev5+gbcbcbccf-py3-none-any.whl: PASSED; Checking dist/scanpy-1.8.1.dev5+gbcbcbccf.tar.gz: PASSED, with warnings; warning: `long_description_content_type` missing. defaulting to `text/x-rst`.; warning: `long_description` missing.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874177090
https://github.com/scverse/scanpy/issues/1909#issuecomment-874368328:156,Deployability,configurat,configuration,156,"Yeah, the task is running fine, but it's not including the license locally. It's also including a different set of files than flit does, which seems like a configuration issue. I think we need to add some more checks to the build task.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874368328
https://github.com/scverse/scanpy/issues/1909#issuecomment-874368328:156,Modifiability,config,configuration,156,"Yeah, the task is running fine, but it's not including the license locally. It's also including a different set of files than flit does, which seems like a configuration issue. I think we need to add some more checks to the build task.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874368328
https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324:141,Deployability,install,install,141,"The short answer is that `flit_core`, which provides the PEP 517 hooks, makes a minimal sdist which should always have the files you need to install the module, but may leave out e.g. tests and docs. The Flit CLI tries to make a 'publication quality' sdist. It's kind of an ugly compromise, because how I approached sdists (before PEP 517) wasn't a good fit for the PEP 517 `build_sdist` hook. I view sdists on PyPI as like a snapshot of the development process, so it should (by default) include everything that you'd get if you checked out the corresponding tag from git (except the git history). But using git assumes that it's something the maintainer makes once and publishes on PyPI. PEP 517 defined a `build_sdist` hook which user tools (like pip) can call. I didn't want this to depend on git, so I gave it a way to make working but minimal sdists. Specifying includes & excludes under `[tool.flit.sdist]` should affect both the Flit CLI and the PEP 517 hooks. So if you want to make the sdists to publish with `python -m build` or similar, you'll need to use those to determine what goes in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324
https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324:787,Integrability,depend,depend,787,"The short answer is that `flit_core`, which provides the PEP 517 hooks, makes a minimal sdist which should always have the files you need to install the module, but may leave out e.g. tests and docs. The Flit CLI tries to make a 'publication quality' sdist. It's kind of an ugly compromise, because how I approached sdists (before PEP 517) wasn't a good fit for the PEP 517 `build_sdist` hook. I view sdists on PyPI as like a snapshot of the development process, so it should (by default) include everything that you'd get if you checked out the corresponding tag from git (except the git history). But using git assumes that it's something the maintainer makes once and publishes on PyPI. PEP 517 defined a `build_sdist` hook which user tools (like pip) can call. I didn't want this to depend on git, so I gave it a way to make working but minimal sdists. Specifying includes & excludes under `[tool.flit.sdist]` should affect both the Flit CLI and the PEP 517 hooks. So if you want to make the sdists to publish with `python -m build` or similar, you'll need to use those to determine what goes in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324
https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324:184,Testability,test,tests,184,"The short answer is that `flit_core`, which provides the PEP 517 hooks, makes a minimal sdist which should always have the files you need to install the module, but may leave out e.g. tests and docs. The Flit CLI tries to make a 'publication quality' sdist. It's kind of an ugly compromise, because how I approached sdists (before PEP 517) wasn't a good fit for the PEP 517 `build_sdist` hook. I view sdists on PyPI as like a snapshot of the development process, so it should (by default) include everything that you'd get if you checked out the corresponding tag from git (except the git history). But using git assumes that it's something the maintainer makes once and publishes on PyPI. PEP 517 defined a `build_sdist` hook which user tools (like pip) can call. I didn't want this to depend on git, so I gave it a way to make working but minimal sdists. Specifying includes & excludes under `[tool.flit.sdist]` should affect both the Flit CLI and the PEP 517 hooks. So if you want to make the sdists to publish with `python -m build` or similar, you'll need to use those to determine what goes in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324
https://github.com/scverse/scanpy/issues/1909#issuecomment-875415985:322,Deployability,install,installed,322,"@flying-sheep, figured out (part of) why I'm seeing the wrong version for the wheel, but the right version for the sdist via `flit build`. When the wheel is built, it hits this case:. https://github.com/theislab/scanpy/blob/3c7256560f53374ecf960bc329405912da8d6efe/scanpy/_metadata.py#L35-L41. Where it sees the currently installed version, which may not be aware of new tag (since I'm making the build in a clean directory). I imagine this is why using `python -m build` get's it right, since it's building everything in a fresh environment. However, getting this issue with the METADATA for the sdist generating warnings, but not having this problem with the wheel: . ```; $ twine check dist/*; Checking dist/scanpy-1.8.1-py3-none-any.whl: PASSED; Checking dist/scanpy-1.8.1.tar.gz: PASSED, with warnings; warning: `long_description_content_type` missing. defaulting to `text/x-rst`.; warning: `long_description` missing.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-875415985
https://github.com/scverse/scanpy/issues/1909#issuecomment-875526490:208,Deployability,install,installed,208,"Thank you for the detailed explanation @takluyver, this helps a lot!. @ivirshup A dev environment should successfully execute the `try` block. I designed that `except` clause to be hit when people import the installed production version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-875526490
https://github.com/scverse/scanpy/issues/1913#issuecomment-872712326:253,Deployability,update,update,253,"I was thinking more like:. ```python; sc.pl.dotplot(; ...,; var_ticklabels_kwargs={; ""fontstyle"": ""italic"",; ... ; },; ); ```. With an internal:. ```python; var_ticklabels_params = {""rotation"": 90, ""ha"": 'center', ""minor"": False}; var_ticklabels_params.update(var_ticklabels_kwargs}. dot_ax.set_xticklabels(var_names, **var_ticklabels_params); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-872712326
https://github.com/scverse/scanpy/issues/1913#issuecomment-873024842:151,Modifiability,flexible,flexible,151,"Hmm ok, I mean that'd work too, I guess this is more like the difference between programmer's view and practitioner's view 😄 . This is definitely more flexible and cleaner in terms of API design, but if we imagine a notebook with e.g. five sc.pl.dotplot calls, wouldn't it be painful to write this for each call (and memorize)? There are two reasons I suggested `set_figure_params` . 1. It already has all the tricks to make figures more publication-ready e.g. rasterization, Arial font etc. ; 2. Italicized gene names is likely going to be a session-wide (or project-wide) decision, where one would use it in all figures not just in one, which brings us to the importance of conciseness. . Last suggestion is like `theme(axis_text_x=element_text(angle=90, hjust=1))` kind of argument but `rotation=90` also works 😃 Anyway, my two cents, happy with any option that achieves this, the community will appreciate it I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-873024842
https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647:606,Modifiability,config,config,606,"> if we imagine a notebook with e.g. five sc.pl.dotplot calls. To me, italicizing the genes is more of a ""customizing plots for publication"" than ""looking at data"", so I think it's okay for it to have some extra steps as it happens less often. I personally have used illustrator for this kind of step. Alternatively, I tend to have functions like:. ```python; def customized_dotplot(adata, ..., dotplot_kwargs={}):; ... # Preprocess data, maybe add some fields; ret = sc.pl.dotplot(adata, show=False, ..., **dotplot_kwargs); ... # do additional things; ```. That said, I'm not against also having a global config for it, I would just want to see the structure for handling this global config for extra setting options sketched out in more detail. Also a PR implementing it 😉. > Last suggestion is like theme(axis_text_x=element_text(angle=90, hjust=1)). I do think it would make sense to have this outside of `set_figure_params`. `set_figure_params` does some global setting through the matplotlib config. Were you thinking of this as `DotPlot(...).theme(...)` or `sc.settings.theme(...)`? Maybe both, with the method overriding the config?. 1. I don't really like that `set_figure_params` modifies plots not generated by scanpy; 2. matplotlib doesn't have a concept of the ""variable axis"", so we'd have to handle this ourselves.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647
https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647:685,Modifiability,config,config,685,"> if we imagine a notebook with e.g. five sc.pl.dotplot calls. To me, italicizing the genes is more of a ""customizing plots for publication"" than ""looking at data"", so I think it's okay for it to have some extra steps as it happens less often. I personally have used illustrator for this kind of step. Alternatively, I tend to have functions like:. ```python; def customized_dotplot(adata, ..., dotplot_kwargs={}):; ... # Preprocess data, maybe add some fields; ret = sc.pl.dotplot(adata, show=False, ..., **dotplot_kwargs); ... # do additional things; ```. That said, I'm not against also having a global config for it, I would just want to see the structure for handling this global config for extra setting options sketched out in more detail. Also a PR implementing it 😉. > Last suggestion is like theme(axis_text_x=element_text(angle=90, hjust=1)). I do think it would make sense to have this outside of `set_figure_params`. `set_figure_params` does some global setting through the matplotlib config. Were you thinking of this as `DotPlot(...).theme(...)` or `sc.settings.theme(...)`? Maybe both, with the method overriding the config?. 1. I don't really like that `set_figure_params` modifies plots not generated by scanpy; 2. matplotlib doesn't have a concept of the ""variable axis"", so we'd have to handle this ourselves.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647
https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647:998,Modifiability,config,config,998,"> if we imagine a notebook with e.g. five sc.pl.dotplot calls. To me, italicizing the genes is more of a ""customizing plots for publication"" than ""looking at data"", so I think it's okay for it to have some extra steps as it happens less often. I personally have used illustrator for this kind of step. Alternatively, I tend to have functions like:. ```python; def customized_dotplot(adata, ..., dotplot_kwargs={}):; ... # Preprocess data, maybe add some fields; ret = sc.pl.dotplot(adata, show=False, ..., **dotplot_kwargs); ... # do additional things; ```. That said, I'm not against also having a global config for it, I would just want to see the structure for handling this global config for extra setting options sketched out in more detail. Also a PR implementing it 😉. > Last suggestion is like theme(axis_text_x=element_text(angle=90, hjust=1)). I do think it would make sense to have this outside of `set_figure_params`. `set_figure_params` does some global setting through the matplotlib config. Were you thinking of this as `DotPlot(...).theme(...)` or `sc.settings.theme(...)`? Maybe both, with the method overriding the config?. 1. I don't really like that `set_figure_params` modifies plots not generated by scanpy; 2. matplotlib doesn't have a concept of the ""variable axis"", so we'd have to handle this ourselves.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647
https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647:1133,Modifiability,config,config,1133,"> if we imagine a notebook with e.g. five sc.pl.dotplot calls. To me, italicizing the genes is more of a ""customizing plots for publication"" than ""looking at data"", so I think it's okay for it to have some extra steps as it happens less often. I personally have used illustrator for this kind of step. Alternatively, I tend to have functions like:. ```python; def customized_dotplot(adata, ..., dotplot_kwargs={}):; ... # Preprocess data, maybe add some fields; ret = sc.pl.dotplot(adata, show=False, ..., **dotplot_kwargs); ... # do additional things; ```. That said, I'm not against also having a global config for it, I would just want to see the structure for handling this global config for extra setting options sketched out in more detail. Also a PR implementing it 😉. > Last suggestion is like theme(axis_text_x=element_text(angle=90, hjust=1)). I do think it would make sense to have this outside of `set_figure_params`. `set_figure_params` does some global setting through the matplotlib config. Were you thinking of this as `DotPlot(...).theme(...)` or `sc.settings.theme(...)`? Maybe both, with the method overriding the config?. 1. I don't really like that `set_figure_params` modifies plots not generated by scanpy; 2. matplotlib doesn't have a concept of the ""variable axis"", so we'd have to handle this ourselves.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647
https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647:1275,Modifiability,variab,variable,1275,"> if we imagine a notebook with e.g. five sc.pl.dotplot calls. To me, italicizing the genes is more of a ""customizing plots for publication"" than ""looking at data"", so I think it's okay for it to have some extra steps as it happens less often. I personally have used illustrator for this kind of step. Alternatively, I tend to have functions like:. ```python; def customized_dotplot(adata, ..., dotplot_kwargs={}):; ... # Preprocess data, maybe add some fields; ret = sc.pl.dotplot(adata, show=False, ..., **dotplot_kwargs); ... # do additional things; ```. That said, I'm not against also having a global config for it, I would just want to see the structure for handling this global config for extra setting options sketched out in more detail. Also a PR implementing it 😉. > Last suggestion is like theme(axis_text_x=element_text(angle=90, hjust=1)). I do think it would make sense to have this outside of `set_figure_params`. `set_figure_params` does some global setting through the matplotlib config. Were you thinking of this as `DotPlot(...).theme(...)` or `sc.settings.theme(...)`? Maybe both, with the method overriding the config?. 1. I don't really like that `set_figure_params` modifies plots not generated by scanpy; 2. matplotlib doesn't have a concept of the ""variable axis"", so we'd have to handle this ourselves.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647
https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906:864,Integrability,depend,dependency,864,"> To me, italicizing the genes is more of a ""customizing plots for publication"" than ""looking at data"", so I think it's okay for it to have some extra steps as it happens less often. I personally have used illustrator for this kind of step. I disagree very much :) First of all I didn't say that these 10 dotplots are for ""looking at data,"" most of them are indeed for a paper (we can count the number of dotplots in sc papers, if we want to have a more accurate estimate of the time cost) and all of them need to be edited in a pdf editing software. Once the preprint is out, I can write in this post which dotplots I am talking about, how many there are in reality and how much time it took us to manually edit such minor things that can be easily addressed in the plotting code 😄 . What we are discussing is maybe a minor thing here, but if we can minimize the dependency on Illustrator (which is a pricey proprietary software with a highly unintuitive interface in my very subjective opinion) to make publication-ready figures, I think it's a HUGE plus for the community. I think this is related to the philosophy of scanpy. To sum it up in a broader context, I think enabling people to have high-quality, publication-ready figures without mastering matplotlib and/or Illustrator must be one of the top items of the `scanpy constitution` :). I know many colleagues who already nicely memorized the entire scanpy API but asking them also to know bits and pieces of a beast like mpl might be too much IMO. Based on your final suggestion, I can imagine myself trying to remember ""Was it `var_ticklabels_kwargs={""fontstyle"": ""italic""}` or `var_ticklabels_kwargs={""font_style"": ""italic""}` or `var_ticklabels_kwds={""fontstyle"": ""italic""}` or `ticklabels_var_kwargs={""fontstyle"": ""italic""}` etc. I even spend 45 seconds everyday to remember this damn thing here `plt.rcParams[""figure.figsize""]` :). > I don't really like that `set_figure_params` modifies plots not generated by scanpy. I totally understa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906
https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906:956,Integrability,interface,interface,956,"> To me, italicizing the genes is more of a ""customizing plots for publication"" than ""looking at data"", so I think it's okay for it to have some extra steps as it happens less often. I personally have used illustrator for this kind of step. I disagree very much :) First of all I didn't say that these 10 dotplots are for ""looking at data,"" most of them are indeed for a paper (we can count the number of dotplots in sc papers, if we want to have a more accurate estimate of the time cost) and all of them need to be edited in a pdf editing software. Once the preprint is out, I can write in this post which dotplots I am talking about, how many there are in reality and how much time it took us to manually edit such minor things that can be easily addressed in the plotting code 😄 . What we are discussing is maybe a minor thing here, but if we can minimize the dependency on Illustrator (which is a pricey proprietary software with a highly unintuitive interface in my very subjective opinion) to make publication-ready figures, I think it's a HUGE plus for the community. I think this is related to the philosophy of scanpy. To sum it up in a broader context, I think enabling people to have high-quality, publication-ready figures without mastering matplotlib and/or Illustrator must be one of the top items of the `scanpy constitution` :). I know many colleagues who already nicely memorized the entire scanpy API but asking them also to know bits and pieces of a beast like mpl might be too much IMO. Based on your final suggestion, I can imagine myself trying to remember ""Was it `var_ticklabels_kwargs={""fontstyle"": ""italic""}` or `var_ticklabels_kwargs={""font_style"": ""italic""}` or `var_ticklabels_kwds={""fontstyle"": ""italic""}` or `ticklabels_var_kwargs={""fontstyle"": ""italic""}` etc. I even spend 45 seconds everyday to remember this damn thing here `plt.rcParams[""figure.figsize""]` :). > I don't really like that `set_figure_params` modifies plots not generated by scanpy. I totally understa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906
https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906:2206,Modifiability,config,config,2206,"manually edit such minor things that can be easily addressed in the plotting code 😄 . What we are discussing is maybe a minor thing here, but if we can minimize the dependency on Illustrator (which is a pricey proprietary software with a highly unintuitive interface in my very subjective opinion) to make publication-ready figures, I think it's a HUGE plus for the community. I think this is related to the philosophy of scanpy. To sum it up in a broader context, I think enabling people to have high-quality, publication-ready figures without mastering matplotlib and/or Illustrator must be one of the top items of the `scanpy constitution` :). I know many colleagues who already nicely memorized the entire scanpy API but asking them also to know bits and pieces of a beast like mpl might be too much IMO. Based on your final suggestion, I can imagine myself trying to remember ""Was it `var_ticklabels_kwargs={""fontstyle"": ""italic""}` or `var_ticklabels_kwargs={""font_style"": ""italic""}` or `var_ticklabels_kwds={""fontstyle"": ""italic""}` or `ticklabels_var_kwargs={""fontstyle"": ""italic""}` etc. I even spend 45 seconds everyday to remember this damn thing here `plt.rcParams[""figure.figsize""]` :). > I don't really like that `set_figure_params` modifies plots not generated by scanpy. I totally understand this from the coding and engineering perspective, very ugly and violates several principles of good design. But on the other hand, it makes the life of many practitioners easier by setting the plotting config early on in a ""Scanpy notebook session"", which is clearly created to do research on single-cell genomics with scanpy, without rerunning things several times. For example, I use plotnine sometimes for publications too, but I hate writing `theme(text=element_text(family='Arial'))` every time I make a figure. plotnine is a general-purpose plotting library so it's not their problem, but people use Scanpy for both data exploration AND publications. So I think we can do better than that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906
https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906:2263,Usability,clear,clearly,2263,"manually edit such minor things that can be easily addressed in the plotting code 😄 . What we are discussing is maybe a minor thing here, but if we can minimize the dependency on Illustrator (which is a pricey proprietary software with a highly unintuitive interface in my very subjective opinion) to make publication-ready figures, I think it's a HUGE plus for the community. I think this is related to the philosophy of scanpy. To sum it up in a broader context, I think enabling people to have high-quality, publication-ready figures without mastering matplotlib and/or Illustrator must be one of the top items of the `scanpy constitution` :). I know many colleagues who already nicely memorized the entire scanpy API but asking them also to know bits and pieces of a beast like mpl might be too much IMO. Based on your final suggestion, I can imagine myself trying to remember ""Was it `var_ticklabels_kwargs={""fontstyle"": ""italic""}` or `var_ticklabels_kwargs={""font_style"": ""italic""}` or `var_ticklabels_kwds={""fontstyle"": ""italic""}` or `ticklabels_var_kwargs={""fontstyle"": ""italic""}` etc. I even spend 45 seconds everyday to remember this damn thing here `plt.rcParams[""figure.figsize""]` :). > I don't really like that `set_figure_params` modifies plots not generated by scanpy. I totally understand this from the coding and engineering perspective, very ugly and violates several principles of good design. But on the other hand, it makes the life of many practitioners easier by setting the plotting config early on in a ""Scanpy notebook session"", which is clearly created to do research on single-cell genomics with scanpy, without rerunning things several times. For example, I use plotnine sometimes for publications too, but I hate writing `theme(text=element_text(family='Arial'))` every time I make a figure. plotnine is a general-purpose plotting library so it's not their problem, but people use Scanpy for both data exploration AND publications. So I think we can do better than that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906
https://github.com/scverse/scanpy/issues/1914#issuecomment-871916461:138,Deployability,release,releases,138,"You can use the `categories_order` argument, though would have to check which version that was added in. Are you having issues with newer releases of scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1914#issuecomment-871916461
https://github.com/scverse/scanpy/issues/1916#issuecomment-874096004:149,Availability,error,error,149,"GSE145328 is from NCBI GEO. [GSE139555_RAW.tar](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE139555) is another example that gives the same error. These are the output files generated by Cell Ranger and uploaded to NCBI GEO, based on the authors' notes in their [paper](https://www.nature.com/articles/s41586-020-2056-8). . Some examples from NCBI GEO that can be read without errors: [GSE173231](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE173231) and [GSE158803](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE158803).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-874096004
https://github.com/scverse/scanpy/issues/1916#issuecomment-874096004:386,Availability,error,errors,386,"GSE145328 is from NCBI GEO. [GSE139555_RAW.tar](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE139555) is another example that gives the same error. These are the output files generated by Cell Ranger and uploaded to NCBI GEO, based on the authors' notes in their [paper](https://www.nature.com/articles/s41586-020-2056-8). . Some examples from NCBI GEO that can be read without errors: [GSE173231](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE173231) and [GSE158803](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE158803).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-874096004
https://github.com/scverse/scanpy/issues/1916#issuecomment-927497782:67,Availability,error,error,67,"Hello,. I am having a similar issue with `read_10x_mtx`, except my error is showing `Key Error: 1` and it is from 10X data we produced ourselves, do doesn't involve a GEO incompabitibility. Error below:. ```. >>> juno = sc.read_10x_mtx(path = ""../../Data/juno_influenza_pilot/hash_t_cells/umi_count""); Traceback (most recent call last):; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc; return self._engine.get_loc(casted_key); File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 2131, in pandas._libs.hashtable.Int64HashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 2140, in pandas._libs.hashtable.Int64HashTable.get_item; KeyError: 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/scanpy/readwrite.py"", line 481, in read_10x_mtx; adata = read(; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/scanpy/readwrite.py"", line 552, in _read_v3_10x_mtx; var_names = genes[1].values; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/frame.py"", line 3455, in __getitem__; indexer = self.columns.get_loc(key); File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc; raise KeyError(key) from err; KeyError: 1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-927497782
https://github.com/scverse/scanpy/issues/1916#issuecomment-927497782:89,Availability,Error,Error,89,"Hello,. I am having a similar issue with `read_10x_mtx`, except my error is showing `Key Error: 1` and it is from 10X data we produced ourselves, do doesn't involve a GEO incompabitibility. Error below:. ```. >>> juno = sc.read_10x_mtx(path = ""../../Data/juno_influenza_pilot/hash_t_cells/umi_count""); Traceback (most recent call last):; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc; return self._engine.get_loc(casted_key); File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 2131, in pandas._libs.hashtable.Int64HashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 2140, in pandas._libs.hashtable.Int64HashTable.get_item; KeyError: 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/scanpy/readwrite.py"", line 481, in read_10x_mtx; adata = read(; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/scanpy/readwrite.py"", line 552, in _read_v3_10x_mtx; var_names = genes[1].values; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/frame.py"", line 3455, in __getitem__; indexer = self.columns.get_loc(key); File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc; raise KeyError(key) from err; KeyError: 1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-927497782
https://github.com/scverse/scanpy/issues/1916#issuecomment-927497782:190,Availability,Error,Error,190,"Hello,. I am having a similar issue with `read_10x_mtx`, except my error is showing `Key Error: 1` and it is from 10X data we produced ourselves, do doesn't involve a GEO incompabitibility. Error below:. ```. >>> juno = sc.read_10x_mtx(path = ""../../Data/juno_influenza_pilot/hash_t_cells/umi_count""); Traceback (most recent call last):; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc; return self._engine.get_loc(casted_key); File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 2131, in pandas._libs.hashtable.Int64HashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 2140, in pandas._libs.hashtable.Int64HashTable.get_item; KeyError: 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/scanpy/readwrite.py"", line 481, in read_10x_mtx; adata = read(; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/scanpy/readwrite.py"", line 552, in _read_v3_10x_mtx; var_names = genes[1].values; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/frame.py"", line 3455, in __getitem__; indexer = self.columns.get_loc(key); File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc; raise KeyError(key) from err; KeyError: 1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-927497782
https://github.com/scverse/scanpy/issues/1916#issuecomment-927497782:741,Security,hash,hashtable,741,"Hello,. I am having a similar issue with `read_10x_mtx`, except my error is showing `Key Error: 1` and it is from 10X data we produced ourselves, do doesn't involve a GEO incompabitibility. Error below:. ```. >>> juno = sc.read_10x_mtx(path = ""../../Data/juno_influenza_pilot/hash_t_cells/umi_count""); Traceback (most recent call last):; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc; return self._engine.get_loc(casted_key); File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 2131, in pandas._libs.hashtable.Int64HashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 2140, in pandas._libs.hashtable.Int64HashTable.get_item; KeyError: 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/scanpy/readwrite.py"", line 481, in read_10x_mtx; adata = read(; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/scanpy/readwrite.py"", line 552, in _read_v3_10x_mtx; var_names = genes[1].values; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/frame.py"", line 3455, in __getitem__; indexer = self.columns.get_loc(key); File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc; raise KeyError(key) from err; KeyError: 1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-927497782
https://github.com/scverse/scanpy/issues/1916#issuecomment-927497782:851,Security,hash,hashtable,851,"Hello,. I am having a similar issue with `read_10x_mtx`, except my error is showing `Key Error: 1` and it is from 10X data we produced ourselves, do doesn't involve a GEO incompabitibility. Error below:. ```. >>> juno = sc.read_10x_mtx(path = ""../../Data/juno_influenza_pilot/hash_t_cells/umi_count""); Traceback (most recent call last):; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 3361, in get_loc; return self._engine.get_loc(casted_key); File ""pandas/_libs/index.pyx"", line 76, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 2131, in pandas._libs.hashtable.Int64HashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 2140, in pandas._libs.hashtable.Int64HashTable.get_item; KeyError: 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/scanpy/readwrite.py"", line 481, in read_10x_mtx; adata = read(; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/scanpy/readwrite.py"", line 552, in _read_v3_10x_mtx; var_names = genes[1].values; File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/frame.py"", line 3455, in __getitem__; indexer = self.columns.get_loc(key); File ""/home/daniel/miniconda/envs/scvi/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 3363, in get_loc; raise KeyError(key) from err; KeyError: 1. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-927497782
https://github.com/scverse/scanpy/issues/1916#issuecomment-1102500257:53,Usability,simpl,simply,53,"pandas transparently handles gzipping. We could also simply add support for gzipped files:. ```py; def maybe_zipped_path(path: Path) -> Path:; zipped_path = Path(f'{path}.gz'); if zipped_path.is_file(): return zipped_path; return path; ```. And then we use it:. https://github.com/scverse/scanpy/blob/a38a22ab85074f17788b8d1effa89c1373e0c978/scanpy/readwrite.py#L488. to. ```py; genefile_exists = maybe_zipped_path(path / f'{prefix}genes.tsv').is_file(); ```. and . https://github.com/scverse/scanpy/blob/a38a22ab85074f17788b8d1effa89c1373e0c978/scanpy/readwrite.py#L525. to. ```py; genes = pd.read_csv(maybe_zipped_path(path / f'{prefix}genes.tsv'), header=None, sep='\t'); ```. and so on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1102500257
https://github.com/scverse/scanpy/issues/1916#issuecomment-1286404697:37,Availability,error,error,37,"I've figured out what was causing my error. The scanpy function to read in `features.tsv.gz` expects three columns: `['gene_symbols, 'gene_ids', 'feature_types']`; Where 'feature types' is a text string like 'Gene Expression' and usually repeated along the whole length of the file.; The file I was reading in was from HTO data and only had one column:. > Hashtag1-GTCAACTCTTTAGCG; > Hashtag2-TTCCGCCTCTCTTTG; > Hashtag3-AAGTATCGTTTCGCA; > unmapped. So if others run into this same error, just add in some extra columns to the `features.tsv` file so it doesn't error out when looking for the extra columns. Something like this (different features file):. >RP11-34P13.7	RP11-34P13.7	Gene Expression; >FO538757.3	FO538757.3	Gene Expression; >FO538757.2	FO538757.2	Gene Expression; >AP006222.2	AP006222.2	Gene Expression; >RP4-669L17.10	RP4-669L17.10	Gene Expression. It would also be helpful if scanpy would validate the number of columns at the start. At the moment it looks like it reads in the whole `.mtx` file before trying to map the feature names and producing this error, so it takes a while to fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1286404697
https://github.com/scverse/scanpy/issues/1916#issuecomment-1286404697:482,Availability,error,error,482,"I've figured out what was causing my error. The scanpy function to read in `features.tsv.gz` expects three columns: `['gene_symbols, 'gene_ids', 'feature_types']`; Where 'feature types' is a text string like 'Gene Expression' and usually repeated along the whole length of the file.; The file I was reading in was from HTO data and only had one column:. > Hashtag1-GTCAACTCTTTAGCG; > Hashtag2-TTCCGCCTCTCTTTG; > Hashtag3-AAGTATCGTTTCGCA; > unmapped. So if others run into this same error, just add in some extra columns to the `features.tsv` file so it doesn't error out when looking for the extra columns. Something like this (different features file):. >RP11-34P13.7	RP11-34P13.7	Gene Expression; >FO538757.3	FO538757.3	Gene Expression; >FO538757.2	FO538757.2	Gene Expression; >AP006222.2	AP006222.2	Gene Expression; >RP4-669L17.10	RP4-669L17.10	Gene Expression. It would also be helpful if scanpy would validate the number of columns at the start. At the moment it looks like it reads in the whole `.mtx` file before trying to map the feature names and producing this error, so it takes a while to fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1286404697
https://github.com/scverse/scanpy/issues/1916#issuecomment-1286404697:561,Availability,error,error,561,"I've figured out what was causing my error. The scanpy function to read in `features.tsv.gz` expects three columns: `['gene_symbols, 'gene_ids', 'feature_types']`; Where 'feature types' is a text string like 'Gene Expression' and usually repeated along the whole length of the file.; The file I was reading in was from HTO data and only had one column:. > Hashtag1-GTCAACTCTTTAGCG; > Hashtag2-TTCCGCCTCTCTTTG; > Hashtag3-AAGTATCGTTTCGCA; > unmapped. So if others run into this same error, just add in some extra columns to the `features.tsv` file so it doesn't error out when looking for the extra columns. Something like this (different features file):. >RP11-34P13.7	RP11-34P13.7	Gene Expression; >FO538757.3	FO538757.3	Gene Expression; >FO538757.2	FO538757.2	Gene Expression; >AP006222.2	AP006222.2	Gene Expression; >RP4-669L17.10	RP4-669L17.10	Gene Expression. It would also be helpful if scanpy would validate the number of columns at the start. At the moment it looks like it reads in the whole `.mtx` file before trying to map the feature names and producing this error, so it takes a while to fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1286404697
https://github.com/scverse/scanpy/issues/1916#issuecomment-1286404697:1071,Availability,error,error,1071,"I've figured out what was causing my error. The scanpy function to read in `features.tsv.gz` expects three columns: `['gene_symbols, 'gene_ids', 'feature_types']`; Where 'feature types' is a text string like 'Gene Expression' and usually repeated along the whole length of the file.; The file I was reading in was from HTO data and only had one column:. > Hashtag1-GTCAACTCTTTAGCG; > Hashtag2-TTCCGCCTCTCTTTG; > Hashtag3-AAGTATCGTTTCGCA; > unmapped. So if others run into this same error, just add in some extra columns to the `features.tsv` file so it doesn't error out when looking for the extra columns. Something like this (different features file):. >RP11-34P13.7	RP11-34P13.7	Gene Expression; >FO538757.3	FO538757.3	Gene Expression; >FO538757.2	FO538757.2	Gene Expression; >AP006222.2	AP006222.2	Gene Expression; >RP4-669L17.10	RP4-669L17.10	Gene Expression. It would also be helpful if scanpy would validate the number of columns at the start. At the moment it looks like it reads in the whole `.mtx` file before trying to map the feature names and producing this error, so it takes a while to fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1286404697
https://github.com/scverse/scanpy/issues/1916#issuecomment-1286404697:906,Security,validat,validate,906,"I've figured out what was causing my error. The scanpy function to read in `features.tsv.gz` expects three columns: `['gene_symbols, 'gene_ids', 'feature_types']`; Where 'feature types' is a text string like 'Gene Expression' and usually repeated along the whole length of the file.; The file I was reading in was from HTO data and only had one column:. > Hashtag1-GTCAACTCTTTAGCG; > Hashtag2-TTCCGCCTCTCTTTG; > Hashtag3-AAGTATCGTTTCGCA; > unmapped. So if others run into this same error, just add in some extra columns to the `features.tsv` file so it doesn't error out when looking for the extra columns. Something like this (different features file):. >RP11-34P13.7	RP11-34P13.7	Gene Expression; >FO538757.3	FO538757.3	Gene Expression; >FO538757.2	FO538757.2	Gene Expression; >AP006222.2	AP006222.2	Gene Expression; >RP4-669L17.10	RP4-669L17.10	Gene Expression. It would also be helpful if scanpy would validate the number of columns at the start. At the moment it looks like it reads in the whole `.mtx` file before trying to map the feature names and producing this error, so it takes a while to fail.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1286404697
https://github.com/scverse/scanpy/issues/1916#issuecomment-1296918392:222,Availability,error,error,222,"Faced exactly the same problem with file ""GSE185477_GSM3178784_C41_SC_raw_counts.zip"" from [GSE185477](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE185477). The advice from @hurleyLi helped, thanks a lot! But the error is quite confusing. It would be great to read files without extra actions for the user, but is it possible to at least change the error message? E.g.; ```python; try:; ....; except KeyError:; raise KeyError(""Unexpected error, probably due to Cellranger version. Make sure to unarchive gzipped file coming from Cellranger v2""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1296918392
https://github.com/scverse/scanpy/issues/1916#issuecomment-1296918392:358,Availability,error,error,358,"Faced exactly the same problem with file ""GSE185477_GSM3178784_C41_SC_raw_counts.zip"" from [GSE185477](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE185477). The advice from @hurleyLi helped, thanks a lot! But the error is quite confusing. It would be great to read files without extra actions for the user, but is it possible to at least change the error message? E.g.; ```python; try:; ....; except KeyError:; raise KeyError(""Unexpected error, probably due to Cellranger version. Make sure to unarchive gzipped file coming from Cellranger v2""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1296918392
https://github.com/scverse/scanpy/issues/1916#issuecomment-1296918392:447,Availability,error,error,447,"Faced exactly the same problem with file ""GSE185477_GSM3178784_C41_SC_raw_counts.zip"" from [GSE185477](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE185477). The advice from @hurleyLi helped, thanks a lot! But the error is quite confusing. It would be great to read files without extra actions for the user, but is it possible to at least change the error message? E.g.; ```python; try:; ....; except KeyError:; raise KeyError(""Unexpected error, probably due to Cellranger version. Make sure to unarchive gzipped file coming from Cellranger v2""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1296918392
https://github.com/scverse/scanpy/issues/1916#issuecomment-1296918392:364,Integrability,message,message,364,"Faced exactly the same problem with file ""GSE185477_GSM3178784_C41_SC_raw_counts.zip"" from [GSE185477](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE185477). The advice from @hurleyLi helped, thanks a lot! But the error is quite confusing. It would be great to read files without extra actions for the user, but is it possible to at least change the error message? E.g.; ```python; try:; ....; except KeyError:; raise KeyError(""Unexpected error, probably due to Cellranger version. Make sure to unarchive gzipped file coming from Cellranger v2""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1296918392
https://github.com/scverse/scanpy/issues/1916#issuecomment-1433364355:39,Availability,error,error,39,"> I've figured out what was causing my error. The scanpy function to read in `features.tsv.gz` expects three columns: `['gene_symbols, 'gene_ids', 'feature_types']` Where 'feature types' is a text string like 'Gene Expression' and usually repeated along the whole length of the file. The file I was reading in was from HTO data and only had one column:; > ; > > Hashtag1-GTCAACTCTTTAGCG; > > Hashtag2-TTCCGCCTCTCTTTG; > > Hashtag3-AAGTATCGTTTCGCA; > > unmapped; > ; > So if others run into this same error, just add in some extra columns to the `features.tsv` file so it doesn't error out when looking for the extra columns. Something like this (different features file):; > ; > > RP11-34P13.7	RP11-34P13.7	Gene Expression; > > FO538757.3	FO538757.3	Gene Expression; > > FO538757.2	FO538757.2	Gene Expression; > > AP006222.2	AP006222.2	Gene Expression; > > RP4-669L17.10	RP4-669L17.10	Gene Expression; > ; > It would also be helpful if scanpy would validate the number of columns at the start. At the moment it looks like it reads in the whole `.mtx` file before trying to map the feature names and producing this error, so it takes a while to fail. when you do add those columns it makes the number of genes to 0 while reading it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1433364355
https://github.com/scverse/scanpy/issues/1916#issuecomment-1433364355:500,Availability,error,error,500,"> I've figured out what was causing my error. The scanpy function to read in `features.tsv.gz` expects three columns: `['gene_symbols, 'gene_ids', 'feature_types']` Where 'feature types' is a text string like 'Gene Expression' and usually repeated along the whole length of the file. The file I was reading in was from HTO data and only had one column:; > ; > > Hashtag1-GTCAACTCTTTAGCG; > > Hashtag2-TTCCGCCTCTCTTTG; > > Hashtag3-AAGTATCGTTTCGCA; > > unmapped; > ; > So if others run into this same error, just add in some extra columns to the `features.tsv` file so it doesn't error out when looking for the extra columns. Something like this (different features file):; > ; > > RP11-34P13.7	RP11-34P13.7	Gene Expression; > > FO538757.3	FO538757.3	Gene Expression; > > FO538757.2	FO538757.2	Gene Expression; > > AP006222.2	AP006222.2	Gene Expression; > > RP4-669L17.10	RP4-669L17.10	Gene Expression; > ; > It would also be helpful if scanpy would validate the number of columns at the start. At the moment it looks like it reads in the whole `.mtx` file before trying to map the feature names and producing this error, so it takes a while to fail. when you do add those columns it makes the number of genes to 0 while reading it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1433364355
https://github.com/scverse/scanpy/issues/1916#issuecomment-1433364355:579,Availability,error,error,579,"> I've figured out what was causing my error. The scanpy function to read in `features.tsv.gz` expects three columns: `['gene_symbols, 'gene_ids', 'feature_types']` Where 'feature types' is a text string like 'Gene Expression' and usually repeated along the whole length of the file. The file I was reading in was from HTO data and only had one column:; > ; > > Hashtag1-GTCAACTCTTTAGCG; > > Hashtag2-TTCCGCCTCTCTTTG; > > Hashtag3-AAGTATCGTTTCGCA; > > unmapped; > ; > So if others run into this same error, just add in some extra columns to the `features.tsv` file so it doesn't error out when looking for the extra columns. Something like this (different features file):; > ; > > RP11-34P13.7	RP11-34P13.7	Gene Expression; > > FO538757.3	FO538757.3	Gene Expression; > > FO538757.2	FO538757.2	Gene Expression; > > AP006222.2	AP006222.2	Gene Expression; > > RP4-669L17.10	RP4-669L17.10	Gene Expression; > ; > It would also be helpful if scanpy would validate the number of columns at the start. At the moment it looks like it reads in the whole `.mtx` file before trying to map the feature names and producing this error, so it takes a while to fail. when you do add those columns it makes the number of genes to 0 while reading it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1433364355
https://github.com/scverse/scanpy/issues/1916#issuecomment-1433364355:1114,Availability,error,error,1114,"> I've figured out what was causing my error. The scanpy function to read in `features.tsv.gz` expects three columns: `['gene_symbols, 'gene_ids', 'feature_types']` Where 'feature types' is a text string like 'Gene Expression' and usually repeated along the whole length of the file. The file I was reading in was from HTO data and only had one column:; > ; > > Hashtag1-GTCAACTCTTTAGCG; > > Hashtag2-TTCCGCCTCTCTTTG; > > Hashtag3-AAGTATCGTTTCGCA; > > unmapped; > ; > So if others run into this same error, just add in some extra columns to the `features.tsv` file so it doesn't error out when looking for the extra columns. Something like this (different features file):; > ; > > RP11-34P13.7	RP11-34P13.7	Gene Expression; > > FO538757.3	FO538757.3	Gene Expression; > > FO538757.2	FO538757.2	Gene Expression; > > AP006222.2	AP006222.2	Gene Expression; > > RP4-669L17.10	RP4-669L17.10	Gene Expression; > ; > It would also be helpful if scanpy would validate the number of columns at the start. At the moment it looks like it reads in the whole `.mtx` file before trying to map the feature names and producing this error, so it takes a while to fail. when you do add those columns it makes the number of genes to 0 while reading it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1433364355
https://github.com/scverse/scanpy/issues/1916#issuecomment-1433364355:949,Security,validat,validate,949,"> I've figured out what was causing my error. The scanpy function to read in `features.tsv.gz` expects three columns: `['gene_symbols, 'gene_ids', 'feature_types']` Where 'feature types' is a text string like 'Gene Expression' and usually repeated along the whole length of the file. The file I was reading in was from HTO data and only had one column:; > ; > > Hashtag1-GTCAACTCTTTAGCG; > > Hashtag2-TTCCGCCTCTCTTTG; > > Hashtag3-AAGTATCGTTTCGCA; > > unmapped; > ; > So if others run into this same error, just add in some extra columns to the `features.tsv` file so it doesn't error out when looking for the extra columns. Something like this (different features file):; > ; > > RP11-34P13.7	RP11-34P13.7	Gene Expression; > > FO538757.3	FO538757.3	Gene Expression; > > FO538757.2	FO538757.2	Gene Expression; > > AP006222.2	AP006222.2	Gene Expression; > > RP4-669L17.10	RP4-669L17.10	Gene Expression; > ; > It would also be helpful if scanpy would validate the number of columns at the start. At the moment it looks like it reads in the whole `.mtx` file before trying to map the feature names and producing this error, so it takes a while to fail. when you do add those columns it makes the number of genes to 0 while reading it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1433364355
https://github.com/scverse/scanpy/issues/1916#issuecomment-1551477163:198,Availability,down,download,198,"Hello,. my issue is similar to the ones above, so I hope you can help me with that. I tried to read in some sample data from a liver cell database ([Liver Cell Atlas](https://www.livercellatlas.org/download.php)) with the function sc.read_10x_mtx. When I pass in the data just like that, I get a KeyError: 1. ; I adjusted the file to have two more columns (with numbers 1:x as gene IDs and a feature types column with ""Gene Expression"" in all rows) , but then I get the following Error: ValueError: Length of passed value for var_names is 31054, but this AnnData has shape: (389056, 31053). I think this is because the function treats the column names as values, but I don’t know why. When I look at the features file with pandas, it is displayed correctly. Any suggestions? Thanks a lot in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1551477163
https://github.com/scverse/scanpy/issues/1916#issuecomment-1551477163:480,Availability,Error,Error,480,"Hello,. my issue is similar to the ones above, so I hope you can help me with that. I tried to read in some sample data from a liver cell database ([Liver Cell Atlas](https://www.livercellatlas.org/download.php)) with the function sc.read_10x_mtx. When I pass in the data just like that, I get a KeyError: 1. ; I adjusted the file to have two more columns (with numbers 1:x as gene IDs and a feature types column with ""Gene Expression"" in all rows) , but then I get the following Error: ValueError: Length of passed value for var_names is 31054, but this AnnData has shape: (389056, 31053). I think this is because the function treats the column names as values, but I don’t know why. When I look at the features file with pandas, it is displayed correctly. Any suggestions? Thanks a lot in advance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1551477163
https://github.com/scverse/scanpy/issues/1916#issuecomment-2241799568:657,Availability,down,downstream,657,"> Not sure whether it is resolved, just put here another solution to read_mtx and add to anndata one by one; > ; > ```; > import pandas as pd; > import scanpy as sc; > ```; > ; > ```; > adata = sc.read_mtx('./matrix.mtx'); > adata_bc=pd.read_csv('./barcodes.tsv',header=None); > adata_features=pd.read_csv('./features.tsv',header=None); > adata= adata.T; > adata.obs['cell_id']= adata_bc; > adata.var['gene_name']= adata_features[0].tolist(); > adata.var.index= adata.var['gene_name']; > ```. set the delimiter for the features to tab, then use the second column which contains the gene names and not the gene ensembl id. Using the gene names is better for downstream qc since the scanpy recommended pipeline uses the gene name prefixes to identify mitochondrial genes. ```; adata_features = pd.read_csv('./barcodes.tsv', header = None, delimiter = '\t'); ... # technically don't need to use .values or tolist() since the mtx and features file should ; # have same number of rows resulting in same index in the adata.var and adata_features dataframes. adata.var['gene_name'] = adata_features[1].values; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-2241799568
https://github.com/scverse/scanpy/issues/1916#issuecomment-2241799568:700,Deployability,pipeline,pipeline,700,"> Not sure whether it is resolved, just put here another solution to read_mtx and add to anndata one by one; > ; > ```; > import pandas as pd; > import scanpy as sc; > ```; > ; > ```; > adata = sc.read_mtx('./matrix.mtx'); > adata_bc=pd.read_csv('./barcodes.tsv',header=None); > adata_features=pd.read_csv('./features.tsv',header=None); > adata= adata.T; > adata.obs['cell_id']= adata_bc; > adata.var['gene_name']= adata_features[0].tolist(); > adata.var.index= adata.var['gene_name']; > ```. set the delimiter for the features to tab, then use the second column which contains the gene names and not the gene ensembl id. Using the gene names is better for downstream qc since the scanpy recommended pipeline uses the gene name prefixes to identify mitochondrial genes. ```; adata_features = pd.read_csv('./barcodes.tsv', header = None, delimiter = '\t'); ... # technically don't need to use .values or tolist() since the mtx and features file should ; # have same number of rows resulting in same index in the adata.var and adata_features dataframes. adata.var['gene_name'] = adata_features[1].values; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-2241799568
https://github.com/scverse/scanpy/pull/1918#issuecomment-925478453:87,Availability,error,errors,87,"Just a heads up, it looks like Pandas 1.3.3 might break things again, was experiencing errors that I was able to resolve by downgrading. I can create a new issue if you'd like. Error below so you can determine if this is the same issue or not:; ```; adata.obs['log_counts'] = np.log(adata.obs['n_counts']); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py"", line 3612, in __setitem__; self._set_item(key, value); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py"", line 3784, in _set_item; value = self._sanitize_column(value); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py"", line 4509, in _sanitize_column; com.require_length_match(value, self.index); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/common.py"", line 531, in require_length_match; raise ValueError(; ValueError: Length of values (1) does not match length of index (38978); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1918#issuecomment-925478453
https://github.com/scverse/scanpy/pull/1918#issuecomment-925478453:124,Availability,down,downgrading,124,"Just a heads up, it looks like Pandas 1.3.3 might break things again, was experiencing errors that I was able to resolve by downgrading. I can create a new issue if you'd like. Error below so you can determine if this is the same issue or not:; ```; adata.obs['log_counts'] = np.log(adata.obs['n_counts']); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py"", line 3612, in __setitem__; self._set_item(key, value); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py"", line 3784, in _set_item; value = self._sanitize_column(value); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py"", line 4509, in _sanitize_column; com.require_length_match(value, self.index); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/common.py"", line 531, in require_length_match; raise ValueError(; ValueError: Length of values (1) does not match length of index (38978); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1918#issuecomment-925478453
https://github.com/scverse/scanpy/pull/1918#issuecomment-925478453:177,Availability,Error,Error,177,"Just a heads up, it looks like Pandas 1.3.3 might break things again, was experiencing errors that I was able to resolve by downgrading. I can create a new issue if you'd like. Error below so you can determine if this is the same issue or not:; ```; adata.obs['log_counts'] = np.log(adata.obs['n_counts']); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py"", line 3612, in __setitem__; self._set_item(key, value); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py"", line 3784, in _set_item; value = self._sanitize_column(value); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py"", line 4509, in _sanitize_column; com.require_length_match(value, self.index); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/common.py"", line 531, in require_length_match; raise ValueError(; ValueError: Length of values (1) does not match length of index (38978); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1918#issuecomment-925478453
https://github.com/scverse/scanpy/pull/1918#issuecomment-925478453:279,Testability,log,log,279,"Just a heads up, it looks like Pandas 1.3.3 might break things again, was experiencing errors that I was able to resolve by downgrading. I can create a new issue if you'd like. Error below so you can determine if this is the same issue or not:; ```; adata.obs['log_counts'] = np.log(adata.obs['n_counts']); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py"", line 3612, in __setitem__; self._set_item(key, value); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py"", line 3784, in _set_item; value = self._sanitize_column(value); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py"", line 4509, in _sanitize_column; com.require_length_match(value, self.index); File ""/opt/conda/lib/python3.8/site-packages/pandas/core/common.py"", line 531, in require_length_match; raise ValueError(; ValueError: Length of values (1) does not match length of index (38978); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1918#issuecomment-925478453
https://github.com/scverse/scanpy/pull/1920#issuecomment-873816651:328,Availability,error,error-reference,328,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1920?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@8750212`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1920 +/- ##; ========================================; Coverage ? 71.60% ; ========================================; Files ? 92 ; Lines ? 11248 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3194 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1920#issuecomment-873816651
https://github.com/scverse/scanpy/pull/1920#issuecomment-873816651:276,Usability,learn,learn,276,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1920?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@8750212`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1920 +/- ##; ========================================; Coverage ? 71.60% ; ========================================; Files ? 92 ; Lines ? 11248 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3194 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1920#issuecomment-873816651
https://github.com/scverse/scanpy/pull/1923#issuecomment-873930176:328,Availability,error,error-reference,328,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1923?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@d0f851f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1923 +/- ##; ========================================; Coverage ? 71.60% ; ========================================; Files ? 92 ; Lines ? 11248 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3194 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1923#issuecomment-873930176
https://github.com/scverse/scanpy/pull/1923#issuecomment-873930176:276,Usability,learn,learn,276,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1923?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@d0f851f`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1923 +/- ##; ========================================; Coverage ? 71.60% ; ========================================; Files ? 92 ; Lines ? 11248 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3194 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1923#issuecomment-873930176
https://github.com/scverse/scanpy/pull/1924#issuecomment-873973489:328,Availability,error,error-reference,328,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1924?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@1605f63`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1924 +/- ##; ========================================; Coverage ? 71.60% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8053 ; Misses ? 3193 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1924#issuecomment-873973489
https://github.com/scverse/scanpy/pull/1924#issuecomment-873973489:276,Usability,learn,learn,276,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1924?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@1605f63`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1924 +/- ##; ========================================; Coverage ? 71.60% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8053 ; Misses ? 3193 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1924#issuecomment-873973489
https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964:455,Availability,avail,available,455,"We probably have two problems:. 1. CI doesn’t run; 2. Scanpy got harder to use for people. I think the first [is easy to fix](https://github.com/numba/numba/blob/c13c840a8f1f038c1e78472db472a8f19a0bd564/numba/core/config.py#L309): We just `export NUMBA_THREADING_LAYER=workqueue` in our tests. The second is harder, but first I want to note something:. > This was fine in the past, since pynndescent/ umap were forcing a workqueue backend which is always available. I wouldn’t call that situation *fine*, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. This means our solution for the second shouldn’t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. #1933 only fixes CI … also bad issue number, yikes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964
https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964:214,Modifiability,config,config,214,"We probably have two problems:. 1. CI doesn’t run; 2. Scanpy got harder to use for people. I think the first [is easy to fix](https://github.com/numba/numba/blob/c13c840a8f1f038c1e78472db472a8f19a0bd564/numba/core/config.py#L309): We just `export NUMBA_THREADING_LAYER=workqueue` in our tests. The second is harder, but first I want to note something:. > This was fine in the past, since pynndescent/ umap were forcing a workqueue backend which is always available. I wouldn’t call that situation *fine*, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. This means our solution for the second shouldn’t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. #1933 only fixes CI … also bad issue number, yikes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964
https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964:575,Modifiability,config,configurable,575,"We probably have two problems:. 1. CI doesn’t run; 2. Scanpy got harder to use for people. I think the first [is easy to fix](https://github.com/numba/numba/blob/c13c840a8f1f038c1e78472db472a8f19a0bd564/numba/core/config.py#L309): We just `export NUMBA_THREADING_LAYER=workqueue` in our tests. The second is harder, but first I want to note something:. > This was fine in the past, since pynndescent/ umap were forcing a workqueue backend which is always available. I wouldn’t call that situation *fine*, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. This means our solution for the second shouldn’t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. #1933 only fixes CI … also bad issue number, yikes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964
https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964:735,Modifiability,config,configurable,735,"We probably have two problems:. 1. CI doesn’t run; 2. Scanpy got harder to use for people. I think the first [is easy to fix](https://github.com/numba/numba/blob/c13c840a8f1f038c1e78472db472a8f19a0bd564/numba/core/config.py#L309): We just `export NUMBA_THREADING_LAYER=workqueue` in our tests. The second is harder, but first I want to note something:. > This was fine in the past, since pynndescent/ umap were forcing a workqueue backend which is always available. I wouldn’t call that situation *fine*, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. This means our solution for the second shouldn’t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. #1933 only fixes CI … also bad issue number, yikes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964
https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964:287,Testability,test,tests,287,"We probably have two problems:. 1. CI doesn’t run; 2. Scanpy got harder to use for people. I think the first [is easy to fix](https://github.com/numba/numba/blob/c13c840a8f1f038c1e78472db472a8f19a0bd564/numba/core/config.py#L309): We just `export NUMBA_THREADING_LAYER=workqueue` in our tests. The second is harder, but first I want to note something:. > This was fine in the past, since pynndescent/ umap were forcing a workqueue backend which is always available. I wouldn’t call that situation *fine*, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. This means our solution for the second shouldn’t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. #1933 only fixes CI … also bad issue number, yikes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964
https://github.com/scverse/scanpy/issues/1931#issuecomment-874655687:59,Availability,avail,available,59,> Numba can’t correctly detect when a threading backend is available. Is there a numba issue for this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874655687
https://github.com/scverse/scanpy/issues/1931#issuecomment-874655687:24,Safety,detect,detect,24,> Numba can’t correctly detect when a threading backend is available. Is there a numba issue for this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874655687
https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678:621,Integrability,depend,dependency,621,"> I wouldn’t call that situation fine, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. It was fine in that it made UMAP work more places than it would have otherwise. > This means our solution for the second shouldn’t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. I don't really want to have to touch it, especially since I believe it's immutable after any parallel code is called – so we can't actually control in. Our hands are a little tied by having a dependency do this. I would like to know why this is still considered necessary for pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678
https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678:109,Modifiability,config,configurable,109,"> I wouldn’t call that situation fine, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. It was fine in that it made UMAP work more places than it would have otherwise. > This means our solution for the second shouldn’t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. I don't really want to have to touch it, especially since I believe it's immutable after any parallel code is called – so we can't actually control in. Our hands are a little tied by having a dependency do this. I would like to know why this is still considered necessary for pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678
https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678:351,Modifiability,config,configurable,351,"> I wouldn’t call that situation fine, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. It was fine in that it made UMAP work more places than it would have otherwise. > This means our solution for the second shouldn’t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. I don't really want to have to touch it, especially since I believe it's immutable after any parallel code is called – so we can't actually control in. Our hands are a little tied by having a dependency do this. I would like to know why this is still considered necessary for pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678
https://github.com/scverse/scanpy/issues/1931#issuecomment-874667061:34,Usability,simpl,simply,34,"The way pynndescent does it looks simply wrong: They knew that one specific backend caused problems, but instead of changing it only when that backend is set, they do it indiscriminately, which is pretty rude. I might be missiong something, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874667061
https://github.com/scverse/scanpy/issues/1931#issuecomment-874667150:61,Availability,avail,available,61,"> > Numba can’t correctly detect when a threading backend is available; >; > Is there a numba issue for this?. https://github.com/numba/numba/issues/6108, This issue may be specific to `tbb`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874667150
https://github.com/scverse/scanpy/issues/1931#issuecomment-874667150:26,Safety,detect,detect,26,"> > Numba can’t correctly detect when a threading backend is available; >; > Is there a numba issue for this?. https://github.com/numba/numba/issues/6108, This issue may be specific to `tbb`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874667150
https://github.com/scverse/scanpy/issues/1931#issuecomment-874674744:304,Modifiability,layers,layers,304,"> I might be missiong something, though. I personally have only had problems with the intel backends on my MacBook. I see that pinning the parallel backend would be a good way to make the code work for more people. Especially if this used to be worse. I think the capabilities of the different threading layers have also been tweaked over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874674744
https://github.com/scverse/scanpy/issues/1931#issuecomment-875409508:79,Deployability,release,release,79,"Fixed by reverting the change on pynndescent, so users should just get the new release when they install",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-875409508
https://github.com/scverse/scanpy/issues/1931#issuecomment-875409508:97,Deployability,install,install,97,"Fixed by reverting the change on pynndescent, so users should just get the new release when they install",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-875409508
https://github.com/scverse/scanpy/issues/1932#issuecomment-874647705:110,Testability,log,logging,110,Thanks for the report! Could you let us know a little bit about your environment by showing the output of `sc.logging.print_versions()`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-874647705
https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246:64,Availability,error,error,64,"Hello,; This command (sc.logging.print_versions()) gives me the error pasted below:; AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-19-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246
https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246:256,Integrability,depend,dependencies,256,"Hello,; This command (sc.logging.print_versions()) gives me the error pasted below:; AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-19-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246
https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246:881,Integrability,depend,dependencies,881,"Hello,; This command (sc.logging.print_versions()) gives me the error pasted below:; AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-19-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246
https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246:1061,Integrability,depend,dependencies,1061,"Hello,; This command (sc.logging.print_versions()) gives me the error pasted below:; AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-19-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246
https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246:25,Testability,log,logging,25,"Hello,; This command (sc.logging.print_versions()) gives me the error pasted below:; AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-19-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246
https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246:674,Testability,log,logging,674,"Hello,; This command (sc.logging.print_versions()) gives me the error pasted below:; AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-19-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246
https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246:783,Testability,log,logging,783,"Hello,; This command (sc.logging.print_versions()) gives me the error pasted below:; AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-19-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246
https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028:92,Availability,error,error,92,"Hello, ; I have run this command again in the fresh conda environment. Again I get the same error as before. AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-3-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028
https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028:280,Integrability,depend,dependencies,280,"Hello, ; I have run this command again in the fresh conda environment. Again I get the same error as before. AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-3-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028
https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028:904,Integrability,depend,dependencies,904,"Hello, ; I have run this command again in the fresh conda environment. Again I get the same error as before. AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-3-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028
https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028:1084,Integrability,depend,dependencies,1084,"Hello, ; I have run this command again in the fresh conda environment. Again I get the same error as before. AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-3-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028
https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028:697,Testability,log,logging,697,"Hello, ; I have run this command again in the fresh conda environment. Again I get the same error as before. AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-3-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028
https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028:806,Testability,log,logging,806,"Hello, ; I have run this command again in the fresh conda environment. Again I get the same error as before. AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-3-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028
https://github.com/scverse/scanpy/pull/1933#issuecomment-874659048:95,Modifiability,variab,variable,95,I think pynndescent setting the value at import time will take precedence over the environment variable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1933#issuecomment-874659048
https://github.com/scverse/scanpy/pull/1933#issuecomment-874661288:46,Usability,undo,undo,46,"Oh. Well, that’s just not right. They need to undo that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1933#issuecomment-874661288
https://github.com/scverse/scanpy/pull/1933#issuecomment-874663220:129,Modifiability,config,config,129,"I think it's reasonable that the runtime gets the final say. It's pretty standard for precedence to go: `runtime > environment > config file`, right? I don't think it's reasonable for a library to make the decision, as it should be done by the program.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1933#issuecomment-874663220
https://github.com/scverse/scanpy/pull/1935#issuecomment-875351624:328,Availability,error,error-reference,328,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1935?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@8c51f19`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1935 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1935#issuecomment-875351624
https://github.com/scverse/scanpy/pull/1935#issuecomment-875351624:276,Usability,learn,learn,276,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1935?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@8c51f19`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1935 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1935#issuecomment-875351624
https://github.com/scverse/scanpy/pull/1936#issuecomment-875418541:36,Usability,clear,clearer,36,Looks good to me! Docstring is also clearer now. Do you think it's worth adding a warning at a hard minimum threshold of 5-10 cells per category or sth like this? Although it would probably also just look strange to anyone plotting it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1936#issuecomment-875418541
https://github.com/scverse/scanpy/pull/1936#issuecomment-961318640:37,Deployability,release,release,37,"Bumped so it's up to date, and added release note",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1936#issuecomment-961318640
https://github.com/scverse/scanpy/pull/1938#issuecomment-875422302:328,Availability,error,error-reference,328,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1938?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@2883269`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1938 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1938#issuecomment-875422302
https://github.com/scverse/scanpy/pull/1938#issuecomment-875422302:276,Usability,learn,learn,276,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1938?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@2883269`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1938 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1938#issuecomment-875422302
https://github.com/scverse/scanpy/pull/1939#issuecomment-876054434:546,Deployability,continuous,continuous,546,"I think this is a really good idea. Definitely a good hold over until a real GroupBy object is worked out. I'll review in depth later but have a few thoughts:. * How about `split_by`? Rhymes with `group_by` which this is quite like.; * I would like to be able to do this along the `var` axis; * What do you think about including some of the grouping options from [`_prepare_dataframe`](https://github.com/theislab/scanpy/blob/d072abd05bda07f280ea91f5e7e4a84f9782c118/scanpy/plotting/_anndata.py#L1842)? E.g. grouping by multiple keys, binning by continuous values? It would be nice if this could be used in place of `_prepare_dataframe`. The features would not have to be implemented in this PR, but it would be good to think about what the API for them would be and making sure it could be added elegantly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1939#issuecomment-876054434
https://github.com/scverse/scanpy/issues/1940#issuecomment-877836520:149,Availability,error,error,149,"> `batch1.X.mean(1)` should give you the desired result.; > Or maybe i don't get what you actually need. Thanks for the reply. Sometimes I got index error when I used the subset data view for further analysis, saying the dimension was not matched. ; It could be solved by assigning .copy() when subsetting the adata. So I was just wondering if any memory-efficient way to do this also?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1940#issuecomment-877836520
https://github.com/scverse/scanpy/issues/1940#issuecomment-877836520:355,Energy Efficiency,efficient,efficient,355,"> `batch1.X.mean(1)` should give you the desired result.; > Or maybe i don't get what you actually need. Thanks for the reply. Sometimes I got index error when I used the subset data view for further analysis, saying the dimension was not matched. ; It could be solved by assigning .copy() when subsetting the adata. So I was just wondering if any memory-efficient way to do this also?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1940#issuecomment-877836520
https://github.com/scverse/scanpy/issues/1941#issuecomment-877024437:107,Deployability,update,update,107,"yes, apparently the problem is associated with these 'nan' values. I assumed this has to do with a package update, because it was working before. I rolled back to scanpy 1.7.1 and this block is working fine. The 'nan's are just omitted in the plots. I appreciate you offered to give a code example as my python skills are limited. But I figured a workaround so you don't have to bother with it. . I won't close the issue yet because I assume this behavior was not intended with the update.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1941#issuecomment-877024437
https://github.com/scverse/scanpy/issues/1941#issuecomment-877024437:482,Deployability,update,update,482,"yes, apparently the problem is associated with these 'nan' values. I assumed this has to do with a package update, because it was working before. I rolled back to scanpy 1.7.1 and this block is working fine. The 'nan's are just omitted in the plots. I appreciate you offered to give a code example as my python skills are limited. But I figured a workaround so you don't have to bother with it. . I won't close the issue yet because I assume this behavior was not intended with the update.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1941#issuecomment-877024437
https://github.com/scverse/scanpy/pull/1942#issuecomment-877144170:31,Testability,test,tests,31,No idea why the unrelated plot tests fail.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-877144170
https://github.com/scverse/scanpy/pull/1942#issuecomment-878118310:114,Integrability,depend,depend,114,I am not sure what king of test. I don't want to add another `save_and_compare_images` test because plots seem to depend on the system at least sometimes (i have 3 failing plotting tests locally but they run fine here).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878118310
https://github.com/scverse/scanpy/pull/1942#issuecomment-878118310:27,Testability,test,test,27,I am not sure what king of test. I don't want to add another `save_and_compare_images` test because plots seem to depend on the system at least sometimes (i have 3 failing plotting tests locally but they run fine here).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878118310
https://github.com/scverse/scanpy/pull/1942#issuecomment-878118310:87,Testability,test,test,87,I am not sure what king of test. I don't want to add another `save_and_compare_images` test because plots seem to depend on the system at least sometimes (i have 3 failing plotting tests locally but they run fine here).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878118310
https://github.com/scverse/scanpy/pull/1942#issuecomment-878118310:181,Testability,test,tests,181,I am not sure what king of test. I don't want to add another `save_and_compare_images` test because plots seem to depend on the system at least sometimes (i have 3 failing plotting tests locally but they run fine here).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878118310
https://github.com/scverse/scanpy/pull/1942#issuecomment-878118978:41,Testability,test,tests,41,"This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878118978
https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:812,Availability,resilien,resilient,812,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649
https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:155,Deployability,release,release,155,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649
https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:131,Integrability,depend,dependency,131,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649
https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:291,Integrability,depend,depend,291,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649
https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:43,Testability,test,tests,43,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649
https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:206,Testability,test,test,206,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649
https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:264,Testability,test,test,264,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649
https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:600,Testability,test,tests,600,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649
https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:671,Testability,test,tests,671,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649
https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:758,Testability,test,tests,758,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649
https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:803,Testability,test,tests,803,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649
https://github.com/scverse/scanpy/pull/1942#issuecomment-879832703:124,Testability,test,test,124,"I can add the changes you suggested but. >since if there was a bug in how `filter_rank_genes_groups` sets those values this test would still pass. this is not a test of `filter_rank_genes_groups`, we have a separate test for this. Do you think we need to double check this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-879832703
https://github.com/scverse/scanpy/pull/1942#issuecomment-879832703:161,Testability,test,test,161,"I can add the changes you suggested but. >since if there was a bug in how `filter_rank_genes_groups` sets those values this test would still pass. this is not a test of `filter_rank_genes_groups`, we have a separate test for this. Do you think we need to double check this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-879832703
https://github.com/scverse/scanpy/pull/1942#issuecomment-879832703:216,Testability,test,test,216,"I can add the changes you suggested but. >since if there was a bug in how `filter_rank_genes_groups` sets those values this test would still pass. this is not a test of `filter_rank_genes_groups`, we have a separate test for this. Do you think we need to double check this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-879832703
https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601:411,Modifiability,flexible,flexible,411,"> this is not a test of `filter_rank_genes_groups`. To me the point of the test is that `filter_rank_genes_groups` followed by `.pl.rank_genes_groups_*` returns the right image by comparing it to a reference. I mainly prefer that the reference is generated by as orthogonal a method as possible, since then we're closer to testing functionality than implementation. I also think that this strategy forms a more flexible implementation that can be extended to test other properties. For example, it's easy to modify this to check that the `sc.pl.rank_genes_groups_*(..., min_logfoldchange=)` argument plays well with `filter_rank_genes_groups`. I also like having an example of an equivalent implementation in the test suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601
https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601:447,Modifiability,extend,extended,447,"> this is not a test of `filter_rank_genes_groups`. To me the point of the test is that `filter_rank_genes_groups` followed by `.pl.rank_genes_groups_*` returns the right image by comparing it to a reference. I mainly prefer that the reference is generated by as orthogonal a method as possible, since then we're closer to testing functionality than implementation. I also think that this strategy forms a more flexible implementation that can be extended to test other properties. For example, it's easy to modify this to check that the `sc.pl.rank_genes_groups_*(..., min_logfoldchange=)` argument plays well with `filter_rank_genes_groups`. I also like having an example of an equivalent implementation in the test suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601
https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601:16,Testability,test,test,16,"> this is not a test of `filter_rank_genes_groups`. To me the point of the test is that `filter_rank_genes_groups` followed by `.pl.rank_genes_groups_*` returns the right image by comparing it to a reference. I mainly prefer that the reference is generated by as orthogonal a method as possible, since then we're closer to testing functionality than implementation. I also think that this strategy forms a more flexible implementation that can be extended to test other properties. For example, it's easy to modify this to check that the `sc.pl.rank_genes_groups_*(..., min_logfoldchange=)` argument plays well with `filter_rank_genes_groups`. I also like having an example of an equivalent implementation in the test suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601
https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601:75,Testability,test,test,75,"> this is not a test of `filter_rank_genes_groups`. To me the point of the test is that `filter_rank_genes_groups` followed by `.pl.rank_genes_groups_*` returns the right image by comparing it to a reference. I mainly prefer that the reference is generated by as orthogonal a method as possible, since then we're closer to testing functionality than implementation. I also think that this strategy forms a more flexible implementation that can be extended to test other properties. For example, it's easy to modify this to check that the `sc.pl.rank_genes_groups_*(..., min_logfoldchange=)` argument plays well with `filter_rank_genes_groups`. I also like having an example of an equivalent implementation in the test suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601
https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601:323,Testability,test,testing,323,"> this is not a test of `filter_rank_genes_groups`. To me the point of the test is that `filter_rank_genes_groups` followed by `.pl.rank_genes_groups_*` returns the right image by comparing it to a reference. I mainly prefer that the reference is generated by as orthogonal a method as possible, since then we're closer to testing functionality than implementation. I also think that this strategy forms a more flexible implementation that can be extended to test other properties. For example, it's easy to modify this to check that the `sc.pl.rank_genes_groups_*(..., min_logfoldchange=)` argument plays well with `filter_rank_genes_groups`. I also like having an example of an equivalent implementation in the test suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601
https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601:459,Testability,test,test,459,"> this is not a test of `filter_rank_genes_groups`. To me the point of the test is that `filter_rank_genes_groups` followed by `.pl.rank_genes_groups_*` returns the right image by comparing it to a reference. I mainly prefer that the reference is generated by as orthogonal a method as possible, since then we're closer to testing functionality than implementation. I also think that this strategy forms a more flexible implementation that can be extended to test other properties. For example, it's easy to modify this to check that the `sc.pl.rank_genes_groups_*(..., min_logfoldchange=)` argument plays well with `filter_rank_genes_groups`. I also like having an example of an equivalent implementation in the test suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601
https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601:713,Testability,test,test,713,"> this is not a test of `filter_rank_genes_groups`. To me the point of the test is that `filter_rank_genes_groups` followed by `.pl.rank_genes_groups_*` returns the right image by comparing it to a reference. I mainly prefer that the reference is generated by as orthogonal a method as possible, since then we're closer to testing functionality than implementation. I also think that this strategy forms a more flexible implementation that can be extended to test other properties. For example, it's easy to modify this to check that the `sc.pl.rank_genes_groups_*(..., min_logfoldchange=)` argument plays well with `filter_rank_genes_groups`. I also like having an example of an equivalent implementation in the test suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:45,Availability,error,error,45,"Changing it to a property throws a different error:. <details>; <summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:638,Availability,error,error,638,"Changing it to a property throws a different error:. <details>; <summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:662,Availability,error,error,662,"Changing it to a property throws a different error:. <details>; <summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:821,Availability,Error,Error,821,"Changing it to a property throws a different error:. <details>; <summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:1013,Deployability,release,release,1013,"throws a different error:. <details>; <summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5) from /usr/local/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:1275,Deployability,release,release,1275,"orm; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/serializinghtml/__init__.py; # sphinxcontrib.qthelp (1.0.3) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/qthelp/__init__.py; # alabaster (0.7.12) from /usr/local/lib/python3.8/site-packages/alabaster/__init__.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:668,Integrability,message,message,668,"Changing it to a property throws a different error:. <details>; <summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:1055,Integrability,message,messages,1055,"<summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/seri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:6897,Integrability,message,messages,6897,".read(self.source, self.parser,; File ""/usr/local/lib/python3.8/site-packages/sphinx/io.py"", line 108, in read; self.parse(); File ""/usr/local/lib/python3.8/site-packages/docutils/readers/__init__.py"", line 77, in parse; self.parser.parse(self.input, document); File ""/usr/local/lib/python3.8/site-packages/sphinx/parsers.py"", line 100, in parse; self.statemachine.run(inputlines, document, inliner=self.inliner); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 170, in run; results = StateMachineWS.run(self, input_lines, input_offset,; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 241, in run; context, next_state, result = self.check_line(; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 459, in check_line; return method(match, context, next_state); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2769, in underline; self.section(title, source, style, lineno - 1, messages); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 327, in section; self.new_subsection(title, lineno, messages); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 393, in new_subsection; newabsoffset = self.nested_parse(; File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 281, in nested_parse; state_machine.run(block, input_offset, memo=self.memo,; File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 196, in run; results = StateMachineWS.run(self, input_lines, input_offset); File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 241, in run; context, next_state, result = self.check_line(; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 459, in check_line; return method(match, context, next_state); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2344, in explicit_m",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:7043,Integrability,message,messages,7043,"n3.8/site-packages/docutils/readers/__init__.py"", line 77, in parse; self.parser.parse(self.input, document); File ""/usr/local/lib/python3.8/site-packages/sphinx/parsers.py"", line 100, in parse; self.statemachine.run(inputlines, document, inliner=self.inliner); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 170, in run; results = StateMachineWS.run(self, input_lines, input_offset,; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 241, in run; context, next_state, result = self.check_line(; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 459, in check_line; return method(match, context, next_state); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2769, in underline; self.section(title, source, style, lineno - 1, messages); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 327, in section; self.new_subsection(title, lineno, messages); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 393, in new_subsection; newabsoffset = self.nested_parse(; File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 281, in nested_parse; state_machine.run(block, input_offset, memo=self.memo,; File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 196, in run; results = StateMachineWS.run(self, input_lines, input_offset); File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 241, in run; context, next_state, result = self.check_line(; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 459, in check_line; return method(match, context, next_state); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2344, in explicit_markup; self.explicit_list(blank_finish); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2369, in explicit_list; newl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:1508,Performance,Load,Loaded,1508,"/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/serializinghtml/__init__.py; # sphinxcontrib.qthelp (1.0.3) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/qthelp/__init__.py; # alabaster (0.7.12) from /usr/local/lib/python3.8/site-packages/alabaster/__init__.py; # sphinx.ext.autodoc.preserve_defaults (1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/autodoc/preserve_defaults.py; # sphinx.ext.autodoc.type_comment (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/aut",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:540,Testability,log,log,540,"Changing it to a property throws a different error:. <details>; <summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:895,Testability,log,log,895,"Changing it to a property throws a different error:. <details>; <summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:1315,Testability,test,testing,1315,"eldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/serializinghtml/__init__.py; # sphinxcontrib.qthelp (1.0.3) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/qthelp/__init__.py; # alabaster (0.7.12) from /usr/local/lib/python3.8/site-packages/alabaster/__init__.py; # sphinx.ext.autodoc.preserve_defaul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557
https://github.com/scverse/scanpy/pull/1947#issuecomment-878124023:328,Availability,error,error-reference,328,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1947?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@9360422`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1947 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1947#issuecomment-878124023
https://github.com/scverse/scanpy/pull/1947#issuecomment-878124023:276,Usability,learn,learn,276,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1947?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@9360422`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1947 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1947#issuecomment-878124023
https://github.com/scverse/scanpy/pull/1948#issuecomment-880405295:931,Availability,down,down,931,"So the issue is incompatibilities between versions of sphinx and their `objects.inv`? Is there an open issue in sphinx for this?. > > Do you expect this to be compatible with older sphinx versions?; >; > Of course! Why would it not be?. Mostly because the inner workings of Sphinx are a mystery to me, so I have no idea what features your changes rely on 😆. This was mainly me asking if we should bump the minimum version of sphinx allowed. Maybe we should if there are issues with the `objects.inv`s?. > Maybe we can link to the dev docs?. Or we could add the classes to nitpick ignore? Then once the docs are rebuilt it will do the right thing without any intervention, and we don't have to be keeping an eye out for this. My main concern here is that the `scipy.github.io` address may not be permanent, similar to how numpy temporarily used a github.io address while they revamped their docs. Basically, it may just break or go down without notice. I thought we could even just trigger a new build of the anndata stable docs, but there's an issue there, probably to do with sphinx not being pinned on release. I do want to make a new anndata release soon-ish though. -----------. I'm happy for you to pick one of the approaches and merge it. AnnData could also use a fix for this, I've temporarily just pinned sphinx below 4.1 there too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1948#issuecomment-880405295
https://github.com/scverse/scanpy/pull/1948#issuecomment-880405295:1104,Deployability,release,release,1104,"So the issue is incompatibilities between versions of sphinx and their `objects.inv`? Is there an open issue in sphinx for this?. > > Do you expect this to be compatible with older sphinx versions?; >; > Of course! Why would it not be?. Mostly because the inner workings of Sphinx are a mystery to me, so I have no idea what features your changes rely on 😆. This was mainly me asking if we should bump the minimum version of sphinx allowed. Maybe we should if there are issues with the `objects.inv`s?. > Maybe we can link to the dev docs?. Or we could add the classes to nitpick ignore? Then once the docs are rebuilt it will do the right thing without any intervention, and we don't have to be keeping an eye out for this. My main concern here is that the `scipy.github.io` address may not be permanent, similar to how numpy temporarily used a github.io address while they revamped their docs. Basically, it may just break or go down without notice. I thought we could even just trigger a new build of the anndata stable docs, but there's an issue there, probably to do with sphinx not being pinned on release. I do want to make a new anndata release soon-ish though. -----------. I'm happy for you to pick one of the approaches and merge it. AnnData could also use a fix for this, I've temporarily just pinned sphinx below 4.1 there too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1948#issuecomment-880405295
https://github.com/scverse/scanpy/pull/1948#issuecomment-880405295:1145,Deployability,release,release,1145,"So the issue is incompatibilities between versions of sphinx and their `objects.inv`? Is there an open issue in sphinx for this?. > > Do you expect this to be compatible with older sphinx versions?; >; > Of course! Why would it not be?. Mostly because the inner workings of Sphinx are a mystery to me, so I have no idea what features your changes rely on 😆. This was mainly me asking if we should bump the minimum version of sphinx allowed. Maybe we should if there are issues with the `objects.inv`s?. > Maybe we can link to the dev docs?. Or we could add the classes to nitpick ignore? Then once the docs are rebuilt it will do the right thing without any intervention, and we don't have to be keeping an eye out for this. My main concern here is that the `scipy.github.io` address may not be permanent, similar to how numpy temporarily used a github.io address while they revamped their docs. Basically, it may just break or go down without notice. I thought we could even just trigger a new build of the anndata stable docs, but there's an issue there, probably to do with sphinx not being pinned on release. I do want to make a new anndata release soon-ish though. -----------. I'm happy for you to pick one of the approaches and merge it. AnnData could also use a fix for this, I've temporarily just pinned sphinx below 4.1 there too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1948#issuecomment-880405295
https://github.com/scverse/scanpy/issues/1949#issuecomment-879247652:185,Deployability,pipeline,pipeline,185,"I've ran into this before (`sc.read_10x_mtx()` has the same default of course). One possible issue with defaulting to `gex_only=False` is that someone might accidentally run a 'regular pipeline' with multi-modal data, e.g. log-normalizing RNA+protein+cell hashing counts together without first subsetting the adata based on `.var[""feature_types""]`. By contrast, anyone who know they have multi-modal data would hopefully notice the missing the data with `gex_only=True`. Either way, logging warnings sounds good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1949#issuecomment-879247652
https://github.com/scverse/scanpy/issues/1949#issuecomment-879247652:256,Security,hash,hashing,256,"I've ran into this before (`sc.read_10x_mtx()` has the same default of course). One possible issue with defaulting to `gex_only=False` is that someone might accidentally run a 'regular pipeline' with multi-modal data, e.g. log-normalizing RNA+protein+cell hashing counts together without first subsetting the adata based on `.var[""feature_types""]`. By contrast, anyone who know they have multi-modal data would hopefully notice the missing the data with `gex_only=True`. Either way, logging warnings sounds good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1949#issuecomment-879247652
https://github.com/scverse/scanpy/issues/1949#issuecomment-879247652:223,Testability,log,log-normalizing,223,"I've ran into this before (`sc.read_10x_mtx()` has the same default of course). One possible issue with defaulting to `gex_only=False` is that someone might accidentally run a 'regular pipeline' with multi-modal data, e.g. log-normalizing RNA+protein+cell hashing counts together without first subsetting the adata based on `.var[""feature_types""]`. By contrast, anyone who know they have multi-modal data would hopefully notice the missing the data with `gex_only=True`. Either way, logging warnings sounds good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1949#issuecomment-879247652
https://github.com/scverse/scanpy/issues/1949#issuecomment-879247652:483,Testability,log,logging,483,"I've ran into this before (`sc.read_10x_mtx()` has the same default of course). One possible issue with defaulting to `gex_only=False` is that someone might accidentally run a 'regular pipeline' with multi-modal data, e.g. log-normalizing RNA+protein+cell hashing counts together without first subsetting the adata based on `.var[""feature_types""]`. By contrast, anyone who know they have multi-modal data would hopefully notice the missing the data with `gex_only=True`. Either way, logging warnings sounds good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1949#issuecomment-879247652
https://github.com/scverse/scanpy/issues/1949#issuecomment-879616528:323,Availability,Error,Erroring,323,"For context, the option was added in #334, and I think the scope for other feature types was much more limited at the time. > Would it already be worth either making gex_only a required input?. I'm not sure the `gex_only` argument even entirely makes sense anymore. I think a `feature_type` argument would make more sense. Erroring if nothing is passed and there are multiple kinds sounds reasonable to me, as multimodality should be handled explicitly. For backwards compatibility I think deprecation warnings for a release cycle when either `gex_only` is used or nothing is passed and there are multiple feature types present could work. -----------. Moving the 10x reading functions had been discussed in: #1387",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1949#issuecomment-879616528
https://github.com/scverse/scanpy/issues/1949#issuecomment-879616528:517,Deployability,release,release,517,"For context, the option was added in #334, and I think the scope for other feature types was much more limited at the time. > Would it already be worth either making gex_only a required input?. I'm not sure the `gex_only` argument even entirely makes sense anymore. I think a `feature_type` argument would make more sense. Erroring if nothing is passed and there are multiple kinds sounds reasonable to me, as multimodality should be handled explicitly. For backwards compatibility I think deprecation warnings for a release cycle when either `gex_only` is used or nothing is passed and there are multiple feature types present could work. -----------. Moving the 10x reading functions had been discussed in: #1387",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1949#issuecomment-879616528
https://github.com/scverse/scanpy/pull/1950#issuecomment-887218018:35,Availability,failure,failures,35,"LGTM! . Don't worry about the test failures, those are due to a networkx update changing how plots look, which we'll deal with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1950#issuecomment-887218018
https://github.com/scverse/scanpy/pull/1950#issuecomment-887218018:73,Deployability,update,update,73,"LGTM! . Don't worry about the test failures, those are due to a networkx update changing how plots look, which we'll deal with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1950#issuecomment-887218018
https://github.com/scverse/scanpy/pull/1950#issuecomment-887218018:30,Testability,test,test,30,"LGTM! . Don't worry about the test failures, those are due to a networkx update changing how plots look, which we'll deal with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1950#issuecomment-887218018
https://github.com/scverse/scanpy/issues/1951#issuecomment-882743129:37,Availability,error,error,37,"Hi,. please format your code and the error.; Also, please provide a more in depth description. What were you trying to do? A minimal example helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951#issuecomment-882743129
https://github.com/scverse/scanpy/issues/1951#issuecomment-883853578:99,Deployability,integrat,integrating-data-using-ingest,99,"Emm, the code is just from the scanpy tutorial.; https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html. I run ingest on pbmc dataset, then meet this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951#issuecomment-883853578
https://github.com/scverse/scanpy/issues/1951#issuecomment-883853578:99,Integrability,integrat,integrating-data-using-ingest,99,"Emm, the code is just from the scanpy tutorial.; https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html. I run ingest on pbmc dataset, then meet this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951#issuecomment-883853578
https://github.com/scverse/scanpy/issues/1951#issuecomment-885545697:13,Usability,feedback,feedback,13,Is there any feedback？,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951#issuecomment-885545697
https://github.com/scverse/scanpy/issues/1951#issuecomment-885560293:162,Availability,error,errors,162,"![image](https://user-images.githubusercontent.com/43333475/126772609-6fbdc819-46e1-4fac-8e8c-d73033192991.png). I have tried pynndescent, and I receive the same errors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951#issuecomment-885560293
https://github.com/scverse/scanpy/issues/1951#issuecomment-908461350:17,Deployability,install,installing,17,"Hi @ChineseBest, installing the following versions in google colab worked for me: `scanpy==1.7.1 pynndescent==0.4.8 numba==0.51.2`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951#issuecomment-908461350
https://github.com/scverse/scanpy/issues/1951#issuecomment-908462363:19,Deployability,install,installing,19,"> Hi @ChineseBest, installing the following versions in google colab worked for me: `scanpy==1.7.1 pynndescent==0.4.8 numba==0.51.2`. Ok, thanks. In fact I have already changed to local machine to run it. Thx again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951#issuecomment-908462363
https://github.com/scverse/scanpy/pull/1952#issuecomment-882418271:328,Availability,error,error-reference,328,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1952?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@c9d319e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1952 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1952#issuecomment-882418271
https://github.com/scverse/scanpy/pull/1952#issuecomment-882418271:276,Usability,learn,learn,276,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1952?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@c9d319e`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1952 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1952#issuecomment-882418271
https://github.com/scverse/scanpy/issues/1954#issuecomment-883854761:52,Availability,error,error,52,"Could you please provide more information about his error? How do you create the scanpy tmp?. In addition, the result of sc.tl.umap is stored in the scanpy file as obsm['X_umap']. You can try to display these data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1954#issuecomment-883854761
https://github.com/scverse/scanpy/issues/1955#issuecomment-884979935:34,Deployability,integrat,integration,34,"Hi @pinin4fjords! I understand by integration, you mean access under the scanpy api. We try to advance the scanpy environment by modular extensions, which are packages with their own API, that also work on adata instances. This is currently what diffxpy is and there are no plans to collect all scanpy-related packages under `sc.*` as far as I am aware.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-884979935
https://github.com/scverse/scanpy/issues/1955#issuecomment-884979935:34,Integrability,integrat,integration,34,"Hi @pinin4fjords! I understand by integration, you mean access under the scanpy api. We try to advance the scanpy environment by modular extensions, which are packages with their own API, that also work on adata instances. This is currently what diffxpy is and there are no plans to collect all scanpy-related packages under `sc.*` as far as I am aware.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-884979935
https://github.com/scverse/scanpy/issues/1955#issuecomment-884979935:56,Security,access,access,56,"Hi @pinin4fjords! I understand by integration, you mean access under the scanpy api. We try to advance the scanpy environment by modular extensions, which are packages with their own API, that also work on adata instances. This is currently what diffxpy is and there are no plans to collect all scanpy-related packages under `sc.*` as far as I am aware.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-884979935
https://github.com/scverse/scanpy/issues/1955#issuecomment-885007122:118,Deployability,integrat,integration,118,"We have our [CLI layer for Scanpy](https://github.com/ebi-gene-expression-group/scanpy-scripts), and I could put this integration there, but it'd be a shame to silo code that might be useful to other Scanpy users, so happy to contribute to something in the external API if you guys are willing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-885007122
https://github.com/scverse/scanpy/issues/1955#issuecomment-885007122:118,Integrability,integrat,integration,118,"We have our [CLI layer for Scanpy](https://github.com/ebi-gene-expression-group/scanpy-scripts), and I could put this integration there, but it'd be a shame to silo code that might be useful to other Scanpy users, so happy to contribute to something in the external API if you guys are willing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-885007122
https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954:508,Integrability,wrap,wrapper,508,"@Koncopd has looked at refactoring the `rank_genes_groups` methods, but in the big picture we don't really love the output format that `rank_genes_groups` uses. Maybe an easier path forward would be to be able to directly pass values into the various plotting functions? You can already generate mostly similar plots from `sc.pl.rank_genes_groups_{plot_func}` and `sc.pl.{plot_func}` apart from using logfc and pvalues. If we allowed passing those in, it would be simple enough to make the same plots/ add a wrapper that generates the plots into `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954
https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954:23,Modifiability,refactor,refactoring,23,"@Koncopd has looked at refactoring the `rank_genes_groups` methods, but in the big picture we don't really love the output format that `rank_genes_groups` uses. Maybe an easier path forward would be to be able to directly pass values into the various plotting functions? You can already generate mostly similar plots from `sc.pl.rank_genes_groups_{plot_func}` and `sc.pl.{plot_func}` apart from using logfc and pvalues. If we allowed passing those in, it would be simple enough to make the same plots/ add a wrapper that generates the plots into `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954
https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954:401,Testability,log,logfc,401,"@Koncopd has looked at refactoring the `rank_genes_groups` methods, but in the big picture we don't really love the output format that `rank_genes_groups` uses. Maybe an easier path forward would be to be able to directly pass values into the various plotting functions? You can already generate mostly similar plots from `sc.pl.rank_genes_groups_{plot_func}` and `sc.pl.{plot_func}` apart from using logfc and pvalues. If we allowed passing those in, it would be simple enough to make the same plots/ add a wrapper that generates the plots into `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954
https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954:464,Usability,simpl,simple,464,"@Koncopd has looked at refactoring the `rank_genes_groups` methods, but in the big picture we don't really love the output format that `rank_genes_groups` uses. Maybe an easier path forward would be to be able to directly pass values into the various plotting functions? You can already generate mostly similar plots from `sc.pl.rank_genes_groups_{plot_func}` and `sc.pl.{plot_func}` apart from using logfc and pvalues. If we allowed passing those in, it would be simple enough to make the same plots/ add a wrapper that generates the plots into `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954
https://github.com/scverse/scanpy/issues/1955#issuecomment-886464475:181,Availability,down,downstream,181,"What we're really after is the more general ability to execute different d/e tools without too much extra work, and have the results stored consistently in the annData for whatever downstream applications (plotting or otherwise), or just so that they're available for consumers of our annData objects. But maybe if it's something you guys aren't keen on we can just code it up in our own software layer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886464475
https://github.com/scverse/scanpy/issues/1955#issuecomment-886464475:254,Availability,avail,available,254,"What we're really after is the more general ability to execute different d/e tools without too much extra work, and have the results stored consistently in the annData for whatever downstream applications (plotting or otherwise), or just so that they're available for consumers of our annData objects. But maybe if it's something you guys aren't keen on we can just code it up in our own software layer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886464475
https://github.com/scverse/scanpy/issues/1955#issuecomment-886548222:114,Availability,avail,available,114,"Thanks on that method @ivirshup - our version was written before I was maintainer, and maybe that function wasn't available. . Happy to adopt whatever general approach you recommend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886548222
https://github.com/scverse/scanpy/issues/1957#issuecomment-889126422:73,Integrability,wrap,wrapper,73,"I'll take a look at this to make sure I've not messed up in writing this wrapper (I'm actually doing some more testing myself for production use right now). . But you should know that what we've done here is mirror some of the internals of scrublet, but using Scanpy functions. Scrublet should be supplied with raw counts, but does do its own normalisations internally before doing the actual doublet prediction, which is what we're doing here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957#issuecomment-889126422
https://github.com/scverse/scanpy/issues/1957#issuecomment-889126422:401,Safety,predict,prediction,401,"I'll take a look at this to make sure I've not messed up in writing this wrapper (I'm actually doing some more testing myself for production use right now). . But you should know that what we've done here is mirror some of the internals of scrublet, but using Scanpy functions. Scrublet should be supplied with raw counts, but does do its own normalisations internally before doing the actual doublet prediction, which is what we're doing here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957#issuecomment-889126422
https://github.com/scverse/scanpy/issues/1957#issuecomment-889126422:111,Testability,test,testing,111,"I'll take a look at this to make sure I've not messed up in writing this wrapper (I'm actually doing some more testing myself for production use right now). . But you should know that what we've done here is mirror some of the internals of scrublet, but using Scanpy functions. Scrublet should be supplied with raw counts, but does do its own normalisations internally before doing the actual doublet prediction, which is what we're doing here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957#issuecomment-889126422
https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596:181,Modifiability,layers,layers,181,"Ahh, see what you mean though. Think I'd mis-interpreted the [docs](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html), where talks about normalising layers:. ```; Layer to normalize instead of X. If None, X is normalized.; ```. Implies that .X is the only thing that gets normalised, if you don't specify layers. . **Edit:** . Actually this is just due to references I think:. ```; # Done as per current code; >>> ad = sc.read(""read.h5ad""); >>> ad[1:5,1:5].X.todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); >>> ad.layers['raw'] = ad.X; >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); ; # If we actually copy the matrix; >>> ad = sc.read(""read.h5ad""); >>> ad.layers['raw'] = ad.X.copy()); >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); ```. Shall I add the fix as part of what I'm doing in https://github.com/theislab/scanpy/pull/1965 @ivirshup ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596
https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596:337,Modifiability,layers,layers,337,"Ahh, see what you mean though. Think I'd mis-interpreted the [docs](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html), where talks about normalising layers:. ```; Layer to normalize instead of X. If None, X is normalized.; ```. Implies that .X is the only thing that gets normalised, if you don't specify layers. . **Edit:** . Actually this is just due to references I think:. ```; # Done as per current code; >>> ad = sc.read(""read.h5ad""); >>> ad[1:5,1:5].X.todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); >>> ad.layers['raw'] = ad.X; >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); ; # If we actually copy the matrix; >>> ad = sc.read(""read.h5ad""); >>> ad.layers['raw'] = ad.X.copy()); >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); ```. Shall I add the fix as part of what I'm doing in https://github.com/theislab/scanpy/pull/1965 @ivirshup ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596
https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596:609,Modifiability,layers,layers,609,"Ahh, see what you mean though. Think I'd mis-interpreted the [docs](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html), where talks about normalising layers:. ```; Layer to normalize instead of X. If None, X is normalized.; ```. Implies that .X is the only thing that gets normalised, if you don't specify layers. . **Edit:** . Actually this is just due to references I think:. ```; # Done as per current code; >>> ad = sc.read(""read.h5ad""); >>> ad[1:5,1:5].X.todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); >>> ad.layers['raw'] = ad.X; >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); ; # If we actually copy the matrix; >>> ad = sc.read(""read.h5ad""); >>> ad.layers['raw'] = ad.X.copy()); >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); ```. Shall I add the fix as part of what I'm doing in https://github.com/theislab/scanpy/pull/1965 @ivirshup ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596
https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596:851,Modifiability,layers,layers,851,"Ahh, see what you mean though. Think I'd mis-interpreted the [docs](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html), where talks about normalising layers:. ```; Layer to normalize instead of X. If None, X is normalized.; ```. Implies that .X is the only thing that gets normalised, if you don't specify layers. . **Edit:** . Actually this is just due to references I think:. ```; # Done as per current code; >>> ad = sc.read(""read.h5ad""); >>> ad[1:5,1:5].X.todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); >>> ad.layers['raw'] = ad.X; >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); ; # If we actually copy the matrix; >>> ad = sc.read(""read.h5ad""); >>> ad.layers['raw'] = ad.X.copy()); >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); ```. Shall I add the fix as part of what I'm doing in https://github.com/theislab/scanpy/pull/1965 @ivirshup ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596
https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596:1094,Modifiability,layers,layers,1094,"Ahh, see what you mean though. Think I'd mis-interpreted the [docs](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html), where talks about normalising layers:. ```; Layer to normalize instead of X. If None, X is normalized.; ```. Implies that .X is the only thing that gets normalised, if you don't specify layers. . **Edit:** . Actually this is just due to references I think:. ```; # Done as per current code; >>> ad = sc.read(""read.h5ad""); >>> ad[1:5,1:5].X.todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); >>> ad.layers['raw'] = ad.X; >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); ; # If we actually copy the matrix; >>> ad = sc.read(""read.h5ad""); >>> ad.layers['raw'] = ad.X.copy()); >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); ```. Shall I add the fix as part of what I'm doing in https://github.com/theislab/scanpy/pull/1965 @ivirshup ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596
https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596:1344,Modifiability,layers,layers,1344,"Ahh, see what you mean though. Think I'd mis-interpreted the [docs](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html), where talks about normalising layers:. ```; Layer to normalize instead of X. If None, X is normalized.; ```. Implies that .X is the only thing that gets normalised, if you don't specify layers. . **Edit:** . Actually this is just due to references I think:. ```; # Done as per current code; >>> ad = sc.read(""read.h5ad""); >>> ad[1:5,1:5].X.todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); >>> ad.layers['raw'] = ad.X; >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); ; # If we actually copy the matrix; >>> ad = sc.read(""read.h5ad""); >>> ad.layers['raw'] = ad.X.copy()); >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); ```. Shall I add the fix as part of what I'm doing in https://github.com/theislab/scanpy/pull/1965 @ivirshup ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596
https://github.com/scverse/scanpy/pull/1961#issuecomment-887335708:328,Availability,error,error-reference,328,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1961?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@aeaa07b`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1961 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1961#issuecomment-887335708
https://github.com/scverse/scanpy/pull/1961#issuecomment-887335708:276,Usability,learn,learn,276,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1961?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@aeaa07b`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1961 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1961#issuecomment-887335708
https://github.com/scverse/scanpy/pull/1962#issuecomment-887353669:328,Availability,error,error-reference,328,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1962?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@370d1c6`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1962 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1962#issuecomment-887353669
https://github.com/scverse/scanpy/pull/1962#issuecomment-887353669:276,Usability,learn,learn,276,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1962?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@370d1c6`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1962 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1962#issuecomment-887353669
https://github.com/scverse/scanpy/issues/1963#issuecomment-887392048:13,Deployability,continuous,continuous,13,The color is continuous means that the color is gradient according to the default setting in package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1963#issuecomment-887392048
https://github.com/scverse/scanpy/pull/1965#issuecomment-953614277:146,Deployability,release,released,146,"> Hey, sorry about the late response!. No worries!. > Would you mind separating out the bug fix and feature addition? That was the bug fix can be released more quickly. OK, will do- see https://github.com/theislab/scanpy/pull/2023, https://github.com/theislab/scanpy/pull/2025. > Question about the main idea here: what kind of batches are you expecting to handle here?; > ; > If they were from completely separate sequencing experiments, would you want to have variable expected doublet rates between batches?; > ; > If the batches are multiple samples that were barcoded and multiplexed, would you want to allow for the possibility of multiplets across batches?. So, really, I just want to be able to follow best practice as per the [Scrublet docs](https://github.com/swolock/scrublet#best-practices), to be able to run Scrublet on cells from different samples separately, perhaps batches is the wrong term. Do you have a preferred alternative, or should I just clarify the help text?. > Could you also merge master into this? CI should be fixed now. Done",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-953614277
https://github.com/scverse/scanpy/pull/1965#issuecomment-953614277:462,Modifiability,variab,variable,462,"> Hey, sorry about the late response!. No worries!. > Would you mind separating out the bug fix and feature addition? That was the bug fix can be released more quickly. OK, will do- see https://github.com/theislab/scanpy/pull/2023, https://github.com/theislab/scanpy/pull/2025. > Question about the main idea here: what kind of batches are you expecting to handle here?; > ; > If they were from completely separate sequencing experiments, would you want to have variable expected doublet rates between batches?; > ; > If the batches are multiple samples that were barcoded and multiplexed, would you want to allow for the possibility of multiplets across batches?. So, really, I just want to be able to follow best practice as per the [Scrublet docs](https://github.com/swolock/scrublet#best-practices), to be able to run Scrublet on cells from different samples separately, perhaps batches is the wrong term. Do you have a preferred alternative, or should I just clarify the help text?. > Could you also merge master into this? CI should be fixed now. Done",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-953614277
https://github.com/scverse/scanpy/pull/1965#issuecomment-1017444179:95,Testability,test,tests,95,"@ivirshup I've merged the changes in master from those factored-out PRs, and added a couple of tests. Ready for review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-1017444179
https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656:340,Safety,detect,detects,340,"Oh, and one more thing. It would be great if the test for the batched version could check it was equivalent to computing the doublets separately. E.g. <details>; <summary> modified `test_scrublet_batched` </summary>. ```python; def test_scrublet_batched():; """"""; Test that Scrublet run works with batched data. Check that scrublet runs and detects some doublets.; """"""; pytest.importorskip(""scrublet""). adata = sc.datasets.pbmc3k(); adata.obs['batch'] = 1350 * ['a'] + 1350 * ['b']; split = [adata[adata.obs[""batch""] == x].copy() for x in (""a"", ""b"")]. sce.pp.scrublet(adata, use_approx_neighbors=False, batch_key='batch'). # replace assertions by conditions; assert ""predicted_doublet"" in adata.obs.columns; assert ""doublet_score"" in adata.obs.columns. assert adata.obs[""predicted_doublet""].any(), ""Expect some doublets to be identified""; assert (; 'batches' in adata.uns['scrublet'].keys(); ), ""Expect .uns to contain batch info"". # Check that results are independent; for s in split:; sce.pp.scrublet(s, use_approx_neighbors=False); merged = sc.concat(split). pd.testing.assert_frame_equal(adata.obs[merged.obs.columns], merged.obs); ```. </details>. --------. For the docs, I think you might need to merge from master to get them to build. Sphinx has been acting up a lot recently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656
https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656:49,Testability,test,test,49,"Oh, and one more thing. It would be great if the test for the batched version could check it was equivalent to computing the doublets separately. E.g. <details>; <summary> modified `test_scrublet_batched` </summary>. ```python; def test_scrublet_batched():; """"""; Test that Scrublet run works with batched data. Check that scrublet runs and detects some doublets.; """"""; pytest.importorskip(""scrublet""). adata = sc.datasets.pbmc3k(); adata.obs['batch'] = 1350 * ['a'] + 1350 * ['b']; split = [adata[adata.obs[""batch""] == x].copy() for x in (""a"", ""b"")]. sce.pp.scrublet(adata, use_approx_neighbors=False, batch_key='batch'). # replace assertions by conditions; assert ""predicted_doublet"" in adata.obs.columns; assert ""doublet_score"" in adata.obs.columns. assert adata.obs[""predicted_doublet""].any(), ""Expect some doublets to be identified""; assert (; 'batches' in adata.uns['scrublet'].keys(); ), ""Expect .uns to contain batch info"". # Check that results are independent; for s in split:; sce.pp.scrublet(s, use_approx_neighbors=False); merged = sc.concat(split). pd.testing.assert_frame_equal(adata.obs[merged.obs.columns], merged.obs); ```. </details>. --------. For the docs, I think you might need to merge from master to get them to build. Sphinx has been acting up a lot recently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656
https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656:263,Testability,Test,Test,263,"Oh, and one more thing. It would be great if the test for the batched version could check it was equivalent to computing the doublets separately. E.g. <details>; <summary> modified `test_scrublet_batched` </summary>. ```python; def test_scrublet_batched():; """"""; Test that Scrublet run works with batched data. Check that scrublet runs and detects some doublets.; """"""; pytest.importorskip(""scrublet""). adata = sc.datasets.pbmc3k(); adata.obs['batch'] = 1350 * ['a'] + 1350 * ['b']; split = [adata[adata.obs[""batch""] == x].copy() for x in (""a"", ""b"")]. sce.pp.scrublet(adata, use_approx_neighbors=False, batch_key='batch'). # replace assertions by conditions; assert ""predicted_doublet"" in adata.obs.columns; assert ""doublet_score"" in adata.obs.columns. assert adata.obs[""predicted_doublet""].any(), ""Expect some doublets to be identified""; assert (; 'batches' in adata.uns['scrublet'].keys(); ), ""Expect .uns to contain batch info"". # Check that results are independent; for s in split:; sce.pp.scrublet(s, use_approx_neighbors=False); merged = sc.concat(split). pd.testing.assert_frame_equal(adata.obs[merged.obs.columns], merged.obs); ```. </details>. --------. For the docs, I think you might need to merge from master to get them to build. Sphinx has been acting up a lot recently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656
https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656:632,Testability,assert,assertions,632,"Oh, and one more thing. It would be great if the test for the batched version could check it was equivalent to computing the doublets separately. E.g. <details>; <summary> modified `test_scrublet_batched` </summary>. ```python; def test_scrublet_batched():; """"""; Test that Scrublet run works with batched data. Check that scrublet runs and detects some doublets.; """"""; pytest.importorskip(""scrublet""). adata = sc.datasets.pbmc3k(); adata.obs['batch'] = 1350 * ['a'] + 1350 * ['b']; split = [adata[adata.obs[""batch""] == x].copy() for x in (""a"", ""b"")]. sce.pp.scrublet(adata, use_approx_neighbors=False, batch_key='batch'). # replace assertions by conditions; assert ""predicted_doublet"" in adata.obs.columns; assert ""doublet_score"" in adata.obs.columns. assert adata.obs[""predicted_doublet""].any(), ""Expect some doublets to be identified""; assert (; 'batches' in adata.uns['scrublet'].keys(); ), ""Expect .uns to contain batch info"". # Check that results are independent; for s in split:; sce.pp.scrublet(s, use_approx_neighbors=False); merged = sc.concat(split). pd.testing.assert_frame_equal(adata.obs[merged.obs.columns], merged.obs); ```. </details>. --------. For the docs, I think you might need to merge from master to get them to build. Sphinx has been acting up a lot recently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656
https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656:658,Testability,assert,assert,658,"Oh, and one more thing. It would be great if the test for the batched version could check it was equivalent to computing the doublets separately. E.g. <details>; <summary> modified `test_scrublet_batched` </summary>. ```python; def test_scrublet_batched():; """"""; Test that Scrublet run works with batched data. Check that scrublet runs and detects some doublets.; """"""; pytest.importorskip(""scrublet""). adata = sc.datasets.pbmc3k(); adata.obs['batch'] = 1350 * ['a'] + 1350 * ['b']; split = [adata[adata.obs[""batch""] == x].copy() for x in (""a"", ""b"")]. sce.pp.scrublet(adata, use_approx_neighbors=False, batch_key='batch'). # replace assertions by conditions; assert ""predicted_doublet"" in adata.obs.columns; assert ""doublet_score"" in adata.obs.columns. assert adata.obs[""predicted_doublet""].any(), ""Expect some doublets to be identified""; assert (; 'batches' in adata.uns['scrublet'].keys(); ), ""Expect .uns to contain batch info"". # Check that results are independent; for s in split:; sce.pp.scrublet(s, use_approx_neighbors=False); merged = sc.concat(split). pd.testing.assert_frame_equal(adata.obs[merged.obs.columns], merged.obs); ```. </details>. --------. For the docs, I think you might need to merge from master to get them to build. Sphinx has been acting up a lot recently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656
https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656:707,Testability,assert,assert,707,"Oh, and one more thing. It would be great if the test for the batched version could check it was equivalent to computing the doublets separately. E.g. <details>; <summary> modified `test_scrublet_batched` </summary>. ```python; def test_scrublet_batched():; """"""; Test that Scrublet run works with batched data. Check that scrublet runs and detects some doublets.; """"""; pytest.importorskip(""scrublet""). adata = sc.datasets.pbmc3k(); adata.obs['batch'] = 1350 * ['a'] + 1350 * ['b']; split = [adata[adata.obs[""batch""] == x].copy() for x in (""a"", ""b"")]. sce.pp.scrublet(adata, use_approx_neighbors=False, batch_key='batch'). # replace assertions by conditions; assert ""predicted_doublet"" in adata.obs.columns; assert ""doublet_score"" in adata.obs.columns. assert adata.obs[""predicted_doublet""].any(), ""Expect some doublets to be identified""; assert (; 'batches' in adata.uns['scrublet'].keys(); ), ""Expect .uns to contain batch info"". # Check that results are independent; for s in split:; sce.pp.scrublet(s, use_approx_neighbors=False); merged = sc.concat(split). pd.testing.assert_frame_equal(adata.obs[merged.obs.columns], merged.obs); ```. </details>. --------. For the docs, I think you might need to merge from master to get them to build. Sphinx has been acting up a lot recently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656
https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656:752,Testability,assert,assert,752,"Oh, and one more thing. It would be great if the test for the batched version could check it was equivalent to computing the doublets separately. E.g. <details>; <summary> modified `test_scrublet_batched` </summary>. ```python; def test_scrublet_batched():; """"""; Test that Scrublet run works with batched data. Check that scrublet runs and detects some doublets.; """"""; pytest.importorskip(""scrublet""). adata = sc.datasets.pbmc3k(); adata.obs['batch'] = 1350 * ['a'] + 1350 * ['b']; split = [adata[adata.obs[""batch""] == x].copy() for x in (""a"", ""b"")]. sce.pp.scrublet(adata, use_approx_neighbors=False, batch_key='batch'). # replace assertions by conditions; assert ""predicted_doublet"" in adata.obs.columns; assert ""doublet_score"" in adata.obs.columns. assert adata.obs[""predicted_doublet""].any(), ""Expect some doublets to be identified""; assert (; 'batches' in adata.uns['scrublet'].keys(); ), ""Expect .uns to contain batch info"". # Check that results are independent; for s in split:; sce.pp.scrublet(s, use_approx_neighbors=False); merged = sc.concat(split). pd.testing.assert_frame_equal(adata.obs[merged.obs.columns], merged.obs); ```. </details>. --------. For the docs, I think you might need to merge from master to get them to build. Sphinx has been acting up a lot recently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656
https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656:838,Testability,assert,assert,838,"Oh, and one more thing. It would be great if the test for the batched version could check it was equivalent to computing the doublets separately. E.g. <details>; <summary> modified `test_scrublet_batched` </summary>. ```python; def test_scrublet_batched():; """"""; Test that Scrublet run works with batched data. Check that scrublet runs and detects some doublets.; """"""; pytest.importorskip(""scrublet""). adata = sc.datasets.pbmc3k(); adata.obs['batch'] = 1350 * ['a'] + 1350 * ['b']; split = [adata[adata.obs[""batch""] == x].copy() for x in (""a"", ""b"")]. sce.pp.scrublet(adata, use_approx_neighbors=False, batch_key='batch'). # replace assertions by conditions; assert ""predicted_doublet"" in adata.obs.columns; assert ""doublet_score"" in adata.obs.columns. assert adata.obs[""predicted_doublet""].any(), ""Expect some doublets to be identified""; assert (; 'batches' in adata.uns['scrublet'].keys(); ), ""Expect .uns to contain batch info"". # Check that results are independent; for s in split:; sce.pp.scrublet(s, use_approx_neighbors=False); merged = sc.concat(split). pd.testing.assert_frame_equal(adata.obs[merged.obs.columns], merged.obs); ```. </details>. --------. For the docs, I think you might need to merge from master to get them to build. Sphinx has been acting up a lot recently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656
https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656:1064,Testability,test,testing,1064,"Oh, and one more thing. It would be great if the test for the batched version could check it was equivalent to computing the doublets separately. E.g. <details>; <summary> modified `test_scrublet_batched` </summary>. ```python; def test_scrublet_batched():; """"""; Test that Scrublet run works with batched data. Check that scrublet runs and detects some doublets.; """"""; pytest.importorskip(""scrublet""). adata = sc.datasets.pbmc3k(); adata.obs['batch'] = 1350 * ['a'] + 1350 * ['b']; split = [adata[adata.obs[""batch""] == x].copy() for x in (""a"", ""b"")]. sce.pp.scrublet(adata, use_approx_neighbors=False, batch_key='batch'). # replace assertions by conditions; assert ""predicted_doublet"" in adata.obs.columns; assert ""doublet_score"" in adata.obs.columns. assert adata.obs[""predicted_doublet""].any(), ""Expect some doublets to be identified""; assert (; 'batches' in adata.uns['scrublet'].keys(); ), ""Expect .uns to contain batch info"". # Check that results are independent; for s in split:; sce.pp.scrublet(s, use_approx_neighbors=False); merged = sc.concat(split). pd.testing.assert_frame_equal(adata.obs[merged.obs.columns], merged.obs); ```. </details>. --------. For the docs, I think you might need to merge from master to get them to build. Sphinx has been acting up a lot recently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-1075220656
https://github.com/scverse/scanpy/pull/1965#issuecomment-1076139906:27,Deployability,release,release,27,"(and fingers crossed for a release soon, I currently have a nasty patch in place for our pipelines)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-1076139906
https://github.com/scverse/scanpy/pull/1965#issuecomment-1076139906:66,Deployability,patch,patch,66,"(and fingers crossed for a release soon, I currently have a nasty patch in place for our pipelines)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-1076139906
https://github.com/scverse/scanpy/pull/1965#issuecomment-1076139906:89,Deployability,pipeline,pipelines,89,"(and fingers crossed for a release soon, I currently have a nasty patch in place for our pipelines)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-1076139906
https://github.com/scverse/scanpy/pull/1966#issuecomment-888502703:328,Availability,error,error-reference,328,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1966?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@b9cd8c8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1966 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1966#issuecomment-888502703
https://github.com/scverse/scanpy/pull/1966#issuecomment-888502703:276,Usability,learn,learn,276,# [Codecov](https://codecov.io/gh/theislab/scanpy/pull/1966?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab) Report; > :exclamation: No coverage uploaded for pull request base (`1.8.x@b9cd8c8`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=theislab#section-missing-base-commit).; > The diff coverage is `n/a`. ```diff; @@ Coverage Diff @@; ## 1.8.x #1966 +/- ##; ========================================; Coverage ? 71.61% ; ========================================; Files ? 92 ; Lines ? 11246 ; Branches ? 0 ; ========================================; Hits ? 8054 ; Misses ? 3192 ; Partials ? 0 ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1966#issuecomment-888502703
https://github.com/scverse/scanpy/pull/1969#issuecomment-1291807007:129,Availability,ping,ping,129,"@WeilerP would you be willing to write a tiny test and to add the release note, please?. Thanks! Happy to merge this then if you ping me. @ivirshup generally, I agree. Think that this tiny change doesn't harm though and deprecating the magic ""read"" is something bigger.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1969#issuecomment-1291807007
https://github.com/scverse/scanpy/pull/1969#issuecomment-1291807007:66,Deployability,release,release,66,"@WeilerP would you be willing to write a tiny test and to add the release note, please?. Thanks! Happy to merge this then if you ping me. @ivirshup generally, I agree. Think that this tiny change doesn't harm though and deprecating the magic ""read"" is something bigger.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1969#issuecomment-1291807007
https://github.com/scverse/scanpy/pull/1969#issuecomment-1291807007:46,Testability,test,test,46,"@WeilerP would you be willing to write a tiny test and to add the release note, please?. Thanks! Happy to merge this then if you ping me. @ivirshup generally, I agree. Think that this tiny change doesn't harm though and deprecating the magic ""read"" is something bigger.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1969#issuecomment-1291807007
https://github.com/scverse/scanpy/issues/1972#issuecomment-893092263:4,Testability,log,logic,4,"The logic used in `add_colors_for_categorical_sample_annotation` looks kinda messed up (should check `>` not `<=`). I think `paga` should just use `_get_palette` the same way embedding plots do. It looks like there are a number of plotting functions that will have this behavior. I would note I'm not totally convinced we should allow setting more colors than there are categories, since they could be misaligned. This would be irrelevant if we stored colors in a dict.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1972#issuecomment-893092263
https://github.com/scverse/scanpy/pull/1974#issuecomment-891043388:25,Availability,error,error,25,"```; Warning, treated as error:; failed to reach any of the inventories with the following issues:; intersphinx inventory 'https://docs.scipy.org/doc/scipy/reference/objects.inv' not fetchable due to <class 'requests.exceptions.HTTPError'>: 404 Client Error: Not Found for url: https://docs.scipy.org/doc/scipy/reference/objects.inv; ```. Not my fault I guess",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1974#issuecomment-891043388
https://github.com/scverse/scanpy/pull/1974#issuecomment-891043388:252,Availability,Error,Error,252,"```; Warning, treated as error:; failed to reach any of the inventories with the following issues:; intersphinx inventory 'https://docs.scipy.org/doc/scipy/reference/objects.inv' not fetchable due to <class 'requests.exceptions.HTTPError'>: 404 Client Error: Not Found for url: https://docs.scipy.org/doc/scipy/reference/objects.inv; ```. Not my fault I guess",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1974#issuecomment-891043388
https://github.com/scverse/scanpy/pull/1974#issuecomment-891043388:346,Availability,fault,fault,346,"```; Warning, treated as error:; failed to reach any of the inventories with the following issues:; intersphinx inventory 'https://docs.scipy.org/doc/scipy/reference/objects.inv' not fetchable due to <class 'requests.exceptions.HTTPError'>: 404 Client Error: Not Found for url: https://docs.scipy.org/doc/scipy/reference/objects.inv; ```. Not my fault I guess",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1974#issuecomment-891043388
https://github.com/scverse/scanpy/pull/1974#issuecomment-900226530:73,Availability,Error,Error,73,"```; FAILED scanpy/tests/test_plotting.py::test_violin - AssertionError: Error: Im...; FAILED scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k - AssertionError: E...; ```. Sigh, again not related.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1974#issuecomment-900226530
https://github.com/scverse/scanpy/pull/1974#issuecomment-900226530:19,Testability,test,tests,19,"```; FAILED scanpy/tests/test_plotting.py::test_violin - AssertionError: Error: Im...; FAILED scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k - AssertionError: E...; ```. Sigh, again not related.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1974#issuecomment-900226530
https://github.com/scverse/scanpy/pull/1974#issuecomment-900226530:57,Testability,Assert,AssertionError,57,"```; FAILED scanpy/tests/test_plotting.py::test_violin - AssertionError: Error: Im...; FAILED scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k - AssertionError: E...; ```. Sigh, again not related.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1974#issuecomment-900226530
https://github.com/scverse/scanpy/pull/1974#issuecomment-900226530:101,Testability,test,tests,101,"```; FAILED scanpy/tests/test_plotting.py::test_violin - AssertionError: Error: Im...; FAILED scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k - AssertionError: E...; ```. Sigh, again not related.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1974#issuecomment-900226530
https://github.com/scverse/scanpy/pull/1974#issuecomment-900226530:147,Testability,Assert,AssertionError,147,"```; FAILED scanpy/tests/test_plotting.py::test_violin - AssertionError: Error: Im...; FAILED scanpy/tests/notebooks/test_pbmc3k.py::test_pbmc3k - AssertionError: E...; ```. Sigh, again not related.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1974#issuecomment-900226530
https://github.com/scverse/scanpy/issues/1975#issuecomment-901153925:21,Deployability,update,updates,21,"This might be due to updates in pandas >1.3.0. The command ; `pbmc.rename_categories('phase', new_cluster_names)`; seems to be deprecated. In particular, the ""inplace"" option is no longer valid, so it seems that one can only create a copy of the renamed categories and store it. Hence, the new command should be ; `pbmc.obs['phase'] = pbmc.obs['phase'].cat.rename_categories(new_cluster_names)`.; I checked that this works on a different data set, but haven't checked for pbmc. If this fully fixes the problem, only the tutorial needs to be updated (the command for renaming the clusters) and scanpy doesn't need to be modified. . Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cat.rename_categories.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1975#issuecomment-901153925
https://github.com/scverse/scanpy/issues/1975#issuecomment-901153925:541,Deployability,update,updated,541,"This might be due to updates in pandas >1.3.0. The command ; `pbmc.rename_categories('phase', new_cluster_names)`; seems to be deprecated. In particular, the ""inplace"" option is no longer valid, so it seems that one can only create a copy of the renamed categories and store it. Hence, the new command should be ; `pbmc.obs['phase'] = pbmc.obs['phase'].cat.rename_categories(new_cluster_names)`.; I checked that this works on a different data set, but haven't checked for pbmc. If this fully fixes the problem, only the tutorial needs to be updated (the command for renaming the clusters) and scanpy doesn't need to be modified. . Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cat.rename_categories.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1975#issuecomment-901153925
https://github.com/scverse/scanpy/issues/1975#issuecomment-925158522:23,Deployability,update,updates,23,"> This might be due to updates in pandas >1.3.0. The command; > `pbmc.rename_categories('phase', new_cluster_names)`; > seems to be deprecated. In particular, the ""inplace"" option is no longer valid, so it seems that one can only create a copy of the renamed categories and store it. Hence, the new command should be; > `pbmc.obs['phase'] = pbmc.obs['phase'].cat.rename_categories(new_cluster_names)`.; > I checked that this works on a different data set, but haven't checked for pbmc. If this fully fixes the problem, only the tutorial needs to be updated (the command for renaming the clusters) and scanpy doesn't need to be modified.; > ; > Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cat.rename_categories.html. I have verified that this change works in the scanpy clustering tutorial. The exact change I made was; `adata.rename_categories('leiden', new_cluster_names)`; became; `adata.obs['leiden'] =adata.obs['leiden'].cat.rename_categories(new_cluster_names)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1975#issuecomment-925158522
https://github.com/scverse/scanpy/issues/1975#issuecomment-925158522:549,Deployability,update,updated,549,"> This might be due to updates in pandas >1.3.0. The command; > `pbmc.rename_categories('phase', new_cluster_names)`; > seems to be deprecated. In particular, the ""inplace"" option is no longer valid, so it seems that one can only create a copy of the renamed categories and store it. Hence, the new command should be; > `pbmc.obs['phase'] = pbmc.obs['phase'].cat.rename_categories(new_cluster_names)`.; > I checked that this works on a different data set, but haven't checked for pbmc. If this fully fixes the problem, only the tutorial needs to be updated (the command for renaming the clusters) and scanpy doesn't need to be modified.; > ; > Reference: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.cat.rename_categories.html. I have verified that this change works in the scanpy clustering tutorial. The exact change I made was; `adata.rename_categories('leiden', new_cluster_names)`; became; `adata.obs['leiden'] =adata.obs['leiden'].cat.rename_categories(new_cluster_names)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1975#issuecomment-925158522
https://github.com/scverse/scanpy/issues/1977#issuecomment-953198551:141,Performance,perform,perform,141,There is a really nice version of combat from the sva package that includes a reference batch. If this was added as a feature then you could perform your corrections separately for each sample. This might be a pretty easy addition.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1977#issuecomment-953198551
https://github.com/scverse/scanpy/issues/1977#issuecomment-1949231015:132,Testability,benchmark,benchmarking,132,"I'm wondering if it would be possible to make a gradient descent based version of COMBAT or similar. It would involve some level of benchmarking, but presumably you would be able to get past the memory issue by streaming data in batches, letting the final weights and correction being informed by the whole data while not needing all of it in memory at once. Could possibly implement with a pytorch backend.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1977#issuecomment-1949231015
https://github.com/scverse/scanpy/issues/1978#issuecomment-898340541:32,Deployability,install,install,32,"Hi,. which umap package did you install? Which version is it?; Do you know whether the umap package already has support for Apple M1?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898340541
https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129:31,Deployability,install,install,31,This might be a case of a `pip install umap` rather than `pip install umap-learn`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129
https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129:62,Deployability,install,install,62,This might be a case of a `pip install umap` rather than `pip install umap-learn`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129
https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129:75,Usability,learn,learn,75,This might be a case of a `pip install umap` rather than `pip install umap-learn`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898421129
https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220:33,Deployability,install,install,33,> This might be a case of a `pip install umap` rather than `pip install umap-learn`. Suspecting exactly that :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220
https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220:64,Deployability,install,install,64,> This might be a case of a `pip install umap` rather than `pip install umap-learn`. Suspecting exactly that :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220
https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220:77,Usability,learn,learn,77,> This might be a case of a `pip install umap` rather than `pip install umap-learn`. Suspecting exactly that :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898435220
https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219:48,Deployability,install,installed,48,I uninstalled umap and made sure umap-learn was installed but it did not change anything. . I would guess that the problem comes from modules dependency as I managed to make it work on pycharm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219
https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219:142,Integrability,depend,dependency,142,I uninstalled umap and made sure umap-learn was installed but it did not change anything. . I would guess that the problem comes from modules dependency as I managed to make it work on pycharm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219
https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219:38,Usability,learn,learn,38,I uninstalled umap and made sure umap-learn was installed but it did not change anything. . I would guess that the problem comes from modules dependency as I managed to make it work on pycharm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219
https://github.com/scverse/scanpy/issues/1978#issuecomment-898544845:81,Usability,learn,learn,81,Why are we using `umap.__version__` instead of `importlib.metadata.version('umap-learn')`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898544845
https://github.com/scverse/scanpy/issues/1978#issuecomment-963430528:103,Usability,learn,learn,103,"@flying-sheep, any idea why `__version__` wouldn't work? Are we sure `importlib.metadata.version('umap-learn')` works when `__version__` doesn't? I'd be worried if they were basing the reported version on different sources.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-963430528
https://github.com/scverse/scanpy/issues/1978#issuecomment-963432344:161,Availability,down,down,161,"@hurleyLi, would you mind opening an issue over on umap that you're unable to get a `__version__` from it? It would be nice to have that fixed/ at least tracked down upstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-963432344
https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478:163,Availability,down,down,163,"> @hurleyLi, would you mind opening an issue over on umap that you're unable to get a `__version__` from it? It would be nice to have that fixed/ at least tracked down upstream. Figure it out. In my case it's because I have both `umap` and `umap-learn` installed, see here: https://github.com/theislab/scanpy/issues/2045#issuecomment-963533994",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478
https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478:253,Deployability,install,installed,253,"> @hurleyLi, would you mind opening an issue over on umap that you're unable to get a `__version__` from it? It would be nice to have that fixed/ at least tracked down upstream. Figure it out. In my case it's because I have both `umap` and `umap-learn` installed, see here: https://github.com/theislab/scanpy/issues/2045#issuecomment-963533994",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478
https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478:246,Usability,learn,learn,246,"> @hurleyLi, would you mind opening an issue over on umap that you're unable to get a `__version__` from it? It would be nice to have that fixed/ at least tracked down upstream. Figure it out. In my case it's because I have both `umap` and `umap-learn` installed, see here: https://github.com/theislab/scanpy/issues/2045#issuecomment-963533994",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-963537478
https://github.com/scverse/scanpy/issues/1980#issuecomment-899489219:94,Availability,Error,Error,94,"after I have used bbknn and ump these two steps, when I use the louvain, It gives me a System Error. ; Note: my dataset is too big. I want help!! thx!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1980#issuecomment-899489219
https://github.com/scverse/scanpy/issues/1980#issuecomment-1042669892:23,Availability,error,error,23,"hi @Tycooner, the same error happened to me recently when clustering using leiden, is there any idea what happened here and if you have any solution?. similar situation here: with large dataset and tried to set different resolution parameters when run sc.tl.leiden",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1980#issuecomment-1042669892
https://github.com/scverse/scanpy/issues/1981#issuecomment-1082447079:147,Availability,error,error,147,"Hi [gmoore5](https://github.com/gmoore5),. I assume this might be too late for you, but hopefully it's still useful for someone searching for this error. I could resolve this by:; - Restarting the kernel; - Setting the directory using `sc.settings.figdir = ""path/to/folder/""` (instead of sc._settings.ScanpyConfig.figdir = 'path/to/folder/')",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1981#issuecomment-1082447079
https://github.com/scverse/scanpy/issues/1983#issuecomment-903666863:2900,Availability,error,error,2900,"xception:. SystemError Traceback (most recent call last); /hps/scratch/lsf_tmpdir/hl-codon-13-02/ipykernel_2124423/1009160698.py in <module>; ----> 1 sc.pp.neighbors(adata_pbmc3k). /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)); --> 390 connectivities = fuzzy_simplicial_set(; 391 X,; 392 n_neighbors,. /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/umap/umap_.py in fuzzy_simplicial_set(X, n_neighbors, random_state, metric, metric_kwds, knn_indices, knn_dists, angular, set_op_mix_ratio, local_connectivity, apply_set_operations, verbose); 600 knn_dists = knn_dists.astype(np.float32); 601 ; --> 602 sigmas, rhos = smooth_knn_dist(; 603 knn_dists, float(n_neighbors), local_connectivity=float(local_connectivity),; 604 ). SystemError: CPUDispatcher(<function smooth_knn_dist at 0x150524c6cdc0>) returned a result with an error set. time: 3 s (started: 2021-08-23 11:59:12 +01:00); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983#issuecomment-903666863
https://github.com/scverse/scanpy/issues/1983#issuecomment-903666863:17,Deployability,update,update,17,"I just wanted to update that this issue does not depend on scvelo at all, but I can recreate it by just using scanpy. I suspect it is an issue with running umap. I'm using version '0.4.6'. Any help would be much appreciated:. ### Minimal code sample (that we can copy&paste without having any data). ```python; import os; import scanpy as sc; import numpy as np; import pandas as pd; import copy; import anndata; import matplotlib.pyplot as plt; adata_pbmc3k = sc.datasets.pbmc3k_processed(); #del adata_pbmc3k.obsm['X_pca']; #del adata_pbmc3k.obsm['X_umap']; del adata_pbmc3k.obsp['distances']; del adata_pbmc3k.obsp['connectivities']; #sc.pp.pca(adata_pbmc3k, n_comps=50); sc.pp.neighbors(adata_pbmc3k); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: expected dtype object, got 'numpy.dtype[float32]'. The above exception was the direct cause of the following exception:. SystemError Traceback (most recent call last); /hps/scratch/lsf_tmpdir/hl-codon-13-02/ipykernel_2124423/1009160698.py in <module>; ----> 1 sc.pp.neighbors(adata_pbmc3k). /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. /hps/software/users/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983#issuecomment-903666863
https://github.com/scverse/scanpy/issues/1983#issuecomment-903666863:49,Integrability,depend,depend,49,"I just wanted to update that this issue does not depend on scvelo at all, but I can recreate it by just using scanpy. I suspect it is an issue with running umap. I'm using version '0.4.6'. Any help would be much appreciated:. ### Minimal code sample (that we can copy&paste without having any data). ```python; import os; import scanpy as sc; import numpy as np; import pandas as pd; import copy; import anndata; import matplotlib.pyplot as plt; adata_pbmc3k = sc.datasets.pbmc3k_processed(); #del adata_pbmc3k.obsm['X_pca']; #del adata_pbmc3k.obsm['X_umap']; del adata_pbmc3k.obsp['distances']; del adata_pbmc3k.obsp['connectivities']; #sc.pp.pca(adata_pbmc3k, n_comps=50); sc.pp.neighbors(adata_pbmc3k); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: expected dtype object, got 'numpy.dtype[float32]'. The above exception was the direct cause of the following exception:. SystemError Traceback (most recent call last); /hps/scratch/lsf_tmpdir/hl-codon-13-02/ipykernel_2124423/1009160698.py in <module>; ----> 1 sc.pp.neighbors(adata_pbmc3k). /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. /hps/software/users/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983#issuecomment-903666863
https://github.com/scverse/scanpy/issues/1984#issuecomment-920706904:223,Testability,test,tests,223,"I guess both distance and connectivities matrices are full, and in order to get a sparse matrix, they apply a cutoff. I guess that that cutoff is a hard cutoff on distances, and a softer cutoff on connectivities. From some tests I've done for a particular project, in some datasets there are differences between distances and connectivities, and for others aren't. We'll have to wait to see the answer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1984#issuecomment-920706904
https://github.com/scverse/scanpy/pull/1985#issuecomment-907001063:6,Testability,test,tests,6,"> All tests in test_highly_variable_genes.py pass, but others like test_plotting.py::test_violin fail. I'm not sure why -- anyone have an idea?. They also fail for me for an unrelated change that does not even change code.; @ivirshup got any idea?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1985#issuecomment-907001063
https://github.com/scverse/scanpy/pull/1985#issuecomment-1105164387:50,Testability,test,test,50,"@pedrofale would you be able to add the suggested test for this, please?; If not, @lazappi PR will succeed this one and we will close this one. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1985#issuecomment-1105164387
https://github.com/scverse/scanpy/issues/1986#issuecomment-916368363:49,Availability,error,error,49,"@mumichae ; Thanks for letting us know about the error. For now if you need multiple scatter plots on a single figure you can do something like; `import matplotlib.pyplot as plt`; `f, axs = plt.subplots(2,2)`; `sc.pl.scatter(..., ax=axs[0,0])`; `sc.pl.scatter(..., ax=axs[1,0])`; ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1986#issuecomment-916368363
https://github.com/scverse/scanpy/issues/1988#issuecomment-1372293439:767,Integrability,Message,Message,767,"I completely understand but I’m travailing and have no access to my data. but it’s the basic function and you can use any anndata object and the groupby argument on any observation. Sent from my iPad. On 5. Jan 2023, at 15:11, Lukas Heumos ***@***.***> wrote:. ﻿. By a minimal working example I mean something that we can copy and paste and reproduce your result directly. The easier you make it for us the more likely we can dedicate some time to look at your issue. Thanks. —; Reply to this email directly, view it on GitHub<https://github.com/scverse/scanpy/issues/1988#issuecomment-1372263677>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AOGNBODKUC3S3SZC32VD3TLWQ3JBVANCNFSM5DB5VLAQ>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1988#issuecomment-1372293439
https://github.com/scverse/scanpy/issues/1988#issuecomment-1372293439:55,Security,access,access,55,"I completely understand but I’m travailing and have no access to my data. but it’s the basic function and you can use any anndata object and the groupby argument on any observation. Sent from my iPad. On 5. Jan 2023, at 15:11, Lukas Heumos ***@***.***> wrote:. ﻿. By a minimal working example I mean something that we can copy and paste and reproduce your result directly. The easier you make it for us the more likely we can dedicate some time to look at your issue. Thanks. —; Reply to this email directly, view it on GitHub<https://github.com/scverse/scanpy/issues/1988#issuecomment-1372263677>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AOGNBODKUC3S3SZC32VD3TLWQ3JBVANCNFSM5DB5VLAQ>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1988#issuecomment-1372293439
https://github.com/scverse/scanpy/issues/1988#issuecomment-1372299354:750,Integrability,Message,Message,750,"Here an example for the plot with NA:. The used obs has 2 categories (Levitas and none) and for the individual plots the groupby argument were „Levitas“ and „none“. Sent from my iPad. On 5. Jan 2023, at 15:11, Lukas Heumos ***@***.***> wrote:. ﻿. By a minimal working example I mean something that we can copy and paste and reproduce your result directly. The easier you make it for us the more likely we can dedicate some time to look at your issue. Thanks. —; Reply to this email directly, view it on GitHub<https://github.com/scverse/scanpy/issues/1988#issuecomment-1372263677>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AOGNBODKUC3S3SZC32VD3TLWQ3JBVANCNFSM5DB5VLAQ>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1988#issuecomment-1372299354
https://github.com/scverse/scanpy/issues/1990#issuecomment-916371848:84,Availability,error,errors,84,"Hi, @brianpenghe .; The current version of scanpy is 1.8.1, it is hard to check the errors for the version which is 1.5 year old (1.5.0). Could you try updating scanpy and checking if the error persists?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1990#issuecomment-916371848
https://github.com/scverse/scanpy/issues/1990#issuecomment-916371848:188,Availability,error,error,188,"Hi, @brianpenghe .; The current version of scanpy is 1.8.1, it is hard to check the errors for the version which is 1.5 year old (1.5.0). Could you try updating scanpy and checking if the error persists?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1990#issuecomment-916371848
https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475:95,Deployability,integrat,integrating,95,"Just came across this - is this still relevant?; Scanpy as is does not feature a neat solution integrating with interactive interfaces which would allow to manually tag/select individual points from a plot - for such tasks, [holoviz](https://holoviz.org/) tools might be considered.; As sidenote, a heads-up about considering 2D representations with caution e.g. [here](https://www.sciencedirect.com/science/article/pii/S2405471223002090?via%3Dihub) - considering metrics instead of visual low-dimensional representations to detect or remove outliers might be considered as a viable alternative here in many cases :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475
https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475:95,Integrability,integrat,integrating,95,"Just came across this - is this still relevant?; Scanpy as is does not feature a neat solution integrating with interactive interfaces which would allow to manually tag/select individual points from a plot - for such tasks, [holoviz](https://holoviz.org/) tools might be considered.; As sidenote, a heads-up about considering 2D representations with caution e.g. [here](https://www.sciencedirect.com/science/article/pii/S2405471223002090?via%3Dihub) - considering metrics instead of visual low-dimensional representations to detect or remove outliers might be considered as a viable alternative here in many cases :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475
https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475:124,Integrability,interface,interfaces,124,"Just came across this - is this still relevant?; Scanpy as is does not feature a neat solution integrating with interactive interfaces which would allow to manually tag/select individual points from a plot - for such tasks, [holoviz](https://holoviz.org/) tools might be considered.; As sidenote, a heads-up about considering 2D representations with caution e.g. [here](https://www.sciencedirect.com/science/article/pii/S2405471223002090?via%3Dihub) - considering metrics instead of visual low-dimensional representations to detect or remove outliers might be considered as a viable alternative here in many cases :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475
https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475:525,Safety,detect,detect,525,"Just came across this - is this still relevant?; Scanpy as is does not feature a neat solution integrating with interactive interfaces which would allow to manually tag/select individual points from a plot - for such tasks, [holoviz](https://holoviz.org/) tools might be considered.; As sidenote, a heads-up about considering 2D representations with caution e.g. [here](https://www.sciencedirect.com/science/article/pii/S2405471223002090?via%3Dihub) - considering metrics instead of visual low-dimensional representations to detect or remove outliers might be considered as a viable alternative here in many cases :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475
https://github.com/scverse/scanpy/pull/1994#issuecomment-947760319:14,Deployability,Pipeline,Pipelines,14,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1994#issuecomment-947760319
https://github.com/scverse/scanpy/pull/1994#issuecomment-947760319:55,Deployability,pipeline,pipeline,55,<samp>; Azure Pipelines successfully started running 1 pipeline(s).<br>. </samp>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1994#issuecomment-947760319
https://github.com/scverse/scanpy/issues/2000#issuecomment-920001620:40,Deployability,install,install,40,Dear @jfnavarro . we recommend that you install the latest scanpy version from conda-forge. ; We are currently updating our installation instructions https://github.com/theislab/scanpy/pull/1974,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000#issuecomment-920001620
https://github.com/scverse/scanpy/issues/2000#issuecomment-920001620:124,Deployability,install,installation,124,Dear @jfnavarro . we recommend that you install the latest scanpy version from conda-forge. ; We are currently updating our installation instructions https://github.com/theislab/scanpy/pull/1974,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000#issuecomment-920001620
https://github.com/scverse/scanpy/issues/2000#issuecomment-920976313:60,Availability,error,errors,60,"Hi @Zethson! Thanks for your reply. However, I get the same errors when pulling it from conda-forge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000#issuecomment-920976313
https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267:58,Deployability,install,installed,58,"Those packages are optional dependencies, and also aren't installed with `pip install scanpy`. You'll need to specify those separately if you'd like to use the features that require them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267
https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267:78,Deployability,install,install,78,"Those packages are optional dependencies, and also aren't installed with `pip install scanpy`. You'll need to specify those separately if you'd like to use the features that require them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267
https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267:28,Integrability,depend,dependencies,28,"Those packages are optional dependencies, and also aren't installed with `pip install scanpy`. You'll need to specify those separately if you'd like to use the features that require them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267
https://github.com/scverse/scanpy/issues/2001#issuecomment-1370720993:373,Modifiability,variab,variable,373,"I agree that such an opportunity totally should be, but the example given is not relevant, because to ingest data you have to have all the genes, which were involved in the determination of neighbors, to be present in the query dataset.; The relevant example would be when you somehow managed to count common embeddings (e.g. Harmony-adjusted PCA on the reference's highly variable genes and corresponding Symphony-adjusted PCA on the query), you can then use these embeddings to ingest Umap without the necessity for all the gene_names to be the same.; So maybe the check of equality of gene vars is required only if inside tl.ingest initial embeddings for the query are calculated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2001#issuecomment-1370720993
https://github.com/scverse/scanpy/issues/2003#issuecomment-920879255:18,Availability,error,error,18,I figure out this error by myself.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2003#issuecomment-920879255
https://github.com/scverse/scanpy/issues/2007#issuecomment-931566526:312,Testability,log,log-normalizing,312,"I figured it out. I forgot that I should (or at least, I think I should?) set `adata_sub.raw = adata.sub`. Adding this step before running a new embedding and clustering seemed to have fixed my issue. I guess a follow-up question would be is this an acceptable approach? I stored my full data set in `raw` after log-normalizing my data (so, `adata.raw = adata`) during initial preprocessing. Since `adata_sub` is just a subset of `adata`, I am guessing it is ok to set `adata_sub.raw = adata.sub`?. Apologies for opening this issue based on what was an oversight on my part.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2007#issuecomment-931566526
https://github.com/scverse/scanpy/issues/2008#issuecomment-944874522:291,Availability,error,error,291,"I'm also suddenly having this problem with ""ValueError: Length of values (1) does not match length of index()"" for certain Scanpy functions like `sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac')` and numpy functions `adata.obs['log_counts'] = np.log(adata.obs['n_counts'])`. The error is not due to a problem with my adata file because it reproduces with datasets that were previously error-free.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-944874522
https://github.com/scverse/scanpy/issues/2008#issuecomment-944874522:397,Availability,error,error-free,397,"I'm also suddenly having this problem with ""ValueError: Length of values (1) does not match length of index()"" for certain Scanpy functions like `sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac')` and numpy functions `adata.obs['log_counts'] = np.log(adata.obs['n_counts'])`. The error is not due to a problem with my adata file because it reproduces with datasets that were previously error-free.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-944874522
https://github.com/scverse/scanpy/issues/2008#issuecomment-944874522:258,Testability,log,log,258,"I'm also suddenly having this problem with ""ValueError: Length of values (1) does not match length of index()"" for certain Scanpy functions like `sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac')` and numpy functions `adata.obs['log_counts'] = np.log(adata.obs['n_counts'])`. The error is not due to a problem with my adata file because it reproduces with datasets that were previously error-free.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-944874522
https://github.com/scverse/scanpy/issues/2008#issuecomment-946696084:10,Deployability,update,update,10,"Could you update your version of scanpy and see if the issue persists? I believe this issue was an incompatibility with the 1.3.0 release of pandas https://github.com/pandas-dev/pandas/issues/42376, which was fixed for scanpy 1.8.1 (#1917)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-946696084
https://github.com/scverse/scanpy/issues/2008#issuecomment-946696084:130,Deployability,release,release,130,"Could you update your version of scanpy and see if the issue persists? I believe this issue was an incompatibility with the 1.3.0 release of pandas https://github.com/pandas-dev/pandas/issues/42376, which was fixed for scanpy 1.8.1 (#1917)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-946696084
https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666:448,Availability,error,error,448,"I've encountered similar issue last week and it was because `adata.obs` contained a column which was `n_obs x 1` `scipy.sparse.spmatrix`. The below code reproduces the formatter issue (`pandas==1.3.3`):. ```python; import scanpy as sc; from scipy.sparse import csr_matrix. adata = sc.datasets.pbmc3k(); adata.X = csr_matrix(adata.X); adata.obs['total_counts'] = adata.X.sum(1) # is sparse, pandas doesn't complain; adata.obs # raises the formatter error; ```. <details>. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/IPython/core/formatters.py in __call__(self, obj); 700 type_pprinters=self.type_printers,; 701 deferred_pprinters=self.deferred_printers); --> 702 printer.pretty(obj); 703 printer.flush(); 704 return stream.getvalue(). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/IPython/lib/pretty.py in pretty(self, obj); 392 if cls is not object \; 393 and callable(cls.__dict__.get('__repr__')):; --> 394 return _repr_pprint(obj, self, cycle); 395 ; 396 return _default_pprint(obj, self, cycle). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle); 698 """"""A pprint that just redirects to the normal repr function.""""""; 699 # Find newlines and replace them with p.break_(); --> 700 output = repr(obj); 701 lines = output.splitlines(); 702 with p.group():. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/core/frame.py in __repr__(self); 993 else:; 994 width = None; --> 995 self.to_string(; 996 buf=buf,; 997 max_rows=max_rows,. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/core/frame.py in to_string(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, min_rows, max_cols, show_dimensions, decimal, line_width, max_colwidth, encoding); 1129 decimal=decimal,; 1130 ); -> 1131 return fmt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666
https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666:5538,Availability,mask,mask,5538,"rings(); 1272 return _make_fixed_width(fmt_values, self.justify); 1273 . ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in _format_strings(self); 1516 ; 1517 def _format_strings(self) -> list[str]:; -> 1518 return list(self.get_result_as_array()); 1519 ; 1520 . ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in get_result_as_array(self); 1480 float_format = lambda value: self.float_format % value; 1481 ; -> 1482 formatted_values = format_values_with(float_format); 1483 ; 1484 if not self.fixed_width:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_values_with(float_format); 1454 values = self.values; 1455 is_complex = is_complex_dtype(values); -> 1456 values = format_with_na_rep(values, formatter, na_rep); 1457 ; 1458 if self.fixed_width:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_with_na_rep(values, formatter, na_rep); 1425 mask = isna(values); 1426 formatted = np.array(; -> 1427 [; 1428 formatter(val) if not m else na_rep; 1429 for val, m in zip(values.ravel(), mask.ravel()). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in <listcomp>(.0); 1426 formatted = np.array(; 1427 [; -> 1428 formatter(val) if not m else na_rep; 1429 for val, m in zip(values.ravel(), mask.ravel()); 1430 ]. ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/core/frame.py in _repr_html_(self); 1045 decimal=""."",; 10",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666
https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666:5679,Availability,mask,mask,5679,"ormat.py in _format_strings(self); 1516 ; 1517 def _format_strings(self) -> list[str]:; -> 1518 return list(self.get_result_as_array()); 1519 ; 1520 . ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in get_result_as_array(self); 1480 float_format = lambda value: self.float_format % value; 1481 ; -> 1482 formatted_values = format_values_with(float_format); 1483 ; 1484 if not self.fixed_width:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_values_with(float_format); 1454 values = self.values; 1455 is_complex = is_complex_dtype(values); -> 1456 values = format_with_na_rep(values, formatter, na_rep); 1457 ; 1458 if self.fixed_width:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_with_na_rep(values, formatter, na_rep); 1425 mask = isna(values); 1426 formatted = np.array(; -> 1427 [; 1428 formatter(val) if not m else na_rep; 1429 for val, m in zip(values.ravel(), mask.ravel()). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in <listcomp>(.0); 1426 formatted = np.array(; 1427 [; -> 1428 formatter(val) if not m else na_rep; 1429 for val, m in zip(values.ravel(), mask.ravel()); 1430 ]. ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/core/frame.py in _repr_html_(self); 1045 decimal=""."",; 1046 ); -> 1047 return fmt.DataFrameRenderer(formatter).to_html(notebook=True); 1048 else:; 1049 return None. ~/.miniconda3/envs/cellrank/lib/python3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666
https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666:5917,Availability,mask,mask,5917,"get_result_as_array(self); 1480 float_format = lambda value: self.float_format % value; 1481 ; -> 1482 formatted_values = format_values_with(float_format); 1483 ; 1484 if not self.fixed_width:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_values_with(float_format); 1454 values = self.values; 1455 is_complex = is_complex_dtype(values); -> 1456 values = format_with_na_rep(values, formatter, na_rep); 1457 ; 1458 if self.fixed_width:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_with_na_rep(values, formatter, na_rep); 1425 mask = isna(values); 1426 formatted = np.array(; -> 1427 [; 1428 formatter(val) if not m else na_rep; 1429 for val, m in zip(values.ravel(), mask.ravel()). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in <listcomp>(.0); 1426 formatted = np.array(; 1427 [; -> 1428 formatter(val) if not m else na_rep; 1429 for val, m in zip(values.ravel(), mask.ravel()); 1430 ]. ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/IPython/core/formatters.py in __call__(self, obj); 343 method = get_real_method(obj, self.print_method); 344 if method is not None:; --> 345 return method(); 346 return None; 347 else:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/core/frame.py in _repr_html_(self); 1045 decimal=""."",; 1046 ); -> 1047 return fmt.DataFrameRenderer(formatter).to_html(notebook=True); 1048 else:; 1049 return None. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in to_html(self, buf, encoding, classes, notebook, border, table_id, render_links); 1027 render_links=render_links,; 1028 ); -> 1029 string = html_formatter.to_string(); 1030 return save_to_buf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666
https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666:10529,Availability,mask,mask,10529,"20 formatter,. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_array(values, formatter, float_format, na_rep, digits, space, justify, decimal, leading_space, quoting); 1238 ); 1239 ; -> 1240 return fmt_obj.get_result(); 1241 ; 1242 . ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in get_result(self); 1269 ; 1270 def get_result(self) -> list[str]:; -> 1271 fmt_values = self._format_strings(); 1272 return _make_fixed_width(fmt_values, self.justify); 1273 . ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in _format_strings(self); 1516 ; 1517 def _format_strings(self) -> list[str]:; -> 1518 return list(self.get_result_as_array()); 1519 ; 1520 . ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in get_result_as_array(self); 1480 float_format = lambda value: self.float_format % value; 1481 ; -> 1482 formatted_values = format_values_with(float_format); 1483 ; 1484 if not self.fixed_width:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_values_with(float_format); 1454 values = self.values; 1455 is_complex = is_complex_dtype(values); -> 1456 values = format_with_na_rep(values, formatter, na_rep); 1457 ; 1458 if self.fixed_width:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_with_na_rep(values, formatter, na_rep); 1425 mask = isna(values); 1426 formatted = np.array(; -> 1427 [; 1428 formatter(val) if not m else na_rep; 1429 for val, m in zip(values.ravel(), mask.ravel()). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in <listcomp>(.0); 1426 formatted = np.array(; 1427 [; -> 1428 formatter(val) if not m else na_rep; 1429 for val, m in zip(values.ravel(), mask.ravel()); 1430 ]. ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(); ```; </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666
https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666:10670,Availability,mask,mask,10670,"20 formatter,. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_array(values, formatter, float_format, na_rep, digits, space, justify, decimal, leading_space, quoting); 1238 ); 1239 ; -> 1240 return fmt_obj.get_result(); 1241 ; 1242 . ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in get_result(self); 1269 ; 1270 def get_result(self) -> list[str]:; -> 1271 fmt_values = self._format_strings(); 1272 return _make_fixed_width(fmt_values, self.justify); 1273 . ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in _format_strings(self); 1516 ; 1517 def _format_strings(self) -> list[str]:; -> 1518 return list(self.get_result_as_array()); 1519 ; 1520 . ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in get_result_as_array(self); 1480 float_format = lambda value: self.float_format % value; 1481 ; -> 1482 formatted_values = format_values_with(float_format); 1483 ; 1484 if not self.fixed_width:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_values_with(float_format); 1454 values = self.values; 1455 is_complex = is_complex_dtype(values); -> 1456 values = format_with_na_rep(values, formatter, na_rep); 1457 ; 1458 if self.fixed_width:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_with_na_rep(values, formatter, na_rep); 1425 mask = isna(values); 1426 formatted = np.array(; -> 1427 [; 1428 formatter(val) if not m else na_rep; 1429 for val, m in zip(values.ravel(), mask.ravel()). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in <listcomp>(.0); 1426 formatted = np.array(; 1427 [; -> 1428 formatter(val) if not m else na_rep; 1429 for val, m in zip(values.ravel(), mask.ravel()); 1430 ]. ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(); ```; </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666
https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666:10908,Availability,mask,mask,10908,"20 formatter,. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_array(values, formatter, float_format, na_rep, digits, space, justify, decimal, leading_space, quoting); 1238 ); 1239 ; -> 1240 return fmt_obj.get_result(); 1241 ; 1242 . ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in get_result(self); 1269 ; 1270 def get_result(self) -> list[str]:; -> 1271 fmt_values = self._format_strings(); 1272 return _make_fixed_width(fmt_values, self.justify); 1273 . ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in _format_strings(self); 1516 ; 1517 def _format_strings(self) -> list[str]:; -> 1518 return list(self.get_result_as_array()); 1519 ; 1520 . ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in get_result_as_array(self); 1480 float_format = lambda value: self.float_format % value; 1481 ; -> 1482 formatted_values = format_values_with(float_format); 1483 ; 1484 if not self.fixed_width:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_values_with(float_format); 1454 values = self.values; 1455 is_complex = is_complex_dtype(values); -> 1456 values = format_with_na_rep(values, formatter, na_rep); 1457 ; 1458 if self.fixed_width:. ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in format_with_na_rep(values, formatter, na_rep); 1425 mask = isna(values); 1426 formatted = np.array(; -> 1427 [; 1428 formatter(val) if not m else na_rep; 1429 for val, m in zip(values.ravel(), mask.ravel()). ~/.miniconda3/envs/cellrank/lib/python3.8/site-packages/pandas/io/formats/format.py in <listcomp>(.0); 1426 formatted = np.array(; 1427 [; -> 1428 formatter(val) if not m else na_rep; 1429 for val, m in zip(values.ravel(), mask.ravel()); 1430 ]. ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(); ```; </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-947943666
https://github.com/scverse/scanpy/issues/2008#issuecomment-948025046:509,Availability,error,error,509,"With pandas 1.3.4 and 1.3.3. * I can't replicate the initial issue; * I can replicate @michalk8's example. This looks very upstream in pandas. I will try and submit an issue/ check that this hasn't been reported to pandas already tomorrow. This may be a kinda easy fix (e.g. check value shape better during column assignment in pandas), but it can take a bit to figure out how to fix things there. AFAIK, we removed calls in scanpy which assigned (n x 1) matrices to pandas because of related, non-formatting error. Is the current scanpy release assigning these matrices anywhere?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-948025046
https://github.com/scverse/scanpy/issues/2008#issuecomment-948025046:538,Deployability,release,release,538,"With pandas 1.3.4 and 1.3.3. * I can't replicate the initial issue; * I can replicate @michalk8's example. This looks very upstream in pandas. I will try and submit an issue/ check that this hasn't been reported to pandas already tomorrow. This may be a kinda easy fix (e.g. check value shape better during column assignment in pandas), but it can take a bit to figure out how to fix things there. AFAIK, we removed calls in scanpy which assigned (n x 1) matrices to pandas because of related, non-formatting error. Is the current scanpy release assigning these matrices anywhere?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-948025046
https://github.com/scverse/scanpy/issues/2008#issuecomment-948031374:230,Availability,down,downgrading,230,"Many thanks for everyone's input. The bug is indeed due to an issue with Pandas ≥1.3. I am running Scanpy 1.8.1 and I can confirm that the indexing problem remains with Pandas 1.3.0, 1.3.2, and the latest 1.3.4, but resolves when downgrading to 1.2.5",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-948031374
https://github.com/scverse/scanpy/issues/2008#issuecomment-981383663:62,Availability,down,downgrading,62,"thanks for everyone's input. I tried to solve this problem by downgrading pandas to 1.1.5. the cause of this problem may be that in python 3.9 and above, pandas modifies the matrix function",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008#issuecomment-981383663
https://github.com/scverse/scanpy/issues/2014#issuecomment-944868481:112,Availability,down,download,112,"Hi, . I updated the pipeline to use [this singularity container](https://github.com/icbi-lab/borst2021/releases/download/containers-0.2.0/vanderburg_edger.sif). The problem persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-944868481
https://github.com/scverse/scanpy/issues/2014#issuecomment-944868481:8,Deployability,update,updated,8,"Hi, . I updated the pipeline to use [this singularity container](https://github.com/icbi-lab/borst2021/releases/download/containers-0.2.0/vanderburg_edger.sif). The problem persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-944868481
https://github.com/scverse/scanpy/issues/2014#issuecomment-944868481:20,Deployability,pipeline,pipeline,20,"Hi, . I updated the pipeline to use [this singularity container](https://github.com/icbi-lab/borst2021/releases/download/containers-0.2.0/vanderburg_edger.sif). The problem persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-944868481
https://github.com/scverse/scanpy/issues/2014#issuecomment-944868481:103,Deployability,release,releases,103,"Hi, . I updated the pipeline to use [this singularity container](https://github.com/icbi-lab/borst2021/releases/download/containers-0.2.0/vanderburg_edger.sif). The problem persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-944868481
https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365:336,Availability,down,downstream,336,"That's a good point, and it is not:; ```python; reducer = umap.UMAP(min_dist=0.5); embedding = reducer.fit_transform(adata.obsm[""X_scVI""]); adata.obsm[""X_umap""] = embedding; ```; again produces stable results on only 3/4 CPUs. . Ok, let's forget about UMAP. It's only a nice figure to get an overview of the data and I don't use it for downstream stuff. Irreproducible clustering, on the other hand, is quite a deal-breaker, as for instance cell-type annotations depend on it. I mean, why would I even bother releasing the source code of an analysis alongside the paper if it is not reproducible anyway? . I found out a few more things: ; - the leiden algorithm itself seems deterministic on all 4 nodes, when started from a pre-computed `adata.obsp[""connectivities""]`. ; - when running `pp.neighbors` with `NUMBA_DISABLE_JIT=1`, the clustering is stable on all four nodes (but terribly slow, ofc); - when rounding the connectivities to 3-4 digits, the clustering is also stable (plus the total runtime is reduced from 2:30 to 1:50min). ```python; adata.obsp[""connectivities""] = np.round(adata.obsp[""connectivities""], decimals=3); adata.obsp[""distances""] = np.round(adata.obsp[""distances""], decimals=3); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365
https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365:48,Energy Efficiency,reduce,reducer,48,"That's a good point, and it is not:; ```python; reducer = umap.UMAP(min_dist=0.5); embedding = reducer.fit_transform(adata.obsm[""X_scVI""]); adata.obsm[""X_umap""] = embedding; ```; again produces stable results on only 3/4 CPUs. . Ok, let's forget about UMAP. It's only a nice figure to get an overview of the data and I don't use it for downstream stuff. Irreproducible clustering, on the other hand, is quite a deal-breaker, as for instance cell-type annotations depend on it. I mean, why would I even bother releasing the source code of an analysis alongside the paper if it is not reproducible anyway? . I found out a few more things: ; - the leiden algorithm itself seems deterministic on all 4 nodes, when started from a pre-computed `adata.obsp[""connectivities""]`. ; - when running `pp.neighbors` with `NUMBA_DISABLE_JIT=1`, the clustering is stable on all four nodes (but terribly slow, ofc); - when rounding the connectivities to 3-4 digits, the clustering is also stable (plus the total runtime is reduced from 2:30 to 1:50min). ```python; adata.obsp[""connectivities""] = np.round(adata.obsp[""connectivities""], decimals=3); adata.obsp[""distances""] = np.round(adata.obsp[""distances""], decimals=3); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365
https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365:95,Energy Efficiency,reduce,reducer,95,"That's a good point, and it is not:; ```python; reducer = umap.UMAP(min_dist=0.5); embedding = reducer.fit_transform(adata.obsm[""X_scVI""]); adata.obsm[""X_umap""] = embedding; ```; again produces stable results on only 3/4 CPUs. . Ok, let's forget about UMAP. It's only a nice figure to get an overview of the data and I don't use it for downstream stuff. Irreproducible clustering, on the other hand, is quite a deal-breaker, as for instance cell-type annotations depend on it. I mean, why would I even bother releasing the source code of an analysis alongside the paper if it is not reproducible anyway? . I found out a few more things: ; - the leiden algorithm itself seems deterministic on all 4 nodes, when started from a pre-computed `adata.obsp[""connectivities""]`. ; - when running `pp.neighbors` with `NUMBA_DISABLE_JIT=1`, the clustering is stable on all four nodes (but terribly slow, ofc); - when rounding the connectivities to 3-4 digits, the clustering is also stable (plus the total runtime is reduced from 2:30 to 1:50min). ```python; adata.obsp[""connectivities""] = np.round(adata.obsp[""connectivities""], decimals=3); adata.obsp[""distances""] = np.round(adata.obsp[""distances""], decimals=3); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365
https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365:1006,Energy Efficiency,reduce,reduced,1006,"That's a good point, and it is not:; ```python; reducer = umap.UMAP(min_dist=0.5); embedding = reducer.fit_transform(adata.obsm[""X_scVI""]); adata.obsm[""X_umap""] = embedding; ```; again produces stable results on only 3/4 CPUs. . Ok, let's forget about UMAP. It's only a nice figure to get an overview of the data and I don't use it for downstream stuff. Irreproducible clustering, on the other hand, is quite a deal-breaker, as for instance cell-type annotations depend on it. I mean, why would I even bother releasing the source code of an analysis alongside the paper if it is not reproducible anyway? . I found out a few more things: ; - the leiden algorithm itself seems deterministic on all 4 nodes, when started from a pre-computed `adata.obsp[""connectivities""]`. ; - when running `pp.neighbors` with `NUMBA_DISABLE_JIT=1`, the clustering is stable on all four nodes (but terribly slow, ofc); - when rounding the connectivities to 3-4 digits, the clustering is also stable (plus the total runtime is reduced from 2:30 to 1:50min). ```python; adata.obsp[""connectivities""] = np.round(adata.obsp[""connectivities""], decimals=3); adata.obsp[""distances""] = np.round(adata.obsp[""distances""], decimals=3); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365
https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365:463,Integrability,depend,depend,463,"That's a good point, and it is not:; ```python; reducer = umap.UMAP(min_dist=0.5); embedding = reducer.fit_transform(adata.obsm[""X_scVI""]); adata.obsm[""X_umap""] = embedding; ```; again produces stable results on only 3/4 CPUs. . Ok, let's forget about UMAP. It's only a nice figure to get an overview of the data and I don't use it for downstream stuff. Irreproducible clustering, on the other hand, is quite a deal-breaker, as for instance cell-type annotations depend on it. I mean, why would I even bother releasing the source code of an analysis alongside the paper if it is not reproducible anyway? . I found out a few more things: ; - the leiden algorithm itself seems deterministic on all 4 nodes, when started from a pre-computed `adata.obsp[""connectivities""]`. ; - when running `pp.neighbors` with `NUMBA_DISABLE_JIT=1`, the clustering is stable on all four nodes (but terribly slow, ofc); - when rounding the connectivities to 3-4 digits, the clustering is also stable (plus the total runtime is reduced from 2:30 to 1:50min). ```python; adata.obsp[""connectivities""] = np.round(adata.obsp[""connectivities""], decimals=3); adata.obsp[""distances""] = np.round(adata.obsp[""distances""], decimals=3); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365
https://github.com/scverse/scanpy/issues/2014#issuecomment-946665831:14,Deployability,install,install,14,"I have yet to install the most latest `scanpy` and I do not have CPUs to test for this specific case, but I had some issue of reproducing `leiden` results from `scanpy` from a published paper, and found that running `leiden` 10 times (per `n_iteration` option) resolved any discrepancy. Wonder whether it adds to the discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946665831
https://github.com/scverse/scanpy/issues/2014#issuecomment-946665831:73,Testability,test,test,73,"I have yet to install the most latest `scanpy` and I do not have CPUs to test for this specific case, but I had some issue of reproducing `leiden` results from `scanpy` from a published paper, and found that running `leiden` 10 times (per `n_iteration` option) resolved any discrepancy. Wonder whether it adds to the discussion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946665831
https://github.com/scverse/scanpy/issues/2014#issuecomment-946679078:711,Performance,perform,performance,711,"@grst I don't think `leiden` is the issue here, but `pynndescent`. My guess is this is going to have to do with the CPU that gives different results being much older using a different instruction set that the other intel processors. This could be triggered by either use of any parallelism at all or `pynndescent` being pretty liberal with the use of `numba`'s `fastmath`, and different CPUs having different features. Do you get the same graph out of `pynndescent` if you are make the computation single threaded? If not, we may be able to look at the assembly to see which feature sets give different results. It's possible these can be turned off with a flag. But we may then have to consider what kind of a performance hit we'd be willing to take for exact reproducibility on discontinued chip sets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946679078
https://github.com/scverse/scanpy/issues/2014#issuecomment-946714555:132,Modifiability,variab,variables,132,"Alright, if you would like to achieve reproducibility the next things to play around with would probably be [these CPU feature flag variables](https://numba.pydata.org/numba-doc/dev/reference/envvars.html#compilation-options) for numba. In particular: `NUMBA_CPU_NAME=generic`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946714555
https://github.com/scverse/scanpy/issues/2014#issuecomment-946754682:262,Testability,benchmark,benchmarking,262,"another :100: . with `NUMBA_CPU_NAME=generic` the leiden results are the same on all four nodes and also the UMAPs look a lot more similar (but not identical). . In terms of speed I didn't notice a bit difference (maybe 10s slower, but that would require proper benchmarking ofc).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946754682
https://github.com/scverse/scanpy/issues/2014#issuecomment-2047400078:436,Usability,Guid,Guide,436,"> How about creating a page about reproducibility in the docs, similar to the [one by pytorch](https://pytorch.org/docs/stable/notes/randomness.html)? It could gather all information around reproducibility with scanpy, such as; > ; > * use of containers; > * numba flags. @grst, this would be great. Would you be up for writing this at some point?. I'm thinking it could either go:. * As a how-to page if it's example driven; * A ""User Guide"" page, where we'd start a ""User Guide"" that would contain this and ""Usage principles""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-2047400078
https://github.com/scverse/scanpy/issues/2014#issuecomment-2047400078:474,Usability,Guid,Guide,474,"> How about creating a page about reproducibility in the docs, similar to the [one by pytorch](https://pytorch.org/docs/stable/notes/randomness.html)? It could gather all information around reproducibility with scanpy, such as; > ; > * use of containers; > * numba flags. @grst, this would be great. Would you be up for writing this at some point?. I'm thinking it could either go:. * As a how-to page if it's example driven; * A ""User Guide"" page, where we'd start a ""User Guide"" that would contain this and ""Usage principles""",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-2047400078
https://github.com/scverse/scanpy/issues/2014#issuecomment-2047411696:28,Modifiability,extend,extending,28,We could then also consider extending it with rapids single cell @Intron7 . More of a `scverse reproducibility` page and maybe also bring in scvi tools,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-2047411696
https://github.com/scverse/scanpy/issues/2016#issuecomment-948039275:239,Energy Efficiency,Green,GreenleafLab,239,Yes I think traversing a minimum spanning tree is good enough since you can never perfectly order cell states in one dimension while capturing all the key features.; ArchR's code to do clustering (addClusters) is here:; https://github.com/GreenleafLab/ArchR/blob/master/R/Clustering.R. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2016#issuecomment-948039275
https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348:1021,Availability,down,downsides,1021,"[Here are the specific lines for their cluster ordering](https://github.com/GreenleafLab/ArchR/blob/6765ad962d4d8dcb292a326071c9b5c30c25918e/R/Clustering.R#L368-L383). They do a hierarchical clustering on the mean position of each cluster in the reduced dimensional space. We don't necessarily have access to that space (which may not even exist, e.g. BBKNN graph) at clustering time so we can't use this exact method. ### Current thoughts. My preferences in APIs lean towards modularity and shallowness. I like that the `leiden` function pretty much only computes `leiden` clusters, nothing else. I don't love the idea of adding complexity or computation on top of that. I also think ""gives better label orderings"" is a vague target which is hard to have meaningful tests for, so can be difficult to support. I think this would be a little convenient, but I don't see it being very convenient. I would like to hear if other people would really like this feature. At the moment, I don't think it's utility outweighs it's downsides to me. What I would be more for is some sort of `relabel_clusterings` utility function, which just does the relabelling and could have multiple ways of doing so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348
https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348:76,Energy Efficiency,Green,GreenleafLab,76,"[Here are the specific lines for their cluster ordering](https://github.com/GreenleafLab/ArchR/blob/6765ad962d4d8dcb292a326071c9b5c30c25918e/R/Clustering.R#L368-L383). They do a hierarchical clustering on the mean position of each cluster in the reduced dimensional space. We don't necessarily have access to that space (which may not even exist, e.g. BBKNN graph) at clustering time so we can't use this exact method. ### Current thoughts. My preferences in APIs lean towards modularity and shallowness. I like that the `leiden` function pretty much only computes `leiden` clusters, nothing else. I don't love the idea of adding complexity or computation on top of that. I also think ""gives better label orderings"" is a vague target which is hard to have meaningful tests for, so can be difficult to support. I think this would be a little convenient, but I don't see it being very convenient. I would like to hear if other people would really like this feature. At the moment, I don't think it's utility outweighs it's downsides to me. What I would be more for is some sort of `relabel_clusterings` utility function, which just does the relabelling and could have multiple ways of doing so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348
https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348:246,Energy Efficiency,reduce,reduced,246,"[Here are the specific lines for their cluster ordering](https://github.com/GreenleafLab/ArchR/blob/6765ad962d4d8dcb292a326071c9b5c30c25918e/R/Clustering.R#L368-L383). They do a hierarchical clustering on the mean position of each cluster in the reduced dimensional space. We don't necessarily have access to that space (which may not even exist, e.g. BBKNN graph) at clustering time so we can't use this exact method. ### Current thoughts. My preferences in APIs lean towards modularity and shallowness. I like that the `leiden` function pretty much only computes `leiden` clusters, nothing else. I don't love the idea of adding complexity or computation on top of that. I also think ""gives better label orderings"" is a vague target which is hard to have meaningful tests for, so can be difficult to support. I think this would be a little convenient, but I don't see it being very convenient. I would like to hear if other people would really like this feature. At the moment, I don't think it's utility outweighs it's downsides to me. What I would be more for is some sort of `relabel_clusterings` utility function, which just does the relabelling and could have multiple ways of doing so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348
https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348:299,Security,access,access,299,"[Here are the specific lines for their cluster ordering](https://github.com/GreenleafLab/ArchR/blob/6765ad962d4d8dcb292a326071c9b5c30c25918e/R/Clustering.R#L368-L383). They do a hierarchical clustering on the mean position of each cluster in the reduced dimensional space. We don't necessarily have access to that space (which may not even exist, e.g. BBKNN graph) at clustering time so we can't use this exact method. ### Current thoughts. My preferences in APIs lean towards modularity and shallowness. I like that the `leiden` function pretty much only computes `leiden` clusters, nothing else. I don't love the idea of adding complexity or computation on top of that. I also think ""gives better label orderings"" is a vague target which is hard to have meaningful tests for, so can be difficult to support. I think this would be a little convenient, but I don't see it being very convenient. I would like to hear if other people would really like this feature. At the moment, I don't think it's utility outweighs it's downsides to me. What I would be more for is some sort of `relabel_clusterings` utility function, which just does the relabelling and could have multiple ways of doing so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348
https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348:767,Testability,test,tests,767,"[Here are the specific lines for their cluster ordering](https://github.com/GreenleafLab/ArchR/blob/6765ad962d4d8dcb292a326071c9b5c30c25918e/R/Clustering.R#L368-L383). They do a hierarchical clustering on the mean position of each cluster in the reduced dimensional space. We don't necessarily have access to that space (which may not even exist, e.g. BBKNN graph) at clustering time so we can't use this exact method. ### Current thoughts. My preferences in APIs lean towards modularity and shallowness. I like that the `leiden` function pretty much only computes `leiden` clusters, nothing else. I don't love the idea of adding complexity or computation on top of that. I also think ""gives better label orderings"" is a vague target which is hard to have meaningful tests for, so can be difficult to support. I think this would be a little convenient, but I don't see it being very convenient. I would like to hear if other people would really like this feature. At the moment, I don't think it's utility outweighs it's downsides to me. What I would be more for is some sort of `relabel_clusterings` utility function, which just does the relabelling and could have multiple ways of doing so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2016#issuecomment-948076348
https://github.com/scverse/scanpy/issues/2016#issuecomment-980480553:221,Security,access,access,221,"Hi @brianpenghe ; I've been fiddling with this a little bit and this may be helpful for your request if you haven't come across it already, there is a 'categories_ordered' field in the dendrogram information that you can access as ; adata.uns[DENDROGRAM_NAME]['categories_ordered']; which I think gives what you would want",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2016#issuecomment-980480553
https://github.com/scverse/scanpy/pull/2023#issuecomment-963035287:204,Availability,failure,failure,204,"> * Make a case where a threshold can't be found (not sure how this would be done). Obviously I can test the plotting by directly unsetting the relevant things, but I'm not actually sure how to trigger a failure with the test data I'm afraid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2023#issuecomment-963035287
https://github.com/scverse/scanpy/pull/2023#issuecomment-963035287:100,Testability,test,test,100,"> * Make a case where a threshold can't be found (not sure how this would be done). Obviously I can test the plotting by directly unsetting the relevant things, but I'm not actually sure how to trigger a failure with the test data I'm afraid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2023#issuecomment-963035287
https://github.com/scverse/scanpy/pull/2023#issuecomment-963035287:221,Testability,test,test,221,"> * Make a case where a threshold can't be found (not sure how this would be done). Obviously I can test the plotting by directly unsetting the relevant things, but I'm not actually sure how to trigger a failure with the test data I'm afraid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2023#issuecomment-963035287
https://github.com/scverse/scanpy/pull/2025#issuecomment-959531315:16,Testability,test,test,16,Could you add a test to make sure the correct values are being used?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-959531315
https://github.com/scverse/scanpy/pull/2025#issuecomment-963032563:18,Testability,test,test,18,> Could you add a test to make sure the correct values are being used?. What did you have in mind? A minimum value (e.g. 20?) for the max to check that unlogged values are input?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963032563
https://github.com/scverse/scanpy/pull/2025#issuecomment-963265836:17,Testability,test,test,17,"Basically just a test to make sure the underlying issue is actually corrected. In this example:. ```python; adata_sim = scrublet_simulate_doublets(; adata_obs,; layer='raw',; sim_doublet_ratio=sim_doublet_ratio,; synthetic_doublet_umi_subsampling=synthetic_doublet_umi_subsampling,; ); ```. `.layer[""raw""]` should contain integer data, which you should be able to reconstruct from the parent cells, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963265836
https://github.com/scverse/scanpy/pull/2025#issuecomment-963353519:132,Testability,log,log,132,"Sorry @ivirshup , still not quite getting what you're after. The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Non-integer values don't necessarily mean normalised or log'd data, so I can't add a check in the code for that. I could add a check on the values of the simulated doublets, that they are the sum of the parent counts we send to scrublet's simulate function, but that seems outside the scope of these fixes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963353519
https://github.com/scverse/scanpy/pull/2025#issuecomment-963353519:341,Testability,log,log,341,"Sorry @ivirshup , still not quite getting what you're after. The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Non-integer values don't necessarily mean normalised or log'd data, so I can't add a check in the code for that. I could add a check on the values of the simulated doublets, that they are the sum of the parent counts we send to scrublet's simulate function, but that seems outside the scope of these fixes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963353519
https://github.com/scverse/scanpy/pull/2025#issuecomment-963353519:212,Usability,simpl,simple,212,"Sorry @ivirshup , still not quite getting what you're after. The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Non-integer values don't necessarily mean normalised or log'd data, so I can't add a check in the code for that. I could add a check on the values of the simulated doublets, that they are the sum of the parent counts we send to scrublet's simulate function, but that seems outside the scope of these fixes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963353519
https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664:73,Testability,log,log,73,"> The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process. So a test could be checking that if you passed correctly simulated data into `sc.external.pp.scrublet` as `adata_sim`, you get the same result as letting the function simulate the data itself. You could recreate the simulation using the `.uns['scrublet']['doublet_parents']` field:. ```python; def create_sim_from_parents(adata, parents):; N_sim = parents.shape[0]; I = sparse.coo_matrix(; (; np.ones(2 * N_sim),; (np.repeat(np.arange(N_sim), 2), parents.flat),; ),; (N_sim, adata.n_obs); ); X = I @ adata.X; return ad.AnnData(; X,; var=pd.DataFrame(index=adata.var_names),; obs=pd.DataFrame({""total_counts"": np.ravel(X.sum(axis=1))}),; obsm={""doublet_parents"": parents.copy()},; ); ```. > (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Yeah, I do see from the code what was going wrong. The issue is more that I want a check to be sure it does not go wrong again. These things clearly get through code review, but it's harder for them to get through a test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664
https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664:128,Testability,test,test,128,"> The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process. So a test could be checking that if you passed correctly simulated data into `sc.external.pp.scrublet` as `adata_sim`, you get the same result as letting the function simulate the data itself. You could recreate the simulation using the `.uns['scrublet']['doublet_parents']` field:. ```python; def create_sim_from_parents(adata, parents):; N_sim = parents.shape[0]; I = sparse.coo_matrix(; (; np.ones(2 * N_sim),; (np.repeat(np.arange(N_sim), 2), parents.flat),; ),; (N_sim, adata.n_obs); ); X = I @ adata.X; return ad.AnnData(; X,; var=pd.DataFrame(index=adata.var_names),; obs=pd.DataFrame({""total_counts"": np.ravel(X.sum(axis=1))}),; obsm={""doublet_parents"": parents.copy()},; ); ```. > (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Yeah, I do see from the code what was going wrong. The issue is more that I want a check to be sure it does not go wrong again. These things clearly get through code review, but it's harder for them to get through a test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664
https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664:1133,Testability,test,test,1133,"> The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process. So a test could be checking that if you passed correctly simulated data into `sc.external.pp.scrublet` as `adata_sim`, you get the same result as letting the function simulate the data itself. You could recreate the simulation using the `.uns['scrublet']['doublet_parents']` field:. ```python; def create_sim_from_parents(adata, parents):; N_sim = parents.shape[0]; I = sparse.coo_matrix(; (; np.ones(2 * N_sim),; (np.repeat(np.arange(N_sim), 2), parents.flat),; ),; (N_sim, adata.n_obs); ); X = I @ adata.X; return ad.AnnData(; X,; var=pd.DataFrame(index=adata.var_names),; obs=pd.DataFrame({""total_counts"": np.ravel(X.sum(axis=1))}),; obsm={""doublet_parents"": parents.copy()},; ); ```. > (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Yeah, I do see from the code what was going wrong. The issue is more that I want a check to be sure it does not go wrong again. These things clearly get through code review, but it's harder for them to get through a test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664
https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664:844,Usability,simpl,simple,844,"> The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process. So a test could be checking that if you passed correctly simulated data into `sc.external.pp.scrublet` as `adata_sim`, you get the same result as letting the function simulate the data itself. You could recreate the simulation using the `.uns['scrublet']['doublet_parents']` field:. ```python; def create_sim_from_parents(adata, parents):; N_sim = parents.shape[0]; I = sparse.coo_matrix(; (; np.ones(2 * N_sim),; (np.repeat(np.arange(N_sim), 2), parents.flat),; ),; (N_sim, adata.n_obs); ); X = I @ adata.X; return ad.AnnData(; X,; var=pd.DataFrame(index=adata.var_names),; obs=pd.DataFrame({""total_counts"": np.ravel(X.sum(axis=1))}),; obsm={""doublet_parents"": parents.copy()},; ); ```. > (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Yeah, I do see from the code what was going wrong. The issue is more that I want a check to be sure it does not go wrong again. These things clearly get through code review, but it's harder for them to get through a test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664
https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664:1058,Usability,clear,clearly,1058,"> The underlying issues were with a missing .copy() (now added) and with log'd values getting into the simulation process. So a test could be checking that if you passed correctly simulated data into `sc.external.pp.scrublet` as `adata_sim`, you get the same result as letting the function simulate the data itself. You could recreate the simulation using the `.uns['scrublet']['doublet_parents']` field:. ```python; def create_sim_from_parents(adata, parents):; N_sim = parents.shape[0]; I = sparse.coo_matrix(; (; np.ones(2 * N_sim),; (np.repeat(np.arange(N_sim), 2), parents.flat),; ),; (N_sim, adata.n_obs); ); X = I @ adata.X; return ad.AnnData(; X,; var=pd.DataFrame(index=adata.var_names),; obs=pd.DataFrame({""total_counts"": np.ravel(X.sum(axis=1))}),; obsm={""doublet_parents"": parents.copy()},; ); ```. > (which is now prevented with a simple code rearrangement). I think those fixes are pretty self-evident. Yeah, I do see from the code what was going wrong. The issue is more that I want a check to be sure it does not go wrong again. These things clearly get through code review, but it's harder for them to get through a test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-963418664
https://github.com/scverse/scanpy/pull/2025#issuecomment-969366293:85,Testability,test,tests,85,"@ivirshup okay, I've tried that (thanks for the snazzy function), fingers crossed on tests. This is somewhat involved. When providing adata_sim I had, by design, made the assumption that the user will have done all their own preprocessing. So to make the comparison you requested (sc.external.pp.scrublet with/without adata_sim) I needed to basically replicate all that preprocessing verbatim in the test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-969366293
https://github.com/scverse/scanpy/pull/2025#issuecomment-969366293:400,Testability,test,test,400,"@ivirshup okay, I've tried that (thanks for the snazzy function), fingers crossed on tests. This is somewhat involved. When providing adata_sim I had, by design, made the assumption that the user will have done all their own preprocessing. So to make the comparison you requested (sc.external.pp.scrublet with/without adata_sim) I needed to basically replicate all that preprocessing verbatim in the test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-969366293
https://github.com/scverse/scanpy/pull/2025#issuecomment-987736310:31,Deployability,release,release,31,"Hope that's what you mean by a release note @ivirshup , let me know if it needs anything else.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2025#issuecomment-987736310
https://github.com/scverse/scanpy/issues/2026#issuecomment-959058930:125,Deployability,patch,patch,125,"@Zethson re your comment (https://github.com/theislab/scanpy/pull/2028#issuecomment-956365435), what were you thinking for a patch?. Disallowing `0.5.2`? Or make a fix for that version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2026#issuecomment-959058930
https://github.com/scverse/scanpy/issues/2026#issuecomment-959063737:146,Deployability,patch,patch,146,"> @Zethson re your comment ([#2028 (comment)](https://github.com/theislab/scanpy/pull/2028#issuecomment-956365435)), what were you thinking for a patch?; > ; > Disallowing `0.5.2`? Or make a fix for that version?. Well, I'd accept the [PR](https://github.com/theislab/scanpy/pull/2028) that fixes this. It's targeted enough and I am happy to see people contributing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2026#issuecomment-959063737
https://github.com/scverse/scanpy/pull/2027#issuecomment-955073297:25,Testability,test,tests,25,It looks like all of the tests that failed have to do with ingest functionality and are thus unrelated to this PR. The same ones failed on the PR after this one as well. Let me know what I can do to fix this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-955073297
https://github.com/scverse/scanpy/pull/2027#issuecomment-959614946:77,Deployability,release,release,77,"Hey @esrice, thanks for the PR! Those tests are failing due to a recent umap release. This should be fixed on master now, so just merge master into this branch and they should be fixed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-959614946
https://github.com/scverse/scanpy/pull/2027#issuecomment-959614946:38,Testability,test,tests,38,"Hey @esrice, thanks for the PR! Those tests are failing due to a recent umap release. This should be fixed on master now, so just merge master into this branch and they should be fixed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-959614946
https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242:413,Integrability,wrap,wrapper,413,"Yes, for sure, one of the two commits I just pushed adds a few tests for various cases of `sc.pl.scatter()` with `basis=None`. You're totally right that plotting the transposed matrix (i.e., cells as points in the scatterplot, instead of genes) does not work. I think this is a separate problem from the one I'm fixing here, in response to the issue #1097. It seems to me there are two problems going on:; 1. The wrapper function `sc.pl.scatter()` mistakenly raises a ValueError if x, y, or color are var_names that exist only in raw but not in the base layer, even if `use_raw=True`. The underlying `_scatter_obs()` has no problem dealing with this situation, so to solve this, `sc.pl.scatter()` just needs to call `_scatter_obs()` in this case instead of raising a ValueError. This PR fixes that.; 2. When x, y, and/or color are variables found in `obs.index` or `var.keys()`, `sc.pl.scatter()` makes a transposed version of `adata`, but as you said, `adata.raw` does not get transposed. This leads to an `AttributeError` on this line of `_scatter_obs()`:. https://github.com/theislab/scanpy/blob/cab9f781f9fdee2eeebf05a84c2ce5f717afa514/scanpy/plotting/_anndata.py#L250; `AttributeError: 'NoneType' object has no attribute 'obs_vector'`. I'm not sure what the correct way to handle this is, so for now, I'm; * adding a commit so that this PR does not modify anything in the part of `sc.pl.scatter()` dealing with transposition; * adding a parametrized test to `test_plotting.py` that tests all use-cases of `sc.pl.scatter()` where `basis=None`, _except_ for the one where `use_raw=True` and x/y/color are per-obs variables, as that didn't work before this commit or after it. It's still in the parameter list but commented out for now. I'd be happy to help fix it since I've familiarized myself with this code pretty well, but we might need to discuss how this case should be handled first, and my suggestion would be to deal with this in a separate PR/bug fix/commit. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242
https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242:831,Modifiability,variab,variables,831,"Yes, for sure, one of the two commits I just pushed adds a few tests for various cases of `sc.pl.scatter()` with `basis=None`. You're totally right that plotting the transposed matrix (i.e., cells as points in the scatterplot, instead of genes) does not work. I think this is a separate problem from the one I'm fixing here, in response to the issue #1097. It seems to me there are two problems going on:; 1. The wrapper function `sc.pl.scatter()` mistakenly raises a ValueError if x, y, or color are var_names that exist only in raw but not in the base layer, even if `use_raw=True`. The underlying `_scatter_obs()` has no problem dealing with this situation, so to solve this, `sc.pl.scatter()` just needs to call `_scatter_obs()` in this case instead of raising a ValueError. This PR fixes that.; 2. When x, y, and/or color are variables found in `obs.index` or `var.keys()`, `sc.pl.scatter()` makes a transposed version of `adata`, but as you said, `adata.raw` does not get transposed. This leads to an `AttributeError` on this line of `_scatter_obs()`:. https://github.com/theislab/scanpy/blob/cab9f781f9fdee2eeebf05a84c2ce5f717afa514/scanpy/plotting/_anndata.py#L250; `AttributeError: 'NoneType' object has no attribute 'obs_vector'`. I'm not sure what the correct way to handle this is, so for now, I'm; * adding a commit so that this PR does not modify anything in the part of `sc.pl.scatter()` dealing with transposition; * adding a parametrized test to `test_plotting.py` that tests all use-cases of `sc.pl.scatter()` where `basis=None`, _except_ for the one where `use_raw=True` and x/y/color are per-obs variables, as that didn't work before this commit or after it. It's still in the parameter list but commented out for now. I'd be happy to help fix it since I've familiarized myself with this code pretty well, but we might need to discuss how this case should be handled first, and my suggestion would be to deal with this in a separate PR/bug fix/commit. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242
https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242:1616,Modifiability,variab,variables,1616,"Yes, for sure, one of the two commits I just pushed adds a few tests for various cases of `sc.pl.scatter()` with `basis=None`. You're totally right that plotting the transposed matrix (i.e., cells as points in the scatterplot, instead of genes) does not work. I think this is a separate problem from the one I'm fixing here, in response to the issue #1097. It seems to me there are two problems going on:; 1. The wrapper function `sc.pl.scatter()` mistakenly raises a ValueError if x, y, or color are var_names that exist only in raw but not in the base layer, even if `use_raw=True`. The underlying `_scatter_obs()` has no problem dealing with this situation, so to solve this, `sc.pl.scatter()` just needs to call `_scatter_obs()` in this case instead of raising a ValueError. This PR fixes that.; 2. When x, y, and/or color are variables found in `obs.index` or `var.keys()`, `sc.pl.scatter()` makes a transposed version of `adata`, but as you said, `adata.raw` does not get transposed. This leads to an `AttributeError` on this line of `_scatter_obs()`:. https://github.com/theislab/scanpy/blob/cab9f781f9fdee2eeebf05a84c2ce5f717afa514/scanpy/plotting/_anndata.py#L250; `AttributeError: 'NoneType' object has no attribute 'obs_vector'`. I'm not sure what the correct way to handle this is, so for now, I'm; * adding a commit so that this PR does not modify anything in the part of `sc.pl.scatter()` dealing with transposition; * adding a parametrized test to `test_plotting.py` that tests all use-cases of `sc.pl.scatter()` where `basis=None`, _except_ for the one where `use_raw=True` and x/y/color are per-obs variables, as that didn't work before this commit or after it. It's still in the parameter list but commented out for now. I'd be happy to help fix it since I've familiarized myself with this code pretty well, but we might need to discuss how this case should be handled first, and my suggestion would be to deal with this in a separate PR/bug fix/commit. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242
https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242:63,Testability,test,tests,63,"Yes, for sure, one of the two commits I just pushed adds a few tests for various cases of `sc.pl.scatter()` with `basis=None`. You're totally right that plotting the transposed matrix (i.e., cells as points in the scatterplot, instead of genes) does not work. I think this is a separate problem from the one I'm fixing here, in response to the issue #1097. It seems to me there are two problems going on:; 1. The wrapper function `sc.pl.scatter()` mistakenly raises a ValueError if x, y, or color are var_names that exist only in raw but not in the base layer, even if `use_raw=True`. The underlying `_scatter_obs()` has no problem dealing with this situation, so to solve this, `sc.pl.scatter()` just needs to call `_scatter_obs()` in this case instead of raising a ValueError. This PR fixes that.; 2. When x, y, and/or color are variables found in `obs.index` or `var.keys()`, `sc.pl.scatter()` makes a transposed version of `adata`, but as you said, `adata.raw` does not get transposed. This leads to an `AttributeError` on this line of `_scatter_obs()`:. https://github.com/theislab/scanpy/blob/cab9f781f9fdee2eeebf05a84c2ce5f717afa514/scanpy/plotting/_anndata.py#L250; `AttributeError: 'NoneType' object has no attribute 'obs_vector'`. I'm not sure what the correct way to handle this is, so for now, I'm; * adding a commit so that this PR does not modify anything in the part of `sc.pl.scatter()` dealing with transposition; * adding a parametrized test to `test_plotting.py` that tests all use-cases of `sc.pl.scatter()` where `basis=None`, _except_ for the one where `use_raw=True` and x/y/color are per-obs variables, as that didn't work before this commit or after it. It's still in the parameter list but commented out for now. I'd be happy to help fix it since I've familiarized myself with this code pretty well, but we might need to discuss how this case should be handled first, and my suggestion would be to deal with this in a separate PR/bug fix/commit. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242
https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242:1455,Testability,test,test,1455,"Yes, for sure, one of the two commits I just pushed adds a few tests for various cases of `sc.pl.scatter()` with `basis=None`. You're totally right that plotting the transposed matrix (i.e., cells as points in the scatterplot, instead of genes) does not work. I think this is a separate problem from the one I'm fixing here, in response to the issue #1097. It seems to me there are two problems going on:; 1. The wrapper function `sc.pl.scatter()` mistakenly raises a ValueError if x, y, or color are var_names that exist only in raw but not in the base layer, even if `use_raw=True`. The underlying `_scatter_obs()` has no problem dealing with this situation, so to solve this, `sc.pl.scatter()` just needs to call `_scatter_obs()` in this case instead of raising a ValueError. This PR fixes that.; 2. When x, y, and/or color are variables found in `obs.index` or `var.keys()`, `sc.pl.scatter()` makes a transposed version of `adata`, but as you said, `adata.raw` does not get transposed. This leads to an `AttributeError` on this line of `_scatter_obs()`:. https://github.com/theislab/scanpy/blob/cab9f781f9fdee2eeebf05a84c2ce5f717afa514/scanpy/plotting/_anndata.py#L250; `AttributeError: 'NoneType' object has no attribute 'obs_vector'`. I'm not sure what the correct way to handle this is, so for now, I'm; * adding a commit so that this PR does not modify anything in the part of `sc.pl.scatter()` dealing with transposition; * adding a parametrized test to `test_plotting.py` that tests all use-cases of `sc.pl.scatter()` where `basis=None`, _except_ for the one where `use_raw=True` and x/y/color are per-obs variables, as that didn't work before this commit or after it. It's still in the parameter list but commented out for now. I'd be happy to help fix it since I've familiarized myself with this code pretty well, but we might need to discuss how this case should be handled first, and my suggestion would be to deal with this in a separate PR/bug fix/commit. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242
https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242:1487,Testability,test,tests,1487,"Yes, for sure, one of the two commits I just pushed adds a few tests for various cases of `sc.pl.scatter()` with `basis=None`. You're totally right that plotting the transposed matrix (i.e., cells as points in the scatterplot, instead of genes) does not work. I think this is a separate problem from the one I'm fixing here, in response to the issue #1097. It seems to me there are two problems going on:; 1. The wrapper function `sc.pl.scatter()` mistakenly raises a ValueError if x, y, or color are var_names that exist only in raw but not in the base layer, even if `use_raw=True`. The underlying `_scatter_obs()` has no problem dealing with this situation, so to solve this, `sc.pl.scatter()` just needs to call `_scatter_obs()` in this case instead of raising a ValueError. This PR fixes that.; 2. When x, y, and/or color are variables found in `obs.index` or `var.keys()`, `sc.pl.scatter()` makes a transposed version of `adata`, but as you said, `adata.raw` does not get transposed. This leads to an `AttributeError` on this line of `_scatter_obs()`:. https://github.com/theislab/scanpy/blob/cab9f781f9fdee2eeebf05a84c2ce5f717afa514/scanpy/plotting/_anndata.py#L250; `AttributeError: 'NoneType' object has no attribute 'obs_vector'`. I'm not sure what the correct way to handle this is, so for now, I'm; * adding a commit so that this PR does not modify anything in the part of `sc.pl.scatter()` dealing with transposition; * adding a parametrized test to `test_plotting.py` that tests all use-cases of `sc.pl.scatter()` where `basis=None`, _except_ for the one where `use_raw=True` and x/y/color are per-obs variables, as that didn't work before this commit or after it. It's still in the parameter list but commented out for now. I'd be happy to help fix it since I've familiarized myself with this code pretty well, but we might need to discuss how this case should be handled first, and my suggestion would be to deal with this in a separate PR/bug fix/commit. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242
https://github.com/scverse/scanpy/pull/2027#issuecomment-964269046:190,Modifiability,variab,variables,190,"That's a great idea. It might require some reorganization, though, because currently `use_raw` is checked two places: once in `sc.pl.scatter()`, because it needs to know whether to look for variables in raw or not when deciding how to call `_scatter_obs()`, and again in `_scatter_obs()` itself. And it would probably be bad to do `adata = adata.raw.to_adata()` twice?. On another note, some pytests that are in files I did not edit are now failing because they can't find `anndata.tests` to import. I'm not sure if I messed something up by adding tests to `test_plotting.py` or whether this is a different issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964269046
https://github.com/scverse/scanpy/pull/2027#issuecomment-964269046:482,Testability,test,tests,482,"That's a great idea. It might require some reorganization, though, because currently `use_raw` is checked two places: once in `sc.pl.scatter()`, because it needs to know whether to look for variables in raw or not when deciding how to call `_scatter_obs()`, and again in `_scatter_obs()` itself. And it would probably be bad to do `adata = adata.raw.to_adata()` twice?. On another note, some pytests that are in files I did not edit are now failing because they can't find `anndata.tests` to import. I'm not sure if I messed something up by adding tests to `test_plotting.py` or whether this is a different issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964269046
https://github.com/scverse/scanpy/pull/2027#issuecomment-964269046:548,Testability,test,tests,548,"That's a great idea. It might require some reorganization, though, because currently `use_raw` is checked two places: once in `sc.pl.scatter()`, because it needs to know whether to look for variables in raw or not when deciding how to call `_scatter_obs()`, and again in `_scatter_obs()` itself. And it would probably be bad to do `adata = adata.raw.to_adata()` twice?. On another note, some pytests that are in files I did not edit are now failing because they can't find `anndata.tests` to import. I'm not sure if I messed something up by adding tests to `test_plotting.py` or whether this is a different issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964269046
https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124:707,Deployability,release,release,707,"> That's a great idea. It might require some reorganization, though, because currently use_raw is checked two places: once in sc.pl.scatter(), because it needs to know whether to look for variables in raw or not when deciding how to call _scatter_obs(), and again in _scatter_obs() itself. Would it be possible to not call it again in `scatter_obs`? E.g. could `_scatter_obs` not even need to know about the `raw` field?. > On another note, some pytests that are in files I did not edit are now failing because they can't find anndata.tests to import. I'm not sure if I messed something up by adding tests to test_plotting.py or whether this is a different issue. Aww crap, I think that was me making a new release. On the plus side it means our build system is now working as it's supposed to.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124
https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124:188,Modifiability,variab,variables,188,"> That's a great idea. It might require some reorganization, though, because currently use_raw is checked two places: once in sc.pl.scatter(), because it needs to know whether to look for variables in raw or not when deciding how to call _scatter_obs(), and again in _scatter_obs() itself. Would it be possible to not call it again in `scatter_obs`? E.g. could `_scatter_obs` not even need to know about the `raw` field?. > On another note, some pytests that are in files I did not edit are now failing because they can't find anndata.tests to import. I'm not sure if I messed something up by adding tests to test_plotting.py or whether this is a different issue. Aww crap, I think that was me making a new release. On the plus side it means our build system is now working as it's supposed to.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124
https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124:535,Testability,test,tests,535,"> That's a great idea. It might require some reorganization, though, because currently use_raw is checked two places: once in sc.pl.scatter(), because it needs to know whether to look for variables in raw or not when deciding how to call _scatter_obs(), and again in _scatter_obs() itself. Would it be possible to not call it again in `scatter_obs`? E.g. could `_scatter_obs` not even need to know about the `raw` field?. > On another note, some pytests that are in files I did not edit are now failing because they can't find anndata.tests to import. I'm not sure if I messed something up by adding tests to test_plotting.py or whether this is a different issue. Aww crap, I think that was me making a new release. On the plus side it means our build system is now working as it's supposed to.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124
https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124:600,Testability,test,tests,600,"> That's a great idea. It might require some reorganization, though, because currently use_raw is checked two places: once in sc.pl.scatter(), because it needs to know whether to look for variables in raw or not when deciding how to call _scatter_obs(), and again in _scatter_obs() itself. Would it be possible to not call it again in `scatter_obs`? E.g. could `_scatter_obs` not even need to know about the `raw` field?. > On another note, some pytests that are in files I did not edit are now failing because they can't find anndata.tests to import. I'm not sure if I messed something up by adding tests to test_plotting.py or whether this is a different issue. Aww crap, I think that was me making a new release. On the plus side it means our build system is now working as it's supposed to.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124
https://github.com/scverse/scanpy/pull/2027#issuecomment-964290470:167,Modifiability,variab,variables,167,"> It looks like running adata.raw.to_adata() does not preserve the var table, so that won't work either:. That makes sense though right? Since it's a different set of variables. That said, I do suspect there is some weirdness about handling of `use_raw` here that doesn't actually make sense when you think about it, but could break some code to remove.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964290470
https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514:38,Deployability,integrat,integrate,38,"Yes, I agree. My proposal would be to integrate this PR as it currently stands (after the testing system is working), as it fixes #1097 (namely, you can't currently set x, y, or color to something that is in `adata.raw.var.index` but not `adata.var.index`, even if `use_raw=True`) and leaves the logic flow for the transposition case as is. It also adds some test coverage to use cases of `sc.pl.scatter()` that were not covered before. Then, I can start working on coming up with a strategy for solving the second problem (`use_raw=True` with transposition) separately, as it's mostly unrelated, and seems to be a bit more complicated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514
https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514:38,Integrability,integrat,integrate,38,"Yes, I agree. My proposal would be to integrate this PR as it currently stands (after the testing system is working), as it fixes #1097 (namely, you can't currently set x, y, or color to something that is in `adata.raw.var.index` but not `adata.var.index`, even if `use_raw=True`) and leaves the logic flow for the transposition case as is. It also adds some test coverage to use cases of `sc.pl.scatter()` that were not covered before. Then, I can start working on coming up with a strategy for solving the second problem (`use_raw=True` with transposition) separately, as it's mostly unrelated, and seems to be a bit more complicated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514
https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514:90,Testability,test,testing,90,"Yes, I agree. My proposal would be to integrate this PR as it currently stands (after the testing system is working), as it fixes #1097 (namely, you can't currently set x, y, or color to something that is in `adata.raw.var.index` but not `adata.var.index`, even if `use_raw=True`) and leaves the logic flow for the transposition case as is. It also adds some test coverage to use cases of `sc.pl.scatter()` that were not covered before. Then, I can start working on coming up with a strategy for solving the second problem (`use_raw=True` with transposition) separately, as it's mostly unrelated, and seems to be a bit more complicated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514
https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514:296,Testability,log,logic,296,"Yes, I agree. My proposal would be to integrate this PR as it currently stands (after the testing system is working), as it fixes #1097 (namely, you can't currently set x, y, or color to something that is in `adata.raw.var.index` but not `adata.var.index`, even if `use_raw=True`) and leaves the logic flow for the transposition case as is. It also adds some test coverage to use cases of `sc.pl.scatter()` that were not covered before. Then, I can start working on coming up with a strategy for solving the second problem (`use_raw=True` with transposition) separately, as it's mostly unrelated, and seems to be a bit more complicated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514
https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514:359,Testability,test,test,359,"Yes, I agree. My proposal would be to integrate this PR as it currently stands (after the testing system is working), as it fixes #1097 (namely, you can't currently set x, y, or color to something that is in `adata.raw.var.index` but not `adata.var.index`, even if `use_raw=True`) and leaves the logic flow for the transposition case as is. It also adds some test coverage to use cases of `sc.pl.scatter()` that were not covered before. Then, I can start working on coming up with a strategy for solving the second problem (`use_raw=True` with transposition) separately, as it's mostly unrelated, and seems to be a bit more complicated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514
https://github.com/scverse/scanpy/pull/2027#issuecomment-964557208:0,Testability,Test,Tests,0,Tests are working now,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964557208
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:402,Availability,down,down,402,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:446,Availability,down,down,446,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:520,Availability,mainten,maintenance,520,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:994,Availability,down,down,994,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:558,Deployability,update,updates,558,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:32,Testability,test,tests,32,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:113,Testability,test,tests,113,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:223,Testability,test,tests,223,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:259,Testability,assert,asserting,259,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:311,Testability,test,tested,311,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:321,Testability,test,test,321,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:396,Testability,test,tests,396,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:421,Testability,test,tests,421,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:589,Testability,test,tests,589,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:753,Testability,test,tested,753,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:852,Testability,test,testing,852,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677:77,Usability,feedback,feedback,77,"> First, thanks for adding more tests!. Sure thing. Thanks for all the great feedback!. > 1. Is the file `scanpy/tests/_images/scatter_filtered_genes_raw.png` meant to be here?. No, thanks for catching that. > 2. Could the tests be broken up by what they are asserting? I would prefer to break up what is being tested by test case ; rather than values of parameters. Yes, I've broken both of the tests down into multiple tests. > 3. Could we cut down on the number of reference images generated since those cause manual maintenance burden on some matplotlib updates. These reference based tests are not great for confirming the correct plot is output, only that their output is consistent across commits.; > I think some of these cases could instead be tested with `check_same_image`, e.g. where it doesn't matter whether raw is `True` or `None`. Also testing for checking cases where `use_raw=True` would be equivalent to passing `pbmc.raw.to_adata()`. I've cut the number of reference images down to two. I couldn't figure out a clever way to use `check_same_image()` instead of `save_and_compare_images()` for these as you did for the others. See below for comments about individual suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-966240677
https://github.com/scverse/scanpy/pull/2027#issuecomment-968960149:29,Security,access,access,29,"Awesome, just gave you write access to my fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-968960149
https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435:39,Deployability,release,released,39,Thanks!; @ivirshup umap-learn 0.52 was released 3 days ago. Maybe we should even push out a patch release? I expect that many people will run into this issue and be left confused.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435
https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435:92,Deployability,patch,patch,92,Thanks!; @ivirshup umap-learn 0.52 was released 3 days ago. Maybe we should even push out a patch release? I expect that many people will run into this issue and be left confused.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435
https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435:98,Deployability,release,release,98,Thanks!; @ivirshup umap-learn 0.52 was released 3 days ago. Maybe we should even push out a patch release? I expect that many people will run into this issue and be left confused.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435
https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435:24,Usability,learn,learn,24,Thanks!; @ivirshup umap-learn 0.52 was released 3 days ago. Maybe we should even push out a patch release? I expect that many people will run into this issue and be left confused.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028#issuecomment-956365435
https://github.com/scverse/scanpy/pull/2028#issuecomment-959092651:40,Deployability,release,release,40,Thanks for the fix! . I've just added a release note.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2028#issuecomment-959092651
https://github.com/scverse/scanpy/issues/2029#issuecomment-976191715:28,Availability,error,errors,28,use_raw=False always create errors.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2029#issuecomment-976191715
https://github.com/scverse/scanpy/issues/2029#issuecomment-1012803791:406,Availability,down,downstream,406,"Hello @Zethson ; Thanks for the response.; I read the paper. I understand that using the raw data to calculate the maker genes of clusters is an appropriate way, but the raw data was not regressed out with mitochondrial genes, gene counts, cell cycle scores...So there will be so many mito genes ranked on the top of the marker gene list. What shall we do with these mito genes?. In Seurat, they did every downstream analysis and plotting by using the log-transformed and scaled data (see below, the scaled dots in Seurat violin plot). Scanpy draws all plots by setting `use_raw=True`. I'm wondering which method is better?; ![image](https://user-images.githubusercontent.com/75048821/149460182-c5c11295-ca78-4bfe-aa8b-d13bade4b21f.png). Thanks!; Best,; YJ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2029#issuecomment-1012803791
https://github.com/scverse/scanpy/issues/2029#issuecomment-1012803791:452,Testability,log,log-transformed,452,"Hello @Zethson ; Thanks for the response.; I read the paper. I understand that using the raw data to calculate the maker genes of clusters is an appropriate way, but the raw data was not regressed out with mitochondrial genes, gene counts, cell cycle scores...So there will be so many mito genes ranked on the top of the marker gene list. What shall we do with these mito genes?. In Seurat, they did every downstream analysis and plotting by using the log-transformed and scaled data (see below, the scaled dots in Seurat violin plot). Scanpy draws all plots by setting `use_raw=True`. I'm wondering which method is better?; ![image](https://user-images.githubusercontent.com/75048821/149460182-c5c11295-ca78-4bfe-aa8b-d13bade4b21f.png). Thanks!; Best,; YJ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2029#issuecomment-1012803791
https://github.com/scverse/scanpy/issues/2029#issuecomment-1012982205:475,Availability,mask,masking,475,"Hi @hyjforesight!. As mentioned in the paper, plotting corrected data is definitely the way forward. I'm not sure about regressing out all of these biological covariates (like regressing out MT fraction). In general, it can be quite informative to find that you have different MT gene expression between clusters. This can highlight different respiratory (and maybe metabolic) activity as well as data quality differences. Otherwise, feel free to just ignore the MT genes by masking them from the analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2029#issuecomment-1012982205
https://github.com/scverse/scanpy/issues/2030#issuecomment-993648137:343,Availability,mask,mask,343,"I would agree the results of `sc.pp.filter_genes(..., inplace=False)` are not the most intuitive. Instead of returning a filtered anndata, it returns which cells would have been filtered and the stats which were used to make this decision. This is documented under the `Returns` section for these functions. What you might want is. ```python; mask, _ = sc.pp.filter_cells(adata, min_genes=200, inplace=False); a = adata[mask].copy(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2030#issuecomment-993648137
https://github.com/scverse/scanpy/issues/2030#issuecomment-993648137:420,Availability,mask,mask,420,"I would agree the results of `sc.pp.filter_genes(..., inplace=False)` are not the most intuitive. Instead of returning a filtered anndata, it returns which cells would have been filtered and the stats which were used to make this decision. This is documented under the `Returns` section for these functions. What you might want is. ```python; mask, _ = sc.pp.filter_cells(adata, min_genes=200, inplace=False); a = adata[mask].copy(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2030#issuecomment-993648137
https://github.com/scverse/scanpy/issues/2030#issuecomment-993648137:87,Usability,intuit,intuitive,87,"I would agree the results of `sc.pp.filter_genes(..., inplace=False)` are not the most intuitive. Instead of returning a filtered anndata, it returns which cells would have been filtered and the stats which were used to make this decision. This is documented under the `Returns` section for these functions. What you might want is. ```python; mask, _ = sc.pp.filter_cells(adata, min_genes=200, inplace=False); a = adata[mask].copy(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2030#issuecomment-993648137
https://github.com/scverse/scanpy/issues/2034#issuecomment-1678915525:75,Availability,error,error,75,Hello!. Hoping to reopen this discussion as I've just encountered the same error whilst trying to do some label transfer via `scArches`. My reference object is [AIDA](https://data.humancellatlas.org/explore/projects/f0f89c14-7460-4bab-9d42-22228a91f185) PBMCs and my query data object is a combined object from [here](https://www.ncbi.nlm.nih.gov/bioproject/PRJNA605083) and [here](https://ngdc.cncb.ac.cn/bioproject/browse/PRJCA003616). **Versions**; scanpy v1.9.3; scarches v0.5.9; anndata v0.9.2 ; numpy v1.24.4; pandas v2.0.3 ; scvi-tools v1.0.3,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2034#issuecomment-1678915525
https://github.com/scverse/scanpy/issues/2040#issuecomment-959857056:141,Deployability,install,install,141,We call out to `loompy` to read in loom files. I also am having trouble replicating using `loompy 3.0.6`. Could you try updating your loompy install and seeing if the problem persists?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2040#issuecomment-959857056
https://github.com/scverse/scanpy/pull/2042#issuecomment-960244954:17,Deployability,install,install,17,"This allows `pip install -e` to do a standardized thing! Unclear how much this helps/ hurts overall though, particularly for asking ""what version is this installation"". I think we would be able to tell that an install was ""editable"" through `importlib.metadata.files`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-960244954
https://github.com/scverse/scanpy/pull/2042#issuecomment-960244954:154,Deployability,install,installation,154,"This allows `pip install -e` to do a standardized thing! Unclear how much this helps/ hurts overall though, particularly for asking ""what version is this installation"". I think we would be able to tell that an install was ""editable"" through `importlib.metadata.files`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-960244954
https://github.com/scverse/scanpy/pull/2042#issuecomment-960244954:210,Deployability,install,install,210,"This allows `pip install -e` to do a standardized thing! Unclear how much this helps/ hurts overall though, particularly for asking ""what version is this installation"". I think we would be able to tell that an install was ""editable"" through `importlib.metadata.files`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-960244954
https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897:376,Availability,down,download,376,"Re: quotes: Yes, the difference is that escape sequences work in double quoted strings. So for me a double quoted string in otherwise single quoted TOML means “pay attention, this one has special stuff in it”. Re: Build: The problem is that. 1. we’re installing louvain and it; 2. [doesn’t have a Python 3.9 wheel](https://pypi.org/project/louvain/#files), which causes us to download the sdist,; 3. [Sets `2to3=True` in setup.py](https://github.com/vtraag/louvain-igraph/blob/0.7.0/setup.py#L827-L828), for which [setuptools has removed support](https://setuptools.pypa.io/en/latest/history.html#v58-0-0). I think the best course of action would be to just port louvain to Python 3 only, and until then make sure our build environment as setuptools 57 installed. See https://github.com/vtraag/louvain-igraph/issues/57. Or we can deactivate louvain tests, skip installing it in the tests, and let people who need it deal with that. Or we ask @vtraag to upload Python 3.9 and 3.10 wheels, then we kicked the problem back two releases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897
https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897:251,Deployability,install,installing,251,"Re: quotes: Yes, the difference is that escape sequences work in double quoted strings. So for me a double quoted string in otherwise single quoted TOML means “pay attention, this one has special stuff in it”. Re: Build: The problem is that. 1. we’re installing louvain and it; 2. [doesn’t have a Python 3.9 wheel](https://pypi.org/project/louvain/#files), which causes us to download the sdist,; 3. [Sets `2to3=True` in setup.py](https://github.com/vtraag/louvain-igraph/blob/0.7.0/setup.py#L827-L828), for which [setuptools has removed support](https://setuptools.pypa.io/en/latest/history.html#v58-0-0). I think the best course of action would be to just port louvain to Python 3 only, and until then make sure our build environment as setuptools 57 installed. See https://github.com/vtraag/louvain-igraph/issues/57. Or we can deactivate louvain tests, skip installing it in the tests, and let people who need it deal with that. Or we ask @vtraag to upload Python 3.9 and 3.10 wheels, then we kicked the problem back two releases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897
https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897:753,Deployability,install,installed,753,"Re: quotes: Yes, the difference is that escape sequences work in double quoted strings. So for me a double quoted string in otherwise single quoted TOML means “pay attention, this one has special stuff in it”. Re: Build: The problem is that. 1. we’re installing louvain and it; 2. [doesn’t have a Python 3.9 wheel](https://pypi.org/project/louvain/#files), which causes us to download the sdist,; 3. [Sets `2to3=True` in setup.py](https://github.com/vtraag/louvain-igraph/blob/0.7.0/setup.py#L827-L828), for which [setuptools has removed support](https://setuptools.pypa.io/en/latest/history.html#v58-0-0). I think the best course of action would be to just port louvain to Python 3 only, and until then make sure our build environment as setuptools 57 installed. See https://github.com/vtraag/louvain-igraph/issues/57. Or we can deactivate louvain tests, skip installing it in the tests, and let people who need it deal with that. Or we ask @vtraag to upload Python 3.9 and 3.10 wheels, then we kicked the problem back two releases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897
https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897:861,Deployability,install,installing,861,"Re: quotes: Yes, the difference is that escape sequences work in double quoted strings. So for me a double quoted string in otherwise single quoted TOML means “pay attention, this one has special stuff in it”. Re: Build: The problem is that. 1. we’re installing louvain and it; 2. [doesn’t have a Python 3.9 wheel](https://pypi.org/project/louvain/#files), which causes us to download the sdist,; 3. [Sets `2to3=True` in setup.py](https://github.com/vtraag/louvain-igraph/blob/0.7.0/setup.py#L827-L828), for which [setuptools has removed support](https://setuptools.pypa.io/en/latest/history.html#v58-0-0). I think the best course of action would be to just port louvain to Python 3 only, and until then make sure our build environment as setuptools 57 installed. See https://github.com/vtraag/louvain-igraph/issues/57. Or we can deactivate louvain tests, skip installing it in the tests, and let people who need it deal with that. Or we ask @vtraag to upload Python 3.9 and 3.10 wheels, then we kicked the problem back two releases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897
https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897:1024,Deployability,release,releases,1024,"Re: quotes: Yes, the difference is that escape sequences work in double quoted strings. So for me a double quoted string in otherwise single quoted TOML means “pay attention, this one has special stuff in it”. Re: Build: The problem is that. 1. we’re installing louvain and it; 2. [doesn’t have a Python 3.9 wheel](https://pypi.org/project/louvain/#files), which causes us to download the sdist,; 3. [Sets `2to3=True` in setup.py](https://github.com/vtraag/louvain-igraph/blob/0.7.0/setup.py#L827-L828), for which [setuptools has removed support](https://setuptools.pypa.io/en/latest/history.html#v58-0-0). I think the best course of action would be to just port louvain to Python 3 only, and until then make sure our build environment as setuptools 57 installed. See https://github.com/vtraag/louvain-igraph/issues/57. Or we can deactivate louvain tests, skip installing it in the tests, and let people who need it deal with that. Or we ask @vtraag to upload Python 3.9 and 3.10 wheels, then we kicked the problem back two releases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897
https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897:849,Testability,test,tests,849,"Re: quotes: Yes, the difference is that escape sequences work in double quoted strings. So for me a double quoted string in otherwise single quoted TOML means “pay attention, this one has special stuff in it”. Re: Build: The problem is that. 1. we’re installing louvain and it; 2. [doesn’t have a Python 3.9 wheel](https://pypi.org/project/louvain/#files), which causes us to download the sdist,; 3. [Sets `2to3=True` in setup.py](https://github.com/vtraag/louvain-igraph/blob/0.7.0/setup.py#L827-L828), for which [setuptools has removed support](https://setuptools.pypa.io/en/latest/history.html#v58-0-0). I think the best course of action would be to just port louvain to Python 3 only, and until then make sure our build environment as setuptools 57 installed. See https://github.com/vtraag/louvain-igraph/issues/57. Or we can deactivate louvain tests, skip installing it in the tests, and let people who need it deal with that. Or we ask @vtraag to upload Python 3.9 and 3.10 wheels, then we kicked the problem back two releases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897
https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897:882,Testability,test,tests,882,"Re: quotes: Yes, the difference is that escape sequences work in double quoted strings. So for me a double quoted string in otherwise single quoted TOML means “pay attention, this one has special stuff in it”. Re: Build: The problem is that. 1. we’re installing louvain and it; 2. [doesn’t have a Python 3.9 wheel](https://pypi.org/project/louvain/#files), which causes us to download the sdist,; 3. [Sets `2to3=True` in setup.py](https://github.com/vtraag/louvain-igraph/blob/0.7.0/setup.py#L827-L828), for which [setuptools has removed support](https://setuptools.pypa.io/en/latest/history.html#v58-0-0). I think the best course of action would be to just port louvain to Python 3 only, and until then make sure our build environment as setuptools 57 installed. See https://github.com/vtraag/louvain-igraph/issues/57. Or we can deactivate louvain tests, skip installing it in the tests, and let people who need it deal with that. Or we ask @vtraag to upload Python 3.9 and 3.10 wheels, then we kicked the problem back two releases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2042#issuecomment-967619897
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4,Deployability,install,installed,4,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:105,Deployability,install,install,105,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4727,Deployability,Install,Installing,4727,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:5170,Deployability,install,installation,5170,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:5287,Deployability,install,installed,5287,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1991,Integrability,wrap,wrap,1991," in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:5088,Integrability,wrap,wrap,5088,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:5444,Integrability,wrap,wrap-,5444,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:164,Performance,cache,cached,164,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:243,Performance,cache,cached,243,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:321,Performance,cache,cached,321,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:410,Performance,cache,cached,410,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:491,Performance,cache,cached,491,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:573,Performance,cache,cached,573,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:651,Performance,cache,cached,651,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:736,Performance,cache,cached,736,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:827,Performance,cache,cached,827,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:900,Performance,cache,cached,900,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1122,Performance,cache,cached,1122,*; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.w,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1213,Performance,cache,cached,1213,ecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.wh,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1300,Performance,cache,cached,1300,ing tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecti,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1379,Performance,cache,cached,1379,2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1455,Performance,cache,cached,1455,n_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1528,Performance,cache,cached,1528,3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from import,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1749,Performance,cache,cached,1749,-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1846,Performance,cache,cached,1846,"3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-package",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1921,Performance,cache,cached,1921,"ne-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2003,Performance,cache,cached,2003," in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2084,Performance,cache,cached,2084," (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2174,Performance,cache,cached,2174,"n_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Coll",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2246,Performance,cache,cached,2246,"md64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2306,Performance,cache,cached-property,2306,".1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2329,Performance,cache,cached,2329,".1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2939,Performance,cache,cached,2939,"3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3190,Performance,cache,cached,3190,"h-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3270,Performance,cache,cached,3270,"one-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3361,Performance,cache,cached,3361,"none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3608,Performance,cache,cached,3608,"users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4029,Performance,cache,cached,4029,"ateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4117,Performance,cache,cached,4117,"ib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4197,Performance,cache,cached,4197,"-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4278,Performance,cache,cached,4278,"ow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4368,Performance,cache,cached,4368,"ator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-versio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4451,Performance,cache,cached,4451,"fied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4535,Performance,cache,cached,4535,"canpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4865,Performance,cache,cached-property,4865,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:5311,Performance,cache,cached-property-,5311,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1101,Usability,learn,learn,1101,rmation of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leid,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1825,Usability,learn,learn,1825,"3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.con",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4823,Usability,learn,learn,4823,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:5008,Usability,learn,learn,5008,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:5667,Usability,learn,learn-,5667,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:5822,Usability,learn,learn-,5822,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955
https://github.com/scverse/scanpy/issues/2045#issuecomment-963268069:19,Deployability,release,release,19,We recently made a release (1.8.2) which has a bug fix for an incompatibility with umap 0.5.2. The fix was actually contributed by the author of UMAP!. Upgrading should fix your issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963268069
https://github.com/scverse/scanpy/issues/2045#issuecomment-963422681:57,Availability,error,error,57,"It seems that upgrading from 1.8.1 to 1.8.2 introduce an error on umap version checking, mentioned by #1978 (and a potential solution); > Why are we using umap.__version__ instead of importlib.metadata.version('umap-learn')?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963422681
https://github.com/scverse/scanpy/issues/2045#issuecomment-963422681:216,Usability,learn,learn,216,"It seems that upgrading from 1.8.1 to 1.8.2 introduce an error on umap version checking, mentioned by #1978 (and a potential solution); > Why are we using umap.__version__ instead of importlib.metadata.version('umap-learn')?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963422681
https://github.com/scverse/scanpy/issues/2045#issuecomment-963446967:55,Deployability,upgrade,upgrade,55,"@ivirshup ; Hello ivirshup, thanks for the solution. I upgrade to py 3.8 and install scanpy 1.8.2, problem solved.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963446967
https://github.com/scverse/scanpy/issues/2045#issuecomment-963446967:77,Deployability,install,install,77,"@ivirshup ; Hello ivirshup, thanks for the solution. I upgrade to py 3.8 and install scanpy 1.8.2, problem solved.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963446967
https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994:71,Deployability,install,installed,71,"Just figured it out. It is because I have both `umap` and `umap-learn` installed, but even if I do `pip uninstall umap`, it doesn't totally remove `umap` for whatever reason. I had to uninstall both `umap` and `umap-learn` first, and then re-install `umap-learn` to get it to work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994
https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994:242,Deployability,install,install,242,"Just figured it out. It is because I have both `umap` and `umap-learn` installed, but even if I do `pip uninstall umap`, it doesn't totally remove `umap` for whatever reason. I had to uninstall both `umap` and `umap-learn` first, and then re-install `umap-learn` to get it to work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994
https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994:64,Usability,learn,learn,64,"Just figured it out. It is because I have both `umap` and `umap-learn` installed, but even if I do `pip uninstall umap`, it doesn't totally remove `umap` for whatever reason. I had to uninstall both `umap` and `umap-learn` first, and then re-install `umap-learn` to get it to work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994
https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994:216,Usability,learn,learn,216,"Just figured it out. It is because I have both `umap` and `umap-learn` installed, but even if I do `pip uninstall umap`, it doesn't totally remove `umap` for whatever reason. I had to uninstall both `umap` and `umap-learn` first, and then re-install `umap-learn` to get it to work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994
https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994:256,Usability,learn,learn,256,"Just figured it out. It is because I have both `umap` and `umap-learn` installed, but even if I do `pip uninstall umap`, it doesn't totally remove `umap` for whatever reason. I had to uninstall both `umap` and `umap-learn` first, and then re-install `umap-learn` to get it to work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-963533994
https://github.com/scverse/scanpy/issues/2046#issuecomment-963259525:36,Availability,error,error,36,I'm having trouble reproducing this error. Could you share what versions you have installed (ideally also try updating these to the latest releases) and see if you can replicate the issue on one of the datasets in `sc.datasets`?. I think you should probably do differential expression plots using the same values you used to compute the differential expression in most cases.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963259525
https://github.com/scverse/scanpy/issues/2046#issuecomment-963259525:82,Deployability,install,installed,82,I'm having trouble reproducing this error. Could you share what versions you have installed (ideally also try updating these to the latest releases) and see if you can replicate the issue on one of the datasets in `sc.datasets`?. I think you should probably do differential expression plots using the same values you used to compute the differential expression in most cases.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963259525
https://github.com/scverse/scanpy/issues/2046#issuecomment-963259525:139,Deployability,release,releases,139,I'm having trouble reproducing this error. Could you share what versions you have installed (ideally also try updating these to the latest releases) and see if you can replicate the issue on one of the datasets in `sc.datasets`?. I think you should probably do differential expression plots using the same values you used to compute the differential expression in most cases.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963259525
https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:34,Availability,error,error,34,"@ivirshup ; Hello ivirshup, these error comes from the below environments. When I upgrade to py3.8 and use scanpy 1.8.2, everything works well. Thanks a lot!. <html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Excel.Sheet>; <meta name=Generator content=""Microsoft Excel 15"">; <link id=Main-File rel=Main-File; href=""file:///C:/Users/Yuanjian/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">; <link rel=File-List; href=""file:///C:/Users/Yuanjian/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">; <style>; <!--table; 	{mso-displayed-decimal-separator:""\."";; 	mso-displayed-thousand-separator:""\,"";}; @page; 	{margin:.75in .7in .75in .7in;; 	mso-header-margin:.3in;; 	mso-footer-margin:.3in;}; tr; 	{mso-height-source:auto;; 	mso-ruby-visibility:none;}; col; 	{mso-width-source:auto;; 	mso-ruby-visibility:none;}; br; 	{mso-data-placement:same-cell;}; td; 	{padding-top:1px;; 	padding-right:1px;; 	padding-left:1px;; 	mso-ignore:padding;; 	color:black;; 	font-size:11.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-number-format:General;; 	text-align:general;; 	vertical-align:middle;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;; 	mso-protection:locked visible;; 	white-space:nowrap;; 	mso-rotate:0;}; ruby; 	{ruby-align:left;}; rt; 	{color:windowtext;; 	font-size:9.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-char-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Package | Version; -- | --; Anaconda | 2.1.0; Python | 3.6.13; anndata | 0.7.6; anyio | 2.2.0; argon2-cffi | 20.1.0; async-generator | 1.1; attrs | 21.2.0; Babel | 2.9.1; backcall | 0.2.0; ble",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699
https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:82,Deployability,upgrade,upgrade,82,"@ivirshup ; Hello ivirshup, these error comes from the below environments. When I upgrade to py3.8 and use scanpy 1.8.2, everything works well. Thanks a lot!. <html xmlns:v=""urn:schemas-microsoft-com:vml""; xmlns:o=""urn:schemas-microsoft-com:office:office""; xmlns:x=""urn:schemas-microsoft-com:office:excel""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=Excel.Sheet>; <meta name=Generator content=""Microsoft Excel 15"">; <link id=Main-File rel=Main-File; href=""file:///C:/Users/Yuanjian/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">; <link rel=File-List; href=""file:///C:/Users/Yuanjian/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">; <style>; <!--table; 	{mso-displayed-decimal-separator:""\."";; 	mso-displayed-thousand-separator:""\,"";}; @page; 	{margin:.75in .7in .75in .7in;; 	mso-header-margin:.3in;; 	mso-footer-margin:.3in;}; tr; 	{mso-height-source:auto;; 	mso-ruby-visibility:none;}; col; 	{mso-width-source:auto;; 	mso-ruby-visibility:none;}; br; 	{mso-data-placement:same-cell;}; td; 	{padding-top:1px;; 	padding-right:1px;; 	padding-left:1px;; 	mso-ignore:padding;; 	color:black;; 	font-size:11.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-number-format:General;; 	text-align:general;; 	vertical-align:middle;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;; 	mso-protection:locked visible;; 	white-space:nowrap;; 	mso-rotate:0;}; ruby; 	{ruby-align:left;}; rt; 	{color:windowtext;; 	font-size:9.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-char-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Package | Version; -- | --; Anaconda | 2.1.0; Python | 3.6.13; anndata | 0.7.6; anyio | 2.2.0; argon2-cffi | 20.1.0; async-generator | 1.1; attrs | 21.2.0; Babel | 2.9.1; backcall | 0.2.0; ble",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699
https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:2717,Integrability,wrap,wrap,2717,"ar-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Package | Version; -- | --; Anaconda | 2.1.0; Python | 3.6.13; anndata | 0.7.6; anyio | 2.2.0; argon2-cffi | 20.1.0; async-generator | 1.1; attrs | 21.2.0; Babel | 2.9.1; backcall | 0.2.0; bleach | 4.0.0; brotlipy | 0.7.0; cached-property | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21; pandas | 1.1.5; pandocfilters | 1.4.3; parso | 0.8.2; patsy | 0.5.2; pickleshare | 0.7.5; Pillow | 8.4.0; pip | 21.2.2; prometheus-client | 0.11.0; prompt-toolkit | 3.0.20; pycparser | 2.2; Pygments | 2.10.0; pynndescent | 0.5.5; pyOpenSSL | 21.0.0; **pyparsing | 3.0.4**; pyrsistent | 0.17.3; PySocks | 1.7.1; python-dateutil | 2.8.2; python-igraph | 0.9.8; pytz | 2021.3; pywin32 | 228; pywinpty | 0.5.7; pyzmq | 22.2.1; requests | 2.26.0; scanpy | 1.7.2; scikit-learn | 0.24.2; scipy | 1.5.4; seaborn | 0.11.2; Send2Trash | 1.8.0; setuptools | 58.0.4; sinfo | 0.3.4; six | 1.16.0; sniffio | 1.2.0; statsmodels | 0.12.2; stdlib-list | 0.8.0; tables | 3.6.1; terminado | 0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699
https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:2032,Performance,cache,cached-property,2032,"op:1px;; 	padding-right:1px;; 	padding-left:1px;; 	mso-ignore:padding;; 	color:black;; 	font-size:11.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-number-format:General;; 	text-align:general;; 	vertical-align:middle;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;; 	mso-protection:locked visible;; 	white-space:nowrap;; 	mso-rotate:0;}; ruby; 	{ruby-align:left;}; rt; 	{color:windowtext;; 	font-size:9.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-char-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Package | Version; -- | --; Anaconda | 2.1.0; Python | 3.6.13; anndata | 0.7.6; anyio | 2.2.0; argon2-cffi | 20.1.0; async-generator | 1.1; attrs | 21.2.0; Babel | 2.9.1; backcall | 0.2.0; bleach | 4.0.0; brotlipy | 0.7.0; cached-property | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699
https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:3721,Testability,test,testpath,3721,rty | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21; pandas | 1.1.5; pandocfilters | 1.4.3; parso | 0.8.2; patsy | 0.5.2; pickleshare | 0.7.5; Pillow | 8.4.0; pip | 21.2.2; prometheus-client | 0.11.0; prompt-toolkit | 3.0.20; pycparser | 2.2; Pygments | 2.10.0; pynndescent | 0.5.5; pyOpenSSL | 21.0.0; **pyparsing | 3.0.4**; pyrsistent | 0.17.3; PySocks | 1.7.1; python-dateutil | 2.8.2; python-igraph | 0.9.8; pytz | 2021.3; pywin32 | 228; pywinpty | 0.5.7; pyzmq | 22.2.1; requests | 2.26.0; scanpy | 1.7.2; scikit-learn | 0.24.2; scipy | 1.5.4; seaborn | 0.11.2; Send2Trash | 1.8.0; setuptools | 58.0.4; sinfo | 0.3.4; six | 1.16.0; sniffio | 1.2.0; statsmodels | 0.12.2; stdlib-list | 0.8.0; tables | 3.6.1; terminado | 0.9.4; testpath | 0.5.0; texttable | 1.6.4; threadpoolctl | 3.0.0; tornado | 6.1; tqdm | 4.62.3; traitlets | 4.3.3; typing-extensions | 3.10.0.2; **umap-learn | 0.5.2**; urllib3 | 1.26.7; wcwidth | 0.2.5; webencodings | 0.5.1; wheel | 0.37.0; win-inet-pton | 1.1.0; wincertstore | 0.2; xlrd | 1.2.0; zipp | 3.6.0. </body>. </html>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699
https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:3507,Usability,learn,learn,3507,rty | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21; pandas | 1.1.5; pandocfilters | 1.4.3; parso | 0.8.2; patsy | 0.5.2; pickleshare | 0.7.5; Pillow | 8.4.0; pip | 21.2.2; prometheus-client | 0.11.0; prompt-toolkit | 3.0.20; pycparser | 2.2; Pygments | 2.10.0; pynndescent | 0.5.5; pyOpenSSL | 21.0.0; **pyparsing | 3.0.4**; pyrsistent | 0.17.3; PySocks | 1.7.1; python-dateutil | 2.8.2; python-igraph | 0.9.8; pytz | 2021.3; pywin32 | 228; pywinpty | 0.5.7; pyzmq | 22.2.1; requests | 2.26.0; scanpy | 1.7.2; scikit-learn | 0.24.2; scipy | 1.5.4; seaborn | 0.11.2; Send2Trash | 1.8.0; setuptools | 58.0.4; sinfo | 0.3.4; six | 1.16.0; sniffio | 1.2.0; statsmodels | 0.12.2; stdlib-list | 0.8.0; tables | 3.6.1; terminado | 0.9.4; testpath | 0.5.0; texttable | 1.6.4; threadpoolctl | 3.0.0; tornado | 6.1; tqdm | 4.62.3; traitlets | 4.3.3; typing-extensions | 3.10.0.2; **umap-learn | 0.5.2**; urllib3 | 1.26.7; wcwidth | 0.2.5; webencodings | 0.5.1; wheel | 0.37.0; win-inet-pton | 1.1.0; wincertstore | 0.2; xlrd | 1.2.0; zipp | 3.6.0. </body>. </html>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699
https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:3867,Usability,learn,learn,3867,rty | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21; pandas | 1.1.5; pandocfilters | 1.4.3; parso | 0.8.2; patsy | 0.5.2; pickleshare | 0.7.5; Pillow | 8.4.0; pip | 21.2.2; prometheus-client | 0.11.0; prompt-toolkit | 3.0.20; pycparser | 2.2; Pygments | 2.10.0; pynndescent | 0.5.5; pyOpenSSL | 21.0.0; **pyparsing | 3.0.4**; pyrsistent | 0.17.3; PySocks | 1.7.1; python-dateutil | 2.8.2; python-igraph | 0.9.8; pytz | 2021.3; pywin32 | 228; pywinpty | 0.5.7; pyzmq | 22.2.1; requests | 2.26.0; scanpy | 1.7.2; scikit-learn | 0.24.2; scipy | 1.5.4; seaborn | 0.11.2; Send2Trash | 1.8.0; setuptools | 58.0.4; sinfo | 0.3.4; six | 1.16.0; sniffio | 1.2.0; statsmodels | 0.12.2; stdlib-list | 0.8.0; tables | 3.6.1; terminado | 0.9.4; testpath | 0.5.0; texttable | 1.6.4; threadpoolctl | 3.0.0; tornado | 6.1; tqdm | 4.62.3; traitlets | 4.3.3; typing-extensions | 3.10.0.2; **umap-learn | 0.5.2**; urllib3 | 1.26.7; wcwidth | 0.2.5; webencodings | 0.5.1; wheel | 0.37.0; win-inet-pton | 1.1.0; wincertstore | 0.2; xlrd | 1.2.0; zipp | 3.6.0. </body>. </html>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699
https://github.com/scverse/scanpy/issues/2046#issuecomment-963465126:139,Availability,error,error,139,"Thanks for letting me know!. If it's working in the newest version, there's not much for us to fix. Please let us know if you run into the error on the latest release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963465126
https://github.com/scverse/scanpy/issues/2046#issuecomment-963465126:159,Deployability,release,release,159,"Thanks for letting me know!. If it's working in the newest version, there's not much for us to fix. Please let us know if you run into the error on the latest release!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963465126
https://github.com/scverse/scanpy/issues/2047#issuecomment-969076336:57,Modifiability,config,configs,57,Should be fixed by: https://github.com/algolia/docsearch-configs/pull/4840,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2047#issuecomment-969076336
https://github.com/scverse/scanpy/issues/2048#issuecomment-969078321:49,Testability,test,tests,49,How are you building the package and running the tests? Are you working with a clone of the repo?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969078321
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:2,Availability,down,downloaded,2,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:390,Availability,down,download,390,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:89,Deployability,patch,patches,89,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:182,Deployability,patch,patches,182,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:834,Deployability,install,installed,834,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:1006,Deployability,Install,Installed-Build-Depends,1006,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:2851,Deployability,update,update-icon-cache,2851,"(= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-nondeterminism (= 1.12.0-2),; diffutils (= 1:3.7-5),; dmsetup (= 2:1.02.175-2.1),; docutils-common (= 0.17.1+dfsg-2),; dpkg (= 1.20.9),; dpkg-dev (= 1.20.9),; dwz (= 0.14-1),; file (= 1:5.39-3),; findutils (= 4.8.0-1),; flit (= 3.0.0-1),; fontconfig (= 2.13.1-4.2),; fontconfig-config (= 2.13.1-4.2),; fonts-font-awesome (= 5.0.10+really4.7.0~dfsg-4.1),; fonts-lato (= 2.0-2.1),; fonts-lyx (= 2.3.6-1),; g++ (= 4:11.2.0-2),; g++-11 (= 11.2.0-10),; gcc (= 4:11.2.0-2),; gcc-11 (= 11.2.0-10),; gcc-11-base (= 11.2.0-10),; gettext (= 0.21-4),; gettext-base (= 0.21-4),; gir1.2-atk-1.0 (= 2.36.0-2),; gir1.2-freedesktop (= 1.70.0-2),; gir1.2-gdkpixbuf-2.0 (= 2.42.6+dfsg-2),; gir1.2-glib-2.0 (= 1.70.0-2),; gir1.2-gtk-3.0 (= 3.24.30-3),; gir1.2-harfbuzz-0.0 (= 2.7.4-1),; gir1.2-pango-1.0 (= 1.48.10+ds1-1),; grep (= 3.7-1),; groff-base (= 1.22.4-7),; gtk-update-icon-cache (= 3.24.30-3),; gzip (= 1.10-4),; hicolor-icon-theme (= 0.17-2),; hostname (= 3.23),; imagemagick (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6-common (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6.q16 (= 8:6.9.11.60+dfsg-1.3),; init-system-helpers (= 1.60),; intltool-debian (= 0.35.0+20060710.5),; libacl1 (= 2.3.1-1),; libaec0 (= 1.0.6-1),; libamd2 (= 1:5.10.1+dfsg-2),; libaom3 (= 3.2.0-1),; libapparmor1 (= 3.0.3-5),; libarchive-zip-perl (= 1.68-1),; libargon2-1 (= 0~20171227-0.2),; libarpack2 (= 3.8.0-1),; libasan6 (= 11.2.0-10),; libatk-bridge2.0-0 (= 2.38.0-2),; libatk1.0-0 (= 2.36.0-2),; libatk1.0-data (= 2.36.0-2),; libatlas3-base (= 3.10.3-11),; libatomic1 (= 11.2.0-10),; libatspi2.0-0 (= 2.42.0-2),; libattr1 (= 1:2.5.1-1),; libaudit-common (= 1:3.0.6-1),; libaudit1 (= 1:3.0.6-1),; libavahi-client3 (= 0.8-5),; libavahi-common-data (= 0.8-5),; libavahi-common3 (= 0.8-5),; libbinutils (= 2.37-8),; libblas3 (= 3.10.0-1),; libblkid1 (= 2.37.2-4),; libblosc1 (= 1.21.1+ds1-1),; libbr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:10418,Deployability,patch,patch,10418,"99 (= 3.5-2),; libxau6 (= 1:1.0.9-1),; libxcb-render0 (= 1.14-3),; libxcb-shm0 (= 1.14-3),; libxcb1 (= 1.14-3),; libxcomposite1 (= 1:0.4.5-1),; libxcursor1 (= 1:1.2.0-2),; libxdamage1 (= 1:1.1.5-2),; libxdmcp6 (= 1:1.1.2-3),; libxext6 (= 2:1.3.4-1),; libxfixes3 (= 1:5.0.3-2),; libxft2 (= 2.3.2-2),; libxi6 (= 2:1.8-1),; libxinerama1 (= 2:1.1.4-2),; libxkbcommon0 (= 1.3.1-1),; libxml2 (= 2.9.12+dfsg-5),; libxrandr2 (= 2:1.5.2-1),; libxrender1 (= 1:0.9.10-1),; libxss1 (= 1:1.2.3-1),; libz3-4 (= 4.8.12-1+b1),; libzstd1 (= 1.4.8+dfsg-3),; linux-libc-dev (= 5.14.16-1),; llvm-11 (= 1:11.1.0-4),; llvm-11-linker-tools (= 1:11.1.0-4),; llvm-11-runtime (= 1:11.1.0-4),; login (= 1:4.8.1-2),; lsb-base (= 11.1.0),; m4 (= 1.4.18-5),; mailcap (= 3.70),; make (= 4.3-4.1),; man-db (= 2.9.4-2),; mawk (= 1.3.4.20200120-2),; media-types (= 4.0.0),; mime-support (= 3.66),; mount (= 2.37.2-4),; ncurses-base (= 6.2+20210905-1),; ncurses-bin (= 6.2+20210905-1),; openssl (= 1.1.1l-1),; passwd (= 1:4.8.1-2),; patch (= 2.7.6-7),; perl (= 5.32.1-6),; perl-base (= 5.32.1-6),; perl-modules-5.32 (= 5.32.1-6),; po-debconf (= 1.0.21+nmu1),; procps (= 2:3.3.17-5),; python-matplotlib-data (= 3.3.4-2),; python-tables-data (= 3.6.1-5),; python3 (= 3.9.7-1),; python3-all (= 3.9.7-1),; python3-anndata (= 0.7.5+ds-3),; python3-asciitree (= 0.3.3-3),; python3-attr (= 20.3.0-1),; python3-certifi (= 2020.6.20-1),; python3-cffi-backend (= 1.15.0-1),; python3-chardet (= 4.0.0-1),; python3-cov-core (= 1.15.0-3),; python3-coverage (= 5.1+dfsg.1-2+b2),; python3-cryptography (= 3.3.2-1),; python3-cycler (= 0.11.0-1),; python3-dateutil (= 2.8.1-6),; python3-decorator (= 4.4.2-2),; python3-distutils (= 3.9.8-1),; python3-docutils (= 0.17.1+dfsg-2),; python3-fasteners (= 0.14.1-2),; python3-gi (= 3.42.0-1+b1),; python3-h5py (= 3.3.0-5),; python3-h5py-serial (= 3.3.0-5),; python3-harmony (= 0.7.1-1),; python3-idna (= 2.10-1),; python3-igraph (= 0.9.6-1),; python3-importlib-metadata (= 4.6.4-1),; python3-iniconfig (= 1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:676,Integrability,depend,dependencies,676,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:821,Integrability,depend,dependencies,821,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:1022,Integrability,Depend,Depends,1022,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:864,Modifiability,variab,variables,864,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:2277,Modifiability,config,config,2277,"; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-nondeterminism (= 1.12.0-2),; diffutils (= 1:3.7-5),; dmsetup (= 2:1.02.175-2.1),; docutils-common (= 0.17.1+dfsg-2),; dpkg (= 1.20.9),; dpkg-dev (= 1.20.9),; dwz (= 0.14-1),; file (= 1:5.39-3),; findutils (= 4.8.0-1),; flit (= 3.0.0-1),; fontconfig (= 2.13.1-4.2),; fontconfig-config (= 2.13.1-4.2),; fonts-font-awesome (= 5.0.10+really4.7.0~dfsg-4.1),; fonts-lato (= 2.0-2.1),; fonts-lyx (= 2.3.6-1),; g++ (= 4:11.2.0-2),; g++-11 (= 11.2.0-10),; gcc (= 4:11.2.0-2),; gcc-11 (= 11.2.0-10),; gcc-11-base (= 11.2.0-10),; gettext (= 0.21-4),; gettext-base (= 0.21-4),; gir1.2-atk-1.0 (= 2.36.0-2),; gir1.2-freedesktop (= 1.70.0-2),; gir1.2-gdkpixbuf-2.0 (= 2.42.6+dfsg-2),; gir1.2-glib-2.0 (= 1.70.0-2),; gir1.2-gtk-3.0 (= 3.24.30-3),; gir1.2-harfbuzz-0.0 (= 2.7.4-1),; gir1.2-pango-1.0 (= 1.48.10+ds1-1),; grep (= 3.7-1),; groff-base (= 1.22.4-7),; gtk-update-icon-cache (= 3.24.30-3),; gzip (= 1.10-4),; hicolor-icon-theme (= 0.17-2),; hostname (= 3.23),; imagemagick (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6-common (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6.q16 (= 8:6.9.11.60+dfsg-1.3),; init-system-helpers (= 1.60),; intltool-debian (= 0.35.0+20060710.5),; libacl1 (= 2.3.1-1),; libaec0 (= 1.0.6-1),; libamd2 (= 1:5.10.1+dfsg-2),; libaom3 (= 3.2.0-1),; libapparmor1 (= 3.0.3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:2863,Performance,cache,cache,2863,"(= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-nondeterminism (= 1.12.0-2),; diffutils (= 1:3.7-5),; dmsetup (= 2:1.02.175-2.1),; docutils-common (= 0.17.1+dfsg-2),; dpkg (= 1.20.9),; dpkg-dev (= 1.20.9),; dwz (= 0.14-1),; file (= 1:5.39-3),; findutils (= 4.8.0-1),; flit (= 3.0.0-1),; fontconfig (= 2.13.1-4.2),; fontconfig-config (= 2.13.1-4.2),; fonts-font-awesome (= 5.0.10+really4.7.0~dfsg-4.1),; fonts-lato (= 2.0-2.1),; fonts-lyx (= 2.3.6-1),; g++ (= 4:11.2.0-2),; g++-11 (= 11.2.0-10),; gcc (= 4:11.2.0-2),; gcc-11 (= 11.2.0-10),; gcc-11-base (= 11.2.0-10),; gettext (= 0.21-4),; gettext-base (= 0.21-4),; gir1.2-atk-1.0 (= 2.36.0-2),; gir1.2-freedesktop (= 1.70.0-2),; gir1.2-gdkpixbuf-2.0 (= 2.42.6+dfsg-2),; gir1.2-glib-2.0 (= 1.70.0-2),; gir1.2-gtk-3.0 (= 3.24.30-3),; gir1.2-harfbuzz-0.0 (= 2.7.4-1),; gir1.2-pango-1.0 (= 1.48.10+ds1-1),; grep (= 3.7-1),; groff-base (= 1.22.4-7),; gtk-update-icon-cache (= 3.24.30-3),; gzip (= 1.10-4),; hicolor-icon-theme (= 0.17-2),; hostname (= 3.23),; imagemagick (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6-common (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6.q16 (= 8:6.9.11.60+dfsg-1.3),; init-system-helpers (= 1.60),; intltool-debian (= 0.35.0+20060710.5),; libacl1 (= 2.3.1-1),; libaec0 (= 1.0.6-1),; libamd2 (= 1:5.10.1+dfsg-2),; libaom3 (= 3.2.0-1),; libapparmor1 (= 3.0.3-5),; libarchive-zip-perl (= 1.68-1),; libargon2-1 (= 0~20171227-0.2),; libarpack2 (= 3.8.0-1),; libasan6 (= 11.2.0-10),; libatk-bridge2.0-0 (= 2.38.0-2),; libatk1.0-0 (= 2.36.0-2),; libatk1.0-data (= 2.36.0-2),; libatlas3-base (= 3.10.3-11),; libatomic1 (= 11.2.0-10),; libatspi2.0-0 (= 2.42.0-2),; libattr1 (= 1:2.5.1-1),; libaudit-common (= 1:3.0.6-1),; libaudit1 (= 1:3.0.6-1),; libavahi-client3 (= 0.8-5),; libavahi-common-data (= 0.8-5),; libavahi-common3 (= 0.8-5),; libbinutils (= 2.37-8),; libblas3 (= 3.10.0-1),; libblkid1 (= 2.37.2-4),; libblosc1 (= 1.21.1+ds1-1),; libbr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:6253,Performance,throttle,throttle-debounce,6253,"ixbuf2.0-common (= 2.42.6+dfsg-2),; libgfortran5 (= 11.2.0-10),; libgirepository-1.0-1 (= 1.70.0-2),; libglib2.0-0 (= 2.70.1-1),; libglpk40 (= 5.0-1),; libgmp10 (= 2:6.2.1+dfsg-2),; libgnutls30 (= 3.7.2-2),; libgomp1 (= 11.2.0-10),; libgpg-error0 (= 1.42-3),; libgraphite2-3 (= 1.3.14-1),; libgssapi-krb5-2 (= 1.18.3-7),; libgtk-3-0 (= 3.24.30-3),; libgtk-3-common (= 3.24.30-3),; libharfbuzz0b (= 2.7.4-1),; libhdf5-103-1 (= 1.10.7+repack-4),; libhdf5-hl-100 (= 1.10.7+repack-4),; libheif1 (= 1.12.0-2+b3),; libhogweed6 (= 3.7.3-1),; libicu67 (= 67.1-7),; libidn2-0 (= 2.3.2-2),; libigraph1 (= 0.8.5+ds1-1),; libimagequant0 (= 2.12.2-1.1),; libip4tc2 (= 1.8.7-1),; libisl23 (= 0.24-2),; libitm1 (= 11.2.0-10),; libjbig0 (= 2.1-3.1+b2),; libjpeg62-turbo (= 1:2.1.1-1),; libjs-jquery (= 3.5.1+dfsg+~3.5.5-8),; libjs-jquery-hotkeys (= 0~20130707+git2d51e3a9+dfsg-2.1),; libjs-jquery-isonscreen (= 1.2.0-1.1),; libjs-jquery-metadata (= 12-3),; libjs-jquery-tablesorter (= 1:2.31.3+dfsg1-2),; libjs-jquery-throttle-debounce (= 1.1+dfsg.1-1.1),; libjs-jquery-ui (= 1.13.0+dfsg-1),; libjs-sphinxdoc (= 4.2.0-5),; libjs-underscore (= 1.9.1~dfsg-4),; libjson-c5 (= 0.15-2),; libk5crypto3 (= 1.18.3-7),; libkeyutils1 (= 1.6.1-2),; libkmod2 (= 29-1),; libkrb5-3 (= 1.18.3-7),; libkrb5support0 (= 1.18.3-7),; liblapack3 (= 3.10.0-1),; liblbfgsb0 (= 3.0+dfsg.3-9),; liblcms2-2 (= 2.12~rc1-2),; libldap-2.4-2 (= 2.4.59+dfsg-1),; libllvm11 (= 1:11.1.0-4),; liblqr-1-0 (= 0.4.2-2.1),; liblsan0 (= 11.2.0-10),; libltdl7 (= 2.4.6-15),; liblz4-1 (= 1.9.3-2),; liblzf1 (= 3.6-3),; liblzma5 (= 5.2.5-2),; liblzo2-2 (= 2.10-2),; libmagic-mgc (= 1:5.39-3),; libmagic1 (= 1:5.39-3),; libmagickcore-6.q16-6 (= 8:6.9.11.60+dfsg-1.3),; libmagickwand-6.q16-6 (= 8:6.9.11.60+dfsg-1.3),; libmd0 (= 1.0.4-1),; libmount1 (= 2.37.2-4),; libmpc3 (= 1.2.1-1),; libmpdec3 (= 2.5.1-2),; libmpfr6 (= 4.1.0-3),; libncurses6 (= 6.2+20210905-1),; libncursesw6 (= 6.2+20210905-1),; libnettle8 (= 3.7.3-1),; libnghttp2-14 (= 1.43.0-1),; libnsl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:353,Security,access,access,353,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:1475,Security,certificate,certificates,1475,". The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-nondeterminism (= 1.12.0-2),; diffutils (= 1:3.7-5),; dmsetup (= 2:1.02.175-2.1),; docutils-common (= 0.17.1+dfsg-2),; dpkg (= 1.20.9),; dpkg-dev (= 1.20.9),; dwz (= 0.14-1),; file (= 1:5.39-3),; findutils (= 4.8.0-1),; flit (= 3.0.0-1),; fontconfig (= 2.13.1-4.2),; fontconfig-config (= 2.13.1-4.2),; fonts-font-awesome (= 5.0.10+really4.7.0~dfsg-4.1),; fonts-lato (= 2.0-2.1),; fonts-lyx (= 2.3.6-1),; g++ (= 4:11.2.0-2),; g++-11 (= 11.2.0-10),; gcc (= 4:11.2.0-2),; gcc-11 (= 11.2.0-10),; g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:269,Testability,test,tests,269,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:636,Testability,test,tests,636,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:787,Testability,log,log,787,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:10087,Testability,log,login,10087,"budev1 (= 249.6-2),; libunistring2 (= 0.9.10-6),; libuuid1 (= 2.37.2-4),; libwayland-client0 (= 1.19.0-2+b1),; libwayland-cursor0 (= 1.19.0-2+b1),; libwayland-egl1 (= 1.19.0-2+b1),; libwebp6 (= 0.6.1-2.1),; libwebpdemux2 (= 0.6.1-2.1),; libwebpmux3 (= 0.6.1-2.1),; libx11-6 (= 2:1.7.2-2+b1),; libx11-data (= 2:1.7.2-2),; libx265-199 (= 3.5-2),; libxau6 (= 1:1.0.9-1),; libxcb-render0 (= 1.14-3),; libxcb-shm0 (= 1.14-3),; libxcb1 (= 1.14-3),; libxcomposite1 (= 1:0.4.5-1),; libxcursor1 (= 1:1.2.0-2),; libxdamage1 (= 1:1.1.5-2),; libxdmcp6 (= 1:1.1.2-3),; libxext6 (= 2:1.3.4-1),; libxfixes3 (= 1:5.0.3-2),; libxft2 (= 2.3.2-2),; libxi6 (= 2:1.8-1),; libxinerama1 (= 2:1.1.4-2),; libxkbcommon0 (= 1.3.1-1),; libxml2 (= 2.9.12+dfsg-5),; libxrandr2 (= 2:1.5.2-1),; libxrender1 (= 1:0.9.10-1),; libxss1 (= 1:1.2.3-1),; libz3-4 (= 4.8.12-1+b1),; libzstd1 (= 1.4.8+dfsg-3),; linux-libc-dev (= 5.14.16-1),; llvm-11 (= 1:11.1.0-4),; llvm-11-linker-tools (= 1:11.1.0-4),; llvm-11-runtime (= 1:11.1.0-4),; login (= 1:4.8.1-2),; lsb-base (= 11.1.0),; m4 (= 1.4.18-5),; mailcap (= 3.70),; make (= 4.3-4.1),; man-db (= 2.9.4-2),; mawk (= 1.3.4.20200120-2),; media-types (= 4.0.0),; mime-support (= 3.66),; mount (= 2.37.2-4),; ncurses-base (= 6.2+20210905-1),; ncurses-bin (= 6.2+20210905-1),; openssl (= 1.1.1l-1),; passwd (= 1:4.8.1-2),; patch (= 2.7.6-7),; perl (= 5.32.1-6),; perl-base (= 5.32.1-6),; perl-modules-5.32 (= 5.32.1-6),; po-debconf (= 1.0.21+nmu1),; procps (= 2:3.3.17-5),; python-matplotlib-data (= 3.3.4-2),; python-tables-data (= 3.6.1-5),; python3 (= 3.9.7-1),; python3-all (= 3.9.7-1),; python3-anndata (= 0.7.5+ds-3),; python3-asciitree (= 0.3.3-3),; python3-attr (= 20.3.0-1),; python3-certifi (= 2020.6.20-1),; python3-cffi-backend (= 1.15.0-1),; python3-chardet (= 4.0.0-1),; python3-cov-core (= 1.15.0-3),; python3-coverage (= 5.1+dfsg.1-2+b2),; python3-cryptography (= 3.3.2-1),; python3-cycler (= 0.11.0-1),; python3-dateutil (= 2.8.1-6),; python3-decorator (= 4.4.2-2),; python3-dis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:13854,Usability,learn,learn,13854,".0-1),; python3-pluggy (= 0.13.0-7.1),; python3-ply (= 3.11-5),; python3-py (= 1.10.0-1),; python3-pygments (= 2.7.1+dfsg-2.1),; python3-pyparsing (= 2.4.7-1),; python3-pytest (= 6.2.5-1),; python3-pytest-cov (= 3.0.0-1),; python3-pytoml (= 0.1.21-1),; python3-requests (= 2.25.1+dfsg-2),; python3-roman (= 3.3-1),; python3-scipy (= 1.7.1-2),; python3-seaborn (= 0.11.2-2),; python3-secretstorage (= 3.3.1-1),; python3-setuptools (= 58.2.0-1),; python3-setuptools-scm (= 6.0.1-1),; python3-sinfo (= 0.3.1-2),; python3-six (= 1.16.0-2),; python3-sklearn (= 0.23.2-5),; python3-sklearn-lib (= 0.23.2-5),; python3-skmisc (= 0.1.4+dfsg-1),; python3-slimit (= 0.8.1-4),; python3-statsmodels (= 0.12.2-2),; python3-statsmodels-lib (= 0.12.2-2),; python3-stdlib-list (= 0.8.0-4),; python3-tables (= 3.6.1-5),; python3-tables-lib (= 3.6.1-5),; python3-texttable (= 1.6.3-2),; python3-threadpoolctl (= 2.1.0-1),; python3-tk (= 3.9.8-1),; python3-toml (= 0.10.2-1),; python3-tqdm (= 4.57.0-2),; python3-tz (= 2021.3-1),; python3-urllib3 (= 1.26.5-1~exp1),; python3-wheel (= 0.34.2-1),; python3-zarr (= 2.10.2+ds-2),; python3-zipp (= 1.0.0-3),; python3.9 (= 3.9.8-2),; python3.9-minimal (= 3.9.8-2),; readline-common (= 8.1-2),; rpcsvc-proto (= 1.4.2-4),; sed (= 4.8-1),; sensible-utils (= 0.0.17),; sgml-base (= 1.30),; shared-mime-info (= 2.0-1),; sphinx-rtd-theme-common (= 1.0.0+dfsg-1),; systemd (= 249.6-2),; systemd-sysv (= 249.6-2),; sysvinit-utils (= 3.00-1),; tar (= 1.34+dfsg-1),; tk8.6-blt2.5 (= 2.5.3+dfsg-4.1),; ttf-bitstream-vera (= 1.10-8.1),; tzdata (= 2021e-1),; ucf (= 3.0043),; umap-learn (= 0.4.5+dfsg-2),; util-linux (= 2.37.2-4),; x11-common (= 1:7.7+23),; xkb-data (= 2.33-1),; xml-core (= 0.18+nmu1),; xz-utils (= 5.2.5-2),; zlib1g (= 1:1.2.11.dfsg-2); Environment:; DEB_BUILD_OPTIONS=""parallel=4""; LANG=""en_US.UTF-8""; LC_ALL=""C.UTF-8""; LC_COLLATE=""C.UTF-8""; LC_CTYPE=""en_US.UTF-8""; LC_MESSAGES=""en_US.UTF-8""; LD_LIBRARY_PATH=""/usr/lib/libeatmydata""; SOURCE_DATE_EPOCH=""1625592742""; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616
https://github.com/scverse/scanpy/issues/2048#issuecomment-969888153:180,Testability,test,tests,180,"I can get the data by skipping the spatial plot. . If I run this in a fresh IPython session I get the attached plot. ```; import scanpy; from matplotlib import pyplot; from scanpy.tests.test_embedding_plots import HERE; from scanpy.plotting._tools import scatterplots; adata = scanpy.read_visium(HERE / ""_data"" / ""visium_data"" / ""1.0.0""); adata.obs = adata.obs.astype({'array_row': 'str'}); data_points, components = scatterplots._get_data_points(adata, ""spatial"", None, None, None); pyplot.scatter(data_points[0][:, 0], data_points[0][:, 1]); pyplot.savefig(""visium.png""). ```; ![visium](https://user-images.githubusercontent.com/975038/141928846-993b0fc9-33ad-4edf-b0b3-44b56274494e.png). If I call scanpy.pl.spatial before I call pyplot.scatter I get the black plot, so it's probably some default isn't right.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969888153
https://github.com/scverse/scanpy/issues/2051#issuecomment-993445505:105,Testability,test,tests,105,"Looks like this not working with `multi_panel` would be introduced by #1422. At the moment my matplotlib tests are being flaky, so I may have some trouble fixing this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2051#issuecomment-993445505
https://github.com/scverse/scanpy/issues/2053#issuecomment-973500395:107,Availability,error,errors,107,It seems to be something about the `genes.tsv`. I replaced it with another genes.tsv and it didn't produce errors.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053#issuecomment-973500395
https://github.com/scverse/scanpy/issues/2053#issuecomment-984799011:317,Availability,error,error,317,"> @brianpenghe, do you have a copy of your original file? Any idea what could have been different?; > ; > @dn-ra, would you be able to share the first couple lines of your file, and let me know how it was generated?. I think I found the cause: ; When the genes.tsv only has one column it doesn't work and throws this error. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053#issuecomment-984799011
https://github.com/scverse/scanpy/issues/2053#issuecomment-1180871625:166,Availability,error,error,166,@brianpenghe What column did you add to the genes.tsv so that it worked? I currently have a genes.tsv file with one column for the gene names and am getting the same error as you did. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053#issuecomment-1180871625
https://github.com/scverse/scanpy/issues/2053#issuecomment-1286416876:15,Availability,error,error,15,"I've fixed the error I was getting, which was posted on another issue and referenced here.; Here's the solution that worked for me:; https://github.com/scverse/scanpy/issues/1916#issuecomment-1286404697",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053#issuecomment-1286416876
https://github.com/scverse/scanpy/issues/2053#issuecomment-2133703888:23,Availability,error,error,23,"I encountered the same error (KeyError: 1) when trying to load the .mtx file with scanpy.read_10x_mtx(). After several unsuccessful attempts at renaming the columns and indices in the 'genes.tsv' file in different ways, I found a workaround that worked for me:. 1. Import the .mtx file separately using scanpy.read_mtx().; 2. Convert the imported data to a pandas DataFrame using .to_df().; 3. Manually name the columns and indices using the 'barcodes.tsv' and 'features.tsv' files, respectively. This approach allowed me to bypass the KeyError and successfully load the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053#issuecomment-2133703888
https://github.com/scverse/scanpy/issues/2053#issuecomment-2133703888:58,Performance,load,load,58,"I encountered the same error (KeyError: 1) when trying to load the .mtx file with scanpy.read_10x_mtx(). After several unsuccessful attempts at renaming the columns and indices in the 'genes.tsv' file in different ways, I found a workaround that worked for me:. 1. Import the .mtx file separately using scanpy.read_mtx().; 2. Convert the imported data to a pandas DataFrame using .to_df().; 3. Manually name the columns and indices using the 'barcodes.tsv' and 'features.tsv' files, respectively. This approach allowed me to bypass the KeyError and successfully load the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053#issuecomment-2133703888
https://github.com/scverse/scanpy/issues/2053#issuecomment-2133703888:562,Performance,load,load,562,"I encountered the same error (KeyError: 1) when trying to load the .mtx file with scanpy.read_10x_mtx(). After several unsuccessful attempts at renaming the columns and indices in the 'genes.tsv' file in different ways, I found a workaround that worked for me:. 1. Import the .mtx file separately using scanpy.read_mtx().; 2. Convert the imported data to a pandas DataFrame using .to_df().; 3. Manually name the columns and indices using the 'barcodes.tsv' and 'features.tsv' files, respectively. This approach allowed me to bypass the KeyError and successfully load the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053#issuecomment-2133703888
https://github.com/scverse/scanpy/pull/2055#issuecomment-987012001:67,Modifiability,flexible,flexible,67,"Thanks for the PR!. While I like the idea of being able to be more flexible around how the dot plots are made, I'm not sure I like the idea of adding more argument specific behavior. That is, I don't like that passing a boolean flag changes the meaning of the values for the `var_names` and `groupby` arguments. E.g. the `group_by` is the selection of `obs` corresponding to the plots rows, while `var_names` was the variable for the columns. What could maybe be done instead is to have an argument for column grouping. Instead of. ```python; sc.pl.dotplot(adata, var_names='C1QA', groupby=['louvain', 'sampleid'], groupby_expand=True); ```. It could be more like. ```python; sc.pl.dotplot(adata, var_names='C1QA', group_by='louvain', group_cols='sampleid'); ```. I would ideally like a solution here to work for all the other ""grouped plots"" as well. I'll write a bit more about this in the parent issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2055#issuecomment-987012001
https://github.com/scverse/scanpy/pull/2055#issuecomment-987012001:417,Modifiability,variab,variable,417,"Thanks for the PR!. While I like the idea of being able to be more flexible around how the dot plots are made, I'm not sure I like the idea of adding more argument specific behavior. That is, I don't like that passing a boolean flag changes the meaning of the values for the `var_names` and `groupby` arguments. E.g. the `group_by` is the selection of `obs` corresponding to the plots rows, while `var_names` was the variable for the columns. What could maybe be done instead is to have an argument for column grouping. Instead of. ```python; sc.pl.dotplot(adata, var_names='C1QA', groupby=['louvain', 'sampleid'], groupby_expand=True); ```. It could be more like. ```python; sc.pl.dotplot(adata, var_names='C1QA', group_by='louvain', group_cols='sampleid'); ```. I would ideally like a solution here to work for all the other ""grouped plots"" as well. I'll write a bit more about this in the parent issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2055#issuecomment-987012001
https://github.com/scverse/scanpy/issues/2058#issuecomment-977463620:348,Energy Efficiency,reduce,reduced,348,"Not the developer of scanpy. ; The reason for this is that scanpy will try to initialize the palette automatically if you do not provide it. ; Then this palette will be saved in `adata.uns` . When the number of clusters is large enough, scanpy cannot find a suitable palette, so all colors are initialized to grey.; After the number of clusters is reduced, you can just delete `adata.uns['leiden_colors']` to force the scanpy to reinitialize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2058#issuecomment-977463620
https://github.com/scverse/scanpy/issues/2060#issuecomment-981701546:29,Performance,perform,performance,29,"We're always up for improved performance! Would love to see improvements here. (Btw, I think I've already got your gist bookmarked on twitter). Do you have any benchmarks of performance here? Especially against our current implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2060#issuecomment-981701546
https://github.com/scverse/scanpy/issues/2060#issuecomment-981701546:174,Performance,perform,performance,174,"We're always up for improved performance! Would love to see improvements here. (Btw, I think I've already got your gist bookmarked on twitter). Do you have any benchmarks of performance here? Especially against our current implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2060#issuecomment-981701546
https://github.com/scverse/scanpy/issues/2060#issuecomment-981701546:160,Testability,benchmark,benchmarks,160,"We're always up for improved performance! Would love to see improvements here. (Btw, I think I've already got your gist bookmarked on twitter). Do you have any benchmarks of performance here? Especially against our current implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2060#issuecomment-981701546
https://github.com/scverse/scanpy/issues/2060#issuecomment-981723859:10,Testability,benchmark,benchmarked,10,"I haven't benchmarked against scanpy, only against `scipy.stats.mannwhitneyu` (which at this point can handle arrays, I know it couldn't before). On my laptop (an 8-core Intel MacBook Pro) it's about a 10x speedup. But with more cores it can be a lot more. Even without parallelization, you can get some improvement by just using `numba.njit` on some of the internal bits (e.g. `tiecorrect`). Of course, your code has a lot of options that I didn't bother with, because I didn't need them. Some of them might be harder to JIT than others.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2060#issuecomment-981723859
https://github.com/scverse/scanpy/pull/2061#issuecomment-983857708:36,Testability,test,test,36,"Thanks for the PR!. Could you add a test for this?. Also, do you think the behavior should be different for when `n_points > adata.n_vars`? I'm thinking there could be a block like:. ```python; if n_points is None and adata.n_vars < 30:; n_points = adata.n_vars; elif n_points is None:; n_points = 30; ```. Btw, the tests and docs should succeed if you merge changes from master into this branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2061#issuecomment-983857708
https://github.com/scverse/scanpy/pull/2061#issuecomment-983857708:316,Testability,test,tests,316,"Thanks for the PR!. Could you add a test for this?. Also, do you think the behavior should be different for when `n_points > adata.n_vars`? I'm thinking there could be a block like:. ```python; if n_points is None and adata.n_vars < 30:; n_points = adata.n_vars; elif n_points is None:; n_points = 30; ```. Btw, the tests and docs should succeed if you merge changes from master into this branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2061#issuecomment-983857708
https://github.com/scverse/scanpy/pull/2061#issuecomment-985389044:557,Testability,test,tests,557,"If I catch your point the function prototype would then be. ```; def pca_loadings(; adata: AnnData,; components: Union[str, Sequence[int], None] = None,; include_lowest: bool = True,; n_points: Union[int, None] = None,; show: Optional[bool] = None,; save: Union[str, bool, None] = None,; ):; ```. Then . ```; if n_points is None and adata.n_vars < 30:; n_points = adata.n_vars; elif n_points is None:; n_points = 30. ## should we also prevent user from plotting more than adata.n_vars?. else:; n_points = min(n_points, adata.n_vars); ```; ; considering the tests, I don't see what you expect (sorry, I'm quite new to collaborative work on github):; looking at test_rankings in test_plottings.py, the test should look like; sc.pl.pca(pbmc, n_points=10); save_and_compare_images(""pca_loadings_with_10_points""). but what will it compare to?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2061#issuecomment-985389044
https://github.com/scverse/scanpy/pull/2061#issuecomment-985389044:700,Testability,test,test,700,"If I catch your point the function prototype would then be. ```; def pca_loadings(; adata: AnnData,; components: Union[str, Sequence[int], None] = None,; include_lowest: bool = True,; n_points: Union[int, None] = None,; show: Optional[bool] = None,; save: Union[str, bool, None] = None,; ):; ```. Then . ```; if n_points is None and adata.n_vars < 30:; n_points = adata.n_vars; elif n_points is None:; n_points = 30. ## should we also prevent user from plotting more than adata.n_vars?. else:; n_points = min(n_points, adata.n_vars); ```; ; considering the tests, I don't see what you expect (sorry, I'm quite new to collaborative work on github):; looking at test_rankings in test_plottings.py, the test should look like; sc.pl.pca(pbmc, n_points=10); save_and_compare_images(""pca_loadings_with_10_points""). but what will it compare to?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2061#issuecomment-985389044
https://github.com/scverse/scanpy/pull/2061#issuecomment-986963248:295,Testability,test,testing,295,"We have images which are saved which can be compared against, typically one would add a new image which you have visually confirmed to be correct, so if the behavior changes later we know. There is a little more about this in the [contribution guide](https://scanpy.readthedocs.io/en/stable/dev/testing.html#plotting-tests).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2061#issuecomment-986963248
https://github.com/scverse/scanpy/pull/2061#issuecomment-986963248:317,Testability,test,tests,317,"We have images which are saved which can be compared against, typically one would add a new image which you have visually confirmed to be correct, so if the behavior changes later we know. There is a little more about this in the [contribution guide](https://scanpy.readthedocs.io/en/stable/dev/testing.html#plotting-tests).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2061#issuecomment-986963248
https://github.com/scverse/scanpy/pull/2061#issuecomment-986963248:244,Usability,guid,guide,244,"We have images which are saved which can be compared against, typically one would add a new image which you have visually confirmed to be correct, so if the behavior changes later we know. There is a little more about this in the [contribution guide](https://scanpy.readthedocs.io/en/stable/dev/testing.html#plotting-tests).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2061#issuecomment-986963248
https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860:424,Availability,error,error,424,"Thanks for opening a new issue for this, and the info. Could you let me know a bit more about how you've installed scanpy? E.g. what OS, did you use conda or pip, etc. My guess would be that this is numba related (which, from reporting the cpu flags, I'm guessing you suspect too). Are you able to import `numba`? If so, what about `pynndescent` and `umap`? I'm trying to figure out if some code in scanpy is triggering the error, or if it's one of our dependencies. ---------------. Initially mentioned in https://github.com/theislab/scanpy/issues/1823#issuecomment-983551937",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860
https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860:105,Deployability,install,installed,105,"Thanks for opening a new issue for this, and the info. Could you let me know a bit more about how you've installed scanpy? E.g. what OS, did you use conda or pip, etc. My guess would be that this is numba related (which, from reporting the cpu flags, I'm guessing you suspect too). Are you able to import `numba`? If so, what about `pynndescent` and `umap`? I'm trying to figure out if some code in scanpy is triggering the error, or if it's one of our dependencies. ---------------. Initially mentioned in https://github.com/theislab/scanpy/issues/1823#issuecomment-983551937",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860
https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860:453,Integrability,depend,dependencies,453,"Thanks for opening a new issue for this, and the info. Could you let me know a bit more about how you've installed scanpy? E.g. what OS, did you use conda or pip, etc. My guess would be that this is numba related (which, from reporting the cpu flags, I'm guessing you suspect too). Are you able to import `numba`? If so, what about `pynndescent` and `umap`? I'm trying to figure out if some code in scanpy is triggering the error, or if it's one of our dependencies. ---------------. Initially mentioned in https://github.com/theislab/scanpy/issues/1823#issuecomment-983551937",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860
https://github.com/scverse/scanpy/issues/2062#issuecomment-985144190:36,Integrability,depend,dependencies,36,I did successfully import all three dependencies and (just to make sure) still cannot import scanpy.; ```; >>> import numba; >>> import pynndescent; >>> import umap; >>> import scanpy; Illegal instruction (core dumped); ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2062#issuecomment-985144190
https://github.com/scverse/scanpy/pull/2064#issuecomment-987881759:19,Deployability,install,installing,19,I'm having trouble installing tables into a conda environment. Now may be the time to pull the trigger on this,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2064#issuecomment-987881759
https://github.com/scverse/scanpy/pull/2064#issuecomment-1013003401:42,Integrability,depend,dependency,42,I think I would rather just bump the h5py dependency since it's been over a year.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2064#issuecomment-1013003401
https://github.com/scverse/scanpy/issues/2067#issuecomment-991913417:262,Deployability,release,release,262,"> Trying to figure out how much of a problem this is, how much does PAGA use forceatlas2?; > ; > From my skimming of the code: it's optional – the default is to use one of the `igraph` layout algorithms. Options I see:; 1. We fork forceatlas2 and create our own release with its own name. Not a fan of this.; 2. Somebody finds a way to contact the author. I already failed through several channels. No response.; 3. We remove support for forceatlas2 since we have another option. @ivirshup how comparable are the forceatlas2 and the igraph implementations when it comes to results? Do you have any idea?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-991913417
https://github.com/scverse/scanpy/issues/2067#issuecomment-995647980:83,Deployability,release,release,83,"I was eventually able to contact the maintainer and he's looking into making a new release. Will see what happens. Nevertheless, it might not be a bad idea to simplify the code and to only support `igraph`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-995647980
https://github.com/scverse/scanpy/issues/2067#issuecomment-995647980:159,Usability,simpl,simplify,159,"I was eventually able to contact the maintainer and he's looking into making a new release. Will see what happens. Nevertheless, it might not be a bad idea to simplify the code and to only support `igraph`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-995647980
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1669,Availability,error,error,1669,"orceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:3934,Availability,error,error,3934,as2-0.3.5/examples/geometric_graph.png; x forceatlas2-0.3.5/examples/grid_graph.png; x forceatlas2-0.3.5/fa2/; x forceatlas2-0.3.5/fa2/__init__.py; x forceatlas2-0.3.5/fa2/fa2util.c; x forceatlas2-0.3.5/fa2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; creating fa2.egg-info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; writing manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:3941,Availability,error,error,3941,as2-0.3.5/examples/geometric_graph.png; x forceatlas2-0.3.5/examples/grid_graph.png; x forceatlas2-0.3.5/fa2/; x forceatlas2-0.3.5/fa2/__init__.py; x forceatlas2-0.3.5/fa2/fa2util.c; x forceatlas2-0.3.5/fa2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; creating fa2.egg-info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; writing manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:3971,Availability,error,error,3971,as2-0.3.5/examples/geometric_graph.png; x forceatlas2-0.3.5/examples/grid_graph.png; x forceatlas2-0.3.5/fa2/; x forceatlas2-0.3.5/fa2/__init__.py; x forceatlas2-0.3.5/fa2/fa2util.c; x forceatlas2-0.3.5/fa2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; creating fa2.egg-info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; writing manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:5742,Availability,error,error,5742,ames to fa2.egg-info/top_level.txt; writing manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; creating build/temp.macosx-12.3-x86_64-3.10; creating build/temp.macosx-12.3-x86_64-3.10/fa2; clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/test/.pyenv/versions/3.10.3/include/python3.10 -c fa2/fa2util.c -o build/temp.macosx-12.3-x86_64-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/ve,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:5903,Availability,error,error,5903, 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; creating build/temp.macosx-12.3-x86_64-3.10; creating build/temp.macosx-12.3-x86_64-3.10/fa2; clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/test/.pyenv/versions/3.10.3/include/python3.10 -c fa2/fa2util.c -o build/temp.macosx-12.3-x86_64-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:6064,Availability,error,error,6064,opying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; creating build/temp.macosx-12.3-x86_64-3.10; creating build/temp.macosx-12.3-x86_64-3.10/fa2; clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/test/.pyenv/versions/3.10.3/include/python3.10 -c fa2/fa2util.c -o build/temp.macosx-12.3-x86_64-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __att,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:16733,Availability,error,errors,16733,"te: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:16752,Availability,error,error,16752,"t_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:16839,Availability,error,error,16839,"t_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:16914,Availability,ERROR,ERROR,16914,"ZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17206,Availability,error,error,17206,"PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt';",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17213,Availability,error,error,17213,"PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt';",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17243,Availability,error,error,17243,"PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt';",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:18942,Availability,error,error,18942, to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; creating build/temp.macosx-12.3-x86_64-3.10; creating build/temp.macosx-12.3-x86_64-3.10/fa2; clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/test/.pyenv/versions/3.10.3/include/python3.10 -c fa2/fa2util.c -o build/temp.macosx-12.3-x86_64-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/ve,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:19103,Availability,error,error,19103, 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; creating build/temp.macosx-12.3-x86_64-3.10; creating build/temp.macosx-12.3-x86_64-3.10/fa2; clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/test/.pyenv/versions/3.10.3/include/python3.10 -c fa2/fa2util.c -o build/temp.macosx-12.3-x86_64-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:19264,Availability,error,error,19264,opying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; creating build/temp.macosx-12.3-x86_64-3.10; creating build/temp.macosx-12.3-x86_64-3.10/fa2; clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/test/.pyenv/versions/3.10.3/include/python3.10 -c fa2/fa2util.c -o build/temp.macosx-12.3-x86_64-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __att,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:29933,Availability,error,errors,29933,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:29952,Availability,error,error,29952,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30039,Availability,error,error,30039,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30500,Availability,error,error,30500,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30522,Availability,failure,failure,30522,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30545,Availability,error,error,30545,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30697,Availability,failure,failure,30697,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:164,Deployability,release,release,164,"I am using the latest M1 macbook pro with python 3.10.3. For some reason if you clone the repository then compile it works in python 3.9+; I cannot explain why the release tarball has issues. As per some other documentation, it is because [tp_print has been removed from type objects for python 3.9+.](https://docs.python.org/3/c-api/typeobj.html) See below. So, if you clone the repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:410,Deployability,install,install,410,"I am using the latest M1 macbook pro with python 3.10.3. For some reason if you clone the repository then compile it works in python 3.9+; I cannot explain why the release tarball has issues. As per some other documentation, it is because [tp_print has been removed from type objects for python 3.9+.](https://docs.python.org/3/c-api/typeobj.html) See below. So, if you clone the repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:584,Deployability,install,install,584,"I am using the latest M1 macbook pro with python 3.10.3. For some reason if you clone the repository then compile it works in python 3.9+; I cannot explain why the release tarball has issues. As per some other documentation, it is because [tp_print has been removed from type objects for python 3.9+.](https://docs.python.org/3/c-api/typeobj.html) See below. So, if you clone the repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1491,Deployability,Install,Installing,1491,"e repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1540,Deployability,install,installed,1540,"e repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1630,Deployability,install,install,1630,"orceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1642,Deployability,release,release,1642,"orceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:3390,Deployability,install,install,3390,"d.github.com)|140.82.114.9|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0.3.5.tar.gz’ saved [445420]. test@mac ~/PythonPackages$ tar xvf v0.3.5.tar.gz ; x forceatlas2-0.3.5/; x forceatlas2-0.3.5/.gitignore; x forceatlas2-0.3.5/LICENSE; x forceatlas2-0.3.5/MANIFEST.in; x forceatlas2-0.3.5/README.md; x forceatlas2-0.3.5/examples/; x forceatlas2-0.3.5/examples/forceatlas2-layout.ipynb; x forceatlas2-0.3.5/examples/geometric_graph.png; x forceatlas2-0.3.5/examples/grid_graph.png; x forceatlas2-0.3.5/fa2/; x forceatlas2-0.3.5/fa2/__init__.py; x forceatlas2-0.3.5/fa2/fa2util.c; x forceatlas2-0.3.5/fa2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:4080,Deployability,Install,Installing,4080,a2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; creating fa2.egg-info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; writing manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; r,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:4165,Deployability,install,installed,4165,a2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; creating fa2.egg-info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; writing manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; r,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:4201,Deployability,install,install,4201,a2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; creating fa2.egg-info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; writing manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; r,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17005,Deployability,Install,Installing,17005,"h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17083,Deployability,install,installation,17083,"h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17186,Deployability,install,install,17186,"bject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17271,Deployability,install,install,17271,"op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17357,Deployability,Install,Installing,17357,"get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17442,Deployability,install,installed,17442,"get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17478,Deployability,install,install,17478,"get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:17498,Deployability,install,install,17498,"get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30193,Deployability,Rolling,Rolling,30193,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30514,Deployability,install,install-failure,30514,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30567,Deployability,install,install,30567,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1384,Performance,cache,cache-,1384,"e repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:470,Testability,test,test,470,"I am using the latest M1 macbook pro with python 3.10.3. For some reason if you clone the repository then compile it works in python 3.9+; I cannot explain why the release tarball has issues. As per some other documentation, it is because [tp_print has been removed from type objects for python 3.9+.](https://docs.python.org/3/c-api/typeobj.html) See below. So, if you clone the repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:540,Testability,test,test,540,"I am using the latest M1 macbook pro with python 3.10.3. For some reason if you clone the repository then compile it works in python 3.9+; I cannot explain why the release tarball has issues. As per some other documentation, it is because [tp_print has been removed from type objects for python 3.9+.](https://docs.python.org/3/c-api/typeobj.html) See below. So, if you clone the repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:620,Testability,test,test,620,"I am using the latest M1 macbook pro with python 3.10.3. For some reason if you clone the repository then compile it works in python 3.9+; I cannot explain why the release tarball has issues. As per some other documentation, it is because [tp_print has been removed from type objects for python 3.9+.](https://docs.python.org/3/c-api/typeobj.html) See below. So, if you clone the repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:740,Testability,test,test,740,"I am using the latest M1 macbook pro with python 3.10.3. For some reason if you clone the repository then compile it works in python 3.9+; I cannot explain why the release tarball has issues. As per some other documentation, it is because [tp_print has been removed from type objects for python 3.9+.](https://docs.python.org/3/c-api/typeobj.html) See below. So, if you clone the repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:856,Testability,test,test,856,"I am using the latest M1 macbook pro with python 3.10.3. For some reason if you clone the repository then compile it works in python 3.9+; I cannot explain why the release tarball has issues. As per some other documentation, it is because [tp_print has been removed from type objects for python 3.9+.](https://docs.python.org/3/c-api/typeobj.html) See below. So, if you clone the repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:970,Testability,test,test,970,"I am using the latest M1 macbook pro with python 3.10.3. For some reason if you clone the repository then compile it works in python 3.9+; I cannot explain why the release tarball has issues. As per some other documentation, it is because [tp_print has been removed from type objects for python 3.9+.](https://docs.python.org/3/c-api/typeobj.html) See below. So, if you clone the repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1562,Testability,test,test,1562,"3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.9",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1682,Testability,test,test,1682,"nt already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0.3.5.tar.gz’ saved [445420]. test@mac ~/PythonPackages$ tar xv",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:2670,Testability,test,test,2670,"ac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0.3.5.tar.gz’ saved [445420]. test@mac ~/PythonPackages$ tar xvf v0.3.5.tar.gz ; x forceatlas2-0.3.5/; x forceatlas2-0.3.5/.gitignore; x forceatlas2-0.3.5/LICENSE; x forceatlas2-0.3.5/MANIFEST.in; x forceatlas2-0.3.5/README.md; x forceatlas2-0.3.5/examples/; x forceatlas2-0.3.5/examples/forceatlas2-layout.ipynb; x forceatlas2-0.3.5/examples/geometric_graph.png; x forceatlas2-0.3.5/examples/grid_graph.png; x forceatlas2-0.3.5/fa2/; x forceatlas2-0.3.5/fa2/__init__.py; x forceatlas2-0.3.5/fa2/fa2util.c; x forceatlas2-0.3.5/fa2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:3290,Testability,test,test,3290,"codeload.github.com)... 140.82.114.9; Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0.3.5.tar.gz’ saved [445420]. test@mac ~/PythonPackages$ tar xvf v0.3.5.tar.gz ; x forceatlas2-0.3.5/; x forceatlas2-0.3.5/.gitignore; x forceatlas2-0.3.5/LICENSE; x forceatlas2-0.3.5/MANIFEST.in; x forceatlas2-0.3.5/README.md; x forceatlas2-0.3.5/examples/; x forceatlas2-0.3.5/examples/forceatlas2-layout.ipynb; x forceatlas2-0.3.5/examples/geometric_graph.png; x forceatlas2-0.3.5/examples/grid_graph.png; x forceatlas2-0.3.5/fa2/; x forceatlas2-0.3.5/fa2/__init__.py; x forceatlas2-0.3.5/fa2/fa2util.c; x forceatlas2-0.3.5/fa2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:3340,Testability,test,test,3340," to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0.3.5.tar.gz’ saved [445420]. test@mac ~/PythonPackages$ tar xvf v0.3.5.tar.gz ; x forceatlas2-0.3.5/; x forceatlas2-0.3.5/.gitignore; x forceatlas2-0.3.5/LICENSE; x forceatlas2-0.3.5/MANIFEST.in; x forceatlas2-0.3.5/README.md; x forceatlas2-0.3.5/examples/; x forceatlas2-0.3.5/examples/forceatlas2-layout.ipynb; x forceatlas2-0.3.5/examples/geometric_graph.png; x forceatlas2-0.3.5/examples/grid_graph.png; x forceatlas2-0.3.5/fa2/; x forceatlas2-0.3.5/fa2/__init__.py; x forceatlas2-0.3.5/fa2/fa2util.c; x forceatlas2-0.3.5/fa2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:3426,Testability,test,test,3426,"ected.; HTTP request sent, awaiting response... 200 OK; Length: unspecified [application/x-gzip]; Saving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0.3.5.tar.gz’ saved [445420]. test@mac ~/PythonPackages$ tar xvf v0.3.5.tar.gz ; x forceatlas2-0.3.5/; x forceatlas2-0.3.5/.gitignore; x forceatlas2-0.3.5/LICENSE; x forceatlas2-0.3.5/MANIFEST.in; x forceatlas2-0.3.5/README.md; x forceatlas2-0.3.5/examples/; x forceatlas2-0.3.5/examples/forceatlas2-layout.ipynb; x forceatlas2-0.3.5/examples/geometric_graph.png; x forceatlas2-0.3.5/examples/grid_graph.png; x forceatlas2-0.3.5/fa2/; x forceatlas2-0.3.5/fa2/__init__.py; x forceatlas2-0.3.5/fa2/fa2util.c; x forceatlas2-0.3.5/fa2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:3552,Testability,test,test,3552,aving to: ‘v0.3.5.tar.gz’. v0.3.5.tar.gz [ <=> ] 434.98K 1.03MB/s in 0.4s . 2022-03-24 02:54:22 (1.03 MB/s) - ‘v0.3.5.tar.gz’ saved [445420]. test@mac ~/PythonPackages$ tar xvf v0.3.5.tar.gz ; x forceatlas2-0.3.5/; x forceatlas2-0.3.5/.gitignore; x forceatlas2-0.3.5/LICENSE; x forceatlas2-0.3.5/MANIFEST.in; x forceatlas2-0.3.5/README.md; x forceatlas2-0.3.5/examples/; x forceatlas2-0.3.5/examples/forceatlas2-layout.ipynb; x forceatlas2-0.3.5/examples/geometric_graph.png; x forceatlas2-0.3.5/examples/grid_graph.png; x forceatlas2-0.3.5/fa2/; x forceatlas2-0.3.5/fa2/__init__.py; x forceatlas2-0.3.5/fa2/fa2util.c; x forceatlas2-0.3.5/fa2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:3668,Testability,test,test,3668,.tar.gz’ saved [445420]. test@mac ~/PythonPackages$ tar xvf v0.3.5.tar.gz ; x forceatlas2-0.3.5/; x forceatlas2-0.3.5/.gitignore; x forceatlas2-0.3.5/LICENSE; x forceatlas2-0.3.5/MANIFEST.in; x forceatlas2-0.3.5/README.md; x forceatlas2-0.3.5/examples/; x forceatlas2-0.3.5/examples/forceatlas2-layout.ipynb; x forceatlas2-0.3.5/examples/geometric_graph.png; x forceatlas2-0.3.5/examples/grid_graph.png; x forceatlas2-0.3.5/fa2/; x forceatlas2-0.3.5/fa2/__init__.py; x forceatlas2-0.3.5/fa2/fa2util.c; x forceatlas2-0.3.5/fa2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; creating fa2.egg-info; writing fa2.egg-info/PKG-INFO;,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:3782,Testability,test,test,3782,.5/.gitignore; x forceatlas2-0.3.5/LICENSE; x forceatlas2-0.3.5/MANIFEST.in; x forceatlas2-0.3.5/README.md; x forceatlas2-0.3.5/examples/; x forceatlas2-0.3.5/examples/forceatlas2-layout.ipynb; x forceatlas2-0.3.5/examples/geometric_graph.png; x forceatlas2-0.3.5/examples/grid_graph.png; x forceatlas2-0.3.5/fa2/; x forceatlas2-0.3.5/fa2/__init__.py; x forceatlas2-0.3.5/fa2/fa2util.c; x forceatlas2-0.3.5/fa2/fa2util.pxd; x forceatlas2-0.3.5/fa2/fa2util.py; x forceatlas2-0.3.5/fa2/forceatlas2.py; x forceatlas2-0.3.5/setup.py; test@mac ~/PythonPackages$ cd forceatlas2-0.3.5/; test@mac ~/PythonPackages/forceatlas2-0.3.5$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2-0.3.5; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; error: subprocess-exited-with-error; ; × python setup.py bdist_wheel did not run successfully.; │ exit code: 1; ╰─> [214 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running bdist_wheel; running build; running build_py; creating build; creating build/lib.macosx-12.3-x86_64-3.10; creating build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/__init__.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; creating fa2.egg-info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt;,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:5601,Testability,test,test,5601,; creating fa2.egg-info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; writing manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; creating build/temp.macosx-12.3-x86_64-3.10; creating build/temp.macosx-12.3-x86_64-3.10/fa2; clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/test/.pyenv/versions/3.10.3/include/python3.10 -c fa2/fa2util.c -o build/temp.macosx-12.3-x86_64-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:6383,Testability,test,test,6383,-12.3-x86_64-3.10; creating build/temp.macosx-12.3-x86_64-3.10/fa2; clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/test/.pyenv/versions/3.10.3/include/python3.10 -c fa2/fa2util.c -o build/temp.macosx-12.3-x86_64-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Use,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:6550,Testability,test,test,6550,"acOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/test/.pyenv/versions/3.10.3/include/python3.10 -c fa2/fa2util.c -o build/temp.macosx-12.3-x86_64-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:6767,Testability,test,test,6767,"4-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:6954,Testability,test,test,6954,"ypeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:7306,Testability,test,test,7306,"warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:7493,Testability,test,test,7493,"/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((Py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:7719,Testability,test,test,7719,"icode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:8078,Testability,test,test,8078,"10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:8243,Testability,test,test,8243,"cated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:8460,Testability,test,test,8460,": expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:8647,Testability,test,test,8647,Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: no,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:9006,Testability,test,test,9006,rning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Use,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:9173,Testability,test,test,9173,"ython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:9390,Testability,test,test,9390,"5: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:9577,Testability,test,test,9577,"has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:9929,Testability,test,test,9929,"warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:10116,Testability,test,test,10116,"/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((Py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:10342,Testability,test,test,10342,"icode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:10701,Testability,test,test,10701,"10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:10866,Testability,test,test,10866,"cated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:11083,Testability,test,test,11083,": expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:11270,Testability,test,test,11270,Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:11632,Testability,test,test,11632,ning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:11799,Testability,test,test,11799,"on/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:12016,Testability,test,test,12016,"note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:12203,Testability,test,test,12203," been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:12558,Testability,test,test,12558,"ng: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:12745,Testability,test,test,12745,"icodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:12971,Testability,test,test,12971,"de_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:13333,Testability,test,test,13333,"yport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:13498,Testability,test,test,13498,"[-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:13715,Testability,test,test,13715,"xpanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:13902,Testability,test,test,13902,DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:14264,Testability,test,test,14264,g: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:14431,Testability,test,test,14431,"on/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:14648,Testability,test,test,14648,"note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:14835,Testability,test,test,14835," been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:15190,Testability,test,test,15190,"ng: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:15377,Testability,test,test,15377,"icodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:15603,Testability,test,test,15603,"de_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:15965,Testability,test,test,15965,"yport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:16130,Testability,test,test,16130,"[-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:16347,Testability,test,test,16347,"xpanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py insta",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:16534,Testability,test,test,16534,"DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for fa2; Running setup.py clean for fa2; Failed to build fa2; Installing collected packages: fa2; Attempting uninstall: fa2; Found existing installation: fa2 0.3.5; Uninstalling fa2-0.3.5:; Successfully uninstalled fa2-0.3.5; Running setup.py install for fa2 ... error; error: subprocess-exited-with-error; ; × Running setup.py install for fa2 did not run successfully.; │ exit code: 1; ╰─> [212 lines of output]; Installing fa2 package (fastest forceatlas2 python implementation); ; >>>> Cython is installed?; Yes; ; >>>> Starting to install!; ; running install; running build; running ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:18801,Testability,test,test,18801,forceatlas2.py -> build/lib.macosx-12.3-x86_64-3.10/fa2; running egg_info; writing fa2.egg-info/PKG-INFO; writing dependency_links to fa2.egg-info/dependency_links.txt; writing requirements to fa2.egg-info/requires.txt; writing top-level names to fa2.egg-info/top_level.txt; reading manifest file 'fa2.egg-info/SOURCES.txt'; reading manifest template 'MANIFEST.in'; adding license file 'LICENSE'; writing manifest file 'fa2.egg-info/SOURCES.txt'; copying fa2/fa2util.c -> build/lib.macosx-12.3-x86_64-3.10/fa2; copying fa2/fa2util.pxd -> build/lib.macosx-12.3-x86_64-3.10/fa2; running build_ext; skipping 'fa2/fa2util.c' Cython extension (up-to-date); building 'fa2.fa2util' extension; creating build/temp.macosx-12.3-x86_64-3.10; creating build/temp.macosx-12.3-x86_64-3.10/fa2; clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/test/.pyenv/versions/3.10.3/include/python3.10 -c fa2/fa2util.c -o build/temp.macosx-12.3-x86_64-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:19583,Testability,test,test,19583,-12.3-x86_64-3.10; creating build/temp.macosx-12.3-x86_64-3.10/fa2; clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/test/.pyenv/versions/3.10.3/include/python3.10 -c fa2/fa2util.c -o build/temp.macosx-12.3-x86_64-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Use,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:19750,Testability,test,test,19750,"acOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Users/test/.pyenv/versions/3.10.3/include/python3.10 -c fa2/fa2util.c -o build/temp.macosx-12.3-x86_64-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:19967,Testability,test,test,19967,"4-3.10/fa2/fa2util.o; fa2/fa2util.c:10939:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Node.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10947:33: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:20154,Testability,test,test,20154,"ypeobject'; __pyx_type_3fa2_7fa2util_Edge.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:10960:35: error: no member named 'tp_print' in 'struct _typeobject'; __pyx_type_3fa2_7fa2util_Region.tp_print = 0;; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:20506,Testability,test,test,20506,"warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:20693,Testability,test,test,20693,"/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((Py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:20919,Testability,test,test,20919,"icode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:21278,Testability,test,test,21278,"10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:21443,Testability,test,test,21443,"cated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:21660,Testability,test,test,21660,": expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:21847,Testability,test,test,21847,Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: no,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:22206,Testability,test,test,22206,rning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Use,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:22373,Testability,test,test,22373,"ython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:22590,Testability,test,test,22590,"5: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:22777,Testability,test,test,22777,"has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_w",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:23129,Testability,test,test,23129,"warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:23316,Testability,test,test,23316,"/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((Py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:23542,Testability,test,test,23542,"icode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:23901,Testability,test,test,23901,"10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:24066,Testability,test,test,24066,"cated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:24283,Testability,test,test,24283,": expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:24470,Testability,test,test,24470,Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12133:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:24832,Testability,test,test,24832,ning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:24999,Testability,test,test,24999,"on/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:25216,Testability,test,test,25216,"note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:25403,Testability,test,test,25403," been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:25758,Testability,test,test,25758,"ng: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:25945,Testability,test,test,25945,"icodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:26171,Testability,test,test,26171,"de_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:26533,Testability,test,test,26533,"yport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:26698,Testability,test,test,26698,"[-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:26915,Testability,test,test,26915,"xpanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:27102,Testability,test,test,27102,DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:27464,Testability,test,test,27464,g: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:27631,Testability,test,test,27631,"on/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:27848,Testability,test,test,27848,"note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:28035,Testability,test,test,28035," been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:28390,Testability,test,test,28390,"ng: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:28577,Testability,test,test,28577,"icodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op) : \; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:28803,Testability,test,test,28803,"de_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:29165,Testability,test,test,29165,"yport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:29330,Testability,test,test,29330,"[-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:29547,Testability,test,test,29547,"xpanded from macro 'PyUnicode_GET_SIZE'; ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here; Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:29734,Testability,test,test,29734,"DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackag",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30151,Testability,test,test,30151,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30241,Testability,test,test,30241,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30316,Testability,test,test,30316,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30395,Testability,test,test,30395,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30454,Testability,test,test,30454,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:30707,Testability,test,test,30707,"(Py_UNICODE *) PyUnicode_AsUnicode(; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; fa2/fa2util.c:12149:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]; (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :; ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'; PyUnicode_WSTR_LENGTH(op))); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'; #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here; Py_DEPRECATED(3.3); ^; /Users/test/.pyenv/versions/3.10.3/include/python3.10/pyport.h:513:54: note: expanded from macro 'Py_DEPRECATED'; #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__)); ^; 12 warnings and 3 errors generated.; error: command '/usr/bin/clang' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; WARNING: No metadata found in /Users/test/.local/lib/python3.10/site-packages; Rolling back uninstall of fa2; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2-0.3.5.dist-info/; from /Users/test/.local/lib/python3.10/site-packages/~a2-0.3.5.dist-info; Moving to /Users/test/.local/lib/python3.10/site-packages/fa2/; from /Users/test/.local/lib/python3.10/site-packages/~a2; error: legacy-install-failure. × Encountered error while trying to install package.; ╰─> fa2. note: This is an issue with the package mentioned above, not pip.; hint: See above for output from the failure.; test@mac ~/PythonPackages/forceatlas2-0.3.5$; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096
https://github.com/scverse/scanpy/issues/2068#issuecomment-2268851567:36,Availability,avail,available,36,yup! genes you filtered out are not available unless you use `use_raw`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2068#issuecomment-2268851567
https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912:70,Deployability,install,installed,70,"In the ""requirement already satisfied"" it looks like ""scikit-misc"" is installed in a different location and not within the `site-packages` folder of the anaconda env listed on the line below for `numpy`. From within the `py38` env you could try to reinstall it with `pip install --user scikit-misc --force` and also delete the other one or remove it from your `$PYTHONPATH`? Installing things in the jupyter notebook might be using a different version of pip than the one in the environment (depending on how your kernels are set up) so I think it's sometimes safer to do these things from the command line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912
https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912:271,Deployability,install,install,271,"In the ""requirement already satisfied"" it looks like ""scikit-misc"" is installed in a different location and not within the `site-packages` folder of the anaconda env listed on the line below for `numpy`. From within the `py38` env you could try to reinstall it with `pip install --user scikit-misc --force` and also delete the other one or remove it from your `$PYTHONPATH`? Installing things in the jupyter notebook might be using a different version of pip than the one in the environment (depending on how your kernels are set up) so I think it's sometimes safer to do these things from the command line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912
https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912:375,Deployability,Install,Installing,375,"In the ""requirement already satisfied"" it looks like ""scikit-misc"" is installed in a different location and not within the `site-packages` folder of the anaconda env listed on the line below for `numpy`. From within the `py38` env you could try to reinstall it with `pip install --user scikit-misc --force` and also delete the other one or remove it from your `$PYTHONPATH`? Installing things in the jupyter notebook might be using a different version of pip than the one in the environment (depending on how your kernels are set up) so I think it's sometimes safer to do these things from the command line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912
https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912:492,Integrability,depend,depending,492,"In the ""requirement already satisfied"" it looks like ""scikit-misc"" is installed in a different location and not within the `site-packages` folder of the anaconda env listed on the line below for `numpy`. From within the `py38` env you could try to reinstall it with `pip install --user scikit-misc --force` and also delete the other one or remove it from your `$PYTHONPATH`? Installing things in the jupyter notebook might be using a different version of pip than the one in the environment (depending on how your kernels are set up) so I think it's sometimes safer to do these things from the command line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912
https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912:560,Safety,safe,safer,560,"In the ""requirement already satisfied"" it looks like ""scikit-misc"" is installed in a different location and not within the `site-packages` folder of the anaconda env listed on the line below for `numpy`. From within the `py38` env you could try to reinstall it with `pip install --user scikit-misc --force` and also delete the other one or remove it from your `$PYTHONPATH`? Installing things in the jupyter notebook might be using a different version of pip than the one in the environment (depending on how your kernels are set up) so I think it's sometimes safer to do these things from the command line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912
https://github.com/scverse/scanpy/issues/2073#issuecomment-996241473:57,Deployability,install,install,57,"Hello @davidhbrann,; Thanks for the response.; I did pip install --user scikit-misc --force in the anaconda powershell, but this bug kept the same. Not solved.; Thanks!; Best,; YJ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996241473
https://github.com/scverse/scanpy/issues/2073#issuecomment-996241473:108,Energy Efficiency,power,powershell,108,"Hello @davidhbrann,; Thanks for the response.; I did pip install --user scikit-misc --force in the anaconda powershell, but this bug kept the same. Not solved.; Thanks!; Best,; YJ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996241473
https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340:95,Availability,error,errored,95,"When I did `pip install --user scikit-misc` in my shell and then in python tried the line that errored for you `from skmisc.loess import loess`, everything worked fine for me. Also, depending on how conda is setup `pip install --user` might install it in your home directory, rather than the conda env. So you could also try activating the conda env and then running `pip install scikit-misc --force`. . Can you print out the full traceback of what happens when you run `from skmisc.loess import loess`? If that was causing the `ImportError` it might be easier to see outside of the try/except block. You can also try `import skmisc; print(skmisc.__file__)` to see what that returns. I also see some related issues (https://github.com/has2k1/scikit-misc/issues/12), which could indicate that it did not install correctly because it did not install the cython scripts properly on windows. The solution (install the numpy+mkl .whl first) in https://github.com/has2k1/scikit-misc/issues/4 might work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340
https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340:16,Deployability,install,install,16,"When I did `pip install --user scikit-misc` in my shell and then in python tried the line that errored for you `from skmisc.loess import loess`, everything worked fine for me. Also, depending on how conda is setup `pip install --user` might install it in your home directory, rather than the conda env. So you could also try activating the conda env and then running `pip install scikit-misc --force`. . Can you print out the full traceback of what happens when you run `from skmisc.loess import loess`? If that was causing the `ImportError` it might be easier to see outside of the try/except block. You can also try `import skmisc; print(skmisc.__file__)` to see what that returns. I also see some related issues (https://github.com/has2k1/scikit-misc/issues/12), which could indicate that it did not install correctly because it did not install the cython scripts properly on windows. The solution (install the numpy+mkl .whl first) in https://github.com/has2k1/scikit-misc/issues/4 might work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340
https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340:219,Deployability,install,install,219,"When I did `pip install --user scikit-misc` in my shell and then in python tried the line that errored for you `from skmisc.loess import loess`, everything worked fine for me. Also, depending on how conda is setup `pip install --user` might install it in your home directory, rather than the conda env. So you could also try activating the conda env and then running `pip install scikit-misc --force`. . Can you print out the full traceback of what happens when you run `from skmisc.loess import loess`? If that was causing the `ImportError` it might be easier to see outside of the try/except block. You can also try `import skmisc; print(skmisc.__file__)` to see what that returns. I also see some related issues (https://github.com/has2k1/scikit-misc/issues/12), which could indicate that it did not install correctly because it did not install the cython scripts properly on windows. The solution (install the numpy+mkl .whl first) in https://github.com/has2k1/scikit-misc/issues/4 might work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340
https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340:241,Deployability,install,install,241,"When I did `pip install --user scikit-misc` in my shell and then in python tried the line that errored for you `from skmisc.loess import loess`, everything worked fine for me. Also, depending on how conda is setup `pip install --user` might install it in your home directory, rather than the conda env. So you could also try activating the conda env and then running `pip install scikit-misc --force`. . Can you print out the full traceback of what happens when you run `from skmisc.loess import loess`? If that was causing the `ImportError` it might be easier to see outside of the try/except block. You can also try `import skmisc; print(skmisc.__file__)` to see what that returns. I also see some related issues (https://github.com/has2k1/scikit-misc/issues/12), which could indicate that it did not install correctly because it did not install the cython scripts properly on windows. The solution (install the numpy+mkl .whl first) in https://github.com/has2k1/scikit-misc/issues/4 might work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340
https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340:372,Deployability,install,install,372,"When I did `pip install --user scikit-misc` in my shell and then in python tried the line that errored for you `from skmisc.loess import loess`, everything worked fine for me. Also, depending on how conda is setup `pip install --user` might install it in your home directory, rather than the conda env. So you could also try activating the conda env and then running `pip install scikit-misc --force`. . Can you print out the full traceback of what happens when you run `from skmisc.loess import loess`? If that was causing the `ImportError` it might be easier to see outside of the try/except block. You can also try `import skmisc; print(skmisc.__file__)` to see what that returns. I also see some related issues (https://github.com/has2k1/scikit-misc/issues/12), which could indicate that it did not install correctly because it did not install the cython scripts properly on windows. The solution (install the numpy+mkl .whl first) in https://github.com/has2k1/scikit-misc/issues/4 might work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340
https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340:803,Deployability,install,install,803,"When I did `pip install --user scikit-misc` in my shell and then in python tried the line that errored for you `from skmisc.loess import loess`, everything worked fine for me. Also, depending on how conda is setup `pip install --user` might install it in your home directory, rather than the conda env. So you could also try activating the conda env and then running `pip install scikit-misc --force`. . Can you print out the full traceback of what happens when you run `from skmisc.loess import loess`? If that was causing the `ImportError` it might be easier to see outside of the try/except block. You can also try `import skmisc; print(skmisc.__file__)` to see what that returns. I also see some related issues (https://github.com/has2k1/scikit-misc/issues/12), which could indicate that it did not install correctly because it did not install the cython scripts properly on windows. The solution (install the numpy+mkl .whl first) in https://github.com/has2k1/scikit-misc/issues/4 might work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340
https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340:840,Deployability,install,install,840,"When I did `pip install --user scikit-misc` in my shell and then in python tried the line that errored for you `from skmisc.loess import loess`, everything worked fine for me. Also, depending on how conda is setup `pip install --user` might install it in your home directory, rather than the conda env. So you could also try activating the conda env and then running `pip install scikit-misc --force`. . Can you print out the full traceback of what happens when you run `from skmisc.loess import loess`? If that was causing the `ImportError` it might be easier to see outside of the try/except block. You can also try `import skmisc; print(skmisc.__file__)` to see what that returns. I also see some related issues (https://github.com/has2k1/scikit-misc/issues/12), which could indicate that it did not install correctly because it did not install the cython scripts properly on windows. The solution (install the numpy+mkl .whl first) in https://github.com/has2k1/scikit-misc/issues/4 might work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340
https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340:902,Deployability,install,install,902,"When I did `pip install --user scikit-misc` in my shell and then in python tried the line that errored for you `from skmisc.loess import loess`, everything worked fine for me. Also, depending on how conda is setup `pip install --user` might install it in your home directory, rather than the conda env. So you could also try activating the conda env and then running `pip install scikit-misc --force`. . Can you print out the full traceback of what happens when you run `from skmisc.loess import loess`? If that was causing the `ImportError` it might be easier to see outside of the try/except block. You can also try `import skmisc; print(skmisc.__file__)` to see what that returns. I also see some related issues (https://github.com/has2k1/scikit-misc/issues/12), which could indicate that it did not install correctly because it did not install the cython scripts properly on windows. The solution (install the numpy+mkl .whl first) in https://github.com/has2k1/scikit-misc/issues/4 might work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340
https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340:182,Integrability,depend,depending,182,"When I did `pip install --user scikit-misc` in my shell and then in python tried the line that errored for you `from skmisc.loess import loess`, everything worked fine for me. Also, depending on how conda is setup `pip install --user` might install it in your home directory, rather than the conda env. So you could also try activating the conda env and then running `pip install scikit-misc --force`. . Can you print out the full traceback of what happens when you run `from skmisc.loess import loess`? If that was causing the `ImportError` it might be easier to see outside of the try/except block. You can also try `import skmisc; print(skmisc.__file__)` to see what that returns. I also see some related issues (https://github.com/has2k1/scikit-misc/issues/12), which could indicate that it did not install correctly because it did not install the cython scripts properly on windows. The solution (install the numpy+mkl .whl first) in https://github.com/has2k1/scikit-misc/issues/4 might work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:779,Availability,Down,Downloading,779,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:1082,Availability,ERROR,ERROR,1082,"t force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,. ImportError: DLL load failed while importing _loess: The specified module could not be found. During",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:1398,Availability,error,errors,1398,"\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,. ImportError: DLL load failed while importing _loess: The specified module could not be found. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/1877627730.py in <module>; ----> 1 sc.pp.highly_variable_genes(adata, n_top_genes=50",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4345,Availability,Down,Downloads,4345," `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4386,Availability,Down,Downloads,4386," `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4479,Availability,down,downloads,4479,"al\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4708,Availability,ERROR,ERROR,4708,"52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib impor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:5247,Availability,error,error,5247,"y with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_13940/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\anaconda3\en",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:147,Deployability,install,install,147,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:300,Deployability,install,install,300,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:587,Deployability,install,install,587,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:643,Deployability,install,install,643,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:892,Deployability,Install,Installing,892,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:987,Deployability,install,installation,987,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:1099,Deployability,install,install,1099,"t force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,. ImportError: DLL load failed while importing _loess: The specified module could not be found. During",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:3171,Deployability,install,install,3171,"handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/1877627730.py in <module>; ----> 1 sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); 2 sc.pl.highly_variable_genes(adata); 3 print(sum(adata.var.highly_variable)); 4 adata. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess; 54 except ImportError:; ---> 55 raise ImportError(; 56 'Please install skmisc package via `pip install --user scikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:3203,Deployability,install,install,3203,"handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/1877627730.py in <module>; ----> 1 sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); 2 sc.pl.highly_variable_genes(adata); 3 print(sum(adata.var.highly_variable)); 4 adata. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess; 54 except ImportError:; ---> 55 raise ImportError(; 56 'Please install skmisc package via `pip install --user scikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:3258,Deployability,install,install,3258,"ipykernel_11028/1877627730.py in <module>; ----> 1 sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); 2 sc.pl.highly_variable_genes(adata); 3 print(sum(adata.var.highly_variable)); 4 adata. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess; 54 except ImportError:; ---> 55 raise ImportError(; 56 'Please install skmisc package via `pip install --user scikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>cond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:3290,Deployability,install,install,3290,"ipykernel_11028/1877627730.py in <module>; ----> 1 sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); 2 sc.pl.highly_variable_genes(adata); 3 print(sum(adata.var.highly_variable)); 4 adata. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess; 54 except ImportError:; ---> 55 raise ImportError(; 56 'Please install skmisc package via `pip install --user scikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>cond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4227,Deployability,install,install,4227,"ikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4400,Deployability,install,install,4400," `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4531,Deployability,Install,Installing,4531,"Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4613,Deployability,install,installation,4613,"Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presen",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4804,Deployability,install,installed,4804,"52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib impor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4992,Deployability,install,installed,4992,"```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_13940/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 im",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:5102,Deployability,install,installed,5102,"sc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_13940/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): #",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:110,Energy Efficiency,Power,Powershell,110,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4721,Integrability,depend,dependency,4721,"52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib impor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4861,Integrability,depend,dependency,4861,"rror: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykerne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:702,Performance,cache,cached,702,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:2078,Performance,load,load,2078,"packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,. ImportError: DLL load failed while importing _loess: The specified module could not be found. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/1877627730.py in <module>; ----> 1 sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); 2 sc.pl.highly_variable_genes(adata); 3 print(sum(adata.var.highly_variable)); 4 adata. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:3858,Performance,load,load,3858,"Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess; 54 except ImportError:; ---> 55 raise ImportError(; 56 'Please install skmisc package via `pip install --user scikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:1148,Security,Access,Access,1148,"t force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,. ImportError: DLL load failed while importing _loess: The specified module could not be found. During",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:5441,Testability,log,logging,5441,"whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_13940/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\anaconda3\envs\Python38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\anaconda3\envs\Python38\lib\sit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:5602,Testability,log,logging,5602,"isting installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_13940/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\anaconda3\envs\Python38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\anaconda3\envs\Python38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ----> 5 from numba import njit; 6 from scipy import sparse; 7 . ~\anaconda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:5681,Testability,log,logging,5681,"nstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_13940/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\anaconda3\envs\Python38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\anaconda3\envs\Python38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ----> 5 from numba import njit; 6 from scipy import sparse; 7 . ~\anaconda3\envs\Python38\lib\site-packages\numba\__init__.py in <module>; 196 ; 197 _ens",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:6326,Testability,log,logging,6326,"c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_13940/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\anaconda3\envs\Python38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\anaconda3\envs\Python38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ----> 5 from numba import njit; 6 from scipy import sparse; 7 . ~\anaconda3\envs\Python38\lib\site-packages\numba\__init__.py in <module>; 196 ; 197 _ensure_llvm(); --> 198 _ensure_critical_deps(); 199 ; 200 # we know llvmlite is working as the above tests passed, import it now as SVML. ~\anaconda3\envs\Python38\lib\site-packages\numba\__init__.py in _ensure_critical_deps(); 136 raise ImportError(""Numba needs NumPy 1.17 or greater""); 137 elif numpy_version > (1, 20):; --> 138 raise ImportError(""Numba needs NumPy 1.20 or less""); 139 ; 140 try:. ImportError: Numba needs NumPy 1.20 or less; ```; Could you give me more suggestions?; Thanks!; Best,; YJ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:6337,Testability,log,logg,6337,"c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_13940/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\anaconda3\envs\Python38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\anaconda3\envs\Python38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ----> 5 from numba import njit; 6 from scipy import sparse; 7 . ~\anaconda3\envs\Python38\lib\site-packages\numba\__init__.py in <module>; 196 ; 197 _ensure_llvm(); --> 198 _ensure_critical_deps(); 199 ; 200 # we know llvmlite is working as the above tests passed, import it now as SVML. ~\anaconda3\envs\Python38\lib\site-packages\numba\__init__.py in _ensure_critical_deps(); 136 raise ImportError(""Numba needs NumPy 1.17 or greater""); 137 elif numpy_version > (1, 20):; --> 138 raise ImportError(""Numba needs NumPy 1.20 or less""); 139 ; 140 try:. ImportError: Numba needs NumPy 1.20 or less; ```; Could you give me more suggestions?; Thanks!; Best,; YJ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:6783,Testability,test,tests,6783,"c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_13940/2696797780.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\anaconda3\envs\Python38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\anaconda3\envs\Python38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ----> 5 from numba import njit; 6 from scipy import sparse; 7 . ~\anaconda3\envs\Python38\lib\site-packages\numba\__init__.py in <module>; 196 ; 197 _ensure_llvm(); --> 198 _ensure_critical_deps(); 199 ; 200 # we know llvmlite is working as the above tests passed, import it now as SVML. ~\anaconda3\envs\Python38\lib\site-packages\numba\__init__.py in _ensure_critical_deps(); 136 raise ImportError(""Numba needs NumPy 1.17 or greater""); 137 elif numpy_version > (1, 20):; --> 138 raise ImportError(""Numba needs NumPy 1.20 or less""); 139 ; 140 try:. ImportError: Numba needs NumPy 1.20 or less; ```; Could you give me more suggestions?; Thanks!; Best,; YJ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644:7,Availability,error,error,7,"In the error it looks like numba requires numpy < 1.20, so you could try installing the `numpy‑1.19.5+mkl` whl from https://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy before installing the scikit-misc one from https://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-misc? . When you get the `Requirement already satisfied` error from `pip install`, you might need to first do `pip uninstall <pkg>` before installing",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644:317,Availability,error,error,317,"In the error it looks like numba requires numpy < 1.20, so you could try installing the `numpy‑1.19.5+mkl` whl from https://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy before installing the scikit-misc one from https://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-misc? . When you get the `Requirement already satisfied` error from `pip install`, you might need to first do `pip uninstall <pkg>` before installing",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644:73,Deployability,install,installing,73,"In the error it looks like numba requires numpy < 1.20, so you could try installing the `numpy‑1.19.5+mkl` whl from https://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy before installing the scikit-misc one from https://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-misc? . When you get the `Requirement already satisfied` error from `pip install`, you might need to first do `pip uninstall <pkg>` before installing",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644:173,Deployability,install,installing,173,"In the error it looks like numba requires numpy < 1.20, so you could try installing the `numpy‑1.19.5+mkl` whl from https://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy before installing the scikit-misc one from https://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-misc? . When you get the `Requirement already satisfied` error from `pip install`, you might need to first do `pip uninstall <pkg>` before installing",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644:333,Deployability,install,install,333,"In the error it looks like numba requires numpy < 1.20, so you could try installing the `numpy‑1.19.5+mkl` whl from https://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy before installing the scikit-misc one from https://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-misc? . When you get the `Requirement already satisfied` error from `pip install`, you might need to first do `pip uninstall <pkg>` before installing",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644
https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644:399,Deployability,install,installing,399,"In the error it looks like numba requires numpy < 1.20, so you could try installing the `numpy‑1.19.5+mkl` whl from https://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy before installing the scikit-misc one from https://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-misc? . When you get the `Requirement already satisfied` error from `pip install`, you might need to first do `pip uninstall <pkg>` before installing",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000950644
https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116:43,Availability,down,downloading,43,"I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. ; By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; ```; conda create -n scanpy_env; conda activate scanpy_env; conda install numpy=1.19; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy==1.8.1; ```; Now I can run `sc.pp.highly_variable_genes()` with no problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116
https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116:130,Deployability,install,install,130,"I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. ; By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; ```; conda create -n scanpy_env; conda activate scanpy_env; conda install numpy=1.19; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy==1.8.1; ```; Now I can run `sc.pp.highly_variable_genes()` with no problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116
https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116:266,Deployability,install,install,266,"I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. ; By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; ```; conda create -n scanpy_env; conda activate scanpy_env; conda install numpy=1.19; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy==1.8.1; ```; Now I can run `sc.pp.highly_variable_genes()` with no problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116
https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116:292,Deployability,install,install,292,"I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. ; By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; ```; conda create -n scanpy_env; conda activate scanpy_env; conda install numpy=1.19; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy==1.8.1; ```; Now I can run `sc.pp.highly_variable_genes()` with no problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116
https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116:355,Deployability,install,install,355,"I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. ; By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; ```; conda create -n scanpy_env; conda activate scanpy_env; conda install numpy=1.19; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy==1.8.1; ```; Now I can run `sc.pp.highly_variable_genes()` with no problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116
https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116:407,Deployability,install,install,407,"I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. ; By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; ```; conda create -n scanpy_env; conda activate scanpy_env; conda install numpy=1.19; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy==1.8.1; ```; Now I can run `sc.pp.highly_variable_genes()` with no problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116
https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116:315,Usability,learn,learn,315,"I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. ; By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; ```; conda create -n scanpy_env; conda activate scanpy_env; conda install numpy=1.19; conda install seaborn scikit-learn statsmodels numba pytables; conda install -c conda-forge python-igraph leidenalg; pip install scanpy==1.8.1; ```; Now I can run `sc.pp.highly_variable_genes()` with no problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1020416116
https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241:45,Availability,down,downloading,45,"> I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; > ; > ```; > conda create -n scanpy_env; > conda activate scanpy_env; > conda install numpy=1.19; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > Now I can run `sc.pp.highly_variable_genes()` with no problem. Update: this workaround does not seem to work anymore, at least for scanpy 1.8.2 (you'll need to `pip install scanpy==1.8.1`). ; During `pip install scanpy`, a newer version of numpy is installed and version 1.19 is overwritten. This newer version does not have MKL, leading us back to square one. It's also not possible to `conda install numpy 1.19` as the very last step, because this leads to another error (it's related to the fact that scanpy needs to be compiled with the same version of numpy).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241
https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241:921,Availability,error,error,921,"> I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; > ; > ```; > conda create -n scanpy_env; > conda activate scanpy_env; > conda install numpy=1.19; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > Now I can run `sc.pp.highly_variable_genes()` with no problem. Update: this workaround does not seem to work anymore, at least for scanpy 1.8.2 (you'll need to `pip install scanpy==1.8.1`). ; During `pip install scanpy`, a newer version of numpy is installed and version 1.19 is overwritten. This newer version does not have MKL, leading us back to square one. It's also not possible to `conda install numpy 1.19` as the very last step, because this leads to another error (it's related to the fact that scanpy needs to be compiled with the same version of numpy).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241
https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241:130,Deployability,install,install,130,"> I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; > ; > ```; > conda create -n scanpy_env; > conda activate scanpy_env; > conda install numpy=1.19; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > Now I can run `sc.pp.highly_variable_genes()` with no problem. Update: this workaround does not seem to work anymore, at least for scanpy 1.8.2 (you'll need to `pip install scanpy==1.8.1`). ; During `pip install scanpy`, a newer version of numpy is installed and version 1.19 is overwritten. This newer version does not have MKL, leading us back to square one. It's also not possible to `conda install numpy 1.19` as the very last step, because this leads to another error (it's related to the fact that scanpy needs to be compiled with the same version of numpy).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241
https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241:278,Deployability,install,install,278,"> I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; > ; > ```; > conda create -n scanpy_env; > conda activate scanpy_env; > conda install numpy=1.19; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > Now I can run `sc.pp.highly_variable_genes()` with no problem. Update: this workaround does not seem to work anymore, at least for scanpy 1.8.2 (you'll need to `pip install scanpy==1.8.1`). ; During `pip install scanpy`, a newer version of numpy is installed and version 1.19 is overwritten. This newer version does not have MKL, leading us back to square one. It's also not possible to `conda install numpy 1.19` as the very last step, because this leads to another error (it's related to the fact that scanpy needs to be compiled with the same version of numpy).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241
https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241:306,Deployability,install,install,306,"> I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; > ; > ```; > conda create -n scanpy_env; > conda activate scanpy_env; > conda install numpy=1.19; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > Now I can run `sc.pp.highly_variable_genes()` with no problem. Update: this workaround does not seem to work anymore, at least for scanpy 1.8.2 (you'll need to `pip install scanpy==1.8.1`). ; During `pip install scanpy`, a newer version of numpy is installed and version 1.19 is overwritten. This newer version does not have MKL, leading us back to square one. It's also not possible to `conda install numpy 1.19` as the very last step, because this leads to another error (it's related to the fact that scanpy needs to be compiled with the same version of numpy).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241
https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241:371,Deployability,install,install,371,"> I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; > ; > ```; > conda create -n scanpy_env; > conda activate scanpy_env; > conda install numpy=1.19; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > Now I can run `sc.pp.highly_variable_genes()` with no problem. Update: this workaround does not seem to work anymore, at least for scanpy 1.8.2 (you'll need to `pip install scanpy==1.8.1`). ; During `pip install scanpy`, a newer version of numpy is installed and version 1.19 is overwritten. This newer version does not have MKL, leading us back to square one. It's also not possible to `conda install numpy 1.19` as the very last step, because this leads to another error (it's related to the fact that scanpy needs to be compiled with the same version of numpy).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241
https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241:425,Deployability,install,install,425,"> I found a workaround that does not require downloading the `.whl` file for `numpy=1.19.5`. By default, MKL is included when you install numpy with conda. It's good to do this in a new environment.; > ; > ```; > conda create -n scanpy_env; > conda activate scanpy_env; > conda install numpy=1.19; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > Now I can run `sc.pp.highly_variable_genes()` with no problem. Update: this workaround does not seem to work anymore, at least for scanpy 1.8.2 (you'll need to `pip install scanpy==1.8.1`). ; During `pip install scanpy`, a newer version of numpy is installed and version 1.19 is overwritten. This newer version does not have MKL, leading us back to square one. It's also not possible to `conda install numpy 1.19` as the very last step, because this leads to another error (it's related to the fact that scanpy needs to be compiled with the same version of numpy).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1058514241
